<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dcat%3Acs.LG%26id_list%3D%26start%3D0%26max_results%3D1000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=cat:cs.LG&amp;id_list=&amp;start=0&amp;max_results=1000</title>
  <id>http://arxiv.org/api/CnvybtQa9uYuIE4pdtRKOb0rmDI</id>
  <updated>2018-09-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">23243</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/1808.04256v2</id>
    <updated>2018-09-05T17:56:08Z</updated>
    <published>2018-08-10T05:33:23Z</published>
    <title>CT Super-resolution GAN Constrained by the Identical, Residual, and
  Cycle Learning Ensemble(GAN-CIRCLE)</title>
    <summary>  Computed tomography (CT) is widely used in screening, diagnosis, and
image-guided therapy for both clinical and research purposes. Since CT involves
ionizing radiation, an overarching thrust of related technical research is
development of novel methods enabling ultrahigh quality imaging with fine
structural details while reducing the X-ray radiation. In this paper, we
present a semi-supervised deep learning approach to accurately recover
high-resolution (HR) CT images from low-resolution (LR) counterparts.
Specifically, with the generative adversarial network (GAN) as the building
block, we enforce the cycle-consistency in terms of the Wasserstein distance to
establish a nonlinear end-to-end mapping from noisy LR input images to denoised
and deblurred HR outputs. We also include the joint constraints in the loss
function to facilitate structural preservation. In this deep imaging process,
we incorporate deep convolutional neural network (CNN), residual learning, and
network in network techniques for feature extraction and restoration. In
contrast to the current trend of increasing network depth and complexity to
boost the CT imaging performance, which limit its real-world applications by
imposing considerable computational and memory overheads, we apply a parallel
$1\times1$ CNN to compress the output of the hidden layer and optimize the
number of layers and the number of filters for each convolutional layer.
Quantitative and qualitative evaluations demonstrate that our proposed model is
accurate, efficient and robust for super-resolution (SR) image restoration from
noisy LR input images. In particular, we validate our composite SR networks on
three large-scale CT datasets, and obtain promising results as compared to the
other state-of-the-art methods.
</summary>
    <author>
      <name>Chenyu You</name>
    </author>
    <author>
      <name>Guang Li</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Xiaoliu Zhang</name>
    </author>
    <author>
      <name>Hongming Shan</name>
    </author>
    <author>
      <name>Shenghong Ju</name>
    </author>
    <author>
      <name>Zhen Zhao</name>
    </author>
    <author>
      <name>Zhuiyang Zhang</name>
    </author>
    <author>
      <name>Wenxiang Cong</name>
    </author>
    <author>
      <name>Michael W. Vannier</name>
    </author>
    <author>
      <name>Punam K. Saha</name>
    </author>
    <author>
      <name>Ge Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04256v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04256v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01628v1</id>
    <updated>2018-09-05T17:13:02Z</updated>
    <published>2018-09-05T17:13:02Z</published>
    <title>Online local pool generation for dynamic classifier selection: an
  extended version</title>
    <summary>  Dynamic Classifier Selection (DCS) techniques have difficulty in selecting
the most competent classifier in a pool, even when its presence is assured.
Since the DCS techniques rely only on local data to estimate a classifier's
competence, the manner in which the pool is generated could affect the choice
of the best classifier for a given sample. That is, the global perspective in
which pools are generated may not help the DCS techniques in selecting a
competent classifier for samples that are likely to be mislabelled. Thus, we
propose in this work an online pool generation method that produces a locally
accurate pool for test samples in difficult regions of the feature space. The
difficulty of a given area is determined by the classification difficulty of
the samples in it. That way, by using classifiers that were generated in a
local scope, it could be easier for the DCS techniques to select the best one
for the difficult samples. For the query samples in easy regions, a simple
nearest neighbors rule is used. In the extended version of this work, a deep
analysis on the correlation between instance hardness and the performance of
DCS techniques is presented. An instance hardness measure that conveys the
degree of local class overlap is then used to decide when the local pool is
used in the proposed scheme. The proposed method yielded significantly greater
recognition rates in comparison to a Bagging-generated pool and two other
global pool generation schemes for all DCS techniques evaluated. The proposed
scheme's performance was also significantly superior to three state-of-the-art
classification models and statistically equivalent to five of them. Moreover,
an extended analysis on the computational complexity of the proposed method and
of several DS techniques is presented in this version. We also provide the
implementation of the proposed technique using the DESLib library on GitHub.
</summary>
    <author>
      <name>Mariana A. Souza</name>
    </author>
    <author>
      <name>George D. C. Cavalcanti</name>
    </author>
    <author>
      <name>Rafael M. O. Cruz</name>
    </author>
    <author>
      <name>Robert Sabourin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the paper: M. A. Souza, G. D. Cavalcanti, R. M.
  Cruz, R. Sabourin, Online local pool generation for dynamic classifier
  selection, Pattern Recognition 85 (2019) 132 - 148</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01625v1</id>
    <updated>2018-09-05T17:09:00Z</updated>
    <published>2018-09-05T17:09:00Z</published>
    <title>Gene Shaving using influence function of a kernel method</title>
    <summary>  Identifying significant subsets of the genes, gene shaving is an essential
and challenging issue for biomedical research for a huge number of genes and
the complex nature of biological networks,. Since positive definite kernel
based methods on genomic information can improve the prediction of diseases, in
this paper we proposed a new method, "kernel gene shaving (kernel canonical
correlation analysis (kernel CCA) based gene shaving). This problem is
addressed using the influence function of the kernel CCA. To investigate the
performance of the proposed method in a comparison of three popular gene
selection methods (T-test, SAM and LIMMA), we were used extensive simulated and
real microarray gene expression datasets. The performance measures AUC was
computed for each of the methods. The achievement of the proposed method has
improved than the three well-known gene selection methods. In real data
analysis, the proposed method identified a subsets of $210$ genes out of $2000$
genes. The network of these genes has significantly more interactions than
expected, which indicates that they may function in a concerted effort on colon
cancer.
</summary>
    <author>
      <name>Md. Ashad Alam</name>
    </author>
    <author>
      <name>Mohammad Shahjama</name>
    </author>
    <author>
      <name>Md. Ferdush Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, submitted to ICCIT2018, Bangladesh</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06749v3</id>
    <updated>2018-09-05T17:03:42Z</updated>
    <published>2018-02-19T18:35:39Z</published>
    <title>Leveraged volume sampling for linear regression</title>
    <summary>  Suppose an $n \times d$ design matrix in a linear regression problem is
given, but the response for each point is hidden unless explicitly requested.
The goal is to sample only a small number $k \ll n$ of the responses, and then
produce a weight vector whose sum of squares loss over all points is at most
$1+\epsilon$ times the minimum. When $k$ is very small (e.g., $k=d$), jointly
sampling diverse subsets of points is crucial. One such method called volume
sampling has a unique and desirable property that the weight vector it produces
is an unbiased estimate of the optimum. It is therefore natural to ask if this
method offers the optimal unbiased estimate in terms of the number of responses
$k$ needed to achieve a $1+\epsilon$ loss approximation.
  Surprisingly we show that volume sampling can have poor behavior when we
require a very accurate approximation -- indeed worse than some i.i.d. sampling
techniques whose estimates are biased, such as leverage score sampling. We then
develop a new rescaled variant of volume sampling that produces an unbiased
estimate which avoids this bad behavior and has at least as good a tail bound
as leverage score sampling: sample size $k=O(d\log d + d/\epsilon)$ suffices to
guarantee total loss at most $1+\epsilon$ times the minimum with high
probability. Thus, we improve on the best previously known sample size for an
unbiased estimator, $k=O(d^2/\epsilon)$.
  Our rescaling procedure leads to a new efficient algorithm for volume
sampling which is based on a determinantal rejection sampling technique with
potentially broader applications to determinantal point processes. Other
contributions include introducing the combinatorics needed for rescaled volume
sampling and developing tail bounds for sums of dependent random matrices which
arise in the process.
</summary>
    <author>
      <name>Michał Dereziński</name>
    </author>
    <author>
      <name>Manfred K. Warmuth</name>
    </author>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06749v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06749v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09638v2</id>
    <updated>2018-09-05T16:57:25Z</updated>
    <published>2018-08-29T05:01:00Z</published>
    <title>Replay attack spoofing detection system using replay noise by multi-task
  learning</title>
    <summary>  In this paper, we propose a spoofing detection system for replay attack using
replay noise. In many previous studies across various domains, noise has been
reduced. However, in replay attack, we hypothesize that noise is the prominent
feature which is different with original signal and it can be one of the keys
to find whether a signal has been spoofed. We define the noise that is caused
by the replay attack as replay noise. Specifically, the noise of playback
devices, recording environments, and recording devices, is included in the
replay noise. We explore the effectiveness of training a deep neural network
simultaneously for replay attack spoofing detection and replay noise
classification. Multi-task learning was exploited to embed spoofing detection
and replay noise classification in the code layer. The experiment results on
the ASVspoof2017 datasets demonstrate that the performance of our proposed
system is relatively improved 30% on the evaluation set.
</summary>
    <author>
      <name>Hye-Jin Shim</name>
    </author>
    <author>
      <name>Jee-weon Jung</name>
    </author>
    <author>
      <name>Hee-Soo Heo</name>
    </author>
    <author>
      <name>Sunghyun Yoon</name>
    </author>
    <author>
      <name>Ha-Jin Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to 6th IEEE Global Conference on Signal and
  Information Processing (GlobalSIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09638v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09638v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02893v5</id>
    <updated>2018-09-05T16:55:28Z</updated>
    <published>2017-09-09T01:45:43Z</published>
    <title>Convolutional Dictionary Learning: A Comparative Review and New
  Algorithms</title>
    <summary>  Convolutional sparse representations are a form of sparse representation with
a dictionary that has a structure that is equivalent to convolution with a set
of linear filters. While effective algorithms have recently been developed for
the convolutional sparse coding problem, the corresponding dictionary learning
problem is substantially more challenging. Furthermore, although a number of
different approaches have been proposed, the absence of thorough comparisons
between them makes it difficult to determine which of them represents the
current state of the art. The present work both addresses this deficiency and
proposes some new approaches that outperform existing ones in certain contexts.
A thorough set of performance comparisons indicates a very wide range of
performance differences among the existing and proposed methods, and clearly
identifies those that are the most effective.
</summary>
    <author>
      <name>Cristina Garcia-Cardona</name>
    </author>
    <author>
      <name>Brendt Wohlberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCI.2018.2840334</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCI.2018.2840334" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos in Eq. (18) and (19)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computational Imaging, vol. 4, no. 3, pp.
  366-381, Sep 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.02893v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02893v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01357v3</id>
    <updated>2018-09-05T16:50:09Z</updated>
    <published>2018-07-31T08:15:06Z</published>
    <title>A recurrent multi-scale approach to RBG-D Object Recognition</title>
    <summary>  Technological development aims to produce generations of increasingly
efficient robots able to perform complex tasks. This requires considerable
efforts, from the scientific community, to find new algorithms that solve
computer vision problems, such as object recognition. The diffusion of RGB-D
cameras directed the study towards the research of new architectures able to
exploit the RGB and Depth information. The project that is developed in this
thesis concerns the realization of a new end-to-end architecture for the
recognition of RGB-D objects called RCFusion. Our method generates compact and
highly discriminative multi-modal features by combining complementary RGB and
depth information representing different levels of abstraction. We evaluate our
method on standard object recognition datasets, RGB-D Object Dataset and
JHUIT-50. The experiments performed show that our method outperforms the
existing approaches and establishes new state-of-the-art results for both
datasets.
</summary>
    <author>
      <name>Mirco Planamente</name>
    </author>
    <author>
      <name>Mohammad Reza Loghmani</name>
    </author>
    <author>
      <name>Barbara Caputo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master thesis extracted from the paper arXiv:1806.01673 submitted to
  accv 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01357v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01357v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01534v1</id>
    <updated>2018-09-05T16:44:04Z</updated>
    <published>2018-09-05T16:44:04Z</published>
    <title>Utilizing Character and Word Embeddings for Text Normalization with
  Sequence-to-Sequence Models</title>
    <summary>  Text normalization is an important enabling technology for several NLP tasks.
Recently, neural-network-based approaches have outperformed well-established
models in this task. However, in languages other than English, there has been
little exploration in this direction. Both the scarcity of annotated data and
the complexity of the language increase the difficulty of the problem. To
address these challenges, we use a sequence-to-sequence model with
character-based attention, which in addition to its self-learned character
embeddings, uses word embeddings pre-trained with an approach that also models
subword information. This provides the neural model with access to more
linguistic information especially suitable for text normalization, without
large parallel corpora. We show that providing the model with word-level
features bridges the gap for the neural network approach to achieve a
state-of-the-art F1 score on a standard Arabic language correction shared task
dataset.
</summary>
    <author>
      <name>Daniel Watson</name>
    </author>
    <author>
      <name>Nasser Zalmout</name>
    </author>
    <author>
      <name>Nizar Habash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01610v1</id>
    <updated>2018-09-05T16:30:08Z</updated>
    <published>2018-09-05T16:30:08Z</published>
    <title>Bimodal network architectures for automatic generation of image
  annotation from text</title>
    <summary>  Medical image analysis practitioners have embraced big data methodologies.
This has created a need for large annotated datasets. The source of big data is
typically large image collections and clinical reports recorded for these
images. In many cases, however, building algorithms aimed at segmentation and
detection of disease requires a training dataset with markings of the areas of
interest on the image that match with the described anomalies. This process of
annotation is expensive and needs the involvement of clinicians. In this work
we propose two separate deep neural network architectures for automatic marking
of a region of interest (ROI) on the image best representing a finding
location, given a textual report or a set of keywords. One architecture
consists of LSTM and CNN components and is trained end to end with images,
matching text, and markings of ROIs for those images. The output layer
estimates the coordinates of the vertices of a polygonal region. The second
architecture uses a network pre-trained on a large dataset of the same image
types for learning feature representations of the findings of interest. We show
that for a variety of findings from chest X-ray images, both proposed
architectures learn to estimate the ROI, as validated by clinical annotations.
There is a clear advantage obtained from the architecture with pre-trained
imaging network. The centroids of the ROIs marked by this network were on
average at a distance equivalent to 5.1% of the image width from the centroids
of the ground truth ROIs.
</summary>
    <author>
      <name>Mehdi Moradi</name>
    </author>
    <author>
      <name>Ali Madani</name>
    </author>
    <author>
      <name>Yaniv Gur</name>
    </author>
    <author>
      <name>Yufan Guo</name>
    </author>
    <author>
      <name>Tanveer Syeda-Mahmood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to MICCAI 2018, LNCS 11070</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science (LNCS 11070), Proceedings of
  Medical Image Computing &amp; Computer Assisted Intervention (MICCAI 2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.01610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00252v3</id>
    <updated>2018-09-05T16:26:22Z</updated>
    <published>2016-05-01T13:35:15Z</published>
    <title>Fast Rates for General Unbounded Loss Functions: from ERM to Generalized
  Bayes</title>
    <summary>  We present new excess risk bounds for general unbounded loss functions
including log loss and squared loss, where the distribution of the losses may
be heavy-tailed. The bounds hold for general estimators, but they are optimized
when applied to $\eta$-generalized Bayesian, MDL, and empirical risk
minimization estimators. In the case of log loss, the bounds imply convergence
rates for generalized Bayesian inference under misspecification in terms of a
generalization of the Hellinger metric as long as the learning rate $\eta$ is
set correctly. For general loss functions, our bounds rely on two separate
conditions: the $v$-GRIP (generalized reversed information projection)
conditions, which control the lower tail of the excess loss; and the newly
introduced witness condition, which controls the upper tail. The parameter $v$
in the $v$-GRIP conditions determines the achievable rate and is akin to the
exponent in the Tsybakov margin condition and the Bernstein condition for
bounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in
combination with small model complexity leads to $\tilde{O}(1/n)$ rates. The
witness condition allows us to connect the excess risk to an 'annealed' version
thereof, by which we generalize several previous results connecting Hellinger
and R\'enyi divergence to KL divergence.
</summary>
    <author>
      <name>Peter D. Grünwald</name>
    </author>
    <author>
      <name>Nishant A. Mehta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">79 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00252v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00252v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01605v1</id>
    <updated>2018-09-05T16:21:05Z</updated>
    <published>2018-09-05T16:21:05Z</published>
    <title>Anomaly Detection in the Presence of Missing Values</title>
    <summary>  Standard methods for anomaly detection assume that all features are observed
at both learning time and prediction time. Such methods cannot process data
containing missing values. This paper studies five strategies for handling
missing values in test queries: (a) mean imputation, (b) MAP imputation, (c)
reduction (reduced-dimension anomaly detectors via feature bagging), (d)
marginalization (for density estimators only), and (e) proportional
distribution (for tree-based methods only). Our analysis suggests that MAP
imputation and proportional distribution should give better results than mean
imputation, reduction, and marginalization. These hypotheses are largely
confirmed by experimental studies on synthetic data and on anomaly detection
benchmark data sets using the Isolation Forest (IF), LODA, and EGMM anomaly
detection algorithms. However, marginalization worked surprisingly well for
EGMM, and there are exceptions where reduction works well on some benchmark
problems. We recommend proportional distribution for IF, MAP imputation for
LODA, and marginalization for EGMM.
</summary>
    <author>
      <name>Thomas G. Dietterich</name>
    </author>
    <author>
      <name>Tadesse Zemicheal</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01588v1</id>
    <updated>2018-09-05T15:52:35Z</updated>
    <published>2018-09-05T15:52:35Z</published>
    <title>Learning Paths from Signature Tensors</title>
    <summary>  Matrix congruence extends naturally to the setting of tensors. We apply
methods from tensor decomposition, algebraic geometry and numerical
optimization to this group action. Given a tensor in the orbit of another
tensor, we compute a matrix which transforms one to the other. Our primary
application is an inverse problem from stochastic analysis: the recovery of
paths from their signature tensors of order three. We establish identifiability
results and recovery algorithms for piecewise linear paths, polynomial paths,
and generic dictionaries. A detailed analysis of the relevant condition numbers
is presented. We also compute the shortest path with a given signature tensor.
</summary>
    <author>
      <name>Max Pfeffer</name>
    </author>
    <author>
      <name>Anna Seigal</name>
    </author>
    <author>
      <name>Bernd Sturmfels</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01587v1</id>
    <updated>2018-09-05T15:51:50Z</updated>
    <published>2018-09-05T15:51:50Z</published>
    <title>GAN Lab: Understanding Complex Deep Generative Models using Interactive
  Visual Experimentation</title>
    <summary>  Recent success in deep learning has generated immense interest among
practitioners and students, inspiring many to learn about this new technology.
While visual and interactive approaches have been successfully developed to
help people more easily learn deep learning, most existing tools focus on
simpler models. In this work, we present GAN Lab, the first interactive
visualization tool designed for non-experts to learn and experiment with
Generative Adversarial Networks (GANs), a popular class of complex deep
learning models. With GAN Lab, users can interactively train generative models
and visualize the dynamic training process's intermediate results. GAN Lab
tightly integrates an model overview graph that summarizes GAN's structure, and
a layered distributions view that helps users interpret the interplay between
submodels. GAN Lab introduces new interactive experimentation features for
learning complex deep learning models, such as step-by-step training at
multiple levels of abstraction for understanding intricate training dynamics.
Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web
browsers, without the need for installation or specialized hardware, overcoming
a major practical challenge in deploying interactive tools for deep learning.
</summary>
    <author>
      <name>Minsuk Kahng</name>
    </author>
    <author>
      <name>Nikhil Thorat</name>
    </author>
    <author>
      <name>Duen Horng Chau</name>
    </author>
    <author>
      <name>Fernanda Viégas</name>
    </author>
    <author>
      <name>Martin Wattenberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2018.2864500</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2018.2864500" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be published in the IEEE Transactions on
  Visualization and Computer Graphics, 25(1), January 2019, and presented at
  IEEE VAST 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05120v4</id>
    <updated>2018-09-05T15:24:14Z</updated>
    <published>2016-10-17T14:01:25Z</published>
    <title>Lazifying Conditional Gradient Algorithms</title>
    <summary>  Conditional gradient algorithms (also often called Frank-Wolfe algorithms)
are popular due to their simplicity of only requiring a linear optimization
oracle and more recently they also gained significant traction for online
learning. While simple in principle, in many cases the actual implementation of
the linear optimization oracle is costly. We show a general method to lazify
various conditional gradient algorithms, which in actual computations leads to
several orders of magnitude of speedup in wall-clock time. This is achieved by
using a faster separation oracle instead of a linear optimization oracle,
relying only on few linear optimization oracle calls.
</summary>
    <author>
      <name>Gábor Braun</name>
    </author>
    <author>
      <name>Sebastian Pokutta</name>
    </author>
    <author>
      <name>Daniel Zink</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages and 31 pages of computational results</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.05120v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05120v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32, 68W27, 90C52" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01571v1</id>
    <updated>2018-09-05T15:14:08Z</updated>
    <published>2018-09-05T15:14:08Z</published>
    <title>Knowledge Integrated Classifier Design Based on Utility Optimization</title>
    <summary>  This paper proposes a systematic framework to design a classification model
that yields a classifier which optimizes a utility function based on prior
knowledge. Specifically, as the data size grows, we prove that the produced
classifier asymptotically converges to the optimal classifier, an extended
version of the Bayes rule, which maximizes the utility function. Therefore, we
provide a meaningful theoretical interpretation for modeling with the knowledge
incorporated. Our knowledge incorporation method allows domain experts to guide
the classifier towards correctly classifying data that they think to be more
significant.
</summary>
    <author>
      <name>Shaohan Chen</name>
    </author>
    <author>
      <name>Chuanhou Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08323v2</id>
    <updated>2018-09-05T15:09:06Z</updated>
    <published>2018-02-22T22:08:14Z</published>
    <title>Deep learning algorithm for data-driven simulation of noisy dynamical
  system</title>
    <summary>  We present a deep learning model, DE-LSTM, for the simulation of a stochastic
process with an underlying nonlinear dynamics. The deep learning model aims to
approximate the probability density function of a stochastic process via
numerical discretization and the underlying nonlinear dynamics is modeled by
the Long Short-Term Memory (LSTM) network. It is shown that, when the numerical
discretization is used, the function estimation problem can be solved by a
multi-label classification problem. A penalized maximum log likelihood method
is proposed to impose a smoothness condition in the prediction of the
probability distribution. We show that the time evolution of the probability
distribution can be computed by a high-dimensional integration of the
transition probability of the LSTM internal states. A Monte Carlo algorithm to
approximate the high-dimensional integration is outlined. The behavior of
DE-LSTM is thoroughly investigated by using the Ornstein-Uhlenbeck process and
noisy observations of nonlinear dynamical systems; Mackey-Glass time series and
forced Van der Pol oscillator. It is shown that DE-LSTM makes a good prediction
of the probability distribution without assuming any distributional properties
of the stochastic process. For a multiple-step forecast of the Mackey-Glass
time series, the prediction uncertainty, denoted by the 95\% confidence
interval, first grows, then dynamically adjusts following the evolution of the
system, while in the simulation of the forced Van der Pol oscillator, the
prediction uncertainty does not grow in time even for a 3,000-step forecast.
</summary>
    <author>
      <name>Kyongmin Yeo</name>
    </author>
    <author>
      <name>Igor Melnyk</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08323v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08323v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01564v1</id>
    <updated>2018-09-05T15:03:23Z</updated>
    <published>2018-09-05T15:03:23Z</published>
    <title>Traffic Density Estimation using a Convolutional Neural Network</title>
    <summary>  The goal of this project is to introduce and present a machine learning
application that aims to improve the quality of life of people in Singapore. In
particular, we investigate the use of machine learning solutions to tackle the
problem of traffic congestion in Singapore. In layman's terms, we seek to make
Singapore (or any other city) a smoother place. To accomplish this aim, we
present an end-to-end system comprising of 1. A traffic density estimation
algorithm at traffic lights/junctions and 2. a suitable traffic signal control
algorithms that make use of the density information for better traffic control.
Traffic density estimation can be obtained from traffic junction images using
various machine learning techniques (combined with CV tools). After research
into various advanced machine learning methods, we decided on convolutional
neural networks (CNNs). We conducted experiments on our algorithms, using the
publicly available traffic camera dataset published by the Land Transport
Authority (LTA) to demonstrate the feasibility of this approach. With these
traffic density estimates, different traffic algorithms can be applied to
minimize congestion at traffic junctions in general.
</summary>
    <author>
      <name>Julian Nubert</name>
    </author>
    <author>
      <name>Nicholas Giai Truong</name>
    </author>
    <author>
      <name>Abel Lim</name>
    </author>
    <author>
      <name>Herbert Ilhan Tanujaya</name>
    </author>
    <author>
      <name>Leah Lim</name>
    </author>
    <author>
      <name>Mai Anh Vu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning Project National University of Singapore. 6 pages, 5
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01560v1</id>
    <updated>2018-09-05T14:56:09Z</updated>
    <published>2018-09-05T14:56:09Z</published>
    <title>Reinforcement Learning under Threats</title>
    <summary>  In several reinforcement learning (RL) scenarios, mainly in security
settings, there may be adversaries trying to interfere with the reward
generating process. In this paper, we introduce Threatened Markov Decision
Processes (TMDPs), which provide a framework to support a decision maker
against a potential adversary in RL. Furthermore, we propose a level-$k$
thinking scheme resulting in a new learning framework to deal with TMDPs. After
introducing our framework and deriving theoretical results, relevant empirical
evidence is given via extensive experiments, showing the benefits of accounting
for adversaries while the agent learns.
</summary>
    <author>
      <name>Víctor Gallego</name>
    </author>
    <author>
      <name>Roi Naveiro</name>
    </author>
    <author>
      <name>David Ríos Insua</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07233v3</id>
    <updated>2018-09-05T14:10:46Z</updated>
    <published>2018-08-22T06:07:03Z</published>
    <title>Neural Architecture Optimization</title>
    <summary>  Automatic neural architecture design has shown its potential in discovering
powerful neural network architectures. Existing methods, no matter based on
reinforcement learning or evolutionary algorithms (EA), conduct architecture
search in a discrete space, which is highly inefficient. In this paper, we
propose a simple and efficient method to automatic neural architecture design
based on continuous optimization. We call this new approach neural architecture
optimization (NAO). There are three key components in our proposed approach:
(1) An encoder embeds/maps neural network architectures into a continuous
space. (2) A predictor takes the continuous representation of a network as
input and predicts its accuracy. (3) A decoder maps a continuous representation
of a network back to its architecture. The performance predictor and the
encoder enable us to perform gradient based optimization in the continuous
space to find the embedding of a new architecture with potentially better
accuracy. Such a better embedding is then decoded to a network by the decoder.
Experiments show that the architecture discovered by our method is very
competitive for image classification task on CIFAR-10 and language modeling
task on PTB, outperforming or on par with the best results of previous
architecture search methods with a significantly reduction of computational
resources. Specifically we obtain $2.07\%$ test set error rate for CIFAR-10
image classification task and $55.9$ test set perplexity of PTB language
modeling task. The best discovered architectures on both tasks are successfully
transferred to other tasks such as CIFAR-100 and WikiText-2.
</summary>
    <author>
      <name>Renqian Luo</name>
    </author>
    <author>
      <name>Fei Tian</name>
    </author>
    <author>
      <name>Tao Qin</name>
    </author>
    <author>
      <name>Enhong Chen</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ongoing work. Will appear at NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07233v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07233v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09540v2</id>
    <updated>2018-09-05T13:49:52Z</updated>
    <published>2018-08-28T20:53:01Z</published>
    <title>Lipschitz regularized Deep Neural Networks converge and generalize</title>
    <summary>  Lipschitz regularized neural networks augment the usual fidelity term used in
training with a regularization term corresponding the excess Lipschitz constant
of the network compared to the Lipschitz constant of the data. We prove that
Lipschitz regularized neural networks converge, and provide a rate, in the
limit as the number of data points $n\to\infty$. We consider the regime where
perfect fitting of data is possible, which means the size of the network grows
with $n$. There are two regimes: in the case of perfect labels, we prove
convergence to the label function which corresponds to zero loss. In the case
of corrupted labels which occurs when the Lipschitz constant of the data blows
up, we prove convergence to a regularized label function which is the solution
of a limiting variational problem.
</summary>
    <author>
      <name>Adam M Oberman</name>
    </author>
    <author>
      <name>Jeff Calder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures. Added a figure and a reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09540v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09540v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05377v2</id>
    <updated>2018-09-05T13:49:26Z</updated>
    <published>2018-08-16T08:45:01Z</published>
    <title>Neural Architecture Search: A Survey</title>
    <summary>  Deep Learning has enabled remarkable progress over the last years on a
variety of tasks, such as image recognition, speech recognition, and machine
translation. One crucial aspect for this progress are novel neural
architectures. Currently employed architectures have mostly been developed
manually by human experts, which is a time-consuming and error-prone process.
Because of this, there is growing interest in automated neural architecture
search methods. We provide an overview of existing work in this field of
research and categorize them according to three dimensions: search space,
search strategy, and performance estimation strategy.
</summary>
    <author>
      <name>Thomas Elsken</name>
    </author>
    <author>
      <name>Jan Hendrik Metzen</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01506v1</id>
    <updated>2018-09-05T13:44:12Z</updated>
    <published>2018-09-05T13:44:12Z</published>
    <title>Deep Reinforcement Learning in High Frequency Trading</title>
    <summary>  The ability to give a precise and fast prediction for the price movement of
stocks is the key to profitability in High Frequency Trading. The main
objective of this paper is to propose a novel way of modeling the high
frequency trading problem using Deep Reinforcement Learning and to argue why
Deep RL can have a lot of potential in the field of High Frequency Trading. We
have analyzed the model's performance based on it's prediction accuracy as well
as prediction speed across full-day trading simulations.
</summary>
    <author>
      <name>Prakhar Ganesh</name>
    </author>
    <author>
      <name>Puneet Rakheja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted in ACM CoDs-COMAD 2019, to be held in kolkata, India in
  January 2019. arXiv admin note: text overlap with arXiv:1708.05866 by other
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05052v6</id>
    <updated>2018-09-05T12:43:31Z</updated>
    <published>2018-05-14T08:08:33Z</published>
    <title>Machine Learning: Basic Principles</title>
    <summary>  This tutorial is based on the lecture notes for the courses "Machine
Learning: Basic Principles" and "Artificial Intelligence", which I have
(co-)taught since 2015 at Aalto University. The aim is to provide an accessible
introduction to some of the main concepts and methods within machine learning.
Many of the current systems which are considered (artificial) intelligent are
based on combinations of few basic machine learning methods. After formalizing
the main building blocks of a machine learning problem, some popular
algorithmic design patterns for machine learning methods are discussed in some
detail.
</summary>
    <author>
      <name>Alexander Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05052v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05052v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01452v1</id>
    <updated>2018-09-05T12:04:35Z</updated>
    <published>2018-09-05T12:04:35Z</published>
    <title>Sentylic at IEST 2018: Gated Recurrent Neural Network and Capsule
  Network Based Approach for Implicit Emotion Detection</title>
    <summary>  In this paper, we present the system we have used for the Implicit WASSA 2018
Implicit Emotion Shared Task. The task is to predict the emotion of a tweet of
which the explicit mentions of emotion terms have been removed. The idea is to
come up with a model which has the ability to implicitly identify the emotion
expressed given the context words. We have used a Gated Recurrent Neural
Network (GRU) and a Capsule Network based model for the task. Pre-trained word
embeddings have been utilized to incorporate contextual knowledge about words
into the model. GRU layer learns latent representations using the input word
embeddings. Subsequent Capsule Network layer learns high-level features from
that hidden representation. The proposed model managed to achieve a macro-F1
score of 0.692.
</summary>
    <author>
      <name>Prabod Rathnayaka</name>
    </author>
    <author>
      <name>Supun Abeysinghe</name>
    </author>
    <author>
      <name>Chamod Samarajeewa</name>
    </author>
    <author>
      <name>Isura Manchanayake</name>
    </author>
    <author>
      <name>Malaka Walpola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to the 9th Workshop on Computational Approaches to
  Subjectivity, Sentiment &amp; Social Media Analysis, part of the EMNLP 2018
  Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07594v3</id>
    <updated>2018-09-05T11:53:24Z</updated>
    <published>2018-05-19T13:51:34Z</published>
    <title>Generalizing Point Embeddings using the Wasserstein Space of Elliptical
  Distributions</title>
    <summary>  Embedding complex objects as vectors in low dimensional spaces is a
longstanding problem in machine learning. We propose in this work an extension
of that approach, which consists in embedding objects as elliptical probability
distributions, namely distributions whose densities have elliptical level sets.
We endow these measures with the 2-Wasserstein metric, with two important
benefits: (i) For such measures, the squared 2-Wasserstein metric has a closed
form, equal to the sum of the squared Euclidean distance between means and the
squared Bures metric between covariance matrices. The latter is a Riemannian
metric between positive semi-definite matrices, which turns out to be Euclidean
on a suitable factor representation of such matrices, which is valid on the
entire geodesic between these matrices. (ii) The 2-Wasserstein distance boils
down to the usual Euclidean metric when comparing Diracs, and therefore
provides the natural framework to extend point embeddings. We show that for
these reasons Wasserstein elliptical embeddings are more intuitive and yield
tools that are better behaved numerically than the alternative choice of
Gaussian embeddings with the Kullback-Leibler divergence. In particular, and
unlike previous work based on the KL geometry, we learn elliptical
distributions that are not necessarily diagonal. We demonstrate the advantages
of elliptical embeddings by using them for visualization, to compute embeddings
of words, and to reflect entailment or hypernymy.
</summary>
    <author>
      <name>Boris Muzellec</name>
    </author>
    <author>
      <name>Marco Cuturi</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07594v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07594v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07745v3</id>
    <updated>2018-09-05T11:52:08Z</updated>
    <published>2018-04-20T17:41:15Z</published>
    <title>Loss in Translation: Learning Bilingual Word Mapping with a Retrieval
  Criterion</title>
    <summary>  Continuous word representations learned separately on distinct languages can
be aligned so that their words become comparable in a common space. Existing
works typically solve a least-square regression problem to learn a rotation
aligning a small bilingual lexicon, and use a retrieval criterion for
inference. In this paper, we propose an unified formulation that directly
optimizes a retrieval criterion in an end-to-end fashion. Our experiments on
standard benchmarks show that our approach outperforms the state of the art on
word translation, with the biggest improvements observed for distant language
pairs such as English-Chinese.
</summary>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Piotr Bojanowski</name>
    </author>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Herve Jegou</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <link href="http://arxiv.org/abs/1804.07745v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07745v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01434v1</id>
    <updated>2018-09-05T11:06:06Z</updated>
    <published>2018-09-05T11:06:06Z</published>
    <title>Stellar Cluster Detection using GMM with Deep Variational Autoencoder</title>
    <summary>  Detecting stellar clusters have always been an important research problem in
Astronomy. Although images do not convey very detailed information in detecting
stellar density enhancements, we attempt to understand if new machine learning
techniques can reveal patterns that would assist in drawing better inferences
from the available image data. This paper describes an unsupervised approach in
detecting star clusters using Deep Variational Autoencoder combined with a
Gaussian Mixture Model. We show that our method works significantly well in
comparison with state-of-the-art detection algorithm in recognizing a variety
of star clusters even in the presence of noise and distortion.
</summary>
    <author>
      <name>Arnab Karmakar</name>
    </author>
    <author>
      <name>Deepak Mishra</name>
    </author>
    <author>
      <name>Anandmayee Tej</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures, under review in IEEE RAICS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03832v3</id>
    <updated>2018-09-05T10:34:02Z</updated>
    <published>2018-02-11T22:35:28Z</published>
    <title>Quadrature-based features for kernel approximation</title>
    <summary>  We consider the problem of improving kernel approximation via randomized
feature maps. These maps arise as Monte Carlo approximation to integral
representations of kernel functions and scale up kernel methods for larger
datasets. Based on an efficient numerical integration technique, we propose a
unifying approach that reinterprets the previous random features methods and
extends to better estimates of the kernel approximation. We derive the
convergence behaviour and conduct an extensive empirical study that supports
our hypothesis.
</summary>
    <author>
      <name>Marina Munkhoeva</name>
    </author>
    <author>
      <name>Yermek Kapushev</name>
    </author>
    <author>
      <name>Evgeny Burnaev</name>
    </author>
    <author>
      <name>Ivan Oseledets</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, Appendix: 4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03832v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03832v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01407v1</id>
    <updated>2018-09-05T09:41:16Z</updated>
    <published>2018-09-05T09:41:16Z</published>
    <title>Consensus-Driven Propagation in Massive Unlabeled Data for Face
  Recognition</title>
    <summary>  Face recognition has witnessed great progress in recent years, mainly
attributed to the high-capacity model designed and the abundant labeled data
collected. However, it becomes more and more prohibitive to scale up the
current million-level identity annotations. In this work, we show that
unlabeled face data can be as effective as the labeled ones. Here, we consider
a setting closely mimicking the real-world scenario, where the unlabeled data
are collected from unconstrained environments and their identities are
exclusive from the labeled ones. Our main insight is that although the class
information is not available, we can still faithfully approximate these
semantic relationships by constructing a relational graph in a bottom-up
manner. We propose Consensus-Driven Propagation (CDP) to tackle this
challenging problem with two modules, the "committee" and the "mediator", which
select positive face pairs robustly by carefully aggregating multi-view
information. Extensive experiments validate the effectiveness of both modules
to discard outliers and mine hard positives. With CDP, we achieve a compelling
accuracy of 78.18% on MegaFace identification challenge by using only 9% of the
labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all
labels are employed.
</summary>
    <author>
      <name>Xiaohang Zhan</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
    <author>
      <name>Junjie Yan</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <author>
      <name>Chen Change Loy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ECCV 2018. More details at the project page:
  http://mmlab.ie.cuhk.edu.hk/projects/CDP/</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09627v3</id>
    <updated>2018-09-05T09:08:43Z</updated>
    <published>2017-06-29T08:42:44Z</published>
    <title>Deep learning bank distress from news and numerical financial data</title>
    <summary>  In this paper we focus our attention on the exploitation of the information
contained in financial news to enhance the performance of a classifier of bank
distress. Such information should be analyzed and inserted into the predictive
model in the most efficient way and this task deals with all the issues related
to text analysis and specifically analysis of news media. Among the different
models proposed for such purpose, we investigate one of the possible deep
learning approaches, based on a doc2vec representation of the textual data, a
kind of neural network able to map the sequential and symbolic text input onto
a reduced latent semantic space. Afterwards, a second supervised neural network
is trained combining news data with standard financial figures to classify
banks whether in distressed or tranquil states, based on a small set of known
distress events. Then the final aim is not only the improvement of the
predictive performance of the classifier but also to assess the importance of
news data in the classification process. Does news data really bring more
useful information not contained in standard financial variables? Our results
seem to confirm such hypothesis.
</summary>
    <author>
      <name>Paola Cerchiello</name>
    </author>
    <author>
      <name>Giancarlo Nicola</name>
    </author>
    <author>
      <name>Samuel Ronnqvist</name>
    </author>
    <author>
      <name>Peter Sarlin</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09627v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09627v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01382v1</id>
    <updated>2018-09-05T08:32:01Z</updated>
    <published>2018-09-05T08:32:01Z</published>
    <title>Anytime Hedge achieves optimal regret in the stochastic regime</title>
    <summary>  This paper is about a surprising fact: we prove that the anytime Hedge
algorithm with decreasing learning rate, which is one of the simplest algorithm
for the problem of prediction with expert advice, is actually both worst-case
optimal and adaptive to the easier stochastic and adversarial with a gap
problems. This runs counter to the common belief in the literature that this
algorithm is overly conservative, and that only new adaptive algorithms can
simultaneously achieve minimax regret and adapt to the difficulty of the
problem. Moreover, our analysis exhibits qualitative differences with other
variants of the Hedge algorithm, based on the so-called "doubling trick", and
the fixed-horizon version (with constant learning rate).
</summary>
    <author>
      <name>Jaouad Mourtada</name>
    </author>
    <author>
      <name>Stéphane Gaïffas</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01369v1</id>
    <updated>2018-09-05T07:56:11Z</updated>
    <published>2018-09-05T07:56:11Z</published>
    <title>Towards quantitative methods to assess network generative models</title>
    <summary>  Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
</summary>
    <author>
      <name>Vahid Mostofi</name>
    </author>
    <author>
      <name>Sadegh Aliakbary</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01357v1</id>
    <updated>2018-09-05T07:13:30Z</updated>
    <published>2018-09-05T07:13:30Z</published>
    <title>Zero Shot Learning for Code Education: Rubric Sampling with Deep
  Learning Inference</title>
    <summary>  In modern computer science education, massive open online courses (MOOCs) log
thousands of hours of data about how students solve coding challenges. Being so
rich in data, these platforms have garnered the interest of the machine
learning community, with many new algorithms attempting to autonomously provide
feedback to help future students learn. But what about those first hundred
thousand students? In most educational contexts (i.e. classrooms), assignments
do not have enough historical data for supervised learning. In this paper, we
introduce a human-in-the-loop "rubric sampling" approach to tackle the "zero
shot" feedback challenge. We are able to provide autonomous feedback for the
first students working on an introductory programming assignment with accuracy
that substantially outperforms data-hungry algorithms and approaches human
level fidelity. Rubric sampling requires minimal teacher effort, can associate
feedback with specific parts of a student's solution and can articulate a
student's misconceptions in the language of the instructor. Deep learning
inference enables rubric sampling to further improve as more assignment
specific student data is acquired. We demonstrate our results on a novel
dataset from Code.org, the world's largest programming education platform.
</summary>
    <author>
      <name>Mike Wu</name>
    </author>
    <author>
      <name>Milan Mosse</name>
    </author>
    <author>
      <name>Noah Goodman</name>
    </author>
    <author>
      <name>Chris Piech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01354v1</id>
    <updated>2018-09-05T06:50:24Z</updated>
    <published>2018-09-05T06:50:24Z</published>
    <title>Semantic Human Matting</title>
    <summary>  Human matting, high quality extraction of humans from natural images, is
crucial for a wide variety of applications. Since the matting problem is
severely under-constrained, most previous methods require user interactions to
take user designated trimaps or scribbles as constraints. This user-in-the-loop
nature makes them difficult to be applied to large scale data or time-sensitive
scenarios. In this paper, instead of using explicit user input constraints, we
employ implicit semantic constraints learned from data and propose an automatic
human matting algorithm (SHM). SHM is the first algorithm that learns to
jointly fit both semantic information and high quality details with deep
networks. In practice, simultaneously learning both coarse semantics and fine
details is challenging. We propose a novel fusion strategy which naturally
gives a probabilistic estimation of the alpha matte. We also construct a very
large dataset with high quality annotations consisting of 35,513 unique
foregrounds to facilitate the learning and evaluation of human matting.
Extensive experiments on this dataset and plenty of real images show that SHM
achieves comparable results with state-of-the-art interactive matting methods.
</summary>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Tiezheng Ge</name>
    </author>
    <author>
      <name>Yanyu Xu</name>
    </author>
    <author>
      <name>Zhiqiang Zhang</name>
    </author>
    <author>
      <name>Xinxin Yang</name>
    </author>
    <author>
      <name>Kun Gai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01353v1</id>
    <updated>2018-09-05T06:49:12Z</updated>
    <published>2018-09-05T06:49:12Z</published>
    <title>IKA: Independent Kernel Approximator</title>
    <summary>  This paper describes a new method for low rank kernel approximation called
IKA. The main advantage of IKA is that it produces a function $\psi(x)$ defined
as a linear combination of arbitrarily chosen functions. In contrast the
approximation produced by Nystr\"om method is a linear combination of kernel
evaluations. The proposed method consistently outperformed Nystr\"om method in
a comparison on the STL-10 dataset. Numerical results are reproducible using
the source code available at https://gitlab.com/matteo-ronchetti/IKA
</summary>
    <author>
      <name>Matteo Ronchetti</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05978v3</id>
    <updated>2018-09-05T05:37:03Z</updated>
    <published>2018-06-15T13:55:18Z</published>
    <title>Bayesian Convolutional Neural Networks</title>
    <summary>  We introduce Bayesian Convolutional Neural Networks (BayesCNNs), a variant of
Convolutional Neural Networks (CNNs) which is built upon Bayes by Backprop. We
demonstrate how this novel reliable variational inference method can serve as a
fundamental construct for various network architectures. On multiple datasets
in supervised learning settings (MNIST, CIFAR-10, CIFAR-100, and STL-10), our
proposed variational inference method achieves performances equivalent to
frequentist inference in identical architectures, while a measurement for
uncertainties and a regularisation are incorporated naturally. In the past,
Bayes by Backprop has been successfully implemented in feedforward and
recurrent neural networks, but not in convolutional ones. This work symbolises
the extension of Bayesian neural networks which encompasses all three
aforementioned types of network architectures now.
</summary>
    <author>
      <name>Kumar Shridhar</name>
    </author>
    <author>
      <name>Felix Laumann</name>
    </author>
    <author>
      <name>Adrian Llopart Maurin</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1704.02798 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05978v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05978v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01213v3</id>
    <updated>2018-09-05T05:14:37Z</updated>
    <published>2017-07-05T05:21:50Z</published>
    <title>Data-Driven Sparse Structure Selection for Deep Neural Networks</title>
    <summary>  Deep convolutional neural networks have liberated its extraordinary power on
various tasks. However, it is still very challenging to deploy state-of-the-art
models into real-world applications due to their high computational complexity.
How can we design a compact and effective network without massive experiments
and expert knowledge? In this paper, we propose a simple and effective
framework to learn and prune deep models in an end-to-end manner. In our
framework, a new type of parameter -- scaling factor is first introduced to
scale the outputs of specific structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this
optimization problem by a modified stochastic Accelerated Proximal Gradient
(APG) method. By forcing some of the factors to zero, we can safely remove the
corresponding structures, thus prune the unimportant parts of a CNN. Comparing
with other structure selection methods that may need thousands of trials or
iterative fine-tuning, our method is trained fully end-to-end in one training
pass without bells and whistles. We evaluate our method, Sparse Structure
Selection with several state-of-the-art CNNs, and demonstrate very promising
results with adaptive depth and width selection.
</summary>
    <author>
      <name>Zehao Huang</name>
    </author>
    <author>
      <name>Naiyan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV Camera ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01213v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01213v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02081v2</id>
    <updated>2018-09-05T04:25:45Z</updated>
    <published>2018-04-05T23:41:11Z</published>
    <title>Adaptive Diffusions for Scalable Learning over Graphs</title>
    <summary>  Diffusion-based classifiers such as those relying on the Personalized
PageRank and the Heat kernel, enjoy remarkable classification accuracy at
modest computational requirements. Their performance however is affected by the
extent to which the chosen diffusion captures a typically unknown label
propagation mechanism, that can be specific to the underlying graph, and
potentially different for each class. The present work introduces a
disciplined, data-efficient approach to learning class-specific diffusion
functions adapted to the underlying network topology. The novel learning
approach leverages the notion of "landing probabilities" of class-specific
random walks, which can be computed efficiently, thereby ensuring scalability
to large graphs. This is supported by rigorous analysis of the properties of
the model as well as the proposed algorithms. Furthermore, a robust version of
the classifier facilitates learning even in noisy environments.
  Classification tests on real networks demonstrate that adapting the diffusion
function to the given graph and observed labels, significantly improves the
performance over fixed diffusions; reaching -- and many times surpassing -- the
classification accuracy of computationally heavier state-of-the-art competing
methods, that rely on node embeddings and deep neural networks.
</summary>
    <author>
      <name>Dimitris Berberidis</name>
    </author>
    <author>
      <name>Athanasios N. Nikolakopoulos</name>
    </author>
    <author>
      <name>Georgios B. Giannakis</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01316v1</id>
    <updated>2018-09-05T04:15:13Z</updated>
    <published>2018-09-05T04:15:13Z</published>
    <title>Learning User Preferences and Understanding Calendar Contexts for Event
  Scheduling</title>
    <summary>  With online calendar services gaining popularity worldwide, calendar data has
become one of the richest context sources for understanding human behavior.
However, event scheduling is still time-consuming even with the development of
online calendars. Although machine learning based event scheduling models have
automated scheduling processes to some extent, they often fail to understand
subtle user preferences and complex calendar contexts with event titles written
in natural language. In this paper, we propose Neural Event Scheduling
Assistant (NESA) which learns user preferences and understands calendar
contexts, directly from raw online calendars for fully automated and highly
effective event scheduling. We leverage over 593K calendar events for NESA to
learn scheduling personal events, and we further utilize NESA for
multi-attendee event scheduling. NESA successfully incorporates deep neural
networks such as Bidirectional Long Short-Term Memory, Convolutional Neural
Network, and Highway Network for learning the preferences of each user and
understanding calendar context based on natural languages. The experimental
results show that NESA significantly outperforms previous baseline models in
terms of various evaluation metrics on both personal and multi-attendee event
scheduling tasks. Our qualitative analysis demonstrates the effectiveness of
each layer in NESA and learned user preferences.
</summary>
    <author>
      <name>Donghyeon Kim</name>
    </author>
    <author>
      <name>Jinhyuk Lee</name>
    </author>
    <author>
      <name>Donghee Choi</name>
    </author>
    <author>
      <name>Jaehoon Choi</name>
    </author>
    <author>
      <name>Jaewoo Kang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271712</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271712" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CIKM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50, 68U35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00528v3</id>
    <updated>2018-09-05T02:32:12Z</updated>
    <published>2018-04-19T08:17:08Z</published>
    <title>Reconstruction of Simulation-Based Physical Field by Reconstruction
  Neural Network Method</title>
    <summary>  A variety of modeling techniques have been developed in the past decade to
reduce the computational expense and improve the accuracy of modeling. In this
study, a new framework of modeling is suggested. Compared with other popular
methods, a distinctive characteristic is "from image based model to analysis
based model (e.g. stress, strain, and deformation)". In such a framework, a
reconstruction neural network (ReConNN) model designed for simulation-based
physical field's reconstruction is proposed. The ReConNN contains two submodels
that are convolutional neural network (CNN) and generative adversarial net-work
(GAN). The CNN is employed to construct the mapping between contour images of
physical field and objective function. Subsequently, the GAN is utilized to
generate more images which are similar to the existing contour images. Finally,
Lagrange polynomial is applied to complete the reconstruction. However, the
existing CNN models are commonly applied to the classification tasks, which
seem to be difficult to handle with regression tasks of images. Meanwhile, the
existing GAN architectures are insufficient to generate high-accuracy "pseudo
contour images". Therefore, a ReConNN model based on a Convolution in
Convolution (CIC) and a Convolutional AutoEncoder based on Wasserstein
Generative Adversarial Network (WGAN-CAE) is suggested. To evaluate the
performance of the proposed model representatively, a classical topology
optimization procedure is considered. Then the ReConNN is utilized to the
reconstruction of heat transfer process of a pin fin heat sink. It demonstrates
that the proposed ReConNN model is proved to be a potential capability to
reconstruct physical field for multidisciplinary, such as structural
optimization.
</summary>
    <author>
      <name>Yu Li</name>
    </author>
    <author>
      <name>Hu Wang</name>
    </author>
    <author>
      <name>Kangjia Mo</name>
    </author>
    <author>
      <name>Tao Zeng</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00528v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00528v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01293v1</id>
    <updated>2018-09-05T01:55:28Z</updated>
    <published>2018-09-05T01:55:28Z</published>
    <title>Stochastic Particle-Optimization Sampling and the Non-Asymptotic
  Convergence Theory</title>
    <summary>  Particle-optimization sampling (POS) is a recently developed technique to
generate high-quality samples from a target distribution by iteratively
updating a set of interactive particles. A representative algorithm is the
Stein variational gradient descent (SVGD). Though obtaining significant
empirical success, the {\em non-asymptotic} convergence behavior of SVGD
remains unknown. In this paper, we generalize POS to a stochasticity setting by
injecting random noise in particle updates, called stochastic
particle-optimization sampling (SPOS). Standard SVGD can be regarded as a
special case of our framework. Notably, for the first time, we develop
non-asymptotic convergence theory for the SPOS framework (which includes SVGD),
characterizing the bias of a sample approximation w.r.t. the numbers of
particles and iterations under both convex- and noncovex-energy-function
settings. Remarkably, we provide theoretical understand of a pitfall of SVGD
that can be avoided in the proposed SPOS framework, i.e., particles tent to
collapse to a local mode in SVGD under some particular conditions. Our theory
is based on the analysis of nonlinear stochastic differential equations, which
serves as an extension and a complemented development to the asymptotic
convergence theory for SVGD such as [1].
</summary>
    <author>
      <name>Jianyi Zhang</name>
    </author>
    <author>
      <name>Ruiyi Zhang</name>
    </author>
    <author>
      <name>Changyou Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Use NIPS template</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00403v2</id>
    <updated>2018-09-05T01:45:01Z</updated>
    <published>2018-09-02T22:17:52Z</published>
    <title>Effective Exploration for Deep Reinforcement Learning via Bootstrapped
  Q-Ensembles under Tsallis Entropy Regularization</title>
    <summary>  Recently deep reinforcement learning (DRL) has achieved outstanding success
on solving many difficult and large-scale RL problems. However the high sample
cost required for effective learning often makes DRL unaffordable in
resource-limited applications. With the aim of improving sample efficiency and
learning performance, we will develop a new DRL algorithm in this paper that
seamless integrates entropy-induced and bootstrap-induced techniques for
efficient and deep exploration of the learning environment. Specifically, a
general form of Tsallis entropy regularizer will be utilized to drive
entropy-induced exploration based on efficient approximation of optimal
action-selection policies. Different from many existing works that rely on
action dithering strategies for exploration, our algorithm is efficient in
exploring actions with clear exploration value. Meanwhile, by employing an
ensemble of Q-networks under varied Tsallis entropy regularization, the
diversity of the ensemble can be further enhanced to enable effective
bootstrap-induced exploration. Experiments on Atari game playing tasks clearly
demonstrate that our new algorithm can achieve more efficient and effective
exploration for DRL, in comparison to recently proposed exploration methods
including Bootstrapped Deep Q-Network and UCB Q-Ensemble.
</summary>
    <author>
      <name>Gang Chen</name>
    </author>
    <author>
      <name>Yiming Peng</name>
    </author>
    <author>
      <name>Mengjie Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00403v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00403v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01272v1</id>
    <updated>2018-09-04T23:22:28Z</updated>
    <published>2018-09-04T23:22:28Z</published>
    <title>Unsupervised Statistical Machine Translation</title>
    <summary>  While modern machine translation has relied on large parallel corpora, a
recent line of work has managed to train Neural Machine Translation (NMT)
systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al.,
2018). Despite the potential of this approach for low-resource settings,
existing systems are far behind their supervised counterparts, limiting their
practical interest. In this paper, we propose an alternative approach based on
phrase-based Statistical Machine Translation (SMT) that significantly closes
the gap with supervised systems. Our method profits from the modular
architecture of SMT: we first induce a phrase table from monolingual corpora
through cross-lingual embedding mappings, combine it with an n-gram language
model, and fine-tune hyperparameters through an unsupervised MERT variant. In
addition, iterative backtranslation improves results further, yielding, for
instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and
English-French, respectively, an improvement of more than 7-10 BLEU points over
previous unsupervised systems, and closing the gap with supervised SMT (Moses
trained on Europarl) down to 2-5 BLEU points. Our implementation is available
at https://github.com/artetxem/monoses
</summary>
    <author>
      <name>Mikel Artetxe</name>
    </author>
    <author>
      <name>Gorka Labaka</name>
    </author>
    <author>
      <name>Eneko Agirre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09057v4</id>
    <updated>2018-09-04T23:18:35Z</updated>
    <published>2016-12-29T07:26:26Z</published>
    <title>Deep Learning and Hierarchal Generative Models</title>
    <summary>  It is argued that deep learning is efficient for data that is generated from
hierarchal generative models. Examples of such generative models include
wavelet scattering networks, functions of compositional structure, and deep
rendering models. Unfortunately so far, for all such models, it is either not
rigorously known that they can be learned efficiently, or it is not known that
"deep algorithms" are required in order to learn them.
  We propose a simple family of "generative hierarchal models" which can be
efficiently learned and where "deep" algorithm are necessary for learning. Our
definition of "deep" algorithms is based on the empirical observation that deep
nets necessarily use correlations between features. More formally, we show that
in a semi-supervised setting, given access to low-order moments of the labeled
data and all of the unlabeled data, it is information theoretically impossible
to perform classification while at the same time there is an efficient
algorithm, that given all labelled and unlabeled data, perfectly labels all
unlabelled data with high probability.
  For the proof, we use and strengthen the fact that Belief Propagation does
not admit a good approximation in terms of linear functions.
</summary>
    <author>
      <name>Elchanan Mossel</name>
    </author>
    <link href="http://arxiv.org/abs/1612.09057v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09057v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01266v1</id>
    <updated>2018-09-04T23:07:45Z</updated>
    <published>2018-09-04T23:07:45Z</published>
    <title>Coverage-Guided Fuzzing for Deep Neural Networks</title>
    <summary>  In company with the data explosion over the past decade, deep neural network
(DNN) based software has experienced unprecedented leap and is becoming the key
driving force of many novel industrial applications, including many
safety-critical scenarios such as autonomous driving. Despite great success
achieved in various human intelligence tasks, similar to traditional software,
DNNs could also exhibit incorrect behaviors caused by hidden defects causing
severe accidents and losses. In this paper, we propose an automated fuzz
testing framework for hunting potential defects of general-purpose DNNs. It
performs metamorphic mutation to generate new semantically preserved tests, and
leverages multiple plugable coverage criteria as feedback to guide the test
generation from different perspectives. To be scalable towards practical-sized
DNNs, our framework maintains tests in batch, and prioritizes the tests
selection based on active feedback. The effectiveness of our framework is
extensively investigated on 3 popular datasets (MNIST, CIFAR-10, ImageNet) and
7 DNNs with diverse complexities, under large set of 6 coverage criteria as
feedback. The large-scale experiments demonstrate that our fuzzing framework
can (1) significantly boost the coverage with guidance; (2) generate useful
tests to detect erroneous behaviors and facilitate the DNN model quality
evaluation; (3) accurately capture potential defects during DNN quantization
for platform migration.
</summary>
    <author>
      <name>Xiaofei Xie</name>
    </author>
    <author>
      <name>Lei Ma</name>
    </author>
    <author>
      <name>Felix Juefei-Xu</name>
    </author>
    <author>
      <name>Hongxu Chen</name>
    </author>
    <author>
      <name>Minhui Xue</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Jianjun Zhao</name>
    </author>
    <author>
      <name>Jianxiong Yin</name>
    </author>
    <author>
      <name>Simon See</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02436v7</id>
    <updated>2018-09-04T22:39:50Z</updated>
    <published>2017-05-06T03:13:21Z</published>
    <title>Nonlinear Information Bottleneck</title>
    <summary>  Information bottleneck [IB] is a technique for extracting information in some
`input' random variable that is relevant for predicting some different 'output'
random variable. IB works by encoding the input in a compressed 'bottleneck
variable' from which the output can then be accurately decoded. IB can be
difficult to compute in practice, and has been mainly developed for two limited
cases: (1) discrete random variables with small state spaces, and (2)
continuous random variables that are jointly Gaussian distributed (in which
case the encoding and decoding maps are linear). We propose a method to perform
IB in more general domains. Our approach can be applied to discrete or
continuous inputs and outputs, and allows for nonlinear encoding and decoding
maps. The method uses a novel upper bound on the IB objective, derived using a
non-parametric estimator of mutual information and a variational approximation.
We show how to implement the method using neural networks and gradient-based
optimization, and demonstrate its performance on the MNIST dataset.
</summary>
    <author>
      <name>Artemy Kolchinsky</name>
    </author>
    <author>
      <name>Brendan D. Tracey</name>
    </author>
    <author>
      <name>David H. Wolpert</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02436v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02436v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06834v2</id>
    <updated>2018-09-04T22:36:18Z</updated>
    <published>2018-05-17T16:04:39Z</published>
    <title>Subspace Estimation from Incomplete Observations: A High-Dimensional
  Analysis</title>
    <summary>  We present a high-dimensional analysis of three popular algorithms, namely,
Oja's method, GROUSE and PETRELS, for subspace estimation from streaming and
highly incomplete observations. We show that, with proper time scaling, the
time-varying principal angles between the true subspace and its estimates given
by the algorithms converge weakly to deterministic processes when the ambient
dimension $n$ tends to infinity. Moreover, the limiting processes can be
exactly characterized as the unique solutions of certain ordinary differential
equations (ODEs). A finite sample bound is also given, showing that the rate of
convergence towards such limits is $\mathcal{O}(1/\sqrt{n})$. In addition to
providing asymptotically exact predictions of the dynamic performance of the
algorithms, our high-dimensional analysis yields several insights, including an
asymptotic equivalence between Oja's method and GROUSE, and a precise scaling
relationship linking the amount of missing data to the signal-to-noise ratio.
By analyzing the solutions of the limiting ODEs, we also establish phase
transition phenomena associated with the steady-state performance of these
techniques.
</summary>
    <author>
      <name>Chuang Wang</name>
    </author>
    <author>
      <name>Yonina C. Eldar</name>
    </author>
    <author>
      <name>Yue M. Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06834v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06834v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05120v2</id>
    <updated>2018-09-04T21:21:55Z</updated>
    <published>2017-04-17T20:53:18Z</published>
    <title>Does robustness imply tractability? A lower bound for planted clique in
  the semi-random model</title>
    <summary>  We consider a robust analog of the planted clique problem. In this analog, a
set $S$ of vertices is chosen and all edges in $S$ are included; then, edges
between $S$ and the rest of the graph are included with probability
$\frac{1}{2}$, while edges not touching $S$ are allowed to vary arbitrarily.
For this semi-random model, we show that the information-theoretic threshold
for recovery is $\tilde{\Theta}(\sqrt{n})$, in sharp contrast to the classical
information-theoretic threshold of $\Theta(\log(n))$. This matches the
conjectured computational threshold for the classical planted clique problem,
and thus raises the intriguing possibility that, once we require robustness,
there is no computational-statistical gap for planted clique. Our lower bound
involves establishing a result regarding the KL divergence of a family of
perturbed Bernoulli distributions, which may be of independent interest.
</summary>
    <author>
      <name>Jacob Steinhardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved lower bound to give recovery probability tending to zero.
  Factored out and highlighted perturbed Bernoulli argument</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05120v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05120v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03487v2</id>
    <updated>2018-09-04T20:58:56Z</updated>
    <published>2018-02-10T00:49:17Z</published>
    <title>Spurious local minima in neural networks: a critical view</title>
    <summary>  We investigate the loss surface of nonlinear neural networks. We prove that
even for networks with one hidden layer and "slightest" nonlinearity, there can
be spurious local minima. Our results thus indicate that in general "no
spurious local minima" is a property limited to deep linear networks.
Specifically, for ReLU(-like) networks we prove that for almost all (in
contrast to previous results) practical datasets there exist infinitely many
local minima. We also present a counterexample for more general activation
functions (such as sigmoid, tanh, arctan, ReLU, etc.), for which there exists a
local minimum strictly inferior to the global minimum. Our results make the
least restrictive assumptions relative to the existing results on local
optimality in neural networks. We complete our discussion by presenting a
comprehensive characterization of global optimality for deep linear networks.
Our results unify and subsume other results on this topic.
</summary>
    <author>
      <name>Chulhee Yun</name>
    </author>
    <author>
      <name>Suvrit Sra</name>
    </author>
    <author>
      <name>Ali Jadbabaie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages. Updates from v1: change of the title and the organization
  of the paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07561v2</id>
    <updated>2018-09-04T20:10:24Z</updated>
    <published>2018-08-22T20:53:37Z</published>
    <title>Training Deeper Neural Machine Translation Models with Transparent
  Attention</title>
    <summary>  While current state-of-the-art NMT models, such as RNN seq2seq and
Transformers, possess a large number of parameters, they are still shallow in
comparison to convolutional models used for both text and vision applications.
In this work we attempt to train significantly (2-3x) deeper Transformer and
Bi-RNN encoders for machine translation. We propose a simple modification to
the attention mechanism that eases the optimization of deeper models, and
results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT'14
English-German and WMT'15 Czech-English tasks for both architectures.
</summary>
    <author>
      <name>Ankur Bapna</name>
    </author>
    <author>
      <name>Mia Xu Chen</name>
    </author>
    <author>
      <name>Orhan Firat</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01229v1</id>
    <updated>2018-09-04T20:09:01Z</updated>
    <published>2018-09-04T20:09:01Z</published>
    <title>t-Exponential Memory Networks for Question-Answering Machines</title>
    <summary>  Recent advances in deep learning have brought to the fore models that can
make multiple computational steps in the service of completing a task; these
are capable of describ- ing long-term dependencies in sequential data. Novel
recurrent attention models over possibly large external memory modules
constitute the core mechanisms that enable these capabilities. Our work
addresses learning subtler and more complex underlying temporal dynamics in
language modeling tasks that deal with sparse sequential data. To this end, we
improve upon these recent advances, by adopting concepts from the field of
Bayesian statistics, namely variational inference. Our proposed approach
consists in treating the network parameters as latent variables with a prior
distribution imposed over them. Our statistical assumptions go beyond the
standard practice of postulating Gaussian priors. Indeed, to allow for handling
outliers, which are prevalent in long observed sequences of multivariate data,
multivariate t-exponential distributions are imposed. On this basis, we proceed
to infer corresponding posteriors; these can be used for inference and
prediction at test time, in a way that accounts for the uncertainty in the
available sparse training data. Specifically, to allow for our approach to best
exploit the merits of the t-exponential family, our method considers a new
t-divergence measure, which generalizes the concept of the Kullback-Leibler
divergence. We perform an extensive experimental evaluation of our approach,
using challenging language modeling benchmarks, and illustrate its superiority
over existing state-of-the-art techniques.
</summary>
    <author>
      <name>Kyriakos Tolias</name>
    </author>
    <author>
      <name>Sotirios Chatzis</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01225v1</id>
    <updated>2018-09-04T19:58:06Z</updated>
    <published>2018-09-04T19:58:06Z</published>
    <title>Compositional Stochastic Average Gradient for Machine Learning and
  Related Applications</title>
    <summary>  Many machine learning, statistical inference, and portfolio optimization
problems require minimization of a composition of expected value functions
(CEVF). Of particular interest is the finite-sum versions of such compositional
optimization problems (FS-CEVF). Compositional stochastic variance reduced
gradient (C-SVRG) methods that combine stochastic compositional gradient
descent (SCGD) and stochastic variance reduced gradient descent (SVRG) methods
are the state-of-the-art methods for FS-CEVF problems. We introduce
compositional stochastic average gradient descent (C-SAG) a novel extension of
the stochastic average gradient method (SAG) to minimize composition of
finite-sum functions. C-SAG, like SAG, estimates gradient by incorporating
memory of previous gradient information. We present theoretical analyses of
C-SAG which show that C-SAG, like SAG, and C-SVRG, achieves a linear
convergence rate when the objective function is strongly convex; However, C-CAG
achieves lower oracle query complexity per iteration than C-SVRG. Finally, we
present results of experiments showing that C-SAG converges substantially
faster than full gradient (FG), as well as C-SVRG.
</summary>
    <author>
      <name>Tsung-Yu Hsieh</name>
    </author>
    <author>
      <name>Yasser EL-Manzalawy</name>
    </author>
    <author>
      <name>Yiwei Sun</name>
    </author>
    <author>
      <name>Vasant Honavar</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01185v1</id>
    <updated>2018-09-04T18:24:37Z</updated>
    <published>2018-09-04T18:24:37Z</published>
    <title>DeepPINK: reproducible feature selection in deep neural networks</title>
    <summary>  Deep learning has become increasingly popular in both supervised and
unsupervised machine learning thanks to its outstanding empirical performance.
However, because of their intrinsic complexity, most deep learning methods are
largely treated as black box tools with little interpretability. Even though
recent attempts have been made to facilitate the interpretability of deep
neural networks (DNNs), existing methods are susceptible to noise and lack of
robustness.
  Therefore, scientists are justifiably cautious about the reproducibility of
the discoveries, which is often related to the interpretability of the
underlying statistical models. In this paper, we describe a method to increase
the interpretability and reproducibility of DNNs by incorporating the idea of
feature selection with controlled error rate. By designing a new DNN
architecture and integrating it with the recently proposed knockoffs framework,
we perform feature selection with a controlled error rate, while maintaining
high power. This new method, DeepPINK (Deep feature selection using
Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data
sets to demonstrate its empirical utility.
</summary>
    <author>
      <name>Yang Young Lu</name>
    </author>
    <author>
      <name>Jinchi Lv</name>
    </author>
    <author>
      <name>Yingying Fan</name>
    </author>
    <author>
      <name>William Stafford Noble</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08329v2</id>
    <updated>2018-09-04T18:16:40Z</updated>
    <published>2018-05-22T00:16:39Z</published>
    <title>Guided Feature Transformation (GFT): A Neural Language Grounding Module
  for Embodied Agents</title>
    <summary>  Recently there has been a rising interest in training agents, embodied in
virtual environments, to perform language-directed tasks by deep reinforcement
learning. In this paper, we propose a simple but effective neural language
grounding module for embodied agents that can be trained end to end from
scratch taking raw pixels, unstructured linguistic commands, and sparse rewards
as the inputs. We model the language grounding process as a language-guided
transformation of visual features, where latent sentence embeddings are used as
the transformation matrices. In several language-directed navigation tasks that
feature challenging partial observability and require simple reasoning, our
module significantly outperforms the state of the art. We also release
XWorld3D, an easy-to-customize 3D environment that can potentially be modified
to evaluate a variety of embodied agents.
</summary>
    <author>
      <name>Haonan Yu</name>
    </author>
    <author>
      <name>Xiaochen Lian</name>
    </author>
    <author>
      <name>Haichao Zhang</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CoRL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01123v1</id>
    <updated>2018-09-04T17:59:53Z</updated>
    <published>2018-09-04T17:59:53Z</published>
    <title>VideoMatch: Matching based Video Object Segmentation</title>
    <summary>  Video object segmentation is challenging yet important in a wide variety of
applications for video analysis. Recent works formulate video object
segmentation as a prediction task using deep nets to achieve appealing
state-of-the-art performance. Due to the formulation as a prediction task, most
of these methods require fine-tuning during test time, such that the deep nets
memorize the appearance of the objects of interest in the given video. However,
fine-tuning is time-consuming and computationally expensive, hence the
algorithms are far from real time. To address this issue, we develop a novel
matching based algorithm for video object segmentation. In contrast to
memorization based classification techniques, the proposed approach learns to
match extracted features to a provided template without memorizing the
appearance of the objects. We validate the effectiveness and the robustness of
the proposed method on the challenging DAVIS-16, DAVIS-17, Youtube-Objects and
JumpCut datasets. Extensive results show that our method achieves comparable
performance without fine-tuning and is much more favorable in terms of
computational time.
</summary>
    <author>
      <name>Yuan-Ting Hu</name>
    </author>
    <author>
      <name>Jia-Bin Huang</name>
    </author>
    <author>
      <name>Alexander G. Schwing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06546v2</id>
    <updated>2018-09-04T17:57:45Z</updated>
    <published>2018-05-16T22:29:15Z</published>
    <title>Joint Classification and Prediction CNN Framework for Automatic Sleep
  Stage Classification</title>
    <summary>  Correctly identifying sleep stages is important in diagnosing and treating
sleep disorders. This work proposes a joint classification-and-prediction
framework based on CNNs for automatic sleep staging, and, subsequently,
introduces a simple yet efficient CNN architecture to power the framework.
Given a single input epoch, the novel framework jointly determines its label
(classification) and its neighboring epochs' labels (prediction) in the
contextual output. While the proposed framework is orthogonal to the widely
adopted classification schemes, which take one or multiple epochs as contextual
inputs and produce a single classification decision on the target epoch, we
demonstrate its advantages in several ways. First, it leverages the dependency
among consecutive sleep epochs while surpassing the problems experienced with
the common classification schemes. Second, even with a single model, the
framework has the capacity to produce multiple decisions, which are essential
in obtaining a good performance as in ensemble-of-models methods, with very
little induced computational overhead. Probabilistic aggregation techniques are
then proposed to leverage the availability of multiple decisions. We conducted
experiments on two public datasets: Sleep-EDF Expanded with 20 subjects, and
Montreal Archive of Sleep Studies dataset with 200 subjects. The proposed
framework yields an overall classification accuracy of 82.3% and 83.6%,
respectively. We also show that the proposed framework not only is superior to
the baselines based on the common classification schemes but also outperforms
existing deep-learning approaches. To our knowledge, this is the first work
going beyond the standard single-output classification to consider multitask
neural networks for automatic sleep staging. This framework provides avenues
for further studies of different neural-network architectures for automatic
sleep staging.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Fernando Andreotti</name>
    </author>
    <author>
      <name>Navin Cooray</name>
    </author>
    <author>
      <name>Oliver Y. Chén</name>
    </author>
    <author>
      <name>Maarten De Vos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication.
  Changes in this version include additional experiments on Sleep-EDF Expanded,
  comparison with ensemble methods, as well as several other minor changes.
  Source code is available online</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06546v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06546v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01096v1</id>
    <updated>2018-09-04T17:02:35Z</updated>
    <published>2018-09-04T17:02:35Z</published>
    <title>Accelerating Beam Sweeping in mmWave Standalone 5G New Radios using
  Recurrent Neural Networks</title>
    <summary>  Millimeter wave (mmWave) is a key technology to support high data rate
demands for 5G applications. Highly directional transmissions are crucial at
these frequencies to compensate for high isotropic pathloss. This reliance on
di- rectional beamforming, however, makes the cell discovery (cell search)
challenging since both base station (gNB) and user equipment (UE) jointly
perform a search over angular space to locate potential beams to initiate
communication. In the cell discovery phase, sequential beam sweeping is
performed through the angular coverage region in order to transmit
synchronization signals. The sweeping pattern can either be a linear rotation
or a hopping pattern that makes use of additional information. This paper
proposes beam sweeping pattern prediction, based on the dynamic distribution of
user traffic, using a form of recurrent neural networks (RNNs) called Gated
Recurrent Unit (GRU). The spatial distribution of users is inferred from data
in call detail records (CDRs) of the cellular network. Results show that the
users spatial distribution and their approximate location (direction) can be
accurately predicted based on CDRs data using GRU, which is then used to
calculate the sweeping pattern in the angular domain during cell search.
</summary>
    <author>
      <name>Asim Mazin</name>
    </author>
    <author>
      <name>Mohamed Elkourdi</name>
    </author>
    <author>
      <name>Richard D. Gitlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages and 4 Figures. It was presented at VTC 2018-Fall</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01093v1</id>
    <updated>2018-09-04T16:59:53Z</updated>
    <published>2018-09-04T16:59:53Z</published>
    <title>Adversarial Attacks on Node Embeddings</title>
    <summary>  The goal of network representation learning is to learn low-dimensional node
embeddings that capture the graph structure and are useful for solving
downstream tasks. However, despite the proliferation of such methods there is
currently no study of their robustness to adversarial attacks. We provide the
first adversarial vulnerability analysis on the widely used family of methods
based on random walks. We derive efficient adversarial perturbations that
poison the network structure and have a negative effect on both the quality of
the embeddings and the downstream tasks. We further show that our attacks are
transferable -- they generalize to many models -- and are successful even when
the attacker has restricted actions.
</summary>
    <author>
      <name>Aleksandar Bojcheski</name>
    </author>
    <author>
      <name>Stephan Günnemann</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01090v1</id>
    <updated>2018-09-04T16:53:04Z</updated>
    <published>2018-09-04T16:53:04Z</published>
    <title>A Quantum Spatial Graph Convolutional Neural Network using Quantum
  Passing Information</title>
    <summary>  In this paper, we develop a new Quantum Spatial Graph Convolutional Neural
Network (QSGCNN) model that can directly learn a classification function for
graphs of arbitrary sizes. Unlike state-of-the-art Graph Convolutional Neural
Network (GCN) models, the proposed QSGCNN model incorporates the process of
identifying transitive aligned vertices between graphs, and transforms
arbitrary sized graphs into fixed-sized aligned vertex grid structures. To
further learn representative graph characteristics, a new quantum spatial graph
convolution is proposed and employed to extract multi-scale vertex features, in
terms of quantum passing information between grid vertices of each graph. Since
the quantum spatial convolution preserves the property of the input grid
structures, the proposed QSGCNN model allows to directly employ the traditional
convolutional neural network to further learn from the global graph topology,
providing an end-to-end deep learning architecture that integrates the graph
representation and learning in the quantum spatial graph convolution layer and
the traditional convolutional layer for graph classifications. We demonstrate
the effectiveness of the proposed QSGCNN model in terms of the theoretical
connections to state-of-the-art methods. The proposed QSGCNN model addresses
the shortcomings of information loss and imprecise information representation
arising in existing GCN models associated with SortPooling or SumPooling
layers. Experimental results on benchmark graph classification datasets
demonstrate the effectiveness of the proposed QSGCNN model.
</summary>
    <author>
      <name>Lu Bai</name>
    </author>
    <author>
      <name>Yuhang Jiao</name>
    </author>
    <author>
      <name>Luca Rossi</name>
    </author>
    <author>
      <name>Lixin Cui</name>
    </author>
    <author>
      <name>Jian Cheng</name>
    </author>
    <author>
      <name>Edwin R. Hancock</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01079v1</id>
    <updated>2018-09-04T16:38:01Z</updated>
    <published>2018-09-04T16:38:01Z</published>
    <title>Chi-Square Test Neural Network: A New Binary Classifier based on
  Backpropagation Neural Network</title>
    <summary>  We introduce the chi-square test neural network: a single hidden layer
backpropagation neural network using chi-square test theorem to redefine the
cost function and the error function. The weights and thresholds are modified
using standard backpropagation algorithm. The proposed approach has the
advantage of making consistent data distribution over training and testing
sets. It can be used for binary classification. The experimental results on
real world data sets indicate that the proposed algorithm can significantly
improve the classification accuracy comparing to related approaches.
</summary>
    <author>
      <name>Yuan Wu</name>
    </author>
    <author>
      <name>Lingling Li</name>
    </author>
    <author>
      <name>Lian Li</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01074v1</id>
    <updated>2018-09-04T16:28:36Z</updated>
    <published>2018-09-04T16:28:36Z</published>
    <title>A Novel Neural Sequence Model with Multiple Attentions for Word Sense
  Disambiguation</title>
    <summary>  Word sense disambiguation (WSD) is a well researched problem in computational
linguistics. Different research works have approached this problem in different
ways. Some state of the art results that have been achieved for this problem
are by supervised models in terms of accuracy, but they often fall behind
flexible knowledge-based solutions which use engineered features as well as
human annotators to disambiguate every target word. This work focuses on
bridging this gap using neural sequence models incorporating the well-known
attention mechanism. The main gist of our work is to combine multiple
attentions on different linguistic features through weights and to provide a
unified framework for doing this. This weighted attention allows the model to
easily disambiguate the sense of an ambiguous word by attending over a suitable
portion of a sentence. Our extensive experiments show that multiple attention
enables a more versatile encoder-decoder model leading to state of the art
results.
</summary>
    <author>
      <name>Mahtab Ahmed</name>
    </author>
    <author>
      <name>Muhammad Rifayat Samee</name>
    </author>
    <author>
      <name>Robert E. Mercer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 Figures, Accepted as a conference paper in ICMLA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03230v2</id>
    <updated>2018-09-04T16:20:07Z</updated>
    <published>2018-08-09T16:46:51Z</published>
    <title>Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal
  densities?</title>
    <summary>  Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of
Markov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity
of HMC algorithms is their excellent performance as the dimension $d$ of the
target becomes large: under conditions that are satisfied for many common
statistical models, optimally-tuned HMC algorithms have a running time that
scales like $d^{0.25}$. In stark contrast, the running time of the usual
Random-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This
superior scaling of the HMC algorithm with dimension is attributed to the fact
that it, unlike RWM, incorporates the gradient information in the proposal
distribution. In this paper, we investigate a different scaling question: does
HMC beat RWM for highly $\textit{multimodal}$ targets? We find that the answer
is often $\textit{no}$. We compute the spectral gaps for both the algorithms
for a specific class of multimodal target densities, and show that they are
identical. The key reason is that, within one mode, the gradient is effectively
ignorant about other modes, thus negating the advantage the HMC algorithm
enjoys in unimodal targets. We also give heuristic arguments suggesting that
the above observation may hold quite generally. Our main tool for answering
this question is a novel simple formula for the conductance of HMC using
Liouville's theorem. This result allows us to compute the spectral gap of HMC
algorithms, for both the classical HMC with isotropic momentum and the recent
Riemannian HMC, for multimodal targets.
</summary>
    <author>
      <name>Oren Mangoubi</name>
    </author>
    <author>
      <name>Natesh S. Pillai</name>
    </author>
    <author>
      <name>Aaron Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03230v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03230v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01062v1</id>
    <updated>2018-09-04T16:09:15Z</updated>
    <published>2018-09-04T16:09:15Z</published>
    <title>JobComposer: Career Path Optimization via Multicriteria Utility Learning</title>
    <summary>  With online professional network platforms (OPNs, e.g., LinkedIn, Xing, etc.)
becoming popular on the web, people are now turning to these platforms to
create and share their professional profiles, to connect with others who share
similar professional aspirations and to explore new career opportunities. These
platforms however do not offer a long-term roadmap to guide career progression
and improve workforce employability. The career trajectories of OPN users can
serve as a reference but they are not always optimal. A career plan can also be
devised through consultation with career coaches, whose knowledge may however
be limited to a few industries. To address the above limitations, we present a
novel data-driven approach dubbed JobComposer to automate career path planning
and optimization. Its key premise is that the observed career trajectories in
OPNs may not necessarily be optimal, and can be improved by learning to
maximize the sum of payoffs attainable by following a career path. At its
heart, JobComposer features a decomposition-based multicriteria utility
learning procedure to achieve the best tradeoff among different payoff criteria
in career path planning. Extensive studies using a city state-based OPN dataset
demonstrate that JobComposer returns career paths better than other baseline
methods and the actual career paths.
</summary>
    <author>
      <name>Richard J. Oentaryo</name>
    </author>
    <author>
      <name>Xavier Jayaraj Siddarth Ashok</name>
    </author>
    <author>
      <name>Ee-Peng Lim</name>
    </author>
    <author>
      <name>Philips Kokoh Prasetyo</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECML-PKDD Data Science for Human Capital Management 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.01062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01022v1</id>
    <updated>2018-09-04T14:31:42Z</updated>
    <published>2018-09-04T14:31:42Z</published>
    <title>A Neural Network Aided Approach for LDPC Coded DCO-OFDM with Clipping
  Distortion</title>
    <summary>  In this paper, a neural network-aided bit-interleaved coded modulation
(NN-BICM) receiver is designed to mitigate the nonlinear clipping distortion in
the LDPC coded direct currentbiased optical orthogonal frequency division
multiplexing (DCOOFDM) systems. Taking the cross-entropy as loss function, a
feed forward network is trained by backpropagation algorithm to output the
condition probability through the softmax activation function, thereby
assisting in a modified log-likelihood ratio (LLR) improvement. To reduce the
complexity, this feed-forward network simplifies the input layer with a single
symbol and the corresponding Gaussian variance instead of focusing on the
inter-carrier interference between multiple subcarriers. On the basis of the
neural network-aided BICM with Gray labelling, we propose a novel stacked
network architecture of the bitinterleaved coded modulation with iterative
decoding (NN-BICMID). Its performance has been improved further by calculating
the condition probability with the aid of a priori probability that derived
from the extrinsic LLRs in the LDPC decoder at the last iteration, at the
expense of customizing neural network detectors at each iteration time
separately. Utilizing the optimal DC bias as the midpoint of the dynamic
region, the simulation results demonstrate that both the NN-BICM and NN-BICM-ID
schemes achieve noticeable performance gains than other counterparts, in which
the NN-BICM-ID clearly outperforms NN-BICM with various modulation and coding
schemes.
</summary>
    <author>
      <name>Yuan He</name>
    </author>
    <author>
      <name>Ming Jiang</name>
    </author>
    <author>
      <name>Chunming Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01018v1</id>
    <updated>2018-09-04T14:24:19Z</updated>
    <published>2018-09-04T14:24:19Z</published>
    <title>Parameter Transfer Extreme Learning Machine based on Projective Model</title>
    <summary>  Recent years, transfer learning has attracted much attention in the community
of machine learning. In this paper, we mainly focus on the tasks of parameter
transfer under the framework of extreme learning machine (ELM). Unlike the
existing parameter transfer approaches, which incorporate the source model
information into the target by regularizing the di erence between the source
and target domain parameters, an intuitively appealing projective-model is
proposed to bridge the source and target model parameters. Specifically, we
formulate the parameter transfer in the ELM networks by the means of parameter
projection, and train the model by optimizing the projection matrix and
classifier parameters jointly. Further more, the `L2,1-norm structured sparsity
penalty is imposed on the source domain parameters, which encourages the joint
feature selection and parameter transfer. To evaluate the e ectiveness of the
proposed method, comprehensive experiments on several commonly used domain
adaptation datasets are presented. The results show that the proposed method
significantly outperforms the non-transfer ELM networks and other classical
transfer learning methods.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Boyuan Jiang</name>
    </author>
    <author>
      <name>Xinyu Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted as an oral paper by IJCNN 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00114v2</id>
    <updated>2018-09-04T14:19:57Z</updated>
    <published>2018-02-28T22:26:43Z</published>
    <title>SQL-Rank: A Listwise Approach to Collaborative Ranking</title>
    <summary>  In this paper, we propose a listwise approach for constructing user-specific
rankings in recommendation systems in a collaborative fashion. We contrast the
listwise approach to previous pointwise and pairwise approaches, which are
based on treating either each rating or each pairwise comparison as an
independent instance respectively. By extending the work of (Cao et al. 2007),
we cast listwise collaborative ranking as maximum likelihood under a
permutation model which applies probability mass to permutations based on a low
rank latent score matrix. We present a novel algorithm called SQL-Rank, which
can accommodate ties and missing data and can run in linear time. We develop a
theoretical framework for analyzing listwise ranking methods based on a novel
representation theory for the permutation model. Applying this framework to
collaborative ranking, we derive asymptotic statistical rates as the number of
users and items grow together. We conclude by demonstrating that our SQL-Rank
method often outperforms current state-of-the-art algorithms for implicit
feedback such as Weighted-MF and BPR and achieve favorable results when
compared to explicit feedback algorithms such as matrix factorization and
collaborative ranking.
</summary>
    <author>
      <name>Liwei Wu</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <author>
      <name>James Sharpnack</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01017v1</id>
    <updated>2018-09-04T14:19:43Z</updated>
    <published>2018-09-04T14:19:43Z</published>
    <title>Aesthetic Discrimination of Graph Layouts</title>
    <summary>  This paper addresses the following basic question: given two layouts of the
same graph, which one is more aesthetically pleasing? We propose a neural
network-based discriminator model trained on a labeled dataset that decides
which of two layouts has a higher aesthetic quality. The feature vectors used
as inputs to the model are based on known graph drawing quality metrics,
classical statistics, information-theoretical quantities, and two-point
statistics inspired by methods of condensed matter physics. The large corpus of
layout pairs used for training and testing is constructed using force-directed
drawing algorithms and the layouts that naturally stem from the process of
graph generation. It is further extended using data augmentation techniques.
The mean prediction accuracy of our model is 95.70%, outperforming
discriminators based on stress and on the linear combination of popular quality
metrics by a statistically significant margin.
</summary>
    <author>
      <name>Moritz Klammler</name>
    </author>
    <author>
      <name>Tamara Mchedlidze</name>
    </author>
    <author>
      <name>Alexey Pak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the Proceedings of the 26th International Symposium on
  Graph Drawing and Network Visualization (GD 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00973v1</id>
    <updated>2018-09-04T13:56:23Z</updated>
    <published>2018-09-04T13:56:23Z</published>
    <title>Equivalence of approximation by convolutional neural networks and
  fully-connected networks</title>
    <summary>  Convolutional neural networks are the most widely used type of neural
networks in applications. In mathematical analysis, however, mostly
fully-connected networks are studied. In this paper, we establish a connection
between both network architectures. Using this connection, we show that all
upper and lower bounds concerning approximation rates of {fully-connected}
neural networks for functions $f \in \mathcal{C}$---for an arbitrary function
class $\mathcal{C}$---translate to essentially the same bounds on approximation
rates of \emph{convolutional} neural networks for functions $f \in
{\mathcal{C}^{equi}}$, with the class $\mathcal{C}^{equi}$ consisting of all
translation equivariant functions whose first coordinate belongs to
$\mathcal{C}$.
</summary>
    <author>
      <name>Philipp Petersen</name>
    </author>
    <author>
      <name>Felix Voigtlaender</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A25, 44A35, 41A46" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05800v2</id>
    <updated>2018-09-04T13:14:38Z</updated>
    <published>2018-07-16T11:41:32Z</published>
    <title>Deep Generative Model using Unregularized Score for Anomaly Detection
  with Heterogeneous Complexity</title>
    <summary>  Accurate and automated detection of anomalous samples in a natural image
dataset can be accomplished with a probabilistic model for end-to-end modeling
of images. Such images have heterogeneous complexity, however, and a
probabilistic model overlooks simply shaped objects with small anomalies. This
is because the probabilistic model assigns undesirably lower likelihoods to
complexly shaped objects that are nevertheless consistent with set standards.
To overcome this difficulty, we propose an unregularized score for deep
generative models (DGMs), which are generative models leveraging deep neural
networks. We found that the regularization terms of the DGMs considerably
influence the anomaly score depending on the complexity of the samples. By
removing these terms, we obtain an unregularized score, which we evaluated on a
toy dataset and real-world manufacturing datasets. Empirical results
demonstrate that the unregularized score is robust to the inherent complexity
of samples and can be used to better detect anomalies.
</summary>
    <author>
      <name>Takashi Matsubara</name>
    </author>
    <author>
      <name>Kenta Hama</name>
    </author>
    <author>
      <name>Ryosuke Tachibana</name>
    </author>
    <author>
      <name>Kuniaki Uehara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended version of a manuscript in Proc. of The 2018
  International Joint Conference on Neural Networks (IJCNN2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00932v1</id>
    <updated>2018-09-04T13:07:52Z</updated>
    <published>2018-09-04T13:07:52Z</published>
    <title>Faster Balanced Clusterings in High Dimension</title>
    <summary>  The problem of constrained clustering has attracted significant attention in
the past decades. In this paper, we study the balanced $k$-center, $k$-median,
and $k$-means clustering problems where the size of each cluster is constrained
by the given lower and upper bounds. The problems are motivated by the
applications in processing large-scale data in high dimension. Existing methods
often need to compute complicated matchings (or min cost flows) to satisfy the
balance constraint, and thus suffer from high complexities especially in high
dimension. We develop an effective framework for the three balanced clustering
problems to address this issue, and our idea is based on a novel spatial
partition in geometry. For the balanced $k$-center clustering, we provide a
$4$-approximation algorithm that improves the existing approximation factor
$7$; for the balanced $k$-median and $k$-means clusterings, our algorithms
yield constant and $(1+\epsilon)$-approximation factors with any $\epsilon&gt;0$.
More importantly, our algorithms achieve linear or nearly linear running times
when $k$ is a constant, and significantly improve the existing ones. Our
results can be easily extended to metric balanced clusterings and the running
times are sub-linear in terms of the complexity of $n$-point metric.
</summary>
    <author>
      <name>Hu Ding</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00918v1</id>
    <updated>2018-09-04T12:32:54Z</updated>
    <published>2018-09-04T12:32:54Z</published>
    <title>Segmentation-free compositional $n$-gram embedding</title>
    <summary>  Applying conventional word embedding models to unsegmented languages, where
word boundary is not clear, requires word segmentation as preprocessing.
However, word segmentation is difficult and expensive to conduct without
errors. Segmentation error degrades the quality of word embeddings, leading to
performance degradation in downstream applications. In this paper, we propose a
simple segmentation-free method to obtain unsupervised vector representations
for words, phrases and sentences from an unsegmented raw corpus. Our model is
based on subword information skip-gram model, but embedding targets and
contexts are character $n$-grams instead of segmented words. We consider all
possible character $n$-grams in a corpus as targets, and every target is
modeled as the sum of its compositional sub-$n$-grams. Our method completely
ignores word boundaries in a corpus and is not word-segmentation dependent.
This approach may sound reckless, but it was found to work well through
experiments on real-world datasets and benchmarks.
</summary>
    <author>
      <name>Geewook Kim</name>
    </author>
    <author>
      <name>Kazuki Fukui</name>
    </author>
    <author>
      <name>Hidetoshi Shimodaira</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00901v1</id>
    <updated>2018-09-04T11:43:20Z</updated>
    <published>2018-09-04T11:43:20Z</published>
    <title>Parity Crowdsourcing for Cooperative Labeling</title>
    <summary>  Consider a database of $k$ objects, e.g., a set of videos, where each object
has a binary attribute, e.g., a video's suitability for children. The
attributes of the objects are to be determined under a crowdsourcing model: a
worker is queried about the labels of a chosen subset of objects and either
responds with a correct binary answer or declines to respond. Here we propose a
parity response model: the worker is asked to check whether the number of
objects having a given attribute in the chosen subset is even or odd. For
example, if the subset includes two objects, the worker checks whether the two
belong to the same class or not. We propose a method for designing the sequence
of subsets of objects to be queried so that the attributes of the objects can
be identified with high probability using few ($n$) answers. The method is
based on an analogy to the design of Fountain codes for erasure channels. We
define the query difficulty $\bar{d}$ as the average size of the query subsets
and we define the sample complexity $n$ as the minimum number of collected
answers required to attain a given recovery accuracy. We obtain fundamental
tradeoffs between recovery accuracy, query difficulty, and sample complexity.
In particular, the necessary and sufficient sample complexity required for
recovering all $k$ attributes with high probability is $n = c_0 \max\{k, (k
\log k)/\bar{d}\}$ and the sample complexity for recovering a fixed proportion
$(1-\delta)k$ of the attributes for $\delta=o(1)$ is $n = c_1\max\{k, (k
\log(1/\delta))/\bar{d}\}$, where $c_0, c_1&gt;0$.
</summary>
    <author>
      <name>Hye Won Chung</name>
    </author>
    <author>
      <name>Ji Oon Lee</name>
    </author>
    <author>
      <name>Doyeon Kim</name>
    </author>
    <author>
      <name>Alfred O. Hero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09586v2</id>
    <updated>2018-09-04T11:30:12Z</updated>
    <published>2018-07-13T13:43:15Z</published>
    <title>Perturb and Combine to Identify Influential Spreaders in Real-World
  Networks</title>
    <summary>  Recent research has shown that graph degeneracy algorithms, which decompose a
network into a hierarchy of nested subgraphs of decreasing size and increasing
density, are very effective at detecting the good spreaders in a network.
However, it is also known that degeneracy-based decompositions of a graph are
unstable to small perturbations of the network structure. In Machine Learning,
the performance of unstable classification and regression methods, such as
fully-grown decision trees, can be greatly improved by using Perturb and
Combine (P&amp;C) strategies such as bagging (bootstrap aggregating). Therefore, we
propose a P&amp;C procedure for networks that (1) creates many perturbed versions
of a given graph, (2) applies a node scoring function separately to each graph
(such as a degeneracy-based one), and (3) combines the results. We conduct
real-world experiments on the tasks of identifying influential spreaders in
large social networks, and influential words (keywords) in small word
co-occurrence networks. We use the k-core, generalized k-core, and PageRank
algorithms as our vertex scoring functions. In each case, using the aggregated
scores brings significant improvements compared to using the scores computed on
the original graphs. Finally, a bias-variance analysis suggests that our P&amp;C
procedure works mainly by reducing bias, and that therefore, it should be
capable of improving the performance of all vertex scoring functions, not only
unstable ones.
</summary>
    <author>
      <name>Antoine J. -P. Tixier</name>
    </author>
    <author>
      <name>Maria-Evgenia G. Rossi</name>
    </author>
    <author>
      <name>Fragkiskos D. Malliaros</name>
    </author>
    <author>
      <name>Jesse Read</name>
    </author>
    <author>
      <name>Michalis Vazirgiannis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">More compact format</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09586v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09586v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01133v1</id>
    <updated>2018-09-04T10:26:37Z</updated>
    <published>2018-09-04T10:26:37Z</published>
    <title>Automated bird sound recognition in realistic settings</title>
    <summary>  We evaluated the effectiveness of an automated bird sound identification
system in a situation that emulates a realistic, typical application. We
trained classification algorithms on a crowd-sourced collection of bird audio
recording data and restricted our training methods to be completely free of
manual intervention. The approach is hence directly applicable to the analysis
of multiple species collections, with labelling provided by crowd-sourced
collection. We evaluated the performance of the bird sound recognition system
on a realistic number of candidate classes, corresponding to real conditions.
We investigated the use of two canonical classification methods, chosen due to
their widespread use and ease of interpretation, namely a k Nearest Neighbour
(kNN) classifier with histogram-based features and a Support Vector Machine
(SVM) with time-summarisation features. We further investigated the use of a
certainty measure, derived from the output probabilities of the classifiers, to
enhance the interpretability and reliability of the class decisions. Our
results demonstrate that both identification methods achieved similar
performance, but we argue that the use of the kNN classifier offers somewhat
more flexibility. Furthermore, we show that employing an outcome certainty
measure provides a valuable and consistent indicator of the reliability of
classification results. Our use of generic training data and our investigation
of probabilistic classification methodologies that can flexibly address the
variable number of candidate species/classes that are expected to be
encountered in the field, directly contribute to the development of a practical
bird sound identification system with potentially global application. Further,
we show that certainty measures associated with identification outcomes can
significantly contribute to the practical usability of the overall system.
</summary>
    <author>
      <name>Timos Papadopoulos</name>
    </author>
    <author>
      <name>Stephen J. Roberts</name>
    </author>
    <author>
      <name>Katherine J. Willis</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00862v1</id>
    <updated>2018-09-04T09:54:25Z</updated>
    <published>2018-09-04T09:54:25Z</published>
    <title>Handwriting styles: benchmarks and evaluation metrics</title>
    <summary>  Evaluating the style of handwriting generation is a challenging problem,
since it is not well defined. It is a key component in order to develop in
developing systems with more personalized experiences with humans. In this
paper, we propose baseline benchmarks, in order to set anchors to estimate the
relative quality of different handwriting style methods. This will be done
using deep learning techniques, which have shown remarkable results in
different machine learning tasks, learning classification, regression, and most
relevant to our work, generating temporal sequences. We discuss the challenges
associated with evaluating our methods, which is related to evaluation of
generative models in general. We then propose evaluation metrics, which we find
relevant to this problem, and we discuss how we evaluate the evaluation
metrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,
there is no work done before in generating handwriting (either in terms of
methodology or the performance metrics), our in exploring styles using this
dataset.
</summary>
    <author>
      <name>Omar Mohammed</name>
    </author>
    <author>
      <name>Gerard Bailly</name>
    </author>
    <author>
      <name>Damien Pellier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE International Workshop on Deep and Transfer
  Learning (DTL 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00852v1</id>
    <updated>2018-09-04T09:18:19Z</updated>
    <published>2018-09-04T09:18:19Z</published>
    <title>Multi-target Unsupervised Domain Adaptation without Exactly Shared
  Categories</title>
    <summary>  Unsupervised domain adaptation (UDA) aims to learn the unlabeled target
domain by transferring the knowledge from the labeled source domain. To date,
most of the existing works focus on the scenario of one source domain and one
target domain (1S1T), and just a few works concern UDA of multiple source
domains and one target domain (mS1T) for solving the insufficient knowledge
problem with single source domain. While, to the best of our knowledge, almost
no work concerns the scenario of one source domain and multiple target domains
(1SmT). In the 1SmT, these unlabeled target domains may not necessarily share
the same categories, therefore, in contrast to mS1T, 1SmT is more challenging.
In this paper, we study such a new UDA scenario, and accordingly propose a UDA
framework (PA-1SmT) through the model parameter adaptation among these target
domains and the source domain. A key ingredient of our framework is that we
firstly construct a model parameter dictionary which is shared not only between
the source domain and the individual target domains but also among the multiple
target domains. Then we use it to sparsely represent individual target
parameters, which attains knowledge transfer among the domains. Such a new
knowledge transfer is different from existing popular methods for UDA, such as
subspace alignment, distribution matching etc., and can also be directly used
for DA of privacy protection due to the fact that the knowledge is transferred
just via the model parameters rather than data itself. Finally, our
experimental results on three domain adaptation benchmark datasets demonstrate
the superiority of our framework.
</summary>
    <author>
      <name>Huanhuan Yu</name>
    </author>
    <author>
      <name>Menglei Hu</name>
    </author>
    <author>
      <name>Songcan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00846v1</id>
    <updated>2018-09-04T09:01:10Z</updated>
    <published>2018-09-04T09:01:10Z</published>
    <title>Understanding Regularization in Batch Normalization</title>
    <summary>  Batch Normalization (BN) makes output of hidden neuron had zero mean and unit
variance, improving convergence and generalization when training neural
networks. This work understands these phenomena theoretically. We analyze BN by
using a building block of neural networks, which consists of a weight layer, a
BN layer, and a nonlinear activation function. This simple network helps us
understand the characteristics of BN, where the results are generalized to deep
models in numerical studies. We explore BN in three aspects. First, by viewing
BN as a stochastic process, an analytical form of regularization inherited in
BN is derived. Second, the optimization dynamic with this regularization shows
that BN enables training converged with large maximum and effective learning
rates. Third, BN's generalization with regularization is explored by using
random matrix theory and statistical mechanics. Both simulations and
experiments support our analyses.
</summary>
    <author>
      <name>Ping Luo</name>
    </author>
    <author>
      <name>Xinjiang Wang</name>
    </author>
    <author>
      <name>Wenqi Shao</name>
    </author>
    <author>
      <name>Zhanglin Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Work in progress. 17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00804v2</id>
    <updated>2018-09-04T08:59:26Z</updated>
    <published>2018-06-03T14:53:20Z</published>
    <title>NAM: Non-Adversarial Unsupervised Domain Mapping</title>
    <summary>  Several methods were recently proposed for the task of translating images
between domains without prior knowledge in the form of correspondences. The
existing methods apply adversarial learning to ensure that the distribution of
the mapped source domain is indistinguishable from the target domain, which
suffers from known stability issues. In addition, most methods rely heavily on
`cycle' relationships between the domains, which enforce a one-to-one mapping.
In this work, we introduce an alternative method: Non-Adversarial Mapping
(NAM), which separates the task of target domain generative modeling from the
cross-domain mapping task. NAM relies on a pre-trained generative model of the
target domain, and aligns each source image with an image synthesized from the
target domain, while jointly optimizing the domain mapping function. It has
several key advantages: higher quality and resolution image translations,
simpler and more stable training and reusable target models. Extensive
experiments are presented validating the advantages of our method.
</summary>
    <author>
      <name>Yedid Hoshen</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00836v1</id>
    <updated>2018-09-04T08:41:53Z</updated>
    <published>2018-09-04T08:41:53Z</published>
    <title>A Recurrent Neural Network for Sentiment Quantification</title>
    <summary>  Quantification is a supervised learning task that consists in predicting,
given a set of classes C and a set D of unlabelled items, the prevalence (or
relative frequency) p(c|D) of each class c in C. Quantification can in
principle be solved by classifying all the unlabelled items and counting how
many of them have been attributed to each class. However, this "classify and
count" approach has been shown to yield suboptimal quantification accuracy;
this has established quantification as a task of its own, and given rise to a
number of methods specifically devised for it. We propose a recurrent neural
network architecture for quantification (that we call QuaNet) that observes the
classification predictions to learn higher-order "quantification embeddings",
which are then refined by incorporating quantification predictions of simple
classify-and-count-like methods. We test {QuaNet on sentiment quantification on
text, showing that it substantially outperforms several state-of-the-art
baselines.
</summary>
    <author>
      <name>Andrea Esuli</name>
    </author>
    <author>
      <name>Alejandro Moreo Fernández</name>
    </author>
    <author>
      <name>Fabrizio Sebastiani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3269287</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3269287" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at CIKM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01575v1</id>
    <updated>2018-09-04T08:36:09Z</updated>
    <published>2018-09-04T08:36:09Z</published>
    <title>Bounded Rational Decision-Making with Adaptive Neural Network Priors</title>
    <summary>  Bounded rationality investigates utility-optimizing decision-makers with
limited information-processing power. In particular, information theoretic
bounded rationality models formalize resource constraints abstractly in terms
of relative Shannon information, namely the Kullback-Leibler Divergence between
the agents' prior and posterior policy. Between prior and posterior lies an
anytime deliberation process that can be instantiated by sample-based
evaluations of the utility function through Markov Chain Monte Carlo (MCMC)
optimization. The most simple model assumes a fixed prior and can relate
abstract information-theoretic processing costs to the number of sample
evaluations. However, more advanced models would also address the question of
learning, that is how the prior is adapted over time such that generated prior
proposals become more efficient. In this work we investigate generative neural
networks as priors that are optimized concurrently with anytime sample-based
decision-making processes such as MCMC. We evaluate this approach on toy
examples.
</summary>
    <author>
      <name>Heinke Hihn</name>
    </author>
    <author>
      <name>Sebastian Gottwald</name>
    </author>
    <author>
      <name>Daniel A. Braun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-99978-4-17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-99978-4-17" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ANNPR 2018: Artificial Neural Networks in Pattern
  Recognition</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pancioni L., Schwenker F., Trentin E. (eds) Artificial Neural
  Networks in Pattern Recognition. ANNPR 2018. Lecture Notes in Computer
  Science, vol 11081. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.01575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00832v1</id>
    <updated>2018-09-04T08:31:21Z</updated>
    <published>2018-09-04T08:31:21Z</published>
    <title>Improving the Expressiveness of Deep Learning Frameworks with Recursion</title>
    <summary>  Recursive neural networks have widely been used by researchers to handle
applications with recursively or hierarchically structured data. However,
embedded control flow deep learning frameworks such as TensorFlow, Theano,
Caffe2, and MXNet fail to efficiently represent and execute such neural
networks, due to lack of support for recursion. In this paper, we add recursion
to the programming model of existing frameworks by complementing their design
with recursive execution of dataflow graphs as well as additional APIs for
recursive definitions. Unlike iterative implementations, which can only
understand the topological index of each node in recursive data structures, our
recursive implementation is able to exploit the recursive relationships between
nodes for efficient execution based on parallel computation. We present an
implementation on TensorFlow and evaluation results with various recursive
neural network models, showing that our recursive implementation not only
conveys the recursive nature of recursive neural networks better than other
implementations, but also uses given resources more effectively to reduce
training and inference time.
</summary>
    <author>
      <name>Eunji Jeong</name>
    </author>
    <author>
      <name>Joo Seong Jeong</name>
    </author>
    <author>
      <name>Soojeong Kim</name>
    </author>
    <author>
      <name>Gyeong-In Yu</name>
    </author>
    <author>
      <name>Byung-Gon Chun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3190508.3190530</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3190508.3190530" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in EuroSys 2018. 13 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EuroSys 2018: Thirteenth EuroSys Conference, April 23-26, 2018,
  Porto, Portugal</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.00832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00811v1</id>
    <updated>2018-09-04T07:03:58Z</updated>
    <published>2018-09-04T07:03:58Z</published>
    <title>A Deep Learning Spatiotemporal Prediction Framework for Mobile
  Crowdsourced Services</title>
    <summary>  This papers presents a deep learning-based framework to predict crowdsourced
service availability spatially and temporally. A novel two-stage prediction
model is introduced based on historical spatio-temporal traces of mobile
crowdsourced services. The prediction model first clusters mobile crowdsourced
services into regions. The availability prediction of a mobile crowdsourced
service at a certain location and time is then formulated as a classification
problem. To determine the availability duration of predicted mobile
crowdsourced services, we formulate a forecasting task of time series using the
Gramian Angular Field. We validated the effectiveness of the proposed framework
through multiple experiments.
</summary>
    <author>
      <name>Ahmed Ben Said</name>
    </author>
    <author>
      <name>Abdelkarim Erradi</name>
    </author>
    <author>
      <name>Azadeh Ghari Neiat</name>
    </author>
    <author>
      <name>Athman Bouguettaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11036-018-1105-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11036-018-1105-0" rel="related"/>
    <link href="http://arxiv.org/abs/1809.00811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00794v1</id>
    <updated>2018-09-04T04:40:34Z</updated>
    <published>2018-09-04T04:40:34Z</published>
    <title>Texar: A Modularized, Versatile, and Extensible Toolkit for Text
  Generation</title>
    <summary>  We introduce Texar, an open-source toolkit aiming to support the broad set of
text generation tasks that transforms any inputs into natural language, such as
machine translation, summarization, dialog, content manipulation, and so forth.
With the design goals of modularity, versatility, and extensibility in mind,
Texar extracts common patterns underlying the diverse tasks and methodologies,
creates a library of highly reusable modules and functionalities, and allows
arbitrary model architectures and algorithmic paradigms. In Texar, model
architecture, losses, and learning processes are fully decomposed. Modules at
high concept level can be freely assembled or plugged in/swapped out. These
features make Texar particularly suitable for researchers and practitioners to
do fast prototyping and experimentation, as well as foster technique sharing
across different text generation tasks. We provide case studies to demonstrate
the use and advantage of the toolkit. Texar is released under Apache license
2.0 at https://github.com/asyml/texar.
</summary>
    <author>
      <name>Zhiting Hu</name>
    </author>
    <author>
      <name>Haoran Shi</name>
    </author>
    <author>
      <name>Zichao Yang</name>
    </author>
    <author>
      <name>Bowen Tan</name>
    </author>
    <author>
      <name>Tiancheng Zhao</name>
    </author>
    <author>
      <name>Junxian He</name>
    </author>
    <author>
      <name>Wentao Wang</name>
    </author>
    <author>
      <name>Xingjiang Yu</name>
    </author>
    <author>
      <name>Lianhui Qin</name>
    </author>
    <author>
      <name>Di Wang</name>
    </author>
    <author>
      <name>Xuezhe Ma</name>
    </author>
    <author>
      <name>Hector Liu</name>
    </author>
    <author>
      <name>Xiaodan Liang</name>
    </author>
    <author>
      <name>Wanrong Zhu</name>
    </author>
    <author>
      <name>Devendra Singh Sachan</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; Github: https://github.com/asyml/texar</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00782v1</id>
    <updated>2018-09-04T03:15:56Z</updated>
    <published>2018-09-04T03:15:56Z</published>
    <title>Open Domain Question Answering Using Early Fusion of Knowledge Bases and
  Text</title>
    <summary>  Open Domain Question Answering (QA) is evolving from complex pipelined
systems to end-to-end deep neural networks. Specialized neural models have been
developed for extracting answers from either text alone or Knowledge Bases
(KBs) alone. In this paper we look at a more practical setting, namely QA over
the combination of a KB and entity-linked text, which is appropriate when an
incomplete KB is available with a large text corpus. Building on recent
advances in graph representation learning we propose a novel model, GRAFT-Net,
for extracting answers from a question-specific subgraph containing text and KB
entities and relations. We construct a suite of benchmark tasks for this
problem, varying the difficulty of questions, the amount of training data, and
KB completeness. We show that GRAFT-Net is competitive with the
state-of-the-art when tested using either KBs or text alone, and vastly
outperforms existing methods in the combined setting. Source code is available
at https://github.com/OceanskySun/GraftNet .
</summary>
    <author>
      <name>Haitian Sun</name>
    </author>
    <author>
      <name>Bhuwan Dhingra</name>
    </author>
    <author>
      <name>Manzil Zaheer</name>
    </author>
    <author>
      <name>Kathryn Mazaitis</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>William W. Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01129v1</id>
    <updated>2018-09-04T03:12:40Z</updated>
    <published>2018-09-04T03:12:40Z</published>
    <title>Lipschitz Networks and Distributional Robustness</title>
    <summary>  Robust risk minimisation has several advantages: it has been studied with
regards to improving the generalisation properties of models and robustness to
adversarial perturbation. We bound the distributionally robust risk for a model
class rich enough to include deep neural networks by a regularised empirical
risk involving the Lipschitz constant of the model. This allows us to
interpretand quantify the robustness properties of a deep neural network. As an
application we show the distributionally robust risk upperbounds the
adversarial training risk.
</summary>
    <author>
      <name>Zac Cranko</name>
    </author>
    <author>
      <name>Simon Kornblith</name>
    </author>
    <author>
      <name>Zhan Shi</name>
    </author>
    <author>
      <name>Richard Nock</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00770v1</id>
    <updated>2018-09-04T02:13:37Z</updated>
    <published>2018-09-04T02:13:37Z</published>
    <title>Transferring Deep Reinforcement Learning with Adversarial Objective and
  Augmentation</title>
    <summary>  In the past few years, deep reinforcement learning has been proven to solve
problems which have complex states like video games or board games. The next
step of intelligent agents would be able to generalize between tasks, and using
prior experience to pick up new skills more quickly. However, most
reinforcement learning algorithms for now are often suffering from catastrophic
forgetting even when facing a very similar target task. Our approach enables
the agents to generalize knowledge from a single source task, and boost the
learning progress with a semisupervised learning method when facing a new task.
We evaluate this approach on Atari games, which is a popular reinforcement
learning benchmark, and show that it outperforms common baselines based on
pre-training and fine-tuning.
</summary>
    <author>
      <name>Shu-Hsuan Hsu</name>
    </author>
    <author>
      <name>I-Chao Shen</name>
    </author>
    <author>
      <name>Bing-Yu Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00758v1</id>
    <updated>2018-09-04T00:52:25Z</updated>
    <published>2018-09-04T00:52:25Z</published>
    <title>End-to-end Multimodal Emotion and Gender Recognition with Dynamic
  Weights of Joint Loss</title>
    <summary>  Multi-task learning (MTL) is one of the method for improving generalizability
of multiple tasks. In order to perform multiple classification tasks with one
neural network model, the losses of each task should be combined. Previous
studies have mostly focused on prediction of multiple tasks using joint loss
with static weights for training model. Choosing weights between tasks have not
taken any considerations while it is set by uniformly or empirically. In this
study, we propose a method to make joint loss using dynamic weights to improve
total performance not an individual performance of tasks, and apply this method
to end-to-end multimodal emotion and gender recognition model using audio and
video data. This approach provides proper weights for each loss of the tasks
when training ends. In our experiment, a performance of emotion and gender
recognition with proposed method shows lower joint loss which is computed as
negative log-likelihood than the one with static weights of joint loss. Also,
our proposed model shows better generalizability than compared models. In our
best knowledge, this research shows the strength of dynamic weights of joint
loss for maximizing total performance at first in emotion and gender
recognition task.
</summary>
    <author>
      <name>Myungsu Chae</name>
    </author>
    <author>
      <name>Tae-Ho Kim</name>
    </author>
    <author>
      <name>Young Hoon Shin</name>
    </author>
    <author>
      <name>Jun-Woo Kim</name>
    </author>
    <author>
      <name>Soo-Young Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IROS 2018 Workshop on Crossmodal Learning for
  Intelligent Robotics</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10728v2</id>
    <updated>2018-09-03T21:20:50Z</updated>
    <published>2018-06-28T01:12:32Z</published>
    <title>Deep Echo State Networks with Uncertainty Quantification for
  Spatio-Temporal Forecasting</title>
    <summary>  Long-lead forecasting for spatio-temporal systems can often entail complex
nonlinear dynamics that are difficult to specify it a priori. Current
statistical methodologies for modeling these processes are often highly
parameterized and thus, challenging to implement from a computational
perspective. One potential parsimonious solution to this problem is a method
from the dynamical systems and engineering literature referred to as an echo
state network (ESN). ESN models use so-called {\it reservoir computing} to
efficiently compute recurrent neural network (RNN) forecasts. Moreover,
so-called "deep" models have recently been shown to be successful at predicting
high-dimensional complex nonlinear processes, particularly those with multiple
spatial and temporal scales of variability (such as we often find in
spatio-temporal environmental data). Here we introduce a deep ensemble ESN
(D-EESN) model. We present two versions of this model for spatio-temporal
processes that both produce forecasts and associated measures of uncertainty.
The first approach utilizes a bootstrap ensemble framework and the second is
developed within a hierarchical Bayesian framework (BD-EESN). This more general
hierarchical Bayesian framework naturally accommodates non-Gaussian data types
and multiple levels of uncertainties. The methodology is first applied to a
data set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical
system simulation model and then to a long-lead United States (U.S.) soil
moisture forecasting application.
</summary>
    <author>
      <name>Patrick L. McDermott</name>
    </author>
    <author>
      <name>Christopher K. Wikle</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10728v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10728v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00716v1</id>
    <updated>2018-09-03T20:42:27Z</updated>
    <published>2018-09-03T20:42:27Z</published>
    <title>InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes
  Dataset</title>
    <summary>  Datasets have gained an enormous amount of popularity in the computer vision
community, from training and evaluation of Deep Learning-based methods to
benchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt,
synthetic imagery bears a vast potential due to scalability in terms of amounts
of data obtainable without tedious manual ground truth annotations or
measurements. Here, we present a dataset with the aim of providing a higher
degree of photo-realism, larger scale, more variability as well as serving a
wider range of purposes compared to existing datasets. Our dataset leverages
the availability of millions of professional interior designs and millions of
production-level furniture and object assets -- all coming with fine geometric
details and high-resolution texture. We render high-resolution and high
frame-rate video sequences following realistic trajectories while supporting
various camera types as well as providing inertial measurements. Together with
the release of the dataset, we will make executable program of our interactive
simulator software as well as our renderer available at
https://interiornetdataset.github.io. To showcase the usability and uniqueness
of our dataset, we show benchmarking results of both sparse and dense SLAM
algorithms.
</summary>
    <author>
      <name>Wenbin Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, London UK, SW7 2AZ</arxiv:affiliation>
    </author>
    <author>
      <name>Sajad Saeedi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, London UK, SW7 2AZ</arxiv:affiliation>
    </author>
    <author>
      <name>John McCormac</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, London UK, SW7 2AZ</arxiv:affiliation>
    </author>
    <author>
      <name>Ronald Clark</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, London UK, SW7 2AZ</arxiv:affiliation>
    </author>
    <author>
      <name>Dimos Tzoumanikas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, London UK, SW7 2AZ</arxiv:affiliation>
    </author>
    <author>
      <name>Qing Ye</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KooLab, Kujiale.com, Hangzhou China</arxiv:affiliation>
    </author>
    <author>
      <name>Yuzhong Huang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KooLab, Kujiale.com, Hangzhou China</arxiv:affiliation>
    </author>
    <author>
      <name>Rui Tang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KooLab, Kujiale.com, Hangzhou China</arxiv:affiliation>
    </author>
    <author>
      <name>Stefan Leutenegger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, London UK, SW7 2AZ</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">British Machine Vision Conference (BMVC) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04405v5</id>
    <updated>2018-09-03T20:42:21Z</updated>
    <published>2018-01-13T09:41:57Z</published>
    <title>A Survey on Compiler Autotuning using Machine Learning</title>
    <summary>  Since the mid-1990s, researchers have been trying to use machine-learning
based approaches to solve a number of different compiler optimization problems.
These techniques primarily enhance the quality of the obtained results and,
more importantly, make it feasible to tackle two main compiler optimization
problems: optimization selection (choosing which optimizations to apply) and
phase-ordering (choosing the order of applying optimizations). The compiler
optimization space continues to grow due to the advancement of applications,
increasing number of compiler optimizations, and new target architectures.
Generic optimization passes in compilers cannot fully leverage newly introduced
optimizations and, therefore, cannot keep up with the pace of increasing
options. This survey summarizes and classifies the recent advances in using
machine learning for the compiler optimization field, particularly on the two
major problems of (1) selecting the best optimizations and (2) the
phase-ordering of optimizations. The survey highlights the approaches taken so
far, the obtained results, the fine-grain classification among different
approaches and finally, the influential papers of the field.
</summary>
    <author>
      <name>Amir H. Ashouri</name>
    </author>
    <author>
      <name>William Killian</name>
    </author>
    <author>
      <name>John Cavazos</name>
    </author>
    <author>
      <name>Gianluca Palermo</name>
    </author>
    <author>
      <name>Cristina Silvano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3197978</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3197978" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">version 5.0 (updated on September 2018)- Preprint Version For our
  Accepted Journal @ ACM CSUR 2018 (42 pages) - This survey will be updated
  quarterly here (Send me your new published papers to be added in the
  subsequent version) History: Received November 2016; Revised August 2017;
  Revised February 2018; Accepted March 2018-</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04405v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04405v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02808v2</id>
    <updated>2018-09-03T19:24:16Z</updated>
    <published>2018-04-09T04:00:30Z</published>
    <title>Latent Space Policies for Hierarchical Reinforcement Learning</title>
    <summary>  We address the problem of learning hierarchical deep neural network policies
for reinforcement learning. In contrast to methods that explicitly restrict or
cripple lower layers of a hierarchy to force them to use higher-level
modulating signals, each layer in our framework is trained to directly solve
the task, but acquires a range of diverse strategies via a maximum entropy
reinforcement learning objective. Each layer is also augmented with latent
random variables, which are sampled from a prior distribution during the
training of that layer. The maximum entropy objective causes these latent
variables to be incorporated into the layer's policy, and the higher level
layer can directly control the behavior of the lower layer through this latent
space. Furthermore, by constraining the mapping from latent variables to
actions to be invertible, higher layers retain full expressivity: neither the
higher layers nor the lower layers are constrained in their behavior. Our
experimental evaluation demonstrates that we can improve on the performance of
single-layer policies on standard benchmark tasks simply by adding additional
layers, and that our method can solve more complex sparse-reward tasks by
learning higher-level policies on top of high-entropy skills optimized for
simple low-level objectives.
</summary>
    <author>
      <name>Tuomas Haarnoja</name>
    </author>
    <author>
      <name>Kristian Hartikainen</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018; Videos: https://sites.google.com/view/latent-space-deep-rl
  Code: https://github.com/haarnoja/sac</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02808v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02808v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10101v3</id>
    <updated>2018-09-03T18:22:30Z</updated>
    <published>2018-08-30T03:35:50Z</published>
    <title>DP-ADMM: ADMM-based Distributed Learning with Differential Privacy</title>
    <summary>  Privacy-preserving distributed machine learning has become more important
than ever due to the high demand of large-scale data processing. This paper
focuses on a class of machine learning problems that can be formulated as
regularized empirical risk minimization, and develops a privacy-preserving
approach to such learning problems. We use Alternating Direction Method of
Multipliers (ADMM) to decentralize the learning algorithm, and apply Gaussian
mechanisms to provide local differential privacy guarantee. However, simply
combining ADMM and local randomization mechanisms would result in a
nonconvergent algorithm with bad performance even under moderate privacy
guarantees. Besides, this approach cannot be applied when the objective
functions of the learning problems are non-smooth. To address these concerns,
we propose an improved ADMM-based Differentially Private distributed learning
algorithm, DP-ADMM, where an approximate augmented Lagrangian function and
Gaussian mechanisms with time-varying variance are utilized. We also apply the
moment accountant method to bound the total privacy loss. Our theoretical
analysis shows that DP-ADMM can be applied to convex learning problems with
both smooth and non-smooth objectives, provides differential privacy guarantee,
and achieves a convergence rate of $O(1/\sqrt{t})$, where $t$ is the number of
iterations. Our evaluations demonstrate that our approach can achieve good
convergence and accuracy with strong privacy guarantee.
</summary>
    <author>
      <name>Zonghao Huang</name>
    </author>
    <author>
      <name>Rui Hu</name>
    </author>
    <author>
      <name>Eric Chan-Tin</name>
    </author>
    <author>
      <name>Yanmin Gong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 24 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10101v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10101v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04826v4</id>
    <updated>2018-09-03T17:02:21Z</updated>
    <published>2015-01-20T14:41:36Z</published>
    <title>Relative Entailment Among Probabilistic Implications</title>
    <summary>  We study a natural variant of the implicational fragment of propositional
logic. Its formulas are pairs of conjunctions of positive literals, related
together by an implicational-like connective; the semantics of this sort of
implication is defined in terms of a threshold on a conditional probability of
the consequent, given the antecedent: we are dealing with what the data
analysis community calls confidence of partial implications or association
rules. Existing studies of redundancy among these partial implications have
characterized so far only entailment from one premise and entailment from two
premises, both in the stand-alone case and in the case of presence of
additional classical implications (this is what we call "relative entailment").
By exploiting a previously noted alternative view of the entailment in terms of
linear programming duality, we characterize exactly the cases of entailment
from arbitrary numbers of premises, again both in the stand-alone case and in
the case of presence of additional classical implications. As a result, we
obtain decision algorithms of better complexity; additionally, for each
potential case of entailment, we identify a critical confidence threshold and
show that it is, actually, intrinsic to each set of premises and antecedent of
the conclusion.
</summary>
    <author>
      <name>Albert Atserias</name>
    </author>
    <author>
      <name>José L. Balcázar</name>
    </author>
    <author>
      <name>Marie Ely Piceno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revision of the preceding version</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.04826v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04826v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00653v1</id>
    <updated>2018-09-03T16:52:19Z</updated>
    <published>2018-09-03T16:52:19Z</published>
    <title>Towards Dynamic Computation Graphs via Sparse Latent Structure</title>
    <summary>  Deep NLP models benefit from underlying structures in the data---e.g., parse
trees---typically extracted using off-the-shelf parsers. Recent attempts to
jointly learn the latent structure encounter a tradeoff: either make
factorization assumptions that limit expressiveness, or sacrifice end-to-end
differentiability. Using the recently proposed SparseMAP inference, which
retrieves a sparse distribution over latent structures, we propose a novel
approach for end-to-end learning of latent structure predictors jointly with a
downstream predictor. To the best of our knowledge, our method is the first to
enable unrestricted dynamic computation graph construction from the global
latent structure, while maintaining differentiability.
</summary>
    <author>
      <name>Vlad Niculae</name>
    </author>
    <author>
      <name>André F. T. Martins</name>
    </author>
    <author>
      <name>Claire Cardie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018; 9 pages (incl. appendix)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04764v2</id>
    <updated>2018-09-03T14:45:58Z</updated>
    <published>2017-06-15T07:59:57Z</published>
    <title>Efficient Representative Subset Selection over Sliding Windows</title>
    <summary>  Representative subset selection (RSS) is an important tool for users to draw
insights from massive datasets. Existing literature models RSS as the
submodular maximization problem to capture the "diminishing returns" property
of the representativeness of selected subsets, but often only has a single
constraint (e.g., cardinality), which limits its applications in many
real-world problems. To capture the data recency issue and support different
types of constraints, we formulate dynamic RSS in data streams as maximizing
submodular functions subject to general $d$-knapsack constraints (SMDK) over
sliding windows. We propose a \textsc{KnapWindow} framework (KW) for SMDK. KW
utilizes the \textsc{KnapStream} algorithm (KS) for SMDK in append-only streams
as a subroutine. It maintains a sequence of checkpoints and KS instances over
the sliding window. Theoretically, KW is
$\frac{1-\varepsilon}{1+d}$-approximate for SMDK. Furthermore, we propose a
\textsc{KnapWindowPlus} framework (KW$^{+}$) to improve upon KW. KW$^{+}$
builds an index \textsc{SubKnapChk} to manage the checkpoints and KS instances.
\textsc{SubKnapChk} deletes a checkpoint whenever it can be approximated by its
successors. By keeping much fewer checkpoints, KW$^{+}$ achieves higher
efficiency than KW while still guaranteeing a
$\frac{1-\varepsilon'}{2+2d}$-approximate solution for SMDK. Finally, we
evaluate the efficiency and solution quality of KW and KW$^{+}$ in real-world
datasets. The experimental results demonstrate that KW achieves more than two
orders of magnitude speedups over the batch baseline and preserves high-quality
solutions for SMDK over sliding windows. KW$^{+}$ further runs 5-10 times
faster than KW while providing solutions with equivalent or even better
utilities.
</summary>
    <author>
      <name>Yanhao Wang</name>
    </author>
    <author>
      <name>Yuchen Li</name>
    </author>
    <author>
      <name>Kian-Lee Tan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TKDE.2018.2854182</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TKDE.2018.2854182" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 9 figures, to appear in IEEE Transactions on Knowledge and
  Data Engineering (TKDE). 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.04764v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04764v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00615v1</id>
    <updated>2018-09-03T14:25:46Z</updated>
    <published>2018-09-03T14:25:46Z</published>
    <title>Have You Stolen My Model? Evasion Attacks Against Deep Neural Network
  Watermarking Techniques</title>
    <summary>  Deep neural networks have had enormous impact on various domains of computer
science, considerably outperforming previous state of the art machine learning
techniques. To achieve this performance, neural networks need large quantities
of data and huge computational resources, which heavily increases their
construction costs. The increased cost of building a good deep neural network
model gives rise to a need for protecting this investment from potential
copyright infringements. Legitimate owners of a machine learning model want to
be able to reliably track and detect a malicious adversary that tries to steal
the intellectual property related to the model. Recently, this problem was
tackled by introducing in deep neural networks the concept of watermarking,
which allows a legitimate owner to embed some secret information(watermark) in
a given model. The watermark allows the legitimate owner to detect copyright
infringements of his model. This paper focuses on verifying the robustness and
reliability of state-of- the-art deep neural network watermarking schemes. We
show that, a malicious adversary, even in scenarios where the watermark is
difficult to remove, can still evade the verification by the legitimate owners,
thus avoiding the detection of model theft.
</summary>
    <author>
      <name>Dorjan Hitaj</name>
    </author>
    <author>
      <name>Luigi V. Mancini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01479v1</id>
    <updated>2018-09-03T14:06:11Z</updated>
    <published>2018-09-03T14:06:11Z</published>
    <title>UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification</title>
    <summary>  The Fact Extraction and VERification (FEVER) shared task was launched to
support the development of systems able to verify claims by extracting
supporting or refuting facts from raw text. The shared task organizers provide
a large-scale dataset for the consecutive steps involved in claim verification,
in particular, document retrieval, fact extraction, and claim classification.
In this paper, we present our claim verification pipeline approach, which,
according to the preliminary results, scored third in the shared task, out of
23 competing systems. For the document retrieval, we implemented a new entity
linking approach. In order to be able to rank candidate facts and classify a
claim on the basis of several selected facts, we introduce two extensions to
the Enhanced LSTM (ESIM).
</summary>
    <author>
      <name>Andreas Hanselowski</name>
    </author>
    <author>
      <name>Hao Zhang</name>
    </author>
    <author>
      <name>Zile Li</name>
    </author>
    <author>
      <name>Daniil Sorokin</name>
    </author>
    <author>
      <name>Benjamin Schiller</name>
    </author>
    <author>
      <name>Claudia Schulz</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03851v2</id>
    <updated>2018-09-03T13:50:26Z</updated>
    <published>2018-01-11T16:22:07Z</published>
    <title>Autoencoders and Probabilistic Inference with Missing Data: An Exact
  Solution for The Factor Analysis Case</title>
    <summary>  Latent variable models can be used to probabilistically "fill-in" missing
data entries. The variational autoencoder architecture (Kingma and Welling,
2014; Rezende et al., 2014) includes a "recognition" or "encoder" network that
infers the latent variables given the data variables. However, it is not clear
how to handle missing data variables in this network. The factor analysis (FA)
model is a basic autoencoder, using linear encoder and decoder networks. We
show how to calculate exactly the latent posterior distribution for the factor
analysis (FA) model in the presence of missing data, and note that this
solution exhibits a non-trivial dependence on the pattern of missingness. We
also discuss various approximations to the exact solution. Experiments compare
the effectiveness of various approaches to filling in the missing data.
</summary>
    <author>
      <name>Christopher K. I. Williams</name>
    </author>
    <author>
      <name>Charlie Nash</name>
    </author>
    <author>
      <name>Alfredo Nazábal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03851v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03851v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00593v1</id>
    <updated>2018-09-03T13:21:28Z</updated>
    <published>2018-09-03T13:21:28Z</published>
    <title>IoU is not submodular</title>
    <summary>  This short article aims at demonstrate that the Intersection over Union (or
Jaccard index) is not a submodular function. This mistake has been made in an
article which is cited and used as a foundation in another article. The
Intersection of Union is widely used in machine learning as a cost function
especially for imbalance data and semantic segmentation.
</summary>
    <author>
      <name>Tanguy Kerdoncuff</name>
    </author>
    <author>
      <name>Rémi Emonet</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00534v3</id>
    <updated>2018-09-03T11:46:46Z</updated>
    <published>2018-06-01T20:29:47Z</published>
    <title>Run Procrustes, Run! On the convergence of accelerated Procrustes Flow</title>
    <summary>  In this work, we present theoretical results on the convergence of non-convex
accelerated gradient descent in matrix factorization models. The technique is
applied to matrix sensing problems with squared loss, for the estimation of a
rank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show that
the acceleration leads to linear convergence rate, even under non-convex
settings where the variable $X$ is represented as $U U^\top$ for $U \in
\mathbb{R}^{n \times r}$. Our result has the same dependence on the condition
number of the objective --and the optimal solution-- as that of the recent
results on non-accelerated algorithms. However, acceleration is observed in
practice, both in synthetic examples and in two real applications: neuronal
multi-unit activities recovery from single electrode recordings, and quantum
state tomography on quantum computing simulators.
</summary>
    <author>
      <name>Anastasios Kyrillidis</name>
    </author>
    <author>
      <name>Shashanka Ubaru</name>
    </author>
    <author>
      <name>Georgios Kollias</name>
    </author>
    <author>
      <name>Kristofer Bouchard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, footnote 7 removed from previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00534v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00534v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01577v1</id>
    <updated>2018-09-03T11:39:11Z</updated>
    <published>2018-09-03T11:39:11Z</published>
    <title>From Bayesian Inference to Logical Bayesian Inference: A New
  Mathematical Frame for Semantic Communication and Machine Learning</title>
    <summary>  Bayesian Inference (BI) uses the Bayes' posterior whereas Logical Bayesian
Inference (LBI) uses the truth function or membership function as the inference
tool. LBI was proposed because BI was not compatible with the classical Bayes'
prediction and didn't use logical probability and hence couldn't express
semantic meaning. In LBI, statistical probability and logical probability are
strictly distinguished, used at the same time, and linked by the third kind of
Bayes' Theorem. The Shannon channel consists of a set of transition probability
functions whereas the semantic channel consists of a set of truth functions.
When a sample is large enough, we can directly derive the semantic channel from
Shannon's channel. Otherwise, we can use parameters to construct truth
functions and use the Maximum Semantic Information (MSI) criterion to optimize
the truth functions. The MSI criterion is equivalent to the Maximum Likelihood
(ML) criterion, and compatible with the Regularized Least Square (RLS)
criterion. By matching the two channels one with another, we can obtain the
Channels' Matching (CM) algorithm. This algorithm can improve multi-label
classifications, maximum likelihood estimations (including unseen instance
classifications), and mixture models. In comparison with BI, LBI 1) uses the
prior P(X) of X instead of that of Y or {\theta} and fits cases where the
source P(X) changes, 2) can be used to solve the denotations of labels, and 3)
is more compatible with the classical Bayes' prediction and likelihood method.
LBI also provides a confirmation measure between -1 and 1 for induction.
</summary>
    <author>
      <name>Chenguang Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 1 figure, 31 equations</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03B42, 03B48, , 03B52, 03B65, 62F15, 68P30, 94F15, 68T27, 68T37,&#10;  68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.1; F.4.1; I.2.3; I.2.6; I.5.2; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00543v1</id>
    <updated>2018-09-03T10:44:28Z</updated>
    <published>2018-09-03T10:44:28Z</published>
    <title>Learning Vision-based Cohesive Flight in Drone Swarms</title>
    <summary>  This paper presents a data-driven approach to learning vision-based
collective behavior from a simple flocking algorithm. We simulate a swarm of
quadrotor drones and formulate the controller as a regression problem in which
we generate 3D velocity commands directly from raw camera images. The dataset
is created by simultaneously acquiring omnidirectional images and computing the
corresponding control command from the flocking algorithm. We show that a
convolutional neural network trained on the visual inputs of the drone can
learn not only robust collision avoidance but also coherence of the flock in a
sample-efficient manner. The neural controller effectively learns to localize
other agents in the visual input, which we show by visualizing the regions with
the most influence on the motion of an agent. This weakly supervised saliency
map can be computed efficiently and may be used as a prior for subsequent
detection and relative localization of other agents. We remove the dependence
on sharing positions among flock members by taking only local visual
information into account for control. Our work can therefore be seen as the
first step towards a fully decentralized, vision-based flock without the need
for communication or visual markers to aid detection of other agents.
</summary>
    <author>
      <name>Fabian Schilling</name>
    </author>
    <author>
      <name>Julien Lecoeur</name>
    </author>
    <author>
      <name>Fabrizio Schiano</name>
    </author>
    <author>
      <name>Dario Floreano</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00542v1</id>
    <updated>2018-09-03T10:43:02Z</updated>
    <published>2018-09-03T10:43:02Z</published>
    <title>Machine learning for predicting thermal power consumption of the Mars
  Express Spacecraft</title>
    <summary>  The thermal subsystem of the Mars Express (MEX) spacecraft keeps the on-board
equipment within its pre-defined operating temperatures range. To plan and
optimize the scientific operations of MEX, its operators need to estimate in
advance, as accurately as possible, the power consumption of the thermal
subsystem. The remaining power can then be allocated for scientific purposes.
We present a machine learning pipeline for efficiently constructing accurate
predictive models for predicting the power of the thermal subsystem on board
MEX. In particular, we employ state-of-the-art feature engineering approaches
for transforming raw telemetry data, in turn used for constructing accurate
models with different state-of-the-art machine learning methods. We show that
the proposed pipeline considerably improve our previous (competition-winning)
work in terms of time efficiency and predictive performance. Moreover, while
achieving superior predictive performance, the constructed models also provide
important insight into the spacecraft's behavior, allowing for further analyses
and optimal planning of MEX's operation.
</summary>
    <author>
      <name>Matej Petković</name>
    </author>
    <author>
      <name>Redouane Boumghar</name>
    </author>
    <author>
      <name>Martin Breskvar</name>
    </author>
    <author>
      <name>Sašo Džeroski</name>
    </author>
    <author>
      <name>Dragi Kocev</name>
    </author>
    <author>
      <name>Jurica Levatić</name>
    </author>
    <author>
      <name>Luke Lucas</name>
    </author>
    <author>
      <name>Aljaž Osojnik</name>
    </author>
    <author>
      <name>Bernard Ženko</name>
    </author>
    <author>
      <name>Nikola Simidjievski</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00510v1</id>
    <updated>2018-09-03T09:07:30Z</updated>
    <published>2018-09-03T09:07:30Z</published>
    <title>Flatland: a Lightweight First-Person 2-D Environment for Reinforcement
  Learning</title>
    <summary>  We propose Flatland, a simple, lightweight environment for fast prototyping
and testing of reinforcement learning agents. It is of lower complexity
compared to similar 3D platforms, e.g. DeepMind Lab or VizDoom. At the same
time it shares some properties with the real world, such as continuity,
multi-modal partially-observable states with first-person view and coherent
physics. We propose to use it as an intermediary benchmark for problems related
to Lifelong Learning. Flatland is highly customizable and offers a wide range
of task difficulty to extensively evaluate the properties of artificial agents.
We experiment with three reinforcement learning baseline agents and show that
they can rapidly solve a navigation task in Flatland. A video of an agent
acting in Flatland is available here: https://youtu.be/I5y6Y2ZypdA.
</summary>
    <author>
      <name>Hugo Caselles-Dupré</name>
    </author>
    <author>
      <name>Louis Annabi</name>
    </author>
    <author>
      <name>Oksana Hagen</name>
    </author>
    <author>
      <name>Michael Garcia-Ortiz</name>
    </author>
    <author>
      <name>David Filliat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Workshop on Continual Unsupervised Sensorimotor
  Learning (ICDL-EpiRob 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00494v1</id>
    <updated>2018-09-03T08:37:33Z</updated>
    <published>2018-09-03T08:37:33Z</published>
    <title>Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News
  on the Web</title>
    <summary>  With the growth of the internet, the number of fake-news online has been
proliferating every year. The consequences of such phenomena are manifold,
ranging from lousy decision-making process to bullying and violence episodes.
Therefore, fact-checking algorithms became a valuable asset. To this aim, an
important step to detect fake-news is to have access to a credibility score for
a given information source. However, most of the widely used Web indicators
have either been shut-down to the public (e.g., Google PageRank) or are not
free for use (Alexa Rank). Further existing databases are short-manually
curated lists of online sources, which do not scale. Finally, most of the
research on the topic is theoretical-based or explore confidential data in a
restricted simulation environment. In this paper we explore current research,
highlight the challenges and propose solutions to tackle the problem of
classifying websites into a credibility scale. The proposed model automatically
extracts source reputation cues and computes a credibility factor, providing
valuable insights which can help in belittling dubious and confirming trustful
unknown websites. Experimental results outperform state of the art in the
2-classes and 5-classes setting.
</summary>
    <author>
      <name>Diego Esteves</name>
    </author>
    <author>
      <name>Aniketh Janardhan Reddy</name>
    </author>
    <author>
      <name>Piyush Chawla</name>
    </author>
    <author>
      <name>Jens Lehmann</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018: Conference on Empirical Methods in Natural Language
  Processing (The First Workshop on Fact Extraction and Verification)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.00494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07287v3</id>
    <updated>2018-09-03T07:07:53Z</updated>
    <published>2017-07-23T12:07:58Z</published>
    <title>Pairing an arbitrary regressor with an artificial neural network
  estimating aleatoric uncertainty</title>
    <summary>  We suggest a general approach to quantification of different forms of
aleatoric uncertainty in regression tasks performed by artificial neural
networks. It is based on the simultaneous training of two neural networks with
a joint loss function and a specific hyperparameter $\lambda&gt;0$ that allows for
automatically detecting noisy and clean regions in the input space and
controlling their {\em relative contribution} to the loss and its gradients.
After the model has been trained, one of the networks performs predictions and
the other quantifies the uncertainty of these predictions by estimating the
locally averaged loss of the first one. Unlike in many classical uncertainty
quantification methods, we do not assume any a priori knowledge of the ground
truth probability distribution, neither do we, in general, maximize the
likelihood of a chosen parametric family of distributions. We analyze the
learning process and the influence of clean and noisy regions of the input
space on the loss surface, depending on $\lambda$. In particular, we show that
small values of $\lambda$ increase the relative contribution of clean regions
to the loss and its gradients. This explains why choosing small $\lambda$
allows for better predictions compared with neural networks without uncertainty
counterparts and those based on classical likelihood maximization. Finally, we
demonstrate that one can naturally form ensembles of pairs of our networks and
thus capture both aleatoric and epistemic uncertainty and avoid overfitting.
</summary>
    <author>
      <name>Pavel Gurevich</name>
    </author>
    <author>
      <name>Hannes Stuke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07287v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07287v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07215v2</id>
    <updated>2018-09-03T05:46:13Z</updated>
    <published>2018-07-19T02:01:36Z</published>
    <title>A Machine Learning Approach for Detecting Students at Risk of Low
  Academic Achievement</title>
    <summary>  We aim to predict whether a primary school student will perform in the `below
standard' band of a national standardized test. We exploit a data set
containing test performance on the National Assessment Program - Literacy and
Numeracy (NAPLAN); a test given annually to all Australian school students in
grades 3, 5, 7, and 9. We separate the analysis into students in grade 5 and
above, for which previous achievement may be used as a predictor; and students
in grade 3, which must rely on family- and school-level predictors only. We
train and compare a set of classifiers for reading and numeracy learning areas
respectively. The classifiers achieve good predictive power in terms of area
under the ROC curve, suggesting that it is feasible for schools to more
accurately screen a large number of students for academic risk.
</summary>
    <author>
      <name>Sarah Cornell-Farrow</name>
    </author>
    <author>
      <name>Robert Garrard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 tables, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01509v2</id>
    <updated>2018-09-03T04:17:46Z</updated>
    <published>2018-05-03T18:47:43Z</published>
    <title>SURREAL: SUbgraph Robust REpresentAtion Learning</title>
    <summary>  The success of graph embeddings or node representation learning in a variety
of downstream tasks, such as node classification, link prediction, and
recommendation systems, has led to their popularity in recent years.
Representation learning algorithms aim to preserve local and global network
structure by identifying node neighborhood notions. However, many existing
algorithms generate embeddings that fail to properly preserve the network
structure, or lead to unstable representations due to random processes (e.g.,
random walks to generate context) and, thus, cannot generate to multi-graph
problems. In this paper, we propose RECS, a novel, stable graph embedding
algorithmic framework. RECS learns graph representations using connection
subgraphs by employing the analogy of graphs with electrical circuits. It
preserves both local and global connectivity patterns, and addresses the issue
of high-degree nodes. Further, it exploits the strength of weak ties and
meta-data that have been neglected by baselines. The experiments show that RECS
outperforms state-of-the-art algorithms by up to 36.85% on multi-label
classification problem. Further, in contrast to baselines, RECS, being
deterministic, is completely stable.
</summary>
    <author>
      <name>Saba A. Al-Sayouri</name>
    </author>
    <author>
      <name>Danai Koutra</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <author>
      <name>Sarah S. Lam</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05810v2</id>
    <updated>2018-09-03T02:22:39Z</updated>
    <published>2018-04-16T17:29:43Z</published>
    <title>ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object
  Detector</title>
    <summary>  Given the ability to directly manipulate image pixels in the digital input
space, an adversary can easily generate imperceptible perturbations to fool a
Deep Neural Network (DNN) image classifier, as demonstrated in prior work. In
this work, we propose ShapeShifter, an attack that tackles the more challenging
problem of crafting physical adversarial perturbations to fool image-based
object detectors like Faster R-CNN. Attacking an object detector is more
difficult than attacking an image classifier, as it needs to mislead the
classification results in multiple bounding boxes with different scales.
Extending the digital attack to the physical world adds another layer of
difficulty, because it requires the perturbation to be robust enough to survive
real-world distortions due to different viewing distances and angles, lighting
conditions, and camera limitations. We show that the Expectation over
Transformation technique, which was originally proposed to enhance the
robustness of adversarial perturbations in image classification, can be
successfully adapted to the object detection setting. ShapeShifter can generate
adversarially perturbed stop signs that are consistently mis-detected by Faster
R-CNN as other objects, posing a potential threat to autonomous vehicles and
other safety-critical computer vision systems.
</summary>
    <author>
      <name>Shang-Tse Chen</name>
    </author>
    <author>
      <name>Cory Cornelius</name>
    </author>
    <author>
      <name>Jason Martin</name>
    </author>
    <author>
      <name>Duen Horng Chau</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05810v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05810v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06093v2</id>
    <updated>2018-09-03T01:35:59Z</updated>
    <published>2018-07-16T20:23:43Z</published>
    <title>Prognostics Estimations with Dynamic States</title>
    <summary>  The health state assessment and remaining useful life (RUL) estimation play
very important roles in prognostics and health management (PHM), owing to their
abilities to reduce the maintenance and improve the safety of machines or
equipment. However, they generally suffer from this problem of lacking prior
knowledge to pre-define the exact failure thresholds for a machinery operating
in a dynamic environment with a high level of uncertainty. In this case,
dynamic thresholds depicted by the discrete states is a very attractive way to
estimate the RUL of a dynamic machinery. Currently, there are only very few
works considering the dynamic thresholds, and these studies adopted different
algorithms to determine the discrete states and predict the continuous states
separately, which largely increases the complexity of the learning process. In
this paper, we propose a novel prognostics approach for RUL estimation of
aero-engines with self-joint prediction of continuous and discrete states,
wherein the prediction of continuous and discrete states are conducted
simultaneously and dynamically within one learning framework.
</summary>
    <author>
      <name>Rong-Jing Bao</name>
    </author>
    <author>
      <name>Hai-Jun Rong</name>
    </author>
    <author>
      <name>Zhi-Xin Yang</name>
    </author>
    <author>
      <name>Badong Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06093v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06093v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09708v5</id>
    <updated>2018-09-03T01:03:21Z</updated>
    <published>2017-12-27T23:14:46Z</published>
    <title>Learning More Universal Representations for Transfer-Learning</title>
    <summary>  A representation is supposed universal if it encodes any element of the
visual world (e.g., objects, scenes) in any configuration (e.g., scale,
context). While not expecting pure universal representations, the goal in the
literature is to improve the universality level, starting from a representation
with a certain level. To do so, the state-of-the-art consists in learning
CNN-based representations on a diversified training problem (e.g., ImageNet
modified by adding annotated data). While it effectively increases
universality, such approach still requires a large amount of efforts to satisfy
the needs in annotated data. In this work, we propose two methods to improve
universality, but pay special attention to limit the need of annotated data. We
also propose a unified framework of the methods based on the diversifying of
the training problem. Finally, to better match Atkinson's cognitive study about
universal human representations, we proposed to rely on the transfer-learning
scheme as well as a new metric to evaluate universality. This latter, aims us
to demonstrates the interest of our methods on 10 target-problems, relating to
the classification task and a variety of visual domains.
</summary>
    <author>
      <name>Youssef Tamaazousti</name>
    </author>
    <author>
      <name>Hervé Le Borgne</name>
    </author>
    <author>
      <name>Céline Hudelot</name>
    </author>
    <author>
      <name>Mohamed El Amine Seddik</name>
    </author>
    <author>
      <name>Mohamed Tamaazousti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09708v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09708v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04427v2</id>
    <updated>2018-09-03T00:46:35Z</updated>
    <published>2018-07-12T05:14:45Z</published>
    <title>Simultaneous Coherent Structure Coloring facilitates interpretable
  clustering of scientific data by amplifying dissimilarity</title>
    <summary>  The clustering of data into physically meaningful subsets often requires
assumptions regarding the number, size, or shape of the subgroups. Here, we
present a new method, simultaneous Coherent Structure Coloring (sCSC), which
accomplishes the task of unsupervised clustering without a priori guidance
regarding the underlying structure of the data. sCSC performs a sequence of
binary splittings on the dataset such that the most dissimilar data points are
required to be in separate clusters. To achieve this, we obtain a set of
orthogonal coordinates along which dissimilarity in the dataset is maximized
from a generalized eigenvalue problem based on the pairwise dissimilarity
between the data points to be clustered. This sequence of bifurcations produces
a binary tree representation of the system, from which the number of clusters
in the data and their interrelationships naturally emerge. To illustrate the
effectiveness of the method in the absence of a priori assumptions we apply it
to two exemplary problems in fluid dynamics. Then, we illustrate its capacity
for interpretability using a high-dimensional protein folding simulation
dataset. While we restrict our examples to dynamical physical systems in this
work, we anticipate straightforward translation to other fields where existing
analysis tools require ad hoc assumptions on the data structure, lack the
interpretability of the present method, or in which the underlying processes
are less accessible, such as genomics and neuroscience.
</summary>
    <author>
      <name>Brooke E. Husic</name>
    </author>
    <author>
      <name>Kristy L. Schlueter-Kuck</name>
    </author>
    <author>
      <name>John O. Dabiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In revision</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04427v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04427v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00414v1</id>
    <updated>2018-09-03T00:04:49Z</updated>
    <published>2018-09-03T00:04:49Z</published>
    <title>Hypernyms Through Intra-Article Organization in Wikipedia</title>
    <summary>  We introduce a new measure for unsupervised hypernym detection and
directionality. The motivation is to keep the measure computationally light and
portatable across languages. We show that the relative physical location of
words in explanatory articles captures the directionality property. Further,
the phrases in section titles of articles about the word, capture the semantic
similarity needed for hypernym detection task. We experimentally show that the
combination of features coming from these two simple measures suffices to
produce results comparable with the best unsupervised measures in terms of the
average precision.
</summary>
    <author>
      <name>Disha Shrivastava</name>
    </author>
    <author>
      <name>Sreyash Kenkre</name>
    </author>
    <author>
      <name>Santosh Penubothula</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00410v1</id>
    <updated>2018-09-02T23:49:31Z</updated>
    <published>2018-09-02T23:49:31Z</published>
    <title>Modeling Topical Coherence in Discourse without Supervision</title>
    <summary>  Coherence of text is an important attribute to be measured for both manually
and automatically generated discourse; but well-defined quantitative metrics
for it are still elusive. In this paper, we present a metric for scoring
topical coherence of an input paragraph on a real-valued scale by analyzing its
underlying topical structure. We first extract all possible topics that the
sentences of a paragraph of text are related to. Coherence of this text is then
measured by computing: (a) the degree of uncertainty of the topics with respect
to the paragraph, and (b) the relatedness between these topics. All components
of our modular framework rely only on unlabeled data and WordNet, thus making
it completely unsupervised, which is an important feature for general-purpose
usage of any metric. Experiments are conducted on two datasets - a publicly
available dataset for essay grading (representing human discourse), and a
synthetic dataset constructed by mixing content from multiple paragraphs
covering diverse topics. Our evaluation shows that the measured coherence
scores are positively correlated with the ground truth for both the datasets.
Further validation to our coherence scores is provided by conducting human
evaluation on the synthetic data, showing a significant agreement of 79.3%
</summary>
    <author>
      <name>Disha Shrivastava</name>
    </author>
    <author>
      <name>Abhijit Mishra</name>
    </author>
    <author>
      <name>Karthik Sankaranarayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00341v2</id>
    <updated>2018-09-02T22:41:07Z</updated>
    <published>2018-04-01T20:49:56Z</published>
    <title>Sparse Principal Component Analysis via Variable Projection</title>
    <summary>  Sparse principal component analysis (SPCA) has emerged as a powerful
technique for data analysis, providing improved interpretation of low-rank
structures by identifying localized spatial structures in the data and
disambiguating between distinct time scales. We demonstrate a robust and
scalable SPCA algorithm by formulating it as a value-function optimization
problem. This viewpoint leads to a flexible and computationally efficient
algorithm. It can further leverage randomized methods from linear algebra to
extend the approach to the large-scale (big data) setting. Our proposed
innovation also allows for a robust SPCA formulation which can obtain
meaningful sparse components in spite of grossly corrupted input data. The
proposed algorithms are demonstrated using both synthetic and real world data,
showing exceptional computational efficiency and diagnostic performance.
</summary>
    <author>
      <name>N. Benjamin Erichson</name>
    </author>
    <author>
      <name>Peng Zheng</name>
    </author>
    <author>
      <name>Krithika Manohar</name>
    </author>
    <author>
      <name>Steven L. Brunton</name>
    </author>
    <author>
      <name>J. Nathan Kutz</name>
    </author>
    <author>
      <name>Aleksandr Y. Aravkin</name>
    </author>
    <link href="http://arxiv.org/abs/1804.00341v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00341v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00397v1</id>
    <updated>2018-09-02T21:34:28Z</updated>
    <published>2018-09-02T21:34:28Z</published>
    <title>Visual Transfer between Atari Games using Competitive Reinforcement
  Learning</title>
    <summary>  This paper explores the use of deep reinforcement learning agents to transfer
knowledge from one environment to another. More specifically, the method takes
advantage of asynchronous advantage actor critic (A3C) architecture to
generalize a target game using an agent trained on a source game in Atari.
Instead of fine-tuning a pre-trained model for the target game, we propose a
learning approach to update the model using multiple agents trained in parallel
with different representations of the target game. Visual mapping between video
sequences of transfer pairs is used to derive new representations of the target
game; training on these visual representations of the target game improves
model updates in terms of performance, data efficiency and stability. In order
to demonstrate the functionality of the architecture, Atari games Pong-v0 and
Breakout-v0 are being used from the OpenAI gym environment; as the source and
target environment.
</summary>
    <author>
      <name>Akshita Mittel</name>
    </author>
    <author>
      <name>Sowmya Munukutla</name>
    </author>
    <author>
      <name>Himanshi Yadav</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00381v1</id>
    <updated>2018-09-02T20:03:09Z</updated>
    <published>2018-09-02T20:03:09Z</published>
    <title>Multitask Learning for Fundamental Frequency Estimation in Music</title>
    <summary>  Fundamental frequency (f0) estimation from polyphonic music includes the
tasks of multiple-f0, melody, vocal, and bass line estimation. Historically
these problems have been approached separately, and only recently, using
learning-based approaches. We present a multitask deep learning architecture
that jointly estimates outputs for various tasks including multiple-f0, melody,
vocal and bass line estimation, and is trained using a large,
semi-automatically annotated dataset. We show that the multitask model
outperforms its single-task counterparts, and explore the effect of various
design decisions in our approach, and show that it performs better or at least
competitively when compared against strong baseline methods.
</summary>
    <author>
      <name>Rachel M. Bittner</name>
    </author>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Juan P. Bello</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07351v2</id>
    <updated>2018-09-02T20:00:32Z</updated>
    <published>2018-04-19T19:40:47Z</published>
    <title>Sampling-free Uncertainty Estimation in Gated Recurrent Units with
  Exponential Families</title>
    <summary>  There has recently been a concerted effort to derive mechanisms in vision and
machine learning systems to offer uncertainty estimates of the predictions they
make. Clearly, there are enormous benefits to a system that is not only
accurate but also has a sense for when it is not sure. Existing proposals
center around Bayesian interpretations of modern deep architectures -- these
are effective but can often be computationally demanding. We show how classical
ideas in the literature on exponential families on probabilistic networks
provide an excellent starting point to derive uncertainty estimates in Gated
Recurrent Units (GRU). Our proposal directly quantifies uncertainty
deterministically, without the need for costly sampling-based estimation. We
demonstrate how our model can be used to quantitatively and qualitatively
measure uncertainty in unsupervised image sequence prediction. To our
knowledge, this is the first result describing sampling-free uncertainty
estimation for powerful sequential models such as GRUs.
</summary>
    <author>
      <name>Seong Jae Hwang</name>
    </author>
    <author>
      <name>Ronak Mehta</name>
    </author>
    <author>
      <name>Hyunwoo J. Kim</name>
    </author>
    <author>
      <name>Vikas Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00366v1</id>
    <updated>2018-09-02T16:24:38Z</updated>
    <published>2018-09-02T16:24:38Z</published>
    <title>Cold-start recommendations in Collective Matrix Factorization</title>
    <summary>  This work explores the ability of collective matrix factorization models in
recommender systems to make predictions about users and items for which there
is side information available but no feedback or interactions data, and
proposes a new formulation with a faster cold-start prediction formula that can
be used in real-time systems. While these cold-start recommendations are not as
good as warm-start ones, they were found to be of better quality than
non-personalized recommendations, and predictions about new users were found to
be more reliable than those about new items. The formulation proposed here
resulted in improved cold-start recommendations in many scenarios, at the
expense of worse warm-start ones.
</summary>
    <author>
      <name>David Cortes</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06355v2</id>
    <updated>2018-09-02T16:04:16Z</updated>
    <published>2018-02-18T09:15:56Z</published>
    <title>Stochastic Chebyshev Gradient Descent for Spectral Optimization</title>
    <summary>  A large class of machine learning techniques requires the solution of
optimization problems involving spectral functions of parametric matrices, e.g.
log-determinant and nuclear norm. Unfortunately, computing the gradient of a
spectral function is generally of cubic complexity, as such gradient descent
methods are rather expensive for optimizing objectives involving the spectral
function. Thus, one naturally turns to stochastic gradient methods in hope that
they will provide a way to reduce or altogether avoid the computation of full
gradients. However, here a new challenge appears: there is no straightforward
way to compute unbiased stochastic gradients for spectral functions. In this
paper, we develop unbiased stochastic gradients for spectral-sums, an important
subclass of spectral functions. Our unbiased stochastic gradients are based on
combining randomized trace estimators with stochastic truncation of the
Chebyshev expansions. A careful design of the truncation distribution allows us
to offer distributions that are variance-optimal, which is crucial for fast and
stable convergence of stochastic gradient methods. We further leverage our
proposed stochastic gradients to devise stochastic methods for objective
functions involving spectral-sums, and rigorously analyze their convergence
rate. The utility of our methods is demonstrated in numerical experiments.
</summary>
    <author>
      <name>Insu Han</name>
    </author>
    <author>
      <name>Haim Avron</name>
    </author>
    <author>
      <name>Jinwoo Shin</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09334v2</id>
    <updated>2018-09-02T15:12:07Z</updated>
    <published>2017-10-25T16:41:00Z</published>
    <title>A unified framework for manifold landmarking</title>
    <summary>  The success of semi-supervised manifold learning is highly dependent on the
quality of the labeled samples. Active manifold learning aims to select and
label representative landmarks on a manifold from a given set of samples to
improve semi-supervised manifold learning. In this paper, we propose a novel
active manifold learning method based on a unified framework of manifold
landmarking. In particular, our method combines geometric manifold landmarking
methods with algebraic ones. We achieve this by using the Gershgorin circle
theorem to construct an upper bound on the learning error that depends on the
landmarks and the manifold's alignment matrix in a way that captures both the
geometric and algebraic criteria. We then attempt to select landmarks so as to
minimize this bound by iteratively deleting the Gershgorin circles
corresponding to the selected landmarks. We also analyze the complexity,
scalability, and robustness of our method through simulations, and demonstrate
its superiority compared to existing methods. Experiments in regression and
classification further verify that our method performs better than its
competitors.
</summary>
    <author>
      <name>Hongteng Xu</name>
    </author>
    <author>
      <name>Licheng Yu</name>
    </author>
    <author>
      <name>Mark Davenport</name>
    </author>
    <author>
      <name>Hongyuan Zha</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09334v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09334v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00343v1</id>
    <updated>2018-09-02T14:18:40Z</updated>
    <published>2018-09-02T14:18:40Z</published>
    <title>Towards an Intelligent Edge: Wireless Communication Meets Machine
  Learning</title>
    <summary>  The recent revival of artificial intelligence (AI) is revolutionizing almost
every branch of science and technology. Given the ubiquitous smart mobile
gadgets and Internet of Things (IoT) devices, it is expected that a majority of
intelligent applications will be deployed at the edge of wireless networks.
This trend has generated strong interests in realizing an "intelligent edge" to
support AI-enabled applications at various edge devices. Accordingly, a new
research area, called edge learning, emerges, which crosses and revolutionizes
two disciplines: wireless communication and machine learning. A major theme in
edge learning is to overcome the limited computing power, as well as limited
data, at each edge device. This is accomplished by leveraging the mobile edge
computing (MEC) platform and exploiting the massive data distributed over a
large number of edge devices. In such systems, learning from distributed data
and communicating between the edge server and devices are two critical and
coupled aspects, and their fusion poses many new research challenges. This
article advocates a new set of design principles for wireless communication in
edge learning, collectively called learning-driven communication. Illustrative
examples are provided to demonstrate the effectiveness of these design
principles, and unique research opportunities are identified.
</summary>
    <author>
      <name>Guangxu Zhu</name>
    </author>
    <author>
      <name>Dongzhu Liu</name>
    </author>
    <author>
      <name>Yuqing Du</name>
    </author>
    <author>
      <name>Changsheng You</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <author>
      <name>Kaibin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00338v1</id>
    <updated>2018-09-02T13:58:37Z</updated>
    <published>2018-09-02T13:58:37Z</published>
    <title>Look Across Elapse: Disentangled Representation Learning and
  Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition</title>
    <summary>  Despite the remarkable progress in face recognition related technologies,
reliably recognizing faces across ages still remains a big challenge. The
appearance of a human face changes substantially over time, resulting in
significant intra-class variations. As opposed to current techniques for
age-invariant face recognition, which either directly extract age-invariant
features for recognition, or first synthesize a face that matches target age
before feature extraction, we argue that it is more desirable to perform both
tasks jointly so that they can leverage each other. To this end, we propose a
deep Age-Invariant Model (AIM) for face recognition in the wild with three
distinct novelties. First, AIM presents a novel unified deep architecture
jointly performing cross-age face synthesis and recognition in a mutual
boosting way. Second, AIM achieves continuous face rejuvenation/aging with
remarkable photorealistic and identity-preserving properties, avoiding the
requirement of paired data and the true age of testing samples. Third, we
develop effective and novel training strategies for end-to-end learning the
whole deep architecture, which generates powerful age-invariant face
representations explicitly disentangled from the age variation. Moreover, we
propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset
to facilitate existing efforts and push the frontiers of age-invariant face
recognition research. Extensive experiments on both our CAFR and several other
cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the
proposed AIM model over the state-of-the-arts. Benchmarking our model on one of
the most popular unconstrained face recognition datasets IJB-C additionally
verifies the promising generalizability of AIM in recognizing faces in the
wild.
</summary>
    <author>
      <name>Jian Zhao</name>
    </author>
    <author>
      <name>Yu Cheng</name>
    </author>
    <author>
      <name>Yi Cheng</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Haochong Lan</name>
    </author>
    <author>
      <name>Fang Zhao</name>
    </author>
    <author>
      <name>Lin Xiong</name>
    </author>
    <author>
      <name>Yan Xu</name>
    </author>
    <author>
      <name>Jianshu Li</name>
    </author>
    <author>
      <name>Sugiri Pranata</name>
    </author>
    <author>
      <name>Shengmei Shen</name>
    </author>
    <author>
      <name>Junliang Xing</name>
    </author>
    <author>
      <name>Hengzhu Liu</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06640v2</id>
    <updated>2018-09-02T10:29:44Z</updated>
    <published>2018-08-20T18:20:01Z</published>
    <title>Adversarial Removal of Demographic Attributes from Text Data</title>
    <summary>  Recent advances in Representation Learning and Adversarial Training seem to
succeed in removing unwanted features from the learned representation. We show
that demographic information of authors is encoded in -- and can be recovered
from -- the intermediate representations learned by text-based neural
classifiers. The implication is that decisions of classifiers trained on
textual data are not agnostic to -- and likely condition on -- demographic
attributes. When attempting to remove such demographic information using
adversarial training, we find that while the adversarial component achieves
chance-level development-set accuracy during training, a post-hoc classifier,
trained on the encoded sentences from the first part, still manages to reach
substantially higher classification accuracies on the same data. This behavior
is consistent across several tasks, demographic properties and datasets. We
explore several techniques to improve the effectiveness of the adversarial
component. Our main conclusion is a cautionary one: do not rely on the
adversarial training to achieve invariant representation to sensitive features.
</summary>
    <author>
      <name>Yanai Elazar</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06640v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06640v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09964v2</id>
    <updated>2018-09-02T06:17:56Z</updated>
    <published>2018-08-29T14:46:35Z</published>
    <title>Semi-Metrification of the Dynamic Time Warping Distance</title>
    <summary>  The dynamic time warping (dtw) distance fails to satisfy the triangle
inequality and the identity of indiscernibles. As a consequence, the
dtw-distance is not warping-invariant, which in turn results in peculiarities
in data mining applications. This article converts the dtw-distance to a
semi-metric and shows that its canonical extension is warping-invariant.
Empirical results indicate that the nearest-neighbor classifier in the proposed
semi-metric space performs comparably to the same classifier in the standard
dtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result
suggests to further explore the semi-metric space for data mining applications.
</summary>
    <author>
      <name>Brijnesh J. Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09964v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09964v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01478v1</id>
    <updated>2018-09-02T02:56:25Z</updated>
    <published>2018-09-02T02:56:25Z</published>
    <title>Weakly-Supervised Neural Text Classification</title>
    <summary>  Deep neural networks are gaining increasing popularity for the classic text
classification task, due to their strong expressive power and less requirement
for feature engineering. Despite such attractiveness, neural text
classification models suffer from the lack of training data in many real-world
applications. Although many semi-supervised and weakly-supervised text
classification models exist, they cannot be easily applied to deep neural
models and meanwhile support limited supervision types. In this paper, we
propose a weakly-supervised method that addresses the lack of training data in
neural text classification. Our method consists of two modules: (1) a
pseudo-document generator that leverages seed information to generate
pseudo-labeled documents for model pre-training, and (2) a self-training module
that bootstraps on real unlabeled data for model refinement. Our method has the
flexibility to handle different types of weak supervision and can be easily
integrated into existing deep neural models for text classification. We have
performed extensive experiments on three real-world datasets from different
domains. The results demonstrate that our proposed method achieves inspiring
performance without requiring excessive training data and outperforms baseline
methods significantly.
</summary>
    <author>
      <name>Yu Meng</name>
    </author>
    <author>
      <name>Jiaming Shen</name>
    </author>
    <author>
      <name>Chao Zhang</name>
    </author>
    <author>
      <name>Jiawei Han</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271737</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271737" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM 2018 Full Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00263v1</id>
    <updated>2018-09-01T22:58:49Z</updated>
    <published>2018-09-01T22:58:49Z</published>
    <title>Stochastic Video Long-term Interpolation</title>
    <summary>  Video interpolation is aiming to generate intermediate sequence between two
frames. While most existing studies require the two reference frames to be
consecutive, we propose a stochastic learning frame work that can infer a
possible intermediate sequence between a long interval. Therefore, our work
expands the usability of video interpolation in applications such as video
long-term temporal super-resolution, missing frames repair and motion dynamic
inference. Our model includes a deterministic estimation to guarantee the
spatial and temporal coherency among the generated frames and a stochastic
mechanism to sample sequences from possible realities. Like the studies of
stochastic video prediction, our generated sequences are both sharp and varied.
In addition, most of the motions are realistic and can smoothly transition from
the referred start frame to the end frame.
</summary>
    <author>
      <name>Qiangeng Xu</name>
    </author>
    <author>
      <name>Hanwang Zhang</name>
    </author>
    <author>
      <name>Peter N. Belhumeur</name>
    </author>
    <author>
      <name>Ulrich Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00252v1</id>
    <updated>2018-09-01T21:12:09Z</updated>
    <published>2018-09-01T21:12:09Z</published>
    <title>Parameter Sharing Methods for Multilingual Self-Attentional Translation
  Models</title>
    <summary>  In multilingual neural machine translation, it has been shown that sharing a
single translation model between multiple languages can achieve competitive
performance, sometimes even leading to performance gains over bilingually
trained models. However, these improvements are not uniform; often multilingual
parameter sharing results in a decrease in accuracy due to translation models
not being able to accommodate different languages in their limited parameter
space. In this work, we examine parameter sharing techniques that strike a
happy medium between full sharing and individual training, specifically
focusing on the self-attentional Transformer model. We find that the full
parameter sharing approach leads to increases in BLEU scores mainly when the
target languages are from a similar language family. However, even in the case
where target languages are from different families where full parameter sharing
leads to a noticeable drop in BLEU scores, our proposed methods for partial
sharing of parameters can lead to substantial improvements in translation
accuracy.
</summary>
    <author>
      <name>Devendra Singh Sachan</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Third Conference on Machine Translation (WMT 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00251v1</id>
    <updated>2018-09-01T21:00:58Z</updated>
    <published>2018-09-01T21:00:58Z</published>
    <title>Car Monitoring System in Apartment Garages by Small Autonomous Car using
  Deep Learning</title>
    <summary>  Currently in Peru, people prefer to live in apartment instead of houses but
in some cases there are troubles with belongings between tenants who leave
their stuffs in parking lots. For that, the use of an intelligent mobile
mini-robot is proposed to implement a monitoring system of objects, such as
cars in an underground garage inside a building using deep learning models in
order to solve problems of theft of belongings. In addition, the small robot
presents an indoor location system through the use of beacons that allow us to
identify the position of the parking lot corresponding to each tenant of the
building during the route of the robot.
</summary>
    <author>
      <name>Leonardo León</name>
    </author>
    <author>
      <name>Felipe Moreno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures, SimBig 2018 submitted. Improving to get better
  results</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07898v2</id>
    <updated>2018-09-01T20:44:05Z</updated>
    <published>2018-05-21T05:28:22Z</published>
    <title>SmoothOut: Smoothing Out Sharp Minima to Improve Generalization in Deep
  Learning</title>
    <summary>  In Deep Learning, Stochastic Gradient Descent (SGD) is usually selected as
the training method because of its efficiency and scalability; however,
recently, a problem in SGD gains research interest: sharp minima in Deep Neural
Networks (DNNs) have poor generalization [1][2]; especially, large-batch SGD
tends to converge to sharp minima. It becomes an open question whether escaping
sharp minima can improve the generalization. To answer this question, we
proposed SmoothOut to smooth out sharp minima in DNNs and thereby improve
generalization. In a nutshell, SmoothOut perturbs multiple copies of the DNN by
noise injection and averages these copies. Injecting noises to SGD is widely
for exploration, but SmoothOut differs in lots of ways: (1) de-noising process
is applied before parameter updating; (2) uniform noises are injected instead
of Gaussian noises; (3) the goal is to obtain an auxiliary function without
sharp minima for better generalization, instead of higher exploration. We prove
that SmoothOut can eliminate sharp minima. Training multiple DNN copies is
inefficient, we further propose a stochastic version of SmoothOut which only
introduces the overhead of noise injecting and de-noising per batch. We prove
that the Stochastic SmoothOut is an unbiased approximation of the original
SmoothOut. In experiments on a variety of DNNs and datasets, SmoothOut
consistently improve generalization in both small-batch and large-batch
training on the top of state-of-the-art solutions. Our source code is in
https://github.com/wenwei202/smoothout
</summary>
    <author>
      <name>Wei Wen</name>
    </author>
    <author>
      <name>Yandan Wang</name>
    </author>
    <author>
      <name>Feng Yan</name>
    </author>
    <author>
      <name>Cong Xu</name>
    </author>
    <author>
      <name>Chunpeng Wu</name>
    </author>
    <author>
      <name>Yiran Chen</name>
    </author>
    <author>
      <name>Hai Li</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07898v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07898v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00241v1</id>
    <updated>2018-09-01T19:39:06Z</updated>
    <published>2018-09-01T19:39:06Z</published>
    <title>Activity Recognition on a Large Scale in Short Videos - Moments in Time
  Dataset</title>
    <summary>  Moments capture a huge part of our lives. Accurate recognition of these
moments is challenging due to the diverse and complex interpretation of the
moments. Action recognition refers to the act of classifying the desired
action/activity present in a given video. In this work, we perform experiments
on Moments in Time dataset to recognize accurately activities occurring in 3
second clips. We use state of the art techniques for visual, auditory and
spatio temporal localization and develop method to accurately classify the
activity in the Moments in Time dataset. Our novel approach of using Visual
Based Textual features and fusion techniques performs well providing an overall
89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the
Baseline TRN model.
</summary>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Harini Kesavamoorthy</name>
    </author>
    <author>
      <name>Poorva Rane</name>
    </author>
    <author>
      <name>Pramati Kalwad</name>
    </author>
    <author>
      <name>Alexander Hauptmann</name>
    </author>
    <author>
      <name>Florian Metze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Action recognition submission for Moments in Time Dataset - Improved
  results over challenge submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07706v2</id>
    <updated>2018-09-01T19:20:02Z</updated>
    <published>2018-07-20T04:24:51Z</published>
    <title>Efficient Probabilistic Inference in the Quest for Physics Beyond the
  Standard Model</title>
    <summary>  We present a novel framework that enables efficient probabilistic inference
in large-scale scientific models by allowing the execution of existing
domain-specific simulators as probabilistic programs, resulting in highly
interpretable posterior inference. Our framework is general purpose and
scalable, and is based on a cross-platform probabilistic execution protocol
through which an inference engine can control simulators in a language-agnostic
way. We demonstrate the technique in particle physics, on a scientifically
accurate simulation of the tau lepton decay, which is a key ingredient in
establishing the properties of the Higgs boson. High-energy physics has a rich
set of simulators based on quantum field theory and the interaction of
particles in matter. We show how to use probabilistic programming to perform
Bayesian inference in these existing simulator codebases directly, in
particular conditioning on observable outputs from a simulated particle
detector to directly produce an interpretable posterior distribution over decay
pathways. Inference efficiency is achieved via inference compilation where a
deep recurrent neural network is trained to parameterize proposal distributions
and control the stochastic simulator in a sequential importance sampling
scheme, at a fraction of the computational cost of Markov chain Monte Carlo
sampling.
</summary>
    <author>
      <name>Atilim Gunes Baydin</name>
    </author>
    <author>
      <name>Lukas Heinrich</name>
    </author>
    <author>
      <name>Wahid Bhimji</name>
    </author>
    <author>
      <name>Bradley Gram-Hansen</name>
    </author>
    <author>
      <name>Gilles Louppe</name>
    </author>
    <author>
      <name>Lei Shao</name>
    </author>
    <author>
      <name> Prabhat</name>
    </author>
    <author>
      <name>Kyle Cranmer</name>
    </author>
    <author>
      <name>Frank Wood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07706v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07706v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T37, 68T05, 62P35" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.2.6; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00238v1</id>
    <updated>2018-09-01T19:11:53Z</updated>
    <published>2018-09-01T19:11:53Z</published>
    <title>A Machine Learning Driven IoT Solution for Noise Classification in Smart
  Cities</title>
    <summary>  We present a machine learning based method for noise classification using a
low-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients
for audio feature extraction and supervised classification algorithms (that is,
support vector machine and k-nearest neighbors) for noise classification. We
evaluate our approach experimentally with a dataset of about 3000 sound samples
grouped in eight sound classes (such as, car horn, jackhammer, or street
music). We explore the parameter space of support vector machine and k-nearest
neighbors algorithms to estimate the optimal parameter values for
classification of sound samples in the dataset under study. We achieve a noise
classification accuracy in the range 85% -- 100%. Training and testing of our
k-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than
a second for a dataset with features of more than 3000 sound samples.
</summary>
    <author>
      <name>Yasser Alsouda</name>
    </author>
    <author>
      <name>Sabri Pllana</name>
    </author>
    <author>
      <name>Arianit Kurti</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00233v1</id>
    <updated>2018-09-01T18:39:57Z</updated>
    <published>2018-09-01T18:39:57Z</published>
    <title>Sleep Stage Classification: Scalability Evaluations of Distributed
  Approaches</title>
    <summary>  Processing and analyzing of massive clinical data are resource intensive and
time consuming with traditional analytic tools. Electroencephalogram (EEG) is
one of the major technologies in detecting and diagnosing various brain
disorders, and produces huge volume big data to process. In this study, we
propose a big data framework to diagnose sleep disorders by classifying the
sleep stages from EEG signals. The framework is developed with open source
SparkMlib Libraries. We also tested and evaluated the proposed framework by
measuring the scalabilities of well-known classification algorithms on
physionet sleep records.
</summary>
    <author>
      <name>Serife Acikalin</name>
    </author>
    <author>
      <name>Suleyman Eken</name>
    </author>
    <author>
      <name>Ahmet Sayar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of The Third International Conference on Data Mining,
  Internet Computing, and Big Data, Konya, Turkey 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00224v1</id>
    <updated>2018-09-01T17:21:01Z</updated>
    <published>2018-09-01T17:21:01Z</published>
    <title>Finding the Answers with Definition Models</title>
    <summary>  Inspired by a previous attempt to answer crossword questions using neural
networks (Hill, Cho, Korhonen, &amp; Bengio, 2015), this dissertation implements
extensions to improve the performance of this existing definition model on the
task of answering crossword questions. A discussion and evaluation of the
original implementation finds that there are some ways in which the recurrent
neural model could be extended. Insights from related fields neural language
modeling and neural machine translation provide the justification and means
required for these extensions. Two extensions are applied to the LSTM encoder,
first taking the average of LSTM states across the sequence and secondly using
a bidirectional LSTM, both implementations serve to improve model performance
on a definitions and crossword test set. In order to improve performance on
crossword questions, the training data is increased to include crossword
questions and answers, and this serves to improve results on definitions as
well as crossword questions. The final experiments are conducted using sub-word
unit segmentation, first on the source side and then later preliminary
experimentation is conducted to facilitate character-level output. Initially,
an exact reproduction of the baseline results proves unsuccessful. Despite
this, the extensions improve performance, allowing the definition model to
surpass the performance of the recurrent neural network variants of the
previous work (Hill, et al., 2015).
</summary>
    <author>
      <name>Jack Parry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSc Dissertation, University of Edinburgh, &lt;10,000 words</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01630v3</id>
    <updated>2018-09-01T16:29:56Z</updated>
    <published>2018-08-05T14:51:07Z</published>
    <title>A Review of Learning with Deep Generative Models from perspective of
  graphical modeling</title>
    <summary>  This document aims to provide a review on learning with deep generative
models (DGMs), which is an highly-active area in machine learning and more
generally, artificial intelligence. This review is not meant to be a tutorial,
but when necessary, we provide self-contained derivations for completeness.
This review has two features. First, though there are different perspectives to
classify DGMs, we choose to organize this review from the perspective of
graphical modeling, because the learning methods for directed DGMs and
undirected DGMs are fundamentally different. Second, we differentiate model
definitions from model learning algorithms, since different learning algorithms
can be applied to solve the learning problem on the same model, and an
algorithm can be applied to learn different models. We thus separate model
definition and model learning, with more emphasis on reviewing, differentiating
and connecting different learning algorithms. We also discuss promising future
research directions.
</summary>
    <author>
      <name>Zhijian Ou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">add SN-GANs, SA-GANs, conditional generation (cGANs, AC-GANs)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01630v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01630v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05293v3</id>
    <updated>2018-09-01T15:48:05Z</updated>
    <published>2018-08-15T22:10:57Z</published>
    <title>Design-based Analysis in Difference-In-Differences Settings with
  Staggered Adoption</title>
    <summary>  In this paper we study estimation of and inference for average treatment
effects in a setting with panel data. We focus on the setting where units,
e.g., individuals, firms, or states, adopt the policy or treatment of interest
at a particular point in time, and then remain exposed to this treatment at all
times afterwards. We take a design perspective where we investigate the
properties of estimators and procedures given assumptions on the assignment
process. We show that under random assignment of the adoption date the standard
Difference-In-Differences estimator is is an unbiased estimator of a particular
weighted average causal effect. We characterize the proeperties of this
estimand, and show that the standard variance estimator is conservative.
</summary>
    <author>
      <name>Susan Athey</name>
    </author>
    <author>
      <name>Guido Imbens</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05293v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05293v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00175v1</id>
    <updated>2018-09-01T13:33:31Z</updated>
    <published>2018-09-01T13:33:31Z</published>
    <title>Hyperparameter Learning for Conditional Mean Embeddings with Rademacher
  Complexity Bounds</title>
    <summary>  Conditional mean embeddings are nonparametric models that encode conditional
expectations in a reproducing kernel Hilbert space. While they provide a
flexible and powerful framework for probabilistic inference, their performance
is highly dependent on the choice of kernel and regularization hyperparameters.
Nevertheless, current hyperparameter tuning methods predominantly rely on
expensive cross validation or heuristics that is not optimized for the
inference task. For conditional mean embeddings with categorical targets and
arbitrary inputs, we propose a hyperparameter learning framework based on
Rademacher complexity bounds to prevent overfitting by balancing data fit
against model complexity. Our approach only requires batch updates, allowing
scalable kernel hyperparameter tuning without invoking kernel approximations.
Experiments demonstrate that our learning framework outperforms competing
methods, and can be further extended to incorporate and learn deep neural
network weights to improve generalization.
</summary>
    <author>
      <name>Kelvin Hsu</name>
    </author>
    <author>
      <name>Richard Nock</name>
    </author>
    <author>
      <name>Fabio Ramos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the European Conference on Machine Learning (ECML-PKDD
  2018): Currently shortlisted for best student paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02942v2</id>
    <updated>2018-09-01T12:37:58Z</updated>
    <published>2018-06-08T01:58:51Z</published>
    <title>SupportNet: solving catastrophic forgetting in class incremental
  learning with support data</title>
    <summary>  A plain well-trained deep learning model often does not have the ability to
learn new knowledge without forgetting the previously learned knowledge, which
is known as the catastrophic forgetting. Here we propose a novel method,
SupportNet, to solve the catastrophic forgetting problem in class incremental
learning scenario efficiently and effectively. SupportNet combines the strength
of deep learning and support vector machine (SVM), where SVM is used to
identify the support data from the old data, which are fed to the deep learning
model together with the new data for further training so that the model can
review the essential information of the old data when learning the new
information. Two powerful consolidation regularizers are applied to ensure the
robustness of the learned model. Comprehensive experiments on various tasks,
including enzyme function prediction, subcellular structure classification and
breast tumor classification, show that SupportNet drastically outperforms the
state-of-the-art incremental learning methods and even reaches similar
performance as the deep learning model trained from scratch on both old and new
data. Our program is accessible at: https://github.com/lykaust15/SupportNet
</summary>
    <author>
      <name>Yu Li</name>
    </author>
    <author>
      <name>Zhongxiao Li</name>
    </author>
    <author>
      <name>Lizhong Ding</name>
    </author>
    <author>
      <name>Yijie Pan</name>
    </author>
    <author>
      <name>Chao Huang</name>
    </author>
    <author>
      <name>Yuhui Hu</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Xin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02942v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02942v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06371v3</id>
    <updated>2018-09-01T10:28:07Z</updated>
    <published>2018-02-18T13:06:39Z</published>
    <title>Inductive Framework for Multi-Aspect Streaming Tensor Completion with
  Side Information</title>
    <summary>  Low rank tensor completion is a well studied problem and has applications in
various fields. However, in many real world applications the data is dynamic,
i.e., new data arrives at different time intervals. As a result, the tensors
used to represent the data grow in size. Besides the tensors, in many real
world scenarios, side information is also available in the form of matrices
which also grow in size with time. The problem of predicting missing values in
the dynamically growing tensor is called dynamic tensor completion. Most of the
previous work in dynamic tensor completion make an assumption that the tensor
grows only in one mode. To the best of our Knowledge, there is no previous work
which incorporates side information with dynamic tensor completion. We bridge
this gap in this paper by proposing a dynamic tensor completion framework
called Side Information infused Incremental Tensor Analysis (SIITA), which
incorporates side information and works for general incremental tensors. We
also show how non-negative constraints can be incorporated with SIITA, which is
essential for mining interpretable latent clusters. We carry out extensive
experiments on multiple real world datasets to demonstrate the effectiveness of
SIITA in various different settings.
</summary>
    <author>
      <name>Madhav Nimishakavi</name>
    </author>
    <author>
      <name>Bamdev Mishra</name>
    </author>
    <author>
      <name>Manish Gupta</name>
    </author>
    <author>
      <name>Partha Talukdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to International Conference on Information and Knowledge
  Management (CIKM), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06371v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06371v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07169v2</id>
    <updated>2018-09-01T08:27:25Z</updated>
    <published>2018-04-19T13:51:45Z</published>
    <title>Large-scale Nonlinear Variable Selection via Kernel Random Features</title>
    <summary>  We propose a new method for input variable selection in nonlinear regression.
The method is embedded into a kernel regression machine that can model general
nonlinear functions, not being a priori limited to additive models. This is the
first kernel-based variable selection method applicable to large datasets. It
sidesteps the typical poor scaling properties of kernel methods by mapping the
inputs into a relatively low-dimensional space of random features. The
algorithm discovers the variables relevant for the regression task together
with learning the prediction model through learning the appropriate nonlinear
random feature maps. We demonstrate the outstanding performance of our method
on a set of large-scale synthetic and real datasets.
</summary>
    <author>
      <name>Magda Gregorová</name>
    </author>
    <author>
      <name>Jason Ramapuram</name>
    </author>
    <author>
      <name>Alexandros Kalousis</name>
    </author>
    <author>
      <name>Stéphane Marchand-Maillet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version for proceedings of ECML/PKDD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07169v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07169v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00101v1</id>
    <updated>2018-09-01T02:22:57Z</updated>
    <published>2018-09-01T02:22:57Z</published>
    <title>Attentive Crowd Flow Machines</title>
    <summary>  Traffic flow prediction is crucial for urban traffic management and public
safety. Its key challenges lie in how to adaptively integrate the various
factors that affect the flow changes. In this paper, we propose a unified
neural network module to address this problem, called Attentive Crowd Flow
Machine~(ACFM), which is able to infer the evolution of the crowd flow by
learning dynamic representations of temporally-varying data with an attention
mechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units
connected with a convolutional layer for spatial weight prediction. The first
LSTM takes the sequential flow density representation as input and generates a
hidden state at each time-step for attention map inference, while the second
LSTM aims at learning the effective spatial-temporal feature expression from
attentionally weighted crowd flow features. Based on the ACFM, we further build
a deep architecture with the application to citywide crowd flow prediction,
which naturally incorporates the sequential and periodic data as well as other
external influences. Extensive experiments on two standard benchmarks (i.e.,
crowd flow in Beijing and New York City) show that the proposed method achieves
significant improvements over the state-of-the-art methods.
</summary>
    <author>
      <name>Lingbo Liu</name>
    </author>
    <author>
      <name>Ruimao Zhang</name>
    </author>
    <author>
      <name>Jiefeng Peng</name>
    </author>
    <author>
      <name>Guanbin Li</name>
    </author>
    <author>
      <name>Bowen Du</name>
    </author>
    <author>
      <name>Liang Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM MM, full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00095v1</id>
    <updated>2018-09-01T01:28:21Z</updated>
    <published>2018-09-01T01:28:21Z</published>
    <title>Learning Low Precision Deep Neural Networks through Regularization</title>
    <summary>  We consider the quantization of deep neural networks (DNNs) to produce
low-precision models for efficient inference of fixed-point operations.
Compared to previous approaches to training quantized DNNs directly under the
constraints of low-precision weights and activations, we learn the quantization
of DNNs with minimal quantization loss through regularization. In particular,
we introduce the learnable regularization coefficient to find accurate
low-precision models efficiently in training. In our experiments, the proposed
scheme yields the state-of-the-art low-precision models of AlexNet and
ResNet-18, which have better accuracy than their previously available
low-precision models. We also examine our quantization method to produce
low-precision DNNs for image super resolution. We observe only $0.5$~dB peak
signal-to-noise ratio (PSNR) loss when using binary weights and 8-bit
activations. The proposed scheme can be used to train low-precision models from
scratch or to fine-tune a well-trained high-precision model to converge to a
low-precision model. Finally, we discuss how a similar regularization method
can be adopted in DNN weight pruning and compression, and show that $401\times$
compression is achieved for LeNet-5.
</summary>
    <author>
      <name>Yoojin Choi</name>
    </author>
    <author>
      <name>Mostafa El-Khamy</name>
    </author>
    <author>
      <name>Jungwon Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01499v1</id>
    <updated>2018-09-01T00:15:30Z</updated>
    <published>2018-09-01T00:15:30Z</published>
    <title>Extractive Adversarial Networks: High-Recall Explanations for
  Identifying Personal Attacks in Social Media Posts</title>
    <summary>  We introduce an adversarial method for producing high-recall explanations of
neural text classifier decisions. Building on an existing architecture for
extractive explanations via hard attention, we add an adversarial layer which
scans the residual of the attention for remaining predictive signal. Motivated
by the important domain of detecting personal attacks in social media comments,
we additionally demonstrate the importance of manually setting a semantically
appropriate `default' behavior for the model by explicitly manipulating its
bias term. We develop a validation set of human-annotated personal attacks to
evaluate the impact of these changes.
</summary>
    <author>
      <name>Samuel Carton</name>
    </author>
    <author>
      <name>Qiaozhu Mei</name>
    </author>
    <author>
      <name>Paul Resnick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2018; code and data available soon</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00083v1</id>
    <updated>2018-08-31T23:38:41Z</updated>
    <published>2018-08-31T23:38:41Z</published>
    <title>Predicting protein inter-residue contacts using composite likelihood
  maximization and deep learning</title>
    <summary>  Accurate prediction of inter-residue contacts of a protein is important to
calcu- lating its tertiary structure. Analysis of co-evolutionary events among
residues has been proved effective to inferring inter-residue contacts. The
Markov ran- dom field (MRF) technique, although being widely used for contact
prediction, suffers from the following dilemma: the actual likelihood function
of MRF is accurate but time-consuming to calculate, in contrast, approximations
to the actual likelihood, say pseudo-likelihood, are efficient to calculate but
inaccu- rate. Thus, how to achieve both accuracy and efficiency simultaneously
remains a challenge. In this study, we present such an approach (called clmDCA)
for contact prediction. Unlike plmDCA using pseudo-likelihood, i.e., the
product of conditional probability of individual residues, our approach uses
composite- likelihood, i.e., the product of conditional probability of all
residue pairs. Com- posite likelihood has been theoretically proved as a better
approximation to the actual likelihood function than pseudo-likelihood.
Meanwhile, composite likelihood is still efficient to maximize, thus ensuring
the efficiency of clmDCA. We present comprehensive experiments on popular
benchmark datasets, includ- ing PSICOV dataset and CASP-11 dataset, to show
that: i) clmDCA alone outperforms the existing MRF-based approaches in
prediction accuracy. ii) When equipped with deep learning technique for
refinement, the prediction ac- curacy of clmDCA was further significantly
improved, suggesting the suitability of clmDCA for subsequent refinement
procedure. We further present successful application of the predicted contacts
to accurately build tertiary structures for proteins in the PSICOV dataset.
  Accessibility: The software clmDCA and a server are publicly accessible
through http://protein.ict.ac.cn/clmDCA/.
</summary>
    <author>
      <name>Haicang Zhang</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <author>
      <name>Fusong Ju</name>
    </author>
    <author>
      <name>Jianwei Zhu</name>
    </author>
    <author>
      <name>Shiwei Sun</name>
    </author>
    <author>
      <name>Yujuan Gao</name>
    </author>
    <author>
      <name>Ziwei Xie</name>
    </author>
    <author>
      <name>Minghua Deng</name>
    </author>
    <author>
      <name>Shiwei Sun</name>
    </author>
    <author>
      <name>Wei-Mou Zheng</name>
    </author>
    <author>
      <name>Dongbo Bu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00082v1</id>
    <updated>2018-08-31T23:38:00Z</updated>
    <published>2018-08-31T23:38:00Z</published>
    <title>The NEU Meta-Algorithm for Geometric Learning with Applications in
  Finance</title>
    <summary>  We introduce a meta-algorithm, called non-Euclidean upgrading (NEU), which
learns algorithm-specific geometries to improve the training and validation set
performance of a wide class of learning algorithms. Our approach is based on
iteratively performing local reconfigurations of the space in which the data
lie. These reconfigurations build universal approximation and universal
reconfiguration properties into the new algorithm being learned. This allows
any set of features to be learned by the new algorithm to arbitrary precision.
The training and validation set performance of NEU is investigated through
implementations predicting the relationship between select stock prices as well
as finding low-dimensional representations of the German Bond yield curve.
</summary>
    <author>
      <name>Anastasis Kratsios</name>
    </author>
    <author>
      <name>Cody B. Hyndman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60D05, 91G60, 62G08, 65D15, 62H25, 91G80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00072v1</id>
    <updated>2018-08-31T22:22:53Z</updated>
    <published>2018-08-31T22:22:53Z</published>
    <title>Rx-Caffe: Framework for evaluating and training Deep Neural Networks on
  Resistive Crossbars</title>
    <summary>  Deep Neural Networks (DNNs) are widely used to perform machine learning tasks
in speech, image, and natural language processing. The high computation and
storage demands of DNNs have led to a need for energy-efficient
implementations. Resistive crossbar systems have emerged as promising
candidates due to their ability to compactly and efficiently realize the
primitive DNN operation, viz., vector-matrix multiplication. However, in
practice, the functionality of resistive crossbars may deviate considerably
from the ideal abstraction due to the device and circuit level non-idealities
such as driver resistance, sensing resistance, sneak paths, and interconnect
parasitics. Although DNNs are somewhat tolerant to errors in their
computations, it is still essential to evaluate the impact of errors introduced
due to crossbar non-idealities on DNN accuracy. Unfortunately, device and
circuit-level models are not feasible to use in the context of large-scale DNNs
with 2.6-15.5 billion synaptic connections.
  In this work, we present a fast and accurate simulation framework to enable
training and evaluation of large-scale DNNs on resistive crossbar based
hardware fabrics. We propose a Fast Crossbar Model (FCM) that accurately
captures the errors arising due to non-idealities while being five orders of
magnitude faster than circuit simulation. We develop Rx-Caffe, an enhanced
version of the popular Caffe machine learning software framework to train and
evaluate DNNs on crossbars. We use Rx-Caffe to evaluate large-scale image
recognition DNNs designed for the ImageNet dataset. Our experiments reveal that
crossbar non-idealities can significantly degrade DNN accuracy by 9.6%-32%. To
the best of our knowledge, this work is the first evaluation of the accuracy
for large-scale DNNs on resistive crossbars and highlights the need for further
efforts to address the impact of non-idealities.
</summary>
    <author>
      <name>Shubham Jain</name>
    </author>
    <author>
      <name>Abhronil Sengupta</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <author>
      <name>Anand Raghunathan</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00068v1</id>
    <updated>2018-08-31T22:01:45Z</updated>
    <published>2018-08-31T22:01:45Z</published>
    <title>Denoising Neural Machine Translation Training with Trusted Data and
  Online Data Selection</title>
    <summary>  Measuring domain relevance of data and identifying or selecting well-fit
domain data for machine translation (MT) is a well-studied topic, but denoising
is not yet. Denoising is concerned with a different type of data quality and
tries to reduce the negative impact of data noise on MT training, in
particular, neural MT (NMT) training. This paper generalizes methods for
measuring and selecting data for domain MT and applies them to denoising NMT
training. The proposed approach uses trusted data and a denoising curriculum
realized by online data selection. Intrinsic and extrinsic evaluations of the
approach show its significant effectiveness for NMT to train on data with
severe noise.
</summary>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Taro Watanabe</name>
    </author>
    <author>
      <name>Macduff Hughes</name>
    </author>
    <author>
      <name>Tetsuji Nakagawa</name>
    </author>
    <author>
      <name>Ciprian Chelba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2018 Third Conference on Machine Translation (WMT18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00065v1</id>
    <updated>2018-08-31T21:22:52Z</updated>
    <published>2018-08-31T21:22:52Z</published>
    <title>MULDEF: Multi-model-based Defense Against Adversarial Examples for
  Neural Networks</title>
    <summary>  Despite being popularly used in many application domains such as image
recognition and classification, neural network models have been found to be
vulnerable to adversarial examples: given a model and an example correctly
classified by the model, an adversarial example is a new example formed by
applying small perturbation (imperceptible to human) on the given example so
that the model misclassifies the new example. Adversarial examples can pose
potential risks on safety or security in real-world applications. In recent
years, given a vulnerable model, defense approaches, such as adversarial
training and defensive distillation, improve the model to make it more robust
against adversarial examples. However, based on the improved model, attackers
can still generate adversarial examples to successfully attack the model. To
address such limitation, we propose a new defense approach, named MULDEF, based
on the design principle of diversity. Given a target model (as a seed model)
and an attack approach to be defended against, MULDEF constructs additional
models (from the seed model) together with the seed model to form a family of
models, such that the models are complementary to each other to accomplish
robustness diversity (i.e., one model's adversarial examples typically do not
become other models' adversarial examples), while maintaining about the same
accuracy for normal examples. At runtime, given an input example, MULDEF
randomly selects a model from the family to be applied on the given example.
The robustness diversity of the model family and the random selection of a
model from the family together lower the success rate of attacks. Our
evaluation results show that MULDEF substantially improves the target model's
accuracy on adversarial examples by 35-50% and 2-10% in the white-box and
black-box attack scenarios, respectively.
</summary>
    <author>
      <name>Siwakorn Srisakaokul</name>
    </author>
    <author>
      <name>Zexuan Zhong</name>
    </author>
    <author>
      <name>Yuhao Zhang</name>
    </author>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Tao Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01477v1</id>
    <updated>2018-08-31T19:31:05Z</updated>
    <published>2018-08-31T19:31:05Z</published>
    <title>A Supervised Learning Approach For Heading Detection</title>
    <summary>  As the Portable Document Format (PDF) file format increases in popularity,
research in analysing its structure for text extraction and analysis is
necessary. Detecting headings can be a crucial component of classifying and
extracting meaningful data. This research involves training a supervised
learning model to detect headings with features carefully selected through
recursive feature elimination. The best performing classifier had an accuracy
of 96.95%, sensitivity of 0.986 and a specificity of 0.953. This research into
heading detection contributes to the field of PDF based text extraction and can
be applied to the automation of large scale PDF text analysis in a variety of
professional and policy based contexts.
</summary>
    <author>
      <name>Sahib Singh Budhiraja</name>
    </author>
    <author>
      <name>Vijay Mago</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10867v1</id>
    <updated>2018-08-31T17:51:54Z</updated>
    <published>2018-08-31T17:51:54Z</published>
    <title>Tensor Embedding: A Supervised Framework for Human Behavioral Data
  Mining and Prediction</title>
    <summary>  Today's densely instrumented world offers tremendous opportunities for
continuous acquisition and analysis of multimodal sensor data providing
temporal characterization of an individual's behaviors. Is it possible to
efficiently couple such rich sensor data with predictive modeling techniques to
provide contextual, and insightful assessments of individual performance and
wellbeing? Prediction of different aspects of human behavior from these noisy,
incomplete, and heterogeneous bio-behavioral temporal data is a challenging
problem, beyond unsupervised discovery of latent structures. We propose a
Supervised Tensor Embedding (STE) algorithm for high dimension multimodal data
with join decomposition of input and target variable. Furthermore, we show that
features selection will help to reduce the contamination in the prediction and
increase the performance. The efficiently of the methods was tested via two
different real world datasets.
</summary>
    <author>
      <name>Homa Hosseinmardi</name>
    </author>
    <author>
      <name>Amir Ghasemian</name>
    </author>
    <author>
      <name>Shrikanth Narayanan</name>
    </author>
    <author>
      <name>Kristina Lerman</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10862v1</id>
    <updated>2018-08-31T17:43:21Z</updated>
    <published>2018-08-31T17:43:21Z</published>
    <title>Open Source Dataset and Machine Learning Techniques for Automatic
  Recognition of Historical Graffiti</title>
    <summary>  Machine learning techniques are presented for automatic recognition of the
historical letters (XI-XVIII centuries) carved on the stoned walls of St.Sophia
cathedral in Kyiv (Ukraine). A new image dataset of these carved Glagolitic and
Cyrillic letters (CGCL) was assembled and pre-processed for recognition and
prediction by machine learning methods. The dataset consists of more than 4000
images for 34 types of letters. The explanatory data analysis of CGCL and
notMNIST datasets shown that the carved letters can hardly be differentiated by
dimensionality reduction methods, for example, by t-distributed stochastic
neighbor embedding (tSNE) due to the worse letter representation by stone
carving in comparison to hand writing. The multinomial logistic regression
(MLR) and a 2D convolutional neural network (CNN) models were applied. The MLR
model demonstrated the area under curve (AUC) values for receiver operating
characteristic (ROC) are not lower than 0.92 and 0.60 for notMNIST and CGCL,
respectively. The CNN model gave AUC values close to 0.99 for both notMNIST and
CGCL (despite the much smaller size and quality of CGCL in comparison to
notMNIST) under condition of the high lossy data augmentation. CGCL dataset was
published to be available for the data science community as an open source
resource.
</summary>
    <author>
      <name>Nikita Gordienko</name>
    </author>
    <author>
      <name>Peng Gang</name>
    </author>
    <author>
      <name>Yuri Gordienko</name>
    </author>
    <author>
      <name>Wei Zeng</name>
    </author>
    <author>
      <name>Oleg Alienin</name>
    </author>
    <author>
      <name>Oleksandr Rokovyi</name>
    </author>
    <author>
      <name>Sergii Stirenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, accepted for 25th International Conference on
  Neural Information Processing (ICONIP 2018), 14-16 December, 2018 (Siem Reap,
  Cambodia)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01015v1</id>
    <updated>2018-08-31T17:07:31Z</updated>
    <published>2018-08-31T17:07:31Z</published>
    <title>Automated segmentation on the entire cardiac cycle using a deep learning
  work-flow</title>
    <summary>  The segmentation of the left ventricle (LV) from CINE MRI images is essential
to infer important clinical parameters. Typically, machine learning algorithms
for automated LV segmentation use annotated contours from only two cardiac
phases, diastole, and systole. In this work, we present an analysis work-flow
for fully-automated LV segmentation that learns from images acquired through
the cardiac cycle. The workflow consists of three components: first, for each
image in the sequence, we perform an automated localization and subsequent
cropping of the bounding box containing the cardiac silhouette. Second, we
identify the LV contours using a Temporal Fully Convolutional Neural Network
(T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a
recurrent mechanism enforcing temporal coherence across consecutive frames.
Finally, we further defined the boundaries using either one of two components:
fully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials
and Semantic Flow. Our initial experiments suggest that significant improvement
in performance can potentially be achieved by using a recurrent neural network
component that explicitly learns cardiac motion patterns whilst performing LV
segmentation.
</summary>
    <author>
      <name>Nicoló Savioli</name>
    </author>
    <author>
      <name>Miguel Silva Vieira</name>
    </author>
    <author>
      <name>Pablo Lamata</name>
    </author>
    <author>
      <name>Giovanni Montana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, published on IEEE Xplore</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09870v2</id>
    <updated>2018-08-31T16:56:52Z</updated>
    <published>2018-07-25T21:34:55Z</published>
    <title>Do Better ImageNet Models Transfer Better... for Image Recommendation ?</title>
    <summary>  Visual embeddings from Convolutional Neural Networks (CNN) trained on the
ImageNet dataset for the ILSVRC challenge have shown consistently good
performance for transfer learning and are widely used in several tasks,
including image recommendation. However, some important questions have not yet
been answered in order to use these embeddings for a larger scope of
recommendation domains: a) Do CNNs that perform better in ImageNet are also
better for transfer learning in content-based image recommendation?, b) Does
fine-tuning help to improve performance? and c) Which is the best way to
perform the fine-tuning?
  In this paper we compare several CNN models pre-trained with ImageNet to
evaluate their transfer learning performance to an artwork image recommendation
task. Our results indicate that models with better performance in the ImageNet
challenge do not always imply better transfer learning for recommendation tasks
(e.g. NASNet vs. ResNet). Our results also show that fine-tuning can be helpful
even with a small dataset, but not every fine-tuning works. Our results can
inform other researchers and practitioners on how to train their CNNs for
better transfer learning towards image recommendation systems.
</summary>
    <author>
      <name>Felipe del Rio</name>
    </author>
    <author>
      <name>Pablo Messina</name>
    </author>
    <author>
      <name>Vicente Dominguez</name>
    </author>
    <author>
      <name>Denis Parra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to KTL Workshop co-located at RecSys</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09870v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09870v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10792v1</id>
    <updated>2018-08-31T14:55:52Z</updated>
    <published>2018-08-31T14:55:52Z</published>
    <title>Bottom-Up Abstractive Summarization</title>
    <summary>  Neural network-based methods for abstractive summarization produce outputs
that are more fluent than other techniques, but which can be poor at content
selection. This work proposes a simple technique for addressing this issue: use
a data-efficient content selector to over-determine phrases in a source
document that should be part of the summary. We use this selector as a
bottom-up attention step to constrain the model to likely phrases. We show that
this approach improves the ability to compress text, while still generating
fluent summaries. This two-step process is both simpler and higher performing
than other end-to-end content selection models, leading to significant
improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the
content selector can be trained with as little as 1,000 sentences, making it
easy to transfer a trained summarizer to a new domain.
</summary>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <author>
      <name>Yuntian Deng</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00934v1</id>
    <updated>2018-08-31T14:45:33Z</updated>
    <published>2018-08-31T14:45:33Z</published>
    <title>A Deep Neural Network Sentence Level Classification Method with Context
  Information</title>
    <summary>  In the sentence classification task, context formed from sentences adjacent
to the sentence being classified can provide important information for
classification. This context is, however, often ignored. Where methods do make
use of context, only small amounts are considered, making it difficult to
scale. We present a new method for sentence classification, Context-LSTM-CNN,
that makes use of potentially large contexts. The method also utilizes
long-range dependencies within the sentence being classified, using an LSTM,
and short-span features, using a stacked CNN. Our experiments demonstrate that
this approach consistently improves over previous methods on two different
datasets.
</summary>
    <author>
      <name>Xingyi Song</name>
    </author>
    <author>
      <name>Johann Petrak</name>
    </author>
    <author>
      <name>Angus Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EMNLP2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10788v1</id>
    <updated>2018-08-31T14:40:03Z</updated>
    <published>2018-08-31T14:40:03Z</published>
    <title>Data-driven discovery of PDEs in complex datasets</title>
    <summary>  Many processes in science and engineering can be described by partial
differential equations (PDEs). Traditionally, PDEs are derived by considering
first principles of physics to derive the relations between the involved
physical quantities of interest. A different approach is to measure the
quantities of interest and use deep learning to reverse engineer the PDEs which
are describing the physical process.
  In this paper we use machine learning, and deep learning in particular, to
discover PDEs hidden in complex data sets from measurement data. We include
examples of data from a known model problem, and real data from weather station
measurements. We show how necessary transformations of the input data amounts
to coordinate transformations in the discovered PDE, and we elaborate on
feature and model selection. It is shown that the dynamics of a non-linear,
second order PDE can be accurately described by an ordinary differential
equation which is automatically discovered by our deep learning algorithm. Even
more interestingly, we show that similar results apply in the context of more
complex simulations of the Swedish temperature distribution.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nyström</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10776v1</id>
    <updated>2018-08-31T14:32:10Z</updated>
    <published>2018-08-31T14:32:10Z</published>
    <title>Scalable Manifold Learning for Big Data with Apache Spark</title>
    <summary>  Non-linear spectral dimensionality reduction methods, such as Isomap, remain
important technique for learning manifolds. However, due to computational
complexity, exact manifold learning using Isomap is currently impossible from
large-scale data. In this paper, we propose a distributed memory framework
implementing end-to-end exact Isomap under Apache Spark model. We show how each
critical step of the Isomap algorithm can be efficiently realized using basic
Spark model, without the need to provision data in the secondary storage. We
show how the entire method can be implemented using PySpark, offloading compute
intensive linear algebra routines to BLAS. Through experimental results, we
demonstrate excellent scalability of our method, and we show that it can
process datasets orders of magnitude larger than what is currently possible,
using a 25-node parallel~cluster.
</summary>
    <author>
      <name>Frank Schoeneman</name>
    </author>
    <author>
      <name>Jaroslaw Zola</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09382v3</id>
    <updated>2018-08-31T13:31:53Z</updated>
    <published>2018-07-24T23:04:49Z</published>
    <title>On sampling from a log-concave density using kinetic Langevin diffusions</title>
    <summary>  Langevin diffusion processes and their discretizations are often used for
sampling from a target density. The most convenient framework for assessing the
quality of such a sampling scheme corresponds to smooth and strongly
log-concave densities defined on $\mathbb R^p$. The present work focuses on
this framework and studies the behavior of Monte Carlo algorithms based on
discretizations of the kinetic Langevin diffusion. We first prove the geometric
mixing property of the kinetic Langevin diffusion with a mixing rate that is,
in the overdamped regime, optimal in terms of its dependence on the condition
number. We then use this result for obtaining improved guarantees of sampling
using the kinetic Langevin Monte Carlo method, when the quality of sampling is
measured by the Wasserstein distance. We also consider the situation where the
Hessian of the log-density of the target distribution is Lipschitz-continuous.
In this case, we introduce a new discretization of the kinetic Langevin
diffusion and prove that this leads to a substantial improvement of the upper
bound on the sampling error measured in Wasserstein distance.
</summary>
    <author>
      <name>Arnak S. Dalalyan</name>
    </author>
    <author>
      <name>Lionel Riou-Durand</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09382v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09382v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10724v1</id>
    <updated>2018-08-31T13:16:31Z</updated>
    <published>2018-08-31T13:16:31Z</published>
    <title>Learning Data-adaptive Nonparametric Kernels</title>
    <summary>  Traditional kernels or their combinations are often not sufficiently flexible
to fit the data in complicated practical tasks. In this paper, we present a
Data-Adaptive Nonparametric Kernel (DANK) learning framework by imposing an
adaptive matrix on the kernel/Gram matrix in an entry-wise strategy. Since we
do not specify the formulation of the adaptive matrix, each entry in it can be
directly and flexibly learned from the data. Therefore, the solution space of
the learned kernel is largely expanded, which makes DANK flexible to adapt to
the data. Specifically, the proposed kernel learning framework can be
seamlessly embedded to support vector machines (SVM) and support vector
regression (SVR), which has the capability of enlarging the margin between
classes and reducing the model generalization error. Theoretically, we
demonstrate that the objective function of our devised model is
gradient-Lipschitz continuous. Thereby, the training process for kernel and
parameter learning in SVM/SVR can be efficiently optimized in a unified
framework. Further, to address the scalability issue in DANK, a
decomposition-based scalable approach is developed, of which the effectiveness
is demonstrated by both empirical studies and theoretical guarantees.
Experimentally, our method outperforms other representative kernel learning
based algorithms on various classification and regression benchmark datasets.
</summary>
    <author>
      <name>Fanghui Liu</name>
    </author>
    <author>
      <name>Xiaolin Huang</name>
    </author>
    <author>
      <name>Chen Gong</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10705v1</id>
    <updated>2018-08-31T12:24:01Z</updated>
    <published>2018-08-31T12:24:01Z</published>
    <title>Bayesian Classifier for Route Prediction with Markov Chains</title>
    <summary>  We present here a general framework and a specific algorithm for predicting
the destination, route, or more generally a pattern, of an ongoing journey,
building on the recent work of [Y. Lassoued, J. Monteil, Y. Gu, G. Russo, R.
Shorten, and M. Mevissen, "Hidden Markov model for route and destination
prediction," in IEEE International Conference on Intelligent Transportation
Systems, 2017]. In the presented framework, known journey patterns are modelled
as stochastic processes, emitting the road segments visited during the journey,
and the ongoing journey is predicted by updating the posterior probability of
each journey pattern given the road segments visited so far. In this
contribution, we use Markov chains as models for the journey patterns, and
consider the prediction as final, once one of the posterior probabilities
crosses a predefined threshold. Despite the simplicity of both, examples run on
a synthetic dataset demonstrate high accuracy of the made predictions.
</summary>
    <author>
      <name>Jonathan P. Epperlein</name>
    </author>
    <author>
      <name>Julien Monteil</name>
    </author>
    <author>
      <name>Mingming Liu</name>
    </author>
    <author>
      <name>Yingqi Gu</name>
    </author>
    <author>
      <name>Sergiy Zhuk</name>
    </author>
    <author>
      <name>Robert Shorten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at The 21st IEEE International Conference on Intelligent
  Transportation Systems (ITSC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10696v1</id>
    <updated>2018-08-31T11:56:10Z</updated>
    <published>2018-08-31T11:56:10Z</published>
    <title>How agents see things: On visual representations in an emergent language
  game</title>
    <summary>  There is growing interest in the language developed by agents interacting in
emergent-communication settings. Earlier studies have focused on the agents'
symbol usage, rather than on their representation of visual input. In this
paper, we consider the referential games of Lazaridou et al. (2017), and
investigate the representations the agents develop during their evolving
interaction. We find that the agents establish successful communication by
inducing visual representations that almost perfectly align with each other,
but, surprisingly, do not capture the conceptual properties of the objects
depicted in the input images. We conclude that, if we care about developing
language-like communication systems, we must pay more attention to the visual
semantics agents associate to the symbols they use.
</summary>
    <author>
      <name>Diane Bouchacourt</name>
    </author>
    <author>
      <name>Marco Baroni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2018 Conference on Empirical Methods in Natural Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10692v1</id>
    <updated>2018-08-31T11:47:10Z</updated>
    <published>2018-08-31T11:47:10Z</published>
    <title>APES: a Python toolbox for simulating reinforcement learning
  environments</title>
    <summary>  Assisted by neural networks, reinforcement learning agents have been able to
solve increasingly complex tasks over the last years. The simulation
environment in which the agents interact is an essential component in any
reinforcement learning problem. The environment simulates the dynamics of the
agents' world and hence provides feedback to their actions in terms of state
observations and external rewards. To ease the design and simulation of such
environments this work introduces $\texttt{APES}$, a highly customizable and
open source package in Python to create 2D grid-world environments for
reinforcement learning problems. $\texttt{APES}$ equips agents with algorithms
to simulate any field of vision, it allows the creation and positioning of
items and rewards according to user-defined rules, and supports the interaction
of multiple agents.
</summary>
    <author>
      <name>Aqeel Labash</name>
    </author>
    <author>
      <name>Ardi Tampuu</name>
    </author>
    <author>
      <name>Tambet Matiisen</name>
    </author>
    <author>
      <name>Jaan Aru</name>
    </author>
    <author>
      <name>Raul Vicente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03041v2</id>
    <updated>2018-08-31T10:31:57Z</updated>
    <published>2017-03-25T15:46:01Z</published>
    <title>Learning optimal wavelet bases using a neural network approach</title>
    <summary>  A novel method for learning optimal, orthonormal wavelet bases for
representing 1- and 2D signals, based on parallels between the wavelet
transform and fully connected artificial neural networks, is described. The
structural similarities between these two concepts are reviewed and combined to
a "wavenet", allowing for the direct learning of optimal wavelet filter
coefficient through stochastic gradient descent with back-propagation over
ensembles of training inputs, where conditions on the filter coefficients for
constituting orthonormal wavelet bases are cast as quadratic regularisations
terms. We describe the practical implementation of this method, and study its
performance for high-energy physics collision events for QCD $2 \to 2$
processes. It is shown that an optimal solution is found, even in a
high-dimensional search space, and the implications of the result are
discussed.
</summary>
    <author>
      <name>Andreas Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03041v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03041v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10664v1</id>
    <updated>2018-08-31T10:22:32Z</updated>
    <published>2018-08-31T10:22:32Z</published>
    <title>A novel graph-based model for hybrid recommendations in cold-start
  scenarios</title>
    <summary>  Cold-start is a very common and still open problem in the Recommender Systems
literature. Since cold start items do not have any interaction, collaborative
algorithms are not applicable. One of the main strategies is to use pure or
hybrid content-based approaches, which usually yield to lower recommendation
quality than collaborative ones. Some techniques to optimize performance of
this type of approaches have been studied in recent past. One of them is called
feature weighting, which assigns to every feature a real value, called weight,
that estimates its importance. Statistical techniques for feature weighting
commonly used in Information Retrieval, like TF-IDF, have been adapted for
Recommender Systems, but they often do not provide sufficient quality
improvements. More recent approaches, FBSM and LFW, estimate weights by
leveraging collaborative information via machine learning, in order to learn
the importance of a feature based on other users opinions. This type of models
have shown promising results compared to classic statistical analyzes cited
previously. We propose a novel graph, feature-based machine learning model to
face the cold-start item scenario, learning the relevance of features from
probabilities of item-based collaborative filtering algorithms.
</summary>
    <author>
      <name>Cesare Bernardis</name>
    </author>
    <author>
      <name>Maurizio Ferrari Dacrema</name>
    </author>
    <author>
      <name>Paolo Cremonesi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10663v1</id>
    <updated>2018-08-31T10:20:43Z</updated>
    <published>2018-08-31T10:20:43Z</published>
    <title>A Multi-layer Gaussian Process for Motor Symptom Estimation in People
  with Parkinson's Disease</title>
    <summary>  The assessment of Parkinson's disease (PD) poses a significant challenge as
it is influenced by various factors which lead to a complex and fluctuating
symptom manifestation. Thus, a frequent and objective PD assessment is highly
valuable for effective health management of people with Parkinson's disease
(PwP). Here, we propose a method for monitoring PwP by stochastically modeling
the relationships between their wrist movements during unscripted daily
activities and corresponding annotations about clinical displays of movement
abnormalities. We approach the estimation of PD motor signs by independently
modeling and hierarchically stacking Gaussian process models for three classes
of commonly observed movement abnormalities in PwP including tremor,
(non-tremulous) bradykinesia, and (non-tremulous) dyskinesia. We use clinically
adopted severity measures as annotations for training the models, thus allowing
our multi-layer Gaussian process prediction models to estimate not only their
presence but also their severities. The experimental validation of our approach
demonstrates strong agreement of the model predictions with these PD
annotations. Our results show the proposed method produces promising results in
objective monitoring of movement abnormalities of PD in the presence of
arbitrary and unknown voluntary motions, and makes an important step towards
continuous monitoring of PD in the home environment.
</summary>
    <author>
      <name>Muriel Lang</name>
    </author>
    <author>
      <name>Urban Fietzek</name>
    </author>
    <author>
      <name>Jakob Fröhner</name>
    </author>
    <author>
      <name>Franz M. J. Pfister</name>
    </author>
    <author>
      <name>Daniel Pichler</name>
    </author>
    <author>
      <name>Kian Abedinpour</name>
    </author>
    <author>
      <name>Terry T. Um</name>
    </author>
    <author>
      <name>Dana Kulić</name>
    </author>
    <author>
      <name>Satoshi Endo</name>
    </author>
    <author>
      <name>Sandra Hirche</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03726v2</id>
    <updated>2018-08-31T10:14:33Z</updated>
    <published>2018-08-10T23:21:07Z</published>
    <title>Learning to Represent Bilingual Dictionaries</title>
    <summary>  Bilingual word embeddings have been widely used to capture the similarity of
lexical semantics in different human languages. However, many applications,
such as cross-lingual semantic search and question answering, can be largely
benefited from the cross-lingual correspondence between sentences and lexicons.
To bridge this gap, we propose a neural embedding model that leverages
bilingual dictionaries. The proposed model is trained to map the literal word
definitions to the cross-lingual target words, for which we explore with
different sentence encoding techniques. To enhance the learning process on
limited resources, our model adopts several critical learning strategies,
including multi-task learning on different bridges of languages, and joint
learning of the dictionary model with a bilingual word embedding model.
Experimental evaluation focuses on two applications. The results of the
cross-lingual reverse dictionary retrieval task show our model's promising
ability of comprehending bilingual concepts based on descriptions, and
highlight the effectiveness of proposed learning strategies in improving
performance. Meanwhile, our model effectively addresses the bilingual
paraphrase identification problem and significantly outperforms previous
approaches.
</summary>
    <author>
      <name>Muhao Chen</name>
    </author>
    <author>
      <name>Yingtao Tian</name>
    </author>
    <author>
      <name>Haochen Chen</name>
    </author>
    <author>
      <name>Kai-Wei Chang</name>
    </author>
    <author>
      <name>Steven Skiena</name>
    </author>
    <author>
      <name>Carlo Zaniolo</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03726v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03726v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10654v1</id>
    <updated>2018-08-31T09:56:43Z</updated>
    <published>2018-08-31T09:56:43Z</published>
    <title>Gibson Env: Real-World Perception for Embodied Agents</title>
    <summary>  Developing visual perception models for active agents and sensorimotor
control are cumbersome to be done in the physical world, as existing algorithms
are too slow to efficiently learn in real-time and robots are fragile and
costly. This has given rise to learning-in-simulation which consequently casts
a question on whether the results transfer to real-world. In this paper, we are
concerned with the problem of developing real-world perception for active
agents, propose Gibson Virtual Environment for this purpose, and showcase
sample perceptual tasks learned therein. Gibson is based on virtualizing real
spaces, rather than using artificially designed ones, and currently includes
over 1400 floor spaces from 572 full buildings. The main characteristics of
Gibson are: I. being from the real-world and reflecting its semantic
complexity, II. having an internal synthesis mechanism, "Goggles", enabling
deploying the trained models in real-world without needing further domain
adaptation, III. embodiment of agents and making them subject to constraints of
physics and space.
</summary>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Amir Zamir</name>
    </author>
    <author>
      <name>Zhi-Yang He</name>
    </author>
    <author>
      <name>Alexander Sax</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Access the code, dataset, and project website at
  http://gibsonenv.vision/ . CVPR 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.10654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10648v1</id>
    <updated>2018-08-31T09:46:07Z</updated>
    <published>2018-08-31T09:46:07Z</published>
    <title>Adaptation and Robust Learning of Probabilistic Movement Primitives</title>
    <summary>  Probabilistic representations of movement primitives open important new
possibilities for machine learning in robotics. These representations are able
to capture the variability of the demonstrations from a teacher as a
probability distribution over trajectories, providing a sensible region of
exploration and the ability to adapt to changes in the robot environment.
However, to be able to capture variability and correlations between different
joints, a probabilistic movement primitive requires the estimation of a larger
number of parameters compared to their deterministic counterparts, that focus
on modeling only the mean behavior. In this paper, we make use of prior
distributions over the parameters of a probabilistic movement primitive to make
robust estimates of the parameters with few training instances. In addition, we
introduce general purpose operators to adapt movement primitives in joint and
task space. The proposed training method and adaptation operators are tested in
a coffee preparation and in robot table tennis task. In the coffee preparation
task we evaluate the generalization performance to changes in the location of
the coffee grinder and brewing chamber in a target area, achieving the desired
behavior after only two demonstrations. In the table tennis task we evaluate
the hit and return rates, outperforming previous approaches while using fewer
task specific heuristics.
</summary>
    <author>
      <name>Sebastian Gomez-Gonzalez</name>
    </author>
    <author>
      <name>Gerhard Neumann</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <author>
      <name>Jan Peters</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10632v1</id>
    <updated>2018-08-31T08:34:14Z</updated>
    <published>2018-08-31T08:34:14Z</published>
    <title>A novel extension of Generalized Low-Rank Approximation of Matrices
  based on multiple-pairs of transformations</title>
    <summary>  Dimension reduction is a main step in learning process which plays a
essential role in many applications. The most popular methods in this field
like SVD, PCA, and LDA, only can apply to vector data. This means that for
higher order data like matrices or more generally tensors, data should be fold
to a vector. By this folding, the probability of overfitting is increased and
also maybe some important spatial features are ignored. Then, to tackle these
issues, methods are proposed which work directly on data with their own format
like GLRAM, MPCA, and MLDA. In these methods the spatial relationship among
data are preserved and furthermore, the probability of overfitiing has fallen.
Also the time and space complexity are less than vector-based ones. Having said
that, because of the less parameters in multilinear methods, they have a much
smaller search space to find an optimal answer in comparison with vector-based
approach. To overcome this drawback of multilinear methods like GLRAM, we
proposed a new method which is a general form of GLRAM and by preserving the
merits of it have a larger search space. We have done plenty of experiments to
show that our proposed method works better than GLRAM. Also, applying this
approach to other multilinear dimension reduction methods like MPCA and MLDA is
straightforwar
</summary>
    <author>
      <name>Soheil Ahmadi</name>
    </author>
    <author>
      <name>Mansoor Rezghi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07174v3</id>
    <updated>2018-08-31T08:25:11Z</updated>
    <published>2018-06-19T12:01:57Z</published>
    <title>FRnet-DTI: Deep Convolutional Neural Networks with Evolutionary and
  Structural Features for Drug-Target Interaction</title>
    <summary>  The task of drug-target interaction prediction holds significant importance
in pharmacology and therapeutic drug design. In this paper, we present
FRnet-DTI, an auto encoder and a convolutional classifier for feature
manipulation and drug target interaction prediction. Two convolutional neural
neworks are proposed where one model is used for feature manipulation and the
other one for classification. Using the first method FRnet-1, we generate 4096
features for each of the instances in each of the datasets and use the second
method, FRnet-2, to identify interaction probability employing those features.
We have tested our method on four gold standard datasets exhaustively used by
other researchers. Experimental results shows that our method significantly
improves over the state-of-the-art method on three of the four drug-target
interaction gold standard datasets on both area under curve for Receiver
Operating Characteristic(auROC) and area under Precision Recall curve(auPR)
metric. We also introduce twenty new potential drug-target pairs for
interaction based on high prediction scores. Codes Available: https: // github.
com/ farshidrayhanuiu/ FRnet-DTI/ Web Implementation: http: // farshidrayhan.
pythonanywhere. com/ FRnet-DTI/
</summary>
    <author>
      <name>Farshid Rayhan</name>
    </author>
    <author>
      <name>Sajid Ahmed</name>
    </author>
    <author>
      <name>Zaynab Mousavian</name>
    </author>
    <author>
      <name>Dewan Md Farid</name>
    </author>
    <author>
      <name>Swakkhar Shatabda</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07174v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07174v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09856v2</id>
    <updated>2018-08-31T06:57:05Z</updated>
    <published>2018-01-30T05:47:01Z</published>
    <title>ReNN: Rule-embedded Neural Networks</title>
    <summary>  The artificial neural network shows powerful ability of inference, but it is
still criticized for lack of interpretability and prerequisite needs of big
dataset. This paper proposes the Rule-embedded Neural Network (ReNN) to
overcome the shortages. ReNN first makes local-based inferences to detect local
patterns, and then uses rules based on domain knowledge about the local
patterns to generate rule-modulated map. After that, ReNN makes global-based
inferences that synthesizes the local patterns and the rule-modulated map. To
solve the optimization problem caused by rules, we use a two-stage optimization
strategy to train the ReNN model. By introducing rules into ReNN, we can
strengthen traditional neural networks with long-term dependencies which are
difficult to learn with limited empirical dataset, thus improving inference
accuracy. The complexity of neural networks can be reduced since long-term
dependencies are not modeled with neural connections, and thus the amount of
data needed to optimize the neural networks can be reduced. Besides, inferences
from ReNN can be analyzed with both local patterns and rules, and thus have
better interpretability. In this paper, ReNN has been validated with a
time-series detection problem.
</summary>
    <author>
      <name>Hu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">poster paper in ICPR, 6 pages, 4 figures, and 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09856v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09856v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07276v3</id>
    <updated>2018-08-31T05:35:08Z</updated>
    <published>2018-03-20T07:24:40Z</published>
    <title>Removing Confounding Factors Associated Weights in Deep Neural Networks
  Improves the Prediction Accuracy for Healthcare Applications</title>
    <summary>  The proliferation of healthcare data has brought the opportunities of
applying data-driven approaches, such as machine learning methods, to assist
diagnosis. Recently, many deep learning methods have been shown with impressive
successes in predicting disease status with raw input data. However, the
"black-box" nature of deep learning and the high-reliability requirement of
biomedical applications have created new challenges regarding the existence of
confounding factors. In this paper, with a brief argument that inappropriate
handling of confounding factors will lead to models' sub-optimal performance in
real-world applications, we present an efficient method that can remove the
influences of confounding factors such as age or gender to improve the
across-cohort prediction accuracy of neural networks. One distinct advantage of
our method is that it only requires minimal changes of the baseline model's
architecture so that it can be plugged into most of the existing neural
networks. We conduct experiments across CT-scan, MRA, and EEG brain wave with
convolutional neural networks and LSTM to verify the efficiency of our method.
</summary>
    <author>
      <name>Haohan Wang</name>
    </author>
    <author>
      <name>Zhenglin Wu</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07276v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07276v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10594v1</id>
    <updated>2018-08-31T04:17:44Z</updated>
    <published>2018-08-31T04:17:44Z</published>
    <title>Proximity Forest: An effective and scalable distance-based classifier
  for time series</title>
    <summary>  Research into the classification of time series has made enormous progress in
the last decade. The UCR time series archive has played a significant role in
challenging and guiding the development of new learners for time series
classification. The largest dataset in the UCR archive holds 10 thousand time
series only; which may explain why the primary research focus has been in
creating algorithms that have high accuracy on relatively small datasets.
  This paper introduces Proximity Forest, an algorithm that learns accurate
models from datasets with millions of time series, and classifies a time series
in milliseconds. The models are ensembles of highly randomized Proximity Trees.
Whereas conventional decision trees branch on attribute values (and usually
perform poorly on time series), Proximity Trees branch on the proximity of time
series to one exemplar time series or another; allowing us to leverage the
decades of work into developing relevant measures for time series. Proximity
Forest gains both efficiency and accuracy by stochastic selection of both
exemplars and similarity measures.
  Our work is motivated by recent time series applications that provide orders
of magnitude more time series than the UCR benchmarks. Our experiments
demonstrate that Proximity Forest is highly competitive on the UCR archive: it
ranks among the most accurate classifiers while being significantly faster. We
demonstrate on a 1M time series Earth observation dataset that Proximity Forest
retains this accuracy on datasets that are many orders of magnitude greater
than those in the UCR repository, while learning its models at least 100,000
times faster than current state of the art models Elastic Ensemble and COTE.
</summary>
    <author>
      <name>Benjamin Lucas</name>
    </author>
    <author>
      <name>Ahmed Shifaz</name>
    </author>
    <author>
      <name>Charlotte Pelletier</name>
    </author>
    <author>
      <name>Lachlan O'Neill</name>
    </author>
    <author>
      <name>Nayyar Zaidi</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Francois Petitjean</name>
    </author>
    <author>
      <name>Geoffrey I. Webb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10585v1</id>
    <updated>2018-08-31T03:18:00Z</updated>
    <published>2018-08-31T03:18:00Z</published>
    <title>On the Minimal Supervision for Training Any Binary Classifier from Only
  Unlabeled Data</title>
    <summary>  Empirical risk minimization (ERM), with proper loss function and
regularization, is the common practice of supervised classification. In this
paper, we study training arbitrary (from linear to deep) binary classifier from
only unlabeled (U) data by ERM but not by clustering in the geometric space. A
two-step ERM is considered: first an unbiased risk estimator is designed, and
then the empirical training risk is minimized. This approach is advantageous in
that we can also evaluate the empirical validation risk, which is indispensable
for hyperparameter tuning when some validation data is split from U training
data instead of labeled test data. We prove that designing such an estimator is
impossible given a single set of U data, but it becomes possible given two sets
of U data with different class priors. This answers a fundamental question in
weakly-supervised learning, namely what the minimal supervision is for training
any binary classifier from only U data. Since the proposed learning method is
based on unbiased risk estimates, the asymptotic consistency of the learned
classifier is certainly guaranteed. Experiments demonstrate that the proposed
method could successfully train deep models like ResNet and outperform
state-of-the-art methods for learning from two sets of U data.
</summary>
    <author>
      <name>Nan Lu</name>
    </author>
    <author>
      <name>Gang Niu</name>
    </author>
    <author>
      <name>Aditya K. Menon</name>
    </author>
    <author>
      <name>Masashi Sugiyama</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10568v1</id>
    <updated>2018-08-31T01:55:09Z</updated>
    <published>2018-08-31T01:55:09Z</published>
    <title>Multi-Hop Knowledge Graph Reasoning with Reward Shaping</title>
    <summary>  Multi-hop reasoning is an effective approach for query answering (QA) over
incomplete knowledge graphs (KGs). The problem can be formulated in a
reinforcement learning (RL) setup, where a policy-based agent sequentially
extends its inference path until it reaches a target. However, in an incomplete
KG environment, the agent receives low-quality rewards corrupted by false
negatives in the training data, which harms generalization at test time.
Furthermore, since no golden action sequence is used for training, the agent
can be misled by spurious search trajectories that incidentally lead to the
correct answer. We propose two modeling advances to address both issues: (1) we
reduce the impact of false negative supervision by adopting a pretrained
one-hop embedding model to estimate the reward of unobserved facts; (2) we
counter the sensitivity to spurious paths of on-policy RL by forcing the agent
to explore a diverse set of paths using randomly generated edge masks. Our
approach significantly improves over existing path-based KGQA models on several
benchmark datasets and is comparable or better than embedding-based models.
</summary>
    <author>
      <name>Xi Victoria Lin</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2018, 12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10556v1</id>
    <updated>2018-08-31T00:15:53Z</updated>
    <published>2018-08-31T00:15:53Z</published>
    <title>Speaker Fluency Level Classification Using Machine Learning Techniques</title>
    <summary>  Level assessment for foreign language students is necessary for putting them
in the right level group, furthermore, interviewing students is a very
time-consuming task, so we propose to automate the evaluation of speaker
fluency level by implementing machine learning techniques. This work presents
an audio processing system capable of classifying the level of fluency of
non-native English speakers using five different machine learning models. As a
first step, we have built our own dataset, which consists of labeled audio
conversations in English between people ranging in different fluency
domains/classes (low, intermediate, high). We segment the audio conversations
into 5s non-overlapped audio clips to perform feature extraction on them. We
start by extracting Mel cepstral coefficients from the audios, selecting 20
coefficients is an appropriate quantity for our data. We thereafter extracted
zero-crossing rate, root mean square energy and spectral flux features, proving
that this improves model performance. Out of a total of 1424 audio segments,
with 70% training data and 30% test data, one of our trained models (support
vector machine) achieved a classification accuracy of 94.39%, whereas the other
four models passed an 89% classification accuracy threshold.
</summary>
    <author>
      <name>Alan Preciado-Grijalva</name>
    </author>
    <author>
      <name>Ramon F. Brena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10552v1</id>
    <updated>2018-08-31T00:00:22Z</updated>
    <published>2018-08-31T00:00:22Z</published>
    <title>Directed Exploration in PAC Model-Free Reinforcement Learning</title>
    <summary>  We study an exploration method for model-free RL that generalizes the
counter-based exploration bonus methods and takes into account long term
exploratory value of actions rather than a single step look-ahead. We propose a
model-free RL method that modifies Delayed Q-learning and utilizes the
long-term exploration bonus with provable efficiency. We show that our proposed
method finds a near-optimal policy in polynomial time (PAC-MDP), and also
provide experimental evidence that our proposed algorithm is an efficient
exploration method.
</summary>
    <author>
      <name>Min-hwan Oh</name>
    </author>
    <author>
      <name>Garud Iyengar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10551v1</id>
    <updated>2018-08-30T23:57:19Z</updated>
    <published>2018-08-30T23:57:19Z</published>
    <title>Dynamic mode decomposition in vector-valued reproducing kernel Hilbert
  spaces for extracting dynamical structure among observables</title>
    <summary>  Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety
of engineering and scientific fields. Dynamic mode decomposition (DMD), which
is a numerical algorithm for the spectral analysis of Koopman operators, has
been attracting attention as a way of obtaining global modal descriptions of
NLDSs without requiring explicit prior knowledge. However, since existing DMD
algorithms are in principle formulated based on the concatenation of scalar
observables, it is not directly applicable to data with dependent structures
among observables, which take, for example, the form of a sequence of graphs.
In this paper, we formulate Koopman spectral analysis for NLDSs with structures
among observables and propose an estimation algorithm for this problem. This
method can extract and visualize the underlying low-dimensional global dynamics
of NLDSs with structures among observables from data, which can be useful in
understanding the underlying dynamics of such NLDSs. To this end, we first
formulate the problem of estimating spectra of the Koopman operator defined in
vector-valued reproducing kernel Hilbert spaces, and then develop an estimation
procedure for this problem by reformulating tensor-based DMD. As a special case
of our method, we propose the method named as Graph DMD, which is a numerical
algorithm for Koopman spectral analysis of graph dynamical systems, using a
sequence of adjacency matrices. We investigate the empirical performance of our
method by using synthetic and real-world data.
</summary>
    <author>
      <name>Keisuke Fujii</name>
    </author>
    <author>
      <name>Yoshinobu Kawahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages with 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10549v1</id>
    <updated>2018-08-30T23:48:33Z</updated>
    <published>2018-08-30T23:48:33Z</published>
    <title>Fair Algorithms for Learning in Allocation Problems</title>
    <summary>  Settings such as lending and policing can be modeled by a centralized agent
allocating a resource (loans or police officers) amongst several groups, in
order to maximize some objective (loans given that are repaid or criminals that
are apprehended). Often in such problems fairness is also a concern. A natural
notion of fairness, based on general principles of equality of opportunity,
asks that conditional on an individual being a candidate for the resource, the
probability of actually receiving it is approximately independent of the
individual's group. In lending this means that equally creditworthy individuals
in different racial groups have roughly equal chances of receiving a loan. In
policing it means that two individuals committing the same crime in different
districts would have roughly equal chances of being arrested.
  We formalize this fairness notion for allocation problems and investigate its
algorithmic consequences. Our main technical results include an efficient
learning algorithm that converges to an optimal fair allocation even when the
frequency of candidates (creditworthy individuals or criminals) in each group
is unknown. The algorithm operates in a censored feedback model in which only
the number of candidates who received the resource in a given allocation can be
observed, rather than the true number of candidates. This models the fact that
we do not learn the creditworthiness of individuals we do not give loans to nor
learn about crimes committed if the police presence in a district is low.
  As an application of our framework, we consider the predictive policing
problem. The learning algorithm is trained on arrest data gathered from its own
deployments on previous days, resulting in a potential feedback loop that our
algorithm provably overcomes. We empirically investigate the performance of our
algorithm on the Philadelphia Crime Incidents dataset.
</summary>
    <author>
      <name>Hadi Elzayn</name>
    </author>
    <author>
      <name>Shahin Jabbari</name>
    </author>
    <author>
      <name>Christopher Jung</name>
    </author>
    <author>
      <name>Michael Kearns</name>
    </author>
    <author>
      <name>Seth Neel</name>
    </author>
    <author>
      <name>Aaron Roth</name>
    </author>
    <author>
      <name>Zachary Schutzman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10543v1</id>
    <updated>2018-08-30T22:56:46Z</updated>
    <published>2018-08-30T22:56:46Z</published>
    <title>A Self-Attention Network for Hierarchical Data Structures with an
  Application to Claims Management</title>
    <summary>  Insurance companies must manage millions of claims per year. While most of
these claims are non-fraudulent, fraud detection is core for insurance
companies. The ultimate goal is a predictive model to single out the fraudulent
claims and pay out the non-fraudulent ones immediately. Modern machine learning
methods are well suited for this kind of problem. Health care claims often have
a data structure that is hierarchical and of variable length. We propose one
model based on piecewise feed forward neural networks (deep learning) and
another model based on self-attention neural networks for the task of claim
management. We show that the proposed methods outperform bag-of-words based
models, hand designed features, and models based on convolutional neural
networks, on a data set of two million health care claims. The proposed
self-attention method performs the best.
</summary>
    <author>
      <name>Leander Löw</name>
    </author>
    <author>
      <name>Martin Spindler</name>
    </author>
    <author>
      <name>Eike Brechmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00999v1</id>
    <updated>2018-08-30T22:34:29Z</updated>
    <published>2018-08-30T22:34:29Z</published>
    <title>Towards Large Scale Training Of Autoencoders For Collaborative Filtering</title>
    <summary>  In this paper, we apply a mini-batch based negative sampling method to
efficiently train a latent factor autoencoder model on large scale and sparse
data for implicit feedback collaborative filtering. We compare our work against
a state-of-the-art baseline model on different experimental datasets and show
that this method can lead to a good and fast approximation of the baseline
model performance.
</summary>
    <author>
      <name>Abdallah Moussawi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, ACM RecSys 2018 Posters</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05006v3</id>
    <updated>2018-08-30T21:56:28Z</updated>
    <published>2017-09-14T23:06:19Z</published>
    <title>Two-sample Statistics Based on Anisotropic Kernels</title>
    <summary>  The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)
statistic for measuring the distance between two distributions given
finitely-many multivariate samples. When the distributions are locally
low-dimensional, the proposed test can be made more powerful to distinguish
certain alternatives by incorporating local covariance matrices and
constructing an anisotropic kernel. The kernel matrix is asymmetric; it
computes the affinity between $n$ data points and a set of $n_R$ reference
points, where $n_R$ can be drastically smaller than $n$. While the proposed
statistic can be viewed as a special class of Reproducing Kernel Hilbert Space
MMD, the consistency of the test is proved, under mild assumptions of the
kernel, as long as $\|p-q\| \sqrt{n} \to \infty $, and a finite-sample lower
bound of the testing power is obtained. Applications to flow cytometry and
diffusion MRI datasets are demonstrated, which motivate the proposed approach
to compare distributions.
</summary>
    <author>
      <name>Xiuyuan Cheng</name>
    </author>
    <author>
      <name>Alexander Cloninger</name>
    </author>
    <author>
      <name>Ronald R. Coifman</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05006v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05006v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09321v3</id>
    <updated>2018-08-30T21:54:44Z</updated>
    <published>2018-01-29T00:03:54Z</published>
    <title>Document Image Classification with Intra-Domain Transfer Learning and
  Stacked Generalization of Deep Convolutional Neural Networks</title>
    <summary>  In this work, a region-based Deep Convolutional Neural Network framework is
proposed for document structure learning. The contribution of this work
involves efficient training of region based classifiers and effective
ensembling for document image classification. A primary level of `inter-domain'
transfer learning is used by exporting weights from a pre-trained VGG16
architecture on the ImageNet dataset to train a document classifier on whole
document images. Exploiting the nature of region based influence modelling, a
secondary level of `intra-domain' transfer learning is used for rapid training
of deep learning models for image segments. Finally, stacked generalization
based ensembling is utilized for combining the predictions of the base deep
neural network models. The proposed method achieves state-of-the-art accuracy
of 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks
set by existing algorithms.
</summary>
    <author>
      <name>Arindam Das</name>
    </author>
    <author>
      <name>Saikat Roy</name>
    </author>
    <author>
      <name>Ujjwal Bhattacharya</name>
    </author>
    <author>
      <name>Swapan Kumar Parui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in 24th International Conference in Pattern Recognition
  (ICPR), Beijing, China, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09321v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09321v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10532v1</id>
    <updated>2018-08-30T21:53:06Z</updated>
    <published>2018-08-30T21:53:06Z</published>
    <title>Uniform Inference in High-Dimensional Gaussian Graphical Models</title>
    <summary>  Graphical models have become a very popular tool for representing
dependencies within a large set of variables and are key for representing
causal structures. We provide results for uniform inference on high-dimensional
graphical models with the number of target parameters being possible much
larger than sample size. This is in particular important when certain features
or structures of a causal model should be recovered. Our results highlight how
in high-dimensional settings graphical models can be estimated and recovered
with modern machine learning methods in complex data sets. We also demonstrate
in simulation study that our procedure has good small sample properties.
</summary>
    <author>
      <name>Sven Klaassen</name>
    </author>
    <author>
      <name>Jannis Kück</name>
    </author>
    <author>
      <name>Martin Spindler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 2 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H15, 62J07," scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07384v2</id>
    <updated>2018-08-30T21:33:26Z</updated>
    <published>2018-02-21T00:47:32Z</published>
    <title>Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic
  Corrections</title>
    <summary>  We present a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU activations
to change its output. We argue that such a correction is a useful way to
provide feedback to a user when the network's output is different from a
desired output. Our algorithm generates such a correction by solving a series
of linear constraint satisfaction problems. The technique is evaluated on three
neural network models: one predicting whether an applicant will pay a mortgage,
one predicting whether a first-order theorem can be proved efficiently by a
solver using certain heuristics, and the final one judging whether a drawing is
an accurate rendition of a canonical drawing of a cat.
</summary>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Armando Solar-Lezama</name>
    </author>
    <author>
      <name>Rishabh Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07384v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07384v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07450v2</id>
    <updated>2018-08-30T20:27:20Z</updated>
    <published>2017-06-22T18:18:58Z</published>
    <title>Revised Note on Learning Algorithms for Quadratic Assignment with Graph
  Neural Networks</title>
    <summary>  Inverse problems correspond to a certain type of optimization problems
formulated over appropriate input distributions. Recently, there has been a
growing interest in understanding the computational hardness of these
optimization problems, not only in the worst case, but in an average-complexity
sense under this same input distribution.
  In this revised note, we are interested in studying another aspect of
hardness, related to the ability to learn how to solve a problem by simply
observing a collection of previously solved instances. These 'planted
solutions' are used to supervise the training of an appropriate predictive
model that parametrizes a broad class of algorithms, with the hope that the
resulting model will provide good accuracy-complexity tradeoffs in the average
sense.
  We illustrate this setup on the Quadratic Assignment Problem, a fundamental
problem in Network Science. We observe that data-driven models based on Graph
Neural Networks offer intriguingly good performance, even in regimes where
standard relaxation based techniques appear to suffer.
</summary>
    <author>
      <name>Alex Nowak</name>
    </author>
    <author>
      <name>Soledad Villar</name>
    </author>
    <author>
      <name>Afonso S. Bandeira</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised note to arXiv:1706.07450v1 that appeared in IEEE Data Science
  Workshop 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07450v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07450v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10502v1</id>
    <updated>2018-08-30T20:17:21Z</updated>
    <published>2018-08-30T20:17:21Z</published>
    <title>Data-Driven Debugging for Functional Side Channels</title>
    <summary>  Functional side channels arise when an attacker knows that the secret value
of a server stays fixed for a certain time, and can observe the server executes
on a sequence of different public inputs, each paired with the same secret
input. Thus for each secret, the attackers observe a (partial) function from
public values to (for instance) running time, and they can compare these
functions for different secrets. First, we define a notion of noninterference
for functional side channels. We focus on the case of noisy observations, where
we demonstrate on examples that there is a practical functional side channel in
programs that would be deemed information-leak-free using the standard
definition. Second, we develop a framework and techniques for debugging
programs for functional side channels. We adapt existing results and algorithms
in functional data analysis (such as functional clustering) to discover the
existence of side channels. We use a functional extension of standard decision
tree learning to pinpoint the code fragments causing a side channel if there is
one. Finally, we empirically evaluate the performance of our tool Fuschia on a
series of micro-benchmarks, as well as on realistic Java programs with
thousands of methods. Fuschia is able to discover (and locate in the code)
functional side channels, including one that was since fixed by the original
developers.
</summary>
    <author>
      <name>Saeid Tizpaz-Niari</name>
    </author>
    <author>
      <name>Pavol Cerny</name>
    </author>
    <author>
      <name>Ashutosh Trivedi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Page</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09794v2</id>
    <updated>2018-08-30T19:40:56Z</updated>
    <published>2018-08-29T13:25:11Z</published>
    <title>Correlated Time Series Forecasting using Deep Neural Networks: A Summary
  of Results</title>
    <summary>  Cyber-physical systems often consist of entities that interact with each
other over time. Meanwhile, as part of the continued digitization of industrial
processes, various sensor technologies are deployed that enable us to record
time-varying attributes (a.k.a., time series) of such entities, thus producing
correlated time series. To enable accurate forecasting on such correlated time
series, this paper proposes two models that combine convolutional neural
networks (CNNs) and recurrent neural networks (RNNs). The first model employs a
CNN on each individual time series, combines the convoluted features, and then
applies an RNN on top of the convoluted features in the end to enable
forecasting. The second model adds additional auto-encoders into the individual
CNNs, making the second model a multi-task learning model, which provides
accurate and robust forecasting. Experiments on two real-world correlated time
series data set suggest that the proposed two models are effective and
outperform baselines in most settings.
  This report extends the paper "Correlated Time Series Forecasting using
Multi-Task Deep Neural Networks," to appear in ACM CIKM 2018, by providing
additional experimental results.
</summary>
    <author>
      <name>Razvan-Gabriel Cirstea</name>
    </author>
    <author>
      <name>Darius-Valer Micu</name>
    </author>
    <author>
      <name>Gabriel-Marcel Muresan</name>
    </author>
    <author>
      <name>Chenjuan Guo</name>
    </author>
    <author>
      <name>Bin Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09794v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09794v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10430v1</id>
    <updated>2018-08-30T17:48:23Z</updated>
    <published>2018-08-30T17:48:23Z</published>
    <title>Nested multi-instance classification</title>
    <summary>  There are classification tasks that take as inputs groups of images rather
than single images. In order to address such situations, we introduce a nested
multi-instance deep network. The approach is generic in that it is applicable
to general data instances, not just images. The network has several
convolutional neural networks grouped together at different stages. This
primarily differs from other previous works in that we organize instances into
relevant groups that are treated differently. We also introduce a method to
replace instances that are missing which successfully creates neutral input
instances and consistently outperforms standard fill-in methods in real world
use cases. In addition, we propose a method for manual dropout when a whole
group of instances is missing that allows us to use richer training data and
obtain higher accuracy at the end of training. With specific pretraining, we
find that the model works to great effect on our real world and pub-lic
datasets in comparison to baseline methods, justifying the different treatment
among groups of instances.
</summary>
    <author>
      <name>Alexander Stec</name>
    </author>
    <author>
      <name>Diego Klabjan</name>
    </author>
    <author>
      <name>Jean Utke</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05163v3</id>
    <updated>2018-08-30T17:48:04Z</updated>
    <published>2018-08-15T16:04:28Z</published>
    <title>A Simple but Hard-to-Beat Baseline for Session-based Recommendations</title>
    <summary>  Convolutional Neural Networks (CNNs) models have been recently introduced in
the domain of top-$N$ session-based recommendations. An ordered collection of
past items the user has interacted with in a session (or sequence) are embedded
into a 2-dimensional latent matrix, and treated as an image. The convolution
and pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we propose
a simple, but very effective generative model that is capable of learning
high-level representation from both short- and long-range dependencies. The
network architecture of the proposed model is formed of a stack of holed
convolutional layers, which can efficiently increase the receptive fields
without relying on the pooling operation. Another contribution is the effective
use of residual block structure in recommender systems, which can ease the
optimization for much deeper networks. The proposed generative model attains
state-of-the-art accuracy with less training time in the session-based
recommendation task. It accordingly can be used as a powerful session-based
recommendation baseline to beat in future, especially when there are long
sequences of user feedback.
</summary>
    <author>
      <name>Fajie Yuan</name>
    </author>
    <author>
      <name>Alexandros Karatzoglou</name>
    </author>
    <author>
      <name>Ioannis Arapakis</name>
    </author>
    <author>
      <name>Joemon M Jose</name>
    </author>
    <author>
      <name>Xiangnan He</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05163v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05163v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10406v1</id>
    <updated>2018-08-30T17:25:48Z</updated>
    <published>2018-08-30T17:25:48Z</published>
    <title>Towards Reproducible Empirical Research in Meta-Learning</title>
    <summary>  Meta-learning is increasingly used to support the recommendation of machine
learning algorithms and their configurations. Such recommendations are made
based on meta-data, consisting of performance evaluations of algorithms on
prior datasets, as well as characterizations of these datasets. These
characterizations, also called meta-features, describe properties of the data
which are predictive for the performance of machine learning algorithms trained
on them. Unfortunately, despite being used in a large number of studies,
meta-features are not uniformly described and computed, making many empirical
studies irreproducible and hard to compare. This paper aims to remedy this by
systematizing and standardizing data characterization measures used in
meta-learning, and performing an in-depth analysis of their utility. Moreover,
it presents MFE, a new tool for extracting meta-features from datasets and
identify more subtle reproducibility issues in the literature, proposing
guidelines for data characterization that strengthen reproducible empirical
research in meta-learning.
</summary>
    <author>
      <name>Adriano Rivolli</name>
    </author>
    <author>
      <name>Luís P. F. Garcia</name>
    </author>
    <author>
      <name>Carlos Soares</name>
    </author>
    <author>
      <name>Joaquin Vanschoren</name>
    </author>
    <author>
      <name>André C. P. L. F. de Carvalho</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10396v1</id>
    <updated>2018-08-30T17:00:03Z</updated>
    <published>2018-08-30T17:00:03Z</published>
    <title>A Unified Analysis of Stochastic Momentum Methods for Deep Learning</title>
    <summary>  Stochastic momentum methods have been widely adopted in training deep neural
networks. However, their theoretical analysis of convergence of the training
objective and the generalization error for prediction is still under-explored.
This paper aims to bridge the gap between practice and theory by analyzing the
stochastic gradient (SG) method, and the stochastic momentum methods including
two famous variants, i.e., the stochastic heavy-ball (SHB) method and the
stochastic variant of Nesterov's accelerated gradient (SNAG) method. We propose
a framework that unifies the three variants. We then derive the convergence
rates of the norm of gradient for the non-convex optimization problem, and
analyze the generalization performance through the uniform stability approach.
Particularly, the convergence analysis of the training objective exhibits that
SHB and SNAG have no advantage over SG. However, the stability analysis shows
that the momentum term can improve the stability of the learned model and hence
improve the generalization performance. These theoretical insights verify the
common wisdom and are also corroborated by our empirical analysis on deep
learning.
</summary>
    <author>
      <name>Yan Yan</name>
    </author>
    <author>
      <name>Tianbao Yang</name>
    </author>
    <author>
      <name>Zhe Li</name>
    </author>
    <author>
      <name>Qihang Lin</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Previous Technical Report: arXiv:1604.03257</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In IJCAI, pp. 2955-2961. 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.10396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10393v1</id>
    <updated>2018-08-30T16:46:22Z</updated>
    <published>2018-08-30T16:46:22Z</published>
    <title>Learning End-to-end Autonomous Driving using Guided Auxiliary
  Supervision</title>
    <summary>  Learning to drive faithfully in highly stochastic urban settings remains an
open problem. To that end, we propose a Multi-task Learning from Demonstration
(MT-LfD) framework which uses supervised auxiliary task prediction to guide the
main task of predicting the driving commands. Our framework involves an
end-to-end trainable network for imitating the expert demonstrator's driving
commands. The network intermediately predicts visual affordances and action
primitives through direct supervision which provide the aforementioned
auxiliary supervised guidance. We demonstrate that such joint learning and
supervised guidance facilitates hierarchical task decomposition, assisting the
agent to learn faster, achieve better driving performance and increases
transparency of the otherwise black-box end-to-end network. We run our
experiments to validate the MT-LfD framework in CARLA, an open-source urban
driving simulator. We introduce multiple non-player agents in CARLA and induce
temporal noise in them for realistic stochasticity.
</summary>
    <author>
      <name>Ashish Mehta</name>
    </author>
    <author>
      <name>Adithya Subramanian</name>
    </author>
    <author>
      <name>Anbumani Subramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06827v2</id>
    <updated>2018-08-30T16:44:24Z</updated>
    <published>2018-06-18T17:04:04Z</published>
    <title>PAC-Bayes bounds for stable algorithms with instance-dependent priors</title>
    <summary>  PAC-Bayes bounds have been proposed to get risk estimates based on a training
sample. In this paper the PAC-Bayes approach is combined with stability of the
hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting
is used with a Gaussian prior centered at the expected output. Thus a novelty
of our paper is using priors defined in terms of the data-generating
distribution. Our main result estimates the risk of the randomized algorithm in
terms of the hypothesis stability coefficients. We also provide a new bound for
the SVM classifier, which is compared to other known bounds experimentally.
Ours appears to be the first stability-based bound that evaluates to
non-trivial values.
</summary>
    <author>
      <name>Omar Rivasplata</name>
    </author>
    <author>
      <name>Emilio Parrado-Hernandez</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <author>
      <name>Shiliang Sun</name>
    </author>
    <author>
      <name>Csaba Szepesvari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, discussion of theory and experiments in the main body,
  detailed proofs and experimental details in the appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06827v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06827v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00977v1</id>
    <updated>2018-08-30T16:41:58Z</updated>
    <published>2018-08-30T16:41:58Z</published>
    <title>DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal
  Convolutional Autoencoders</title>
    <summary>  Human falls rarely occur; however, detecting falls is very important from the
health and safety perspective. Due to the rarity of falls, it is difficult to
employ supervised classification techniques to detect them. Moreover, in these
highly skewed situations it is also difficult to extract domain specific
features to identify falls. In this paper, we present a novel framework,
\textit{DeepFall}, which formulates the fall detection problem as an anomaly
detection problem. The \textit{DeepFall} framework presents the novel use of
deep spatio-temporal convolutional autoencoders to learn spatial and temporal
features from normal activities using non-invasive sensing modalities. We also
present a new anomaly scoring method that combines the reconstruction score of
frames across a video sequences to detect unseen falls. We tested the
\textit{DeepFall} framework on three publicly available datasets collected
through non-invasive sensing modalities, thermal camera and depth cameras and
show superior results in comparison to traditional autoencoder and
convolutional autoencoder methods to identify unseen falls.
</summary>
    <author>
      <name>Jacob Nogas</name>
    </author>
    <author>
      <name>Shehroz S. Khan</name>
    </author>
    <author>
      <name>Alex Mihailidis</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08294v3</id>
    <updated>2018-08-30T16:29:50Z</updated>
    <published>2017-09-25T02:29:26Z</published>
    <title>Learning Context-Sensitive Convolutional Filters for Text Processing</title>
    <summary>  Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.
</summary>
    <author>
      <name>Dinghan Shen</name>
    </author>
    <author>
      <name>Martin Renqiang Min</name>
    </author>
    <author>
      <name>Yitong Li</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP 2018 as a full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.08294v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08294v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10369v1</id>
    <updated>2018-08-30T15:55:42Z</updated>
    <published>2018-08-30T15:55:42Z</published>
    <title>Robot_gym: accelerated robot training through simulation in the cloud
  with ROS and Gazebo</title>
    <summary>  Rather than programming, training allows robots to achieve behaviors that
generalize better and are capable to respond to real-world needs. However, such
training requires a big amount of experimentation which is not always feasible
for a physical robot. In this work, we present robot_gym, a framework to
accelerate robot training through simulation in the cloud that makes use of
roboticists' tools, simplifying the development and deployment processes on
real robots. We unveil that, for simple tasks, simple 3DoF robots require more
than 140 attempts to learn. For more complex, 6DoF robots, the number of
attempts increases to more than 900 for the same task. We demonstrate that our
framework, for simple tasks, accelerates the robot training time by more than
33% while maintaining similar levels of accuracy and repeatability.
</summary>
    <author>
      <name>Víctor Mayoral Vilches</name>
    </author>
    <author>
      <name>Alejandro Hernández Cordero</name>
    </author>
    <author>
      <name>Asier Bilbao Calvo</name>
    </author>
    <author>
      <name>Irati Zamalloa Ugarte</name>
    </author>
    <author>
      <name>Risto Kojcev</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10356v1</id>
    <updated>2018-08-30T15:32:40Z</updated>
    <published>2018-08-30T15:32:40Z</published>
    <title>Gaussian Mixture Generative Adversarial Networks for Diverse Datasets,
  and the Unsupervised Clustering of Images</title>
    <summary>  Generative Adversarial Networks (GANs) have been shown to produce
realistically looking synthetic images with remarkable success, yet their
performance seems less impressive when the training set is highly diverse. In
order to provide a better fit to the target data distribution when the dataset
includes many different classes, we propose a variant of the basic GAN model,
called Gaussian Mixture GAN (GM-GAN), where the probability distribution over
the latent space is a mixture of Gaussians. We also propose a supervised
variant which is capable of conditional sample synthesis. In order to evaluate
the model's performance, we propose a new scoring method which separately takes
into account two (typically conflicting) measures - diversity vs. quality of
the generated data. Through a series of empirical experiments, using both
synthetic and real-world datasets, we quantitatively show that GM-GANs
outperform baselines, both when evaluated using the commonly used Inception
Score, and when evaluated using our own alternative scoring method. In
addition, we qualitatively demonstrate how the \textit{unsupervised} variant of
GM-GAN tends to map latent vectors sampled from different Gaussians in the
latent space to samples of different classes in the data space. We show how
this phenomenon can be exploited for the task of unsupervised clustering, and
provide quantitative evaluation showing the superiority of our method for the
unsupervised clustering of image datasets. Finally, we demonstrate a feature
which further sets our model apart from other GAN models: the option to control
the quality-diversity trade-off by altering, post-training, the probability
distribution of the latent space. This allows one to sample higher quality and
lower diversity samples, or vice versa, according to one's needs.
</summary>
    <author>
      <name>Matan Ben-Yosef</name>
    </author>
    <author>
      <name>Daphna Weinshall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10350v1</id>
    <updated>2018-08-30T15:24:20Z</updated>
    <published>2018-08-30T15:24:20Z</published>
    <title>IEA: Inner Ensemble Average within a convolutional neural network</title>
    <summary>  Ensemble learning is a method of combining multiple trained models to improve
the model accuracy. We introduce the usage of such methods, specifically
ensemble average inside Convolutional Neural Networks (CNNs) architectures. By
Inner Average Ensemble (IEA) of multiple convolutional neural layers (CNLs)
replacing the single CNLs inside the CNN architecture, the accuracy of the CNN
increased. A visual and a similarity score analysis of the features generated
from IEA explains why it boosts the model performance. Empirical results using
different benchmarking datasets and well-known deep model architectures shows
that IEA outperforms the ordinary CNL used in CNNs.
</summary>
    <author>
      <name>Abduallah A. Mohamed</name>
    </author>
    <author>
      <name>Christian Claudel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10340v1</id>
    <updated>2018-08-30T15:06:03Z</updated>
    <published>2018-08-30T15:06:03Z</published>
    <title>A Coordinate-Free Construction of Scalable Natural Gradient</title>
    <summary>  Most neural networks are trained using first-order optimization methods,
which are sensitive to the parameterization of the model. Natural gradient
descent is invariant to smooth reparameterizations because it is defined in a
coordinate-free way, but tractable approximations are typically defined in
terms of coordinate systems, and hence may lose the invariance properties. We
analyze the invariance properties of the Kronecker-Factored Approximate
Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free
way. We explicitly construct a Riemannian metric under which the natural
gradient matches the K-FAC update; invariance to affine transformations of the
activations follows immediately. We extend our framework to analyze the
invariance properties of K-FAC applied to convolutional networks and recurrent
neural networks, as well as metrics other than the usual Fisher metric.
</summary>
    <author>
      <name>Kevin Luk</name>
    </author>
    <author>
      <name>Roger Grosse</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10322v1</id>
    <updated>2018-08-30T14:45:26Z</updated>
    <published>2018-08-30T14:45:26Z</published>
    <title>PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local
  Descriptors</title>
    <summary>  We present PPF-FoldNet for unsupervised learning of 3D local descriptors on
pure point cloud geometry. Based on the folding-based auto-encoding of well
known point pair features, PPF-FoldNet offers many desirable properties: it
necessitates neither supervision, nor a sensitive local reference frame,
benefits from point-set sparsity, is end-to-end, fast, and can extract powerful
rotation invariant descriptors. Thanks to a novel feature visualization, its
evolution can be monitored to provide interpretable insights. Our extensive
experiments demonstrate that despite having six degree-of-freedom invariance
and lack of training labels, our network achieves state of the art results in
standard benchmark datasets and outperforms its competitors when rotations and
varying point densities are present. PPF-FoldNet achieves $9\%$ higher recall
on standard benchmarks, $23\%$ higher recall when rotations are introduced into
the same datasets and finally, a margin of $&gt;35\%$ is attained when point
density is significantly decreased.
</summary>
    <author>
      <name>Haowen Deng</name>
    </author>
    <author>
      <name>Tolga Birdal</name>
    </author>
    <author>
      <name>Slobodan Ilic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10307v1</id>
    <updated>2018-08-30T14:13:39Z</updated>
    <published>2018-08-30T14:13:39Z</published>
    <title>Backdoor Embedding in Convolutional Neural Network Models via Invisible
  Perturbation</title>
    <summary>  Deep learning models have consistently outperformed traditional machine
learning models in various classification tasks, including image
classification. As such, they have become increasingly prevalent in many real
world applications including those where security is of great concern. Such
popularity, however, may attract attackers to exploit the vulnerabilities of
the deployed deep learning models and launch attacks against security-sensitive
applications. In this paper, we focus on a specific type of data poisoning
attack, which we refer to as a {\em backdoor injection attack}. The main goal
of the adversary performing such attack is to generate and inject a backdoor
into a deep learning model that can be triggered to recognize certain embedded
patterns with a target label of the attacker's choice. Additionally, a backdoor
injection attack should occur in a stealthy manner, without undermining the
efficacy of the victim model. Specifically, we propose two approaches for
generating a backdoor that is hardly perceptible yet effective in poisoning the
model. We consider two attack settings, with backdoor injection carried out
either before model training or during model updating. We carry out extensive
experimental evaluations under various assumptions on the adversary model, and
demonstrate that such attacks can be effective and achieve a high attack
success rate (above $90\%$) at a small cost of model accuracy loss (below
$1\%$) with a small injection rate (around $1\%$), even under the weakest
assumption wherein the adversary has no knowledge either of the original
training data or the classifier model.
</summary>
    <author>
      <name>Cong Liao</name>
    </author>
    <author>
      <name>Haoti Zhong</name>
    </author>
    <author>
      <name>Anna Squicciarini</name>
    </author>
    <author>
      <name>Sencun Zhu</name>
    </author>
    <author>
      <name>David Miller</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01498v1</id>
    <updated>2018-08-30T13:54:45Z</updated>
    <published>2018-08-30T13:54:45Z</published>
    <title>Skip-gram word embeddings in hyperbolic space</title>
    <summary>  Embeddings of tree-like graphs in hyperbolic space were recently shown to
surpass their Euclidean counterparts in performance by a large margin. Inspired
by these results, we present an algorithm for learning word embeddings in
hyperbolic space from free text. An objective function based on the hyperbolic
distance is derived and included in the skip-gram architecture from word2vec.
The hyperbolic word embeddings are then evaluated on word similarity and
analogy benchmarks. The results demonstrate the potential of hyperbolic word
embeddings, particularly in low dimensions, though without clear superiority
over their Euclidean counterparts. We further discuss problems in the
formulation of the analogy task resulting from the curvature of hyperbolic
space.
</summary>
    <author>
      <name>Matthias Leimeister</name>
    </author>
    <author>
      <name>Benjamin J. Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04587v3</id>
    <updated>2018-08-30T13:45:40Z</updated>
    <published>2017-04-15T05:33:32Z</published>
    <title>Deep Learning for Photoacoustic Tomography from Sparse Data</title>
    <summary>  The development of fast and accurate image reconstruction algorithms is a
central aspect of computed tomography. In this paper, we investigate this issue
for the sparse data problem in photoacoustic tomography (PAT). We develop a
direct and highly efficient reconstruction algorithm based on deep learning. In
our approach image reconstruction is performed with a deep convolutional neural
network (CNN), whose weights are adjusted prior to the actual image
reconstruction based on a set of training data. The proposed reconstruction
approach can be interpreted as a network that uses the PAT filtered
backprojection algorithm for the first layer, followed by the U-net
architecture for the remaining layers. Actual image reconstruction with deep
learning consists in one evaluation of the trained CNN, which does not require
time consuming solution of the forward and adjoint problems. At the same time,
our numerical results demonstrate that the proposed deep learning approach
reconstructs images with a quality comparable to state of the art iterative
approaches for PAT from sparse data.
</summary>
    <author>
      <name>Stephan Antholzer</name>
    </author>
    <author>
      <name>Markus Haltmeier</name>
    </author>
    <author>
      <name>Johannes Schwab</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04587v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04587v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01000v1</id>
    <updated>2018-08-30T12:36:36Z</updated>
    <published>2018-08-30T12:36:36Z</published>
    <title>Bayesian Outdoor Defect Detection</title>
    <summary>  We introduce a Bayesian defect detector to facilitate the defect detection on
the motion blurred images on rough texture surfaces. To enhance the accuracy of
Bayesian detection on removing non-defect pixels, we develop a class of
reflected non-local prior distributions, which is constructed by using the mode
of a distribution to subtract its density. The reflected non-local priors
forces the Bayesian detector to approach 0 at the non-defect locations. We
conduct experiments studies to demonstrate the superior performance of the
Bayesian detector in eliminating the non-defect points. We implement the
Bayesian detector in the motion blurred drone images, in which the detector
successfully identifies the hail damages on the rough surface and substantially
enhances the accuracy of the entire defect detection pipeline.
</summary>
    <author>
      <name>Fei Jiang</name>
    </author>
    <author>
      <name>Guosheng Yin</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06791v2</id>
    <updated>2018-08-30T12:33:22Z</updated>
    <published>2018-08-21T07:45:10Z</published>
    <title>LRMM: Learning to Recommend with Missing Modalities</title>
    <summary>  Multimodal learning has shown promising performance in content-based
recommendation due to the auxiliary user and item information of multiple
modalities such as text and images. However, the problem of incomplete and
missing modality is rarely explored and most existing methods fail in learning
a recommendation model with missing or corrupted modalities. In this paper, we
propose LRMM, a novel framework that mitigates not only the problem of missing
modalities but also more generally the cold-start problem of recommender
systems. We propose modality dropout (m-drop) and a multimodal sequential
autoencoder (m-auto) to learn multimodal representations for complementing and
imputing missing modalities. Extensive experiments on real-world Amazon data
show that LRMM achieves state-of-the-art performance on rating prediction
tasks. More importantly, LRMM is more robust to previous methods in alleviating
data-sparsity and the cold-start problem.
</summary>
    <author>
      <name>Cheng Wang</name>
    </author>
    <author>
      <name>Mathias Niepert</name>
    </author>
    <author>
      <name>Hui Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06791v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06791v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00947v1</id>
    <updated>2018-08-30T11:30:19Z</updated>
    <published>2018-08-30T11:30:19Z</published>
    <title>Finding Dory in the Crowd: Detecting Social Interactions using
  Multi-Modal Mobile Sensing</title>
    <summary>  Remembering our day-to-day social interactions is challenging even if you
aren't a blue memory challenged fish. The ability to automatically detect and
remember these types of interactions is not only beneficial for individuals
interested in their behavior in crowded situations, but also of interest to
those who analyze crowd behavior. Currently, detecting social interactions is
often performed using a variety of methods including ethnographic studies,
computer vision techniques and manual annotation-based data analysis. However,
mobile phones offer easier means for data collection that is easy to analyze
and can preserve the user's privacy. In this work, we present a system for
detecting stationary social interactions inside crowds, leveraging multi-modal
mobile sensing data such as Bluetooth Smart (BLE), accelerometer and gyroscope.
To inform the development of such system, we conducted a study with 24
participants, where we asked them to socialize with each other for 45 minutes.
We built a machine learning system based on gradient-boosted trees that
predicts both 1:1 and group interactions with 77.8% precision and 86.5% recall,
a 30.2% performance increase compared to a proximity-based approach. By
utilizing a community detection based method, we further detected the various
group formation that exist within the crowd. Using mobile phone sensors already
carried by the majority of people in a crowd makes our approach particularly
well suited to real-life analysis of crowd behaviour and influence strategies.
</summary>
    <author>
      <name>Kleomenis Katevas</name>
    </author>
    <author>
      <name>Katrin Hänsel</name>
    </author>
    <author>
      <name>Richard Clegg</name>
    </author>
    <author>
      <name>Ilias Leontiadis</name>
    </author>
    <author>
      <name>Hamed Haddadi</name>
    </author>
    <author>
      <name>Laurissa Tokarchuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10442v1</id>
    <updated>2018-08-30T11:26:59Z</updated>
    <published>2018-08-30T11:26:59Z</published>
    <title>Application of Self-Play Reinforcement Learning to a Four-Player Game of
  Imperfect Information</title>
    <summary>  We introduce a new virtual environment for simulating a card game known as
"Big 2". This is a four-player game of imperfect information with a relatively
complicated action space (being allowed to play 1,2,3,4 or 5 card combinations
from an initial starting hand of 13 cards). As such it poses a challenge for
many current reinforcement learning methods. We then use the recently proposed
"Proximal Policy Optimization" algorithm to train a deep neural network to play
the game, purely learning via self-play, and find that it is able to reach a
level which outperforms amateur human players after only a relatively short
amount of training time and without needing to search a tree of future game
states.
</summary>
    <author>
      <name>Henry Charlesworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages + 7 pages SI, 5 figures in total</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02960v2</id>
    <updated>2018-08-30T09:53:41Z</updated>
    <published>2018-04-06T11:42:06Z</published>
    <title>Analysis and development of a novel algorithm for the in-vehicle
  hand-usage of a smartphone</title>
    <summary>  Smartphone usage while driving is unanimously considered to be a really
dangerous habit due to strong correlation with road accidents. In this paper,
the problem of detecting whether the driver is using the phone during a trip is
addressed. To do this, high-frequency data from the triaxial inertial
measurement unit (IMU) integrated in almost all modern phone is processed
without relying on external inputs so as to provide a self-contained approach.
By resorting to a frequency-domain analysis, it is possible to extract from the
raw signals the useful information needed to detect when the driver is using
the phone, without being affected by the effects that vehicle motion has on the
same signals. The selected features are used to train a Support Vector Machine
(SVM) algorithm. The performance of the proposed approach are analyzed and
tested on experimental data collected during mixed naturalistic driving
scenarios, proving the effectiveness of the proposed approach.
</summary>
    <author>
      <name>Simone Gelmini</name>
    </author>
    <author>
      <name>Silvia Strada</name>
    </author>
    <author>
      <name>Mara Tanelli</name>
    </author>
    <author>
      <name>Sergio Savaresi</name>
    </author>
    <author>
      <name>Vincenzo Biase</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02960v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02960v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09902v2</id>
    <updated>2018-08-30T07:51:57Z</updated>
    <published>2018-08-29T16:07:26Z</published>
    <title>Extreme Value Theory for Open Set Classification - GPD and GEV
  Classifiers</title>
    <summary>  Classification tasks usually assume that all possible classes are present
during the training phase. This is restrictive if the algorithm is used over a
long time and possibly encounters samples from unknown classes. The recently
introduced extreme value machine, a classifier motivated by extreme value
theory, addresses this problem and achieves competitive performance in specific
cases. We show that this algorithm can fail when the geometries of known and
unknown classes differ. To overcome this problem, we propose two new algorithms
relying on approximations from extreme value theory. We show the effectiveness
of our classifiers in simulations and on the LETTER and MNIST data sets.
</summary>
    <author>
      <name>Edoardo Vignotto</name>
    </author>
    <author>
      <name>Sebastian Engelke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10151v1</id>
    <updated>2018-08-30T07:21:16Z</updated>
    <published>2018-08-30T07:21:16Z</published>
    <title>VirtualIdentity: Privacy-Preserving User Profiling</title>
    <summary>  User profiling from user generated content (UGC) is a common practice that
supports the business models of many social media companies. Existing systems
require that the UGC is fully exposed to the module that constructs the user
profiles. In this paper we show that it is possible to build user profiles
without ever accessing the user's original data, and without exposing the
trained machine learning models for user profiling -- which are the
intellectual property of the company -- to the users of the social media site.
We present VirtualIdentity, an application that uses secure multi-party
cryptographic protocols to detect the age, gender and personality traits of
users by classifying their user-generated text and personal pictures with
trained support vector machine models in a privacy-preserving manner.
</summary>
    <author>
      <name>Sisi Wang</name>
    </author>
    <author>
      <name>Wing-Sea Poon</name>
    </author>
    <author>
      <name>Golnoosh Farnadi</name>
    </author>
    <author>
      <name>Caleb Horst</name>
    </author>
    <author>
      <name>Kebra Thompson</name>
    </author>
    <author>
      <name>Michael Nickels</name>
    </author>
    <author>
      <name>Rafael Dowsley</name>
    </author>
    <author>
      <name>Anderson C. A. Nascimento</name>
    </author>
    <author>
      <name>Martine De Cock</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10134v1</id>
    <updated>2018-08-30T06:29:10Z</updated>
    <published>2018-08-30T06:29:10Z</published>
    <title>Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and
  Learning based Vehicle Longitude Dynamic Calibrating Algorithm</title>
    <summary>  For any autonomous driving vehicle, control module determines its road
performance and safety, i.e. its precision and stability should stay within a
carefully-designed range. Nonetheless, control algorithms require vehicle
dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are
obscure to calibrate in real time. As a result, to achieve reasonable
performance, most, if not all, research-oriented autonomous vehicles do manual
calibrations in a one-by-one fashion. Since manual calibration is not
sustainable once entering into mass production stage for industrial purposes,
we here introduce a machine-learning based auto-calibration system for
autonomous driving vehicles. In this paper, we will show how we build a
data-driven longitudinal calibration procedure using machine learning
techniques. We first generated offline calibration tables from human driving
data. The offline table serves as an initial guess for later uses and it only
needs twenty-minutes data collection and process. We then used an
online-learning algorithm to appropriately update the initial table (the
offline table) based on real-time performance analysis. This longitudinal
auto-calibration system has been deployed to more than one hundred Baidu Apollo
self-driving vehicles (including hybrid family vehicles and electronic
delivery-only vehicles) since April 2018. By August 27, 2018, it had been
tested for more than two thousands hours, ten thousands kilometers (6,213
miles) and yet proven to be effective.
</summary>
    <author>
      <name>Fan Zhu</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <author>
      <name>Xin Xu</name>
    </author>
    <author>
      <name>Dingfeng Guo</name>
    </author>
    <author>
      <name>Xiao Cui</name>
    </author>
    <author>
      <name>Qi Kong</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10128v1</id>
    <updated>2018-08-30T05:51:30Z</updated>
    <published>2018-08-30T05:51:30Z</published>
    <title>Semi-Supervised Training for Improving Data Efficiency in End-to-End
  Speech Synthesis</title>
    <summary>  Although end-to-end text-to-speech (TTS) models such as Tacotron have shown
excellent results, they typically require a sizable set of high-quality &lt;text,
audio&gt; pairs for training, which are expensive to collect. In this paper, we
propose a semi-supervised training framework to improve the data efficiency of
Tacotron. The idea is to allow Tacotron to utilize textual and acoustic
knowledge contained in large, publicly-available text and speech corpora.
Importantly, these external data are unpaired and potentially noisy.
Specifically, first we embed each word in the input text into word vectors and
condition the Tacotron encoder on them. We then use an unpaired speech corpus
to pre-train the Tacotron decoder in the acoustic domain. Finally, we fine-tune
the model using available paired data. We demonstrate that the proposed
framework enables Tacotron to generate intelligible speech using less than half
an hour of paired training data.
</summary>
    <author>
      <name>Yu-An Chung</name>
    </author>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>Wei-Ning Hsu</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>RJ Skerry-Ryan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10122v1</id>
    <updated>2018-08-30T05:15:42Z</updated>
    <published>2018-08-30T05:15:42Z</published>
    <title>Learning Neural Templates for Text Generation</title>
    <summary>  While neural, encoder-decoder models have had significant empirical success
in text generation, there remain several unaddressed problems with this style
of generation. Encoder-decoder models are largely (a) uninterpretable, and (b)
difficult to control in terms of their phrasing or content. This work proposes
a neural generation system using a hidden semi-markov model (HSMM) decoder,
which learns latent, discrete templates jointly with learning to generate. We
show that this model learns useful templates, and that these templates make
generation both more interpretable and controllable. Furthermore, we show that
this approach scales to real data sets and achieves strong performance nearing
that of encoder-decoder text generation models.
</summary>
    <author>
      <name>Sam Wiseman</name>
    </author>
    <author>
      <name>Stuart M. Shieber</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10120v1</id>
    <updated>2018-08-30T05:04:44Z</updated>
    <published>2018-08-30T05:04:44Z</published>
    <title>ExpIt-OOS: Towards Learning from Planning in Imperfect Information Games</title>
    <summary>  The current state of the art in playing many important perfect information
games, including Chess and Go, combines planning and deep reinforcement
learning with self-play. We extend this approach to imperfect information games
and present ExIt-OOS, a novel approach to playing imperfect information games
within the Expert Iteration framework and inspired by AlphaZero. We use Online
Outcome Sampling, an online search algorithm for imperfect information games in
place of MCTS. While training online, our neural strategy is used to improve
the accuracy of playouts in OOS, allowing a learning and planning feedback loop
for imperfect information games.
</summary>
    <author>
      <name>Andy Kitchen</name>
    </author>
    <author>
      <name>Michela Benedetti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. 1 figure, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10238v2</id>
    <updated>2018-08-30T02:04:16Z</updated>
    <published>2018-02-28T02:39:02Z</published>
    <title>DeepSOFA: A Continuous Acuity Score for Critically Ill Patients using
  Clinically Interpretable Deep Learning</title>
    <summary>  Traditional methods for assessing illness severity and predicting in-hospital
mortality among critically ill patients require time-consuming, error-prone
calculations using static variable thresholds. These methods do not capitalize
on the emerging availability of streaming electronic health record data or
capture time-sensitive individual physiological patterns, a critical task in
the intensive care unit. We propose a novel acuity score framework (DeepSOFA)
that leverages temporal measurements and interpretable deep learning models to
assess illness severity at any point during an ICU stay. We compare DeepSOFA
with SOFA (Sequential Organ Failure Assessment) baseline models using the same
model inputs and find that at any point during an ICU admission, DeepSOFA
yields significantly more accurate predictions of in-hospital mortality. A
DeepSOFA model developed in a public database and validated in a single
institutional cohort had a mean AUC for the entire ICU stay of 0.90 (95% CI
0.90-0.91) compared with baseline SOFA models with mean AUC 0.79 (95% CI
0.79-0.80) and 0.85 (95% CI 0.85-0.86). Deep models are well-suited to identify
ICU patients in need of life-saving interventions prior to the occurrence of an
unexpected adverse event and inform shared decision-making processes among
patients, providers, and families regarding goals of care and optimal resource
utilization.
</summary>
    <author>
      <name>Benjamin Shickel</name>
    </author>
    <author>
      <name>Tyler J. Loftus</name>
    </author>
    <author>
      <name>Lasith Adhikari</name>
    </author>
    <author>
      <name>Tezcan Ozrazgat-Baslanti</name>
    </author>
    <author>
      <name>Azra Bihorac</name>
    </author>
    <author>
      <name>Parisa Rashidi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Currently under review with Scientific Reports</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10238v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10238v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03737v2</id>
    <updated>2018-08-30T01:31:31Z</updated>
    <published>2018-08-11T01:58:19Z</published>
    <title>Learning Multi-touch Conversion Attribution with Dual-attention
  Mechanisms for Online Advertising</title>
    <summary>  In online advertising, the Internet users may be exposed to a sequence of
different ad campaigns, i.e., display ads, search, or referrals from multiple
channels, before led up to any final sales conversion and transaction. For both
campaigners and publishers, it is fundamentally critical to estimate the
contribution from ad campaign touch-points during the customer journey
(conversion funnel) and assign the right credit to the right ad exposure
accordingly. However, the existing research on the multi-touch attribution
problem lacks a principled way of utilizing the users' pre-conversion actions
(i.e., clicks), and quite often fails to model the sequential patterns among
the touch points from a user's behavior data. To make it worse, the current
industry practice is merely employing a set of arbitrary rules as the
attribution model, e.g., the popular last-touch model assigns 100% credit to
the final touch-point regardless of actual attributions. In this paper, we
propose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch
attribution problem. It learns the attribution values through an attention
mechanism directly from the conversion estimation objective. To achieve this,
we utilize sequence-to-sequence prediction for user clicks, and combine both
post-view and post-click attribution patterns together for the final conversion
estimation. To quantitatively benchmark attribution models, we also propose a
novel yet practical attribution evaluation scheme through the proxy of budget
allocation (under the estimated attributions) over ad channels. The
experimental results on two real datasets demonstrate the significant
performance gains of our attribution model against the state of the art.
</summary>
    <author>
      <name>Kan Ren</name>
    </author>
    <author>
      <name>Yuchen Fang</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Shuhao Liu</name>
    </author>
    <author>
      <name>Jiajun Li</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <author>
      <name>Yong Yu</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271677</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271677" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures; CIKM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03737v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03737v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10078v1</id>
    <updated>2018-08-30T01:05:40Z</updated>
    <published>2018-08-30T01:05:40Z</published>
    <title>Discriminative Learning of Similarity and Group Equivariant
  Representations</title>
    <summary>  One of the most fundamental problems in machine learning is to compare
examples: Given a pair of objects we want to return a value which indicates
degree of (dis)similarity. Similarity is often task specific, and pre-defined
distances can perform poorly, leading to work in metric learning. However,
being able to learn a similarity-sensitive distance function also presupposes
access to a rich, discriminative representation for the objects at hand. In
this dissertation we present contributions towards both ends. In the first part
of the thesis, assuming good representations for the data, we present a
formulation for metric learning that makes a more direct attempt to optimize
for the k-NN accuracy as compared to prior work. We also present extensions of
this formulation to metric learning for kNN regression, asymmetric similarity
learning and discriminative learning of Hamming distance. In the second part,
we consider a situation where we are on a limited computational budget i.e.
optimizing over a space of possible metrics would be infeasible, but access to
a label aware distance metric is still desirable. We present a simple, and
computationally inexpensive approach for estimating a well motivated metric
that relies only on gradient estimates, discussing theoretical and experimental
results. In the final part, we address representational issues, considering
group equivariant convolutional neural networks (GCNNs). Equivariance to
symmetry transformations is explicitly encoded in GCNNs; a classical CNN being
the simplest example. In particular, we present a SO(3)-equivariant neural
network architecture for spherical data, that operates entirely in Fourier
space, while also providing a formalism for the design of fully Fourier neural
networks that are equivariant to the action of any continuous compact group.
</summary>
    <author>
      <name>Shubhendu Trivedi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08763v2</id>
    <updated>2018-08-30T00:51:38Z</updated>
    <published>2018-08-27T09:51:03Z</published>
    <title>On the convergence of optimistic policy iteration for stochastic
  shortest path problem</title>
    <summary>  In this paper, we prove some convergence results of a special case of
optimistic policy iteration algorithm for stochastic shortest path problem. We
consider both Monte Carlo and $TD(\lambda)$ methods for the policy evaluation
step under the condition that the termination state will eventually be reached
almost surely.
</summary>
    <author>
      <name>Yuanlong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09830v2</id>
    <updated>2018-08-30T00:35:52Z</updated>
    <published>2018-08-29T13:52:40Z</published>
    <title>Searching Toward Pareto-Optimal Device-Aware Neural Architectures</title>
    <summary>  Recent breakthroughs in Neural Architectural Search (NAS) have achieved
state-of-the-art performance in many tasks such as image classification and
language understanding. However, most existing works only optimize for model
accuracy and largely ignore other important factors imposed by the underlying
hardware and devices, such as latency and energy, when making inference. In
this paper, we first introduce the problem of NAS and provide a survey on
recent works. Then we deep dive into two recent advancements on extending NAS
into multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net
are capable of optimizing accuracy and other objectives imposed by devices,
searching for neural architectures that can be best deployed on a wide spectrum
of devices: from embedded systems and mobile devices to workstations.
Experimental results are poised to show that architectures found by MONAS and
DPP-Net achieves Pareto optimality w.r.t the given objectives for various
devices.
</summary>
    <author>
      <name>An-Chieh Cheng</name>
    </author>
    <author>
      <name>Jin-Dong Dong</name>
    </author>
    <author>
      <name>Chi-Hung Hsu</name>
    </author>
    <author>
      <name>Shu-Huan Chang</name>
    </author>
    <author>
      <name>Min Sun</name>
    </author>
    <author>
      <name>Shih-Chieh Chang</name>
    </author>
    <author>
      <name>Jia-Yu Pan</name>
    </author>
    <author>
      <name>Yu-Ting Chen</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Da-Cheng Juan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCAD'18 Invited Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09830v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09830v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10073v1</id>
    <updated>2018-08-30T00:33:28Z</updated>
    <published>2018-08-30T00:33:28Z</published>
    <title>Rational Neural Networks for Approximating Jump Discontinuities of Graph
  Convolution Operator</title>
    <summary>  For node level graph encoding, a recent important state-of-art method is the
graph convolutional networks (GCN), which nicely integrate local vertex
features and graph topology in the spectral domain. However, current studies
suffer from several drawbacks: (1) graph CNNs relies on Chebyshev polynomial
approximation which results in oscillatory approximation at jump
discontinuities; (2) Increasing the order of Chebyshev polynomial can reduce
the oscillations issue, but also incurs unaffordable computational cost; (3)
Chebyshev polynomials require degree $\Omega$(poly(1/$\epsilon$)) to
approximate a jump signal such as $|x|$, while rational function only needs
$\mathcal{O}$(poly log(1/$\epsilon$))\cite{liang2016deep,telgarsky2017neural}.
However, it's non-trivial to apply rational approximation without increasing
computational complexity due to the denominator. In this paper, the superiority
of rational approximation is exploited for graph signal recovering. RatioanlNet
is proposed to integrate rational function and neural networks. We show that
rational function of eigenvalues can be rewritten as a function of graph
Laplacian, which can avoid multiplication by the eigenvector matrix. Focusing
on the analysis of approximation on graph convolution operation, a graph signal
regression task is formulated. Under graph signal regression task, its time
complexity can be significantly reduced by graph Fourier transform. To overcome
the local minimum problem of neural networks model, a relaxed Remez algorithm
is utilized to initialize the weight parameters. Convergence rate of
RatioanlNet and polynomial based methods on jump signal is analyzed for a
theoretical guarantee. The extensive experimental results demonstrated that our
approach could effectively characterize the jump discontinuities, outperforming
competing methods by a substantial margin on both synthetic and real-world
graphs.
</summary>
    <author>
      <name>Zhiqian Chen</name>
    </author>
    <author>
      <name>Feng Chen</name>
    </author>
    <author>
      <name>Rongjie Lai</name>
    </author>
    <author>
      <name>Xuchao Zhang</name>
    </author>
    <author>
      <name>Chang-Tien Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICDM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07380v3</id>
    <updated>2018-08-30T00:28:59Z</updated>
    <published>2018-08-19T02:53:33Z</published>
    <title>On the Predictability of non-CGM Diabetes Data for Personalized
  Recommendation</title>
    <summary>  With continuous glucose monitoring (CGM), data-driven models on blood glucose
prediction have been shown to be effective in related work. However, such (CGM)
systems are not always available, e.g., for a patient at home. In this work, we
conduct a study on 9 patients and examine the predictability of data-driven
(aka. machine learning) based models on patient-level blood glucose prediction;
with measurements are taken only periodically (i.e., after several hours). To
this end, we propose several post-prediction methods to account for the noise
nature of these data, that marginally improves the performance of the end
system.
</summary>
    <author>
      <name>Tu Nguyen</name>
    </author>
    <author>
      <name>Markus Rokicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of ACM CIKM 2018 Workshops</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07380v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07380v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02129v2</id>
    <updated>2018-08-29T23:31:44Z</updated>
    <published>2018-08-06T21:44:06Z</published>
    <title>Probabilistic Causal Analysis of Social Influence</title>
    <summary>  Mastering the dynamics of social influence requires separating, in a database
of information propagation traces, the genuine causal processes from temporal
correlation, i.e., homophily and other spurious causes. However, most studies
to characterize social influence, and, in general, most data-science analyses
focus on correlations, statistical independence, or conditional independence.
Only recently, there has been a resurgence of interest in "causal data
science", e.g., grounded on causality theories. In this paper we adopt a
principled causal approach to the analysis of social influence from
information-propagation data, rooted in the theory of probabilistic causation.
  Our approach consists of two phases. In the first one, in order to avoid the
pitfalls of misinterpreting causation when the data spans a mixture of several
subtypes ("Simpson's paradox"), we partition the set of propagation traces into
groups, in such a way that each group is as less contradictory as possible in
terms of the hierarchical structure of information propagation. To achieve this
goal, we borrow the notion of "agony" and define the Agony-bounded Partitioning
problem, which we prove being hard, and for which we develop two efficient
algorithms with approximation guarantees. In the second phase, for each group
from the first phase, we apply a constrained MLE approach to ultimately learn a
minimal causal topology. Experiments on synthetic data show that our method is
able to retrieve the genuine causal arcs w.r.t. a ground-truth generative
model. Experiments on real data show that, by focusing only on the extracted
causal structures instead of the whole social graph, the effectiveness of
predicting influence spread is significantly improved.
</summary>
    <author>
      <name>Francesco Bonchi</name>
    </author>
    <author>
      <name>Francesco Gullo</name>
    </author>
    <author>
      <name>Bud Mishra</name>
    </author>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM 18, October 22-26, 2018, Torino, Italy</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.02129v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02129v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07821v4</id>
    <updated>2018-08-29T22:28:04Z</updated>
    <published>2017-07-25T06:05:27Z</published>
    <title>Concept Drift Detection and Adaptation with Hierarchical Hypothesis
  Testing</title>
    <summary>  When using statistical models (such as a classifier) in a streaming
environment, there is often a need to detect and adapt to concept drifts to
mitigate any deterioration in the model's predictive performance over time.
Unfortunately, the ability of popular concept drift approaches in detecting
these drifts in the relationship of the response and predictor variable is
often dependent on the distribution characteristics of the data streams, as
well as its sensitivity on parameter tuning. This paper presents Hierarchical
Linear Four Rates (HLFR), a framework that detects concept drifts for different
data stream distributions (including imbalanced data) by leveraging a
hierarchical set of hypothesis tests in an online setting. The performance of
HLFR is compared to benchmark approaches using both simulated and real-world
datasets spanning the breadth of concept drift types. HLFR significantly
outperforms benchmark approaches in terms of accuracy, G-mean, recall, delay in
detection and adaptability across the various datasets.
</summary>
    <author>
      <name>Shujian Yu</name>
    </author>
    <author>
      <name>Zubin Abraham</name>
    </author>
    <author>
      <name>Heng Wang</name>
    </author>
    <author>
      <name>Mohak Shah</name>
    </author>
    <author>
      <name>Xinge You</name>
    </author>
    <author>
      <name>José C. Príncipe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07821v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07821v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10056v1</id>
    <updated>2018-08-29T22:17:24Z</updated>
    <published>2018-08-29T22:17:24Z</published>
    <title>Differentially Private Change-Point Detection</title>
    <summary>  The change-point detection problem seeks to identify distributional changes
at an unknown change-point k* in a stream of data. This problem appears in many
important practical settings involving personal data, including
biosurveillance, fault detection, finance, signal detection, and security
systems. The field of differential privacy offers data analysis tools that
provide powerful worst-case privacy guarantees. We study the statistical
problem of change-point detection through the lens of differential privacy. We
give private algorithms for both online and offline change-point detection,
analyze these algorithms theoretically, and provide empirical validation of our
results.
</summary>
    <author>
      <name>Rachel Cummings</name>
    </author>
    <author>
      <name>Sara Krehbiel</name>
    </author>
    <author>
      <name>Yajun Mei</name>
    </author>
    <author>
      <name>Rui Tuo</name>
    </author>
    <author>
      <name>Wanrong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08316v2</id>
    <updated>2018-08-29T22:00:03Z</updated>
    <published>2018-08-24T21:29:53Z</published>
    <title>A Trio Neural Model for Dynamic Entity Relatedness Ranking</title>
    <summary>  Measuring entity relatedness is a fundamental task for many natural language
processing and information retrieval applications. Prior work often studies
entity relatedness in static settings and an unsupervised manner. However,
entities in real-world are often involved in many different relationships,
consequently entity-relations are very dynamic over time. In this work, we
propose a neural networkbased approach for dynamic entity relatedness,
leveraging the collective attention as supervision. Our model is capable of
learning rich and different entity representations in a joint framework.
Through extensive experiments on large-scale datasets, we demonstrate that our
method achieves better results than competitive baselines.
</summary>
    <author>
      <name>Tu Nguyen</name>
    </author>
    <author>
      <name>Tuan Tran</name>
    </author>
    <author>
      <name>Wolfgang Nejdl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of CoNLL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08316v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08316v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01496v1</id>
    <updated>2018-08-29T21:11:09Z</updated>
    <published>2018-08-29T21:11:09Z</published>
    <title>Learning Gender-Neutral Word Embeddings</title>
    <summary>  Word embedding models have become a fundamental component in a wide range of
Natural Language Processing (NLP) applications. However, embeddings trained on
human-generated corpora have been demonstrated to inherit strong gender
stereotypes that reflect social constructs. To address this concern, in this
paper, we propose a novel training procedure for learning gender-neutral word
embeddings. Our approach aims to preserve gender information in certain
dimensions of word vectors while compelling other dimensions to be free of
gender influence. Based on the proposed method, we generate a Gender-Neutral
variant of GloVe (GN-GloVe). Quantitative and qualitative experiments
demonstrate that GN-GloVe successfully isolates gender information without
sacrificing the functionality of the embedding model.
</summary>
    <author>
      <name>Jieyu Zhao</name>
    </author>
    <author>
      <name>Yichao Zhou</name>
    </author>
    <author>
      <name>Zeyu Li</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Kai-Wei Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10038v1</id>
    <updated>2018-08-29T20:45:54Z</updated>
    <published>2018-08-29T20:45:54Z</published>
    <title>Theoretical Linear Convergence of Unfolded ISTA and its Practical
  Weights and Thresholds</title>
    <summary>  In recent years, unfolding iterative algorithms as neural networks has become
an empirical success in solving sparse recovery problems. However, its
theoretical understanding is still immature, which prevents us from fully
utilizing the power of neural networks. In this work, we study unfolded ISTA
(Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We
introduce a weight structure that is necessary for asymptotic convergence to
the true sparse signal. With this structure, unfolded ISTA can attain a linear
convergence, which is better than the sublinear convergence of ISTA/FISTA in
general cases. Furthermore, we propose to incorporate thresholding in the
network to perform support selection, which is easy to implement and able to
boost the convergence rate both theoretically and empirically. Extensive
simulations, including sparse vector recovery and a compressive sensing
experiment on real image data, corroborate our theoretical results and
demonstrate their practical usefulness.
</summary>
    <author>
      <name>Xiaohan Chen</name>
    </author>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Zhangyang Wang</name>
    </author>
    <author>
      <name>Wotao Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 6 figures, 1 table. Under review for NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09034v2</id>
    <updated>2018-08-29T20:13:22Z</updated>
    <published>2018-08-27T21:12:47Z</published>
    <title>Importance Weighting and Variational Inference</title>
    <summary>  Recent work used importance sampling ideas for better variational bounds on
likelihoods. We clarify the applicability of these ideas to pure probabilistic
inference, by showing the resulting Importance Weighted Variational Inference
(IWVI) technique is an instance of augmented variational inference, thus
identifying the looseness in previous work. Experiments confirm IWVI's
practicality for probabilistic inference. As a second contribution, we
investigate inference with elliptical distributions, which improves accuracy in
low dimensions, and convergence in high dimensions.
</summary>
    <author>
      <name>Justin Domke</name>
    </author>
    <author>
      <name>Daniel Sheldon</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10026v1</id>
    <updated>2018-08-29T20:02:41Z</updated>
    <published>2018-08-29T20:02:41Z</published>
    <title>Physically-inspired Gaussian processes for transcriptional regulation in
  Drosophila melanogaster</title>
    <summary>  The regulatory process in Drosophila melanogaster is thoroughly studied for
understanding several principles in systems biology. Since transcriptional
regulation of the Drosophila depends on spatiotemporal interactions between
mRNA expressions and gap-gene proteins, proper physically-inspired stochastic
models are required to describe the existing link between both biological
quantities. Many studies have shown that the use of Gaussian processes (GPs)
and differential equations yields promising inference results when modelling
regulatory processes. In order to exploit the benefits of GPs, two types of
physically-inspired GPs based on the reaction-diffusion equation are further
investigated in this paper. The main difference between both approaches lies on
whether the GP prior is placed: either over mRNA expressions or protein
concentrations. Contrarily to other stochastic frameworks, discretising the
spatial space is not required here. Both GP models are tested under different
conditions depending on the availability of biological data. Finally, their
performances are assessed using a high-resolution dataset describing the
blastoderm stage of the early embryo of Drosophila.
</summary>
    <author>
      <name>Andrés F. López-Lopera</name>
    </author>
    <author>
      <name>Nicolas Durrande</name>
    </author>
    <author>
      <name>Mauricio A. Alvarez</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05484v3</id>
    <updated>2018-08-29T19:10:54Z</updated>
    <published>2018-04-16T02:52:45Z</published>
    <title>Block Mean Approximation for Efficient Second Order Optimization</title>
    <summary>  Advanced optimization algorithms such as Newton method and AdaGrad benefit
from second order derivative or second order statistics to achieve better
descent directions and faster convergence rates. At their heart, such
algorithms need to compute the inverse or inverse square root of a matrix whose
size is quadratic of the dimensionality of the search space. For high
dimensional search spaces, the matrix inversion or inversion of square root
becomes overwhelming which in turn demands for approximate methods. In this
work, we propose a new matrix approximation method which divides a matrix into
blocks and represents each block by one or two numbers. The method allows
efficient computation of matrix inverse and inverse square root. We apply our
method to AdaGrad in training deep neural networks. Experiments show
encouraging results compared to the diagonal approximation.
</summary>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Mehrtash Harandi</name>
    </author>
    <author>
      <name>Richard Hartley</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05484v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05484v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00047v4</id>
    <updated>2018-08-29T19:03:53Z</updated>
    <published>2018-01-31T20:03:07Z</published>
    <title>Matrix completion with deterministic pattern - a geometric perspective</title>
    <summary>  We consider the matrix completion problem with a deterministic pattern of
observed entries. In this setting, we aim to answer the question: under what
condition there will be (at least locally) unique solution to the matrix
completion problem, i.e., the underlying true matrix is identifiable. We answer
the question from a certain point of view and outline a geometric perspective.
We give an algebraically verifiable sufficient condition, which we call the
well-posedness condition, for the local uniqueness of MRMC solutions. We argue
that this condition is necessary for local stability of MRMC solutions, and we
show that the condition is generic using the characteristic rank. We also argue
that the low-rank approximation approaches are more stable than MRMC and
further propose a sequential statistical testing procedure to determine the
"true" rank from observed entries. Finally, we provide numerical examples aimed
at verifying validity of the presented theory.
</summary>
    <author>
      <name>Alexander Shapiro</name>
    </author>
    <author>
      <name>Yao Xie</name>
    </author>
    <author>
      <name>Rui Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00047v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00047v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10013v1</id>
    <updated>2018-08-29T18:55:22Z</updated>
    <published>2018-08-29T18:55:22Z</published>
    <title>Group calibration is a byproduct of unconstrained learning</title>
    <summary>  Much recent work on fairness in machine learning has focused on how well a
score function is calibrated in different groups within a given population,
where each group is defined by restricting one or more sensitive attributes.
  We investigate to which extent group calibration follows from unconstrained
empirical risk minimization on its own, without the need for any explicit
intervention. We show that under reasonable conditions, the deviation from
satisfying group calibration is bounded by the excess loss of the empirical
risk minimizer relative to the Bayes optimal score function. As a corollary, it
follows that empirical risk minimization can simultaneously achieve calibration
for many groups, a task that prior work deferred to highly complex algorithms.
We complement our results with a lower bound, and a range of experimental
findings.
  Our results challenge the view that group calibration necessitates an active
intervention, suggesting that often we ought to think of it as a byproduct of
unconstrained machine learning.
</summary>
    <author>
      <name>Lydia T. Liu</name>
    </author>
    <author>
      <name>Max Simchowitz</name>
    </author>
    <author>
      <name>Moritz Hardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02933v2</id>
    <updated>2018-08-29T18:52:44Z</updated>
    <published>2018-08-08T20:40:42Z</published>
    <title>(Sequential) Importance Sampling Bandits</title>
    <summary>  The multi-armed bandit (MAB) problem is a sequential allocation task where
the goal is to learn a policy that maximizes long term payoff, where only the
reward of the executed action is observed; i.e., sequential optimal decisions
are made, while simultaneously learning how the world operates. In the
stochastic setting, the reward for each action is generated from an unknown
distribution. To decide the next optimal action to take, one must compute
sufficient statistics of this unknown reward distribution, e.g.
upper-confidence bounds (UCB), or expectations in Thompson sampling.
Closed-form expressions for these statistics of interest are analytically
intractable except for simple cases. We here propose to leverage Monte Carlo
estimation and, in particular, the flexibility of (sequential) importance
sampling (IS) to allow for accurate estimation of the statistics of interest
within the MAB problem. IS methods estimate posterior densities or expectations
in probabilistic models that are analytically intractable. We first show how IS
can be combined with state-of-the-art MAB algorithms (Thompson sampling and
Bayes-UCB) for classic (Bernoulli and contextual linear-Gaussian) bandit
problems. Furthermore, we leverage the power of sequential IS to extend the
applicability of these algorithms beyond the classic settings, and tackle
additional useful cases. Specifically, we study the dynamic linear-Gaussian
bandit, and both the static and dynamic logistic cases too. The flexibility of
(sequential) importance sampling is shown to be fundamental for obtaining
efficient estimates of the key sufficient statistics in these challenging
scenarios.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02933v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02933v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10009v1</id>
    <updated>2018-08-29T18:40:26Z</updated>
    <published>2018-08-29T18:40:26Z</published>
    <title>Learning a Policy for Opportunistic Active Learning</title>
    <summary>  Active learning identifies data points to label that are expected to be the
most useful in improving a supervised model. Opportunistic active learning
incorporates active learning into interactive tasks that constrain possible
queries during interactions. Prior work has shown that opportunistic active
learning can be used to improve grounding of natural language descriptions in
an interactive object retrieval task. In this work, we use reinforcement
learning for such an object retrieval task, to learn a policy that effectively
trades off task completion with model improvement that would benefit future
tasks.
</summary>
    <author>
      <name>Aishwarya Padmakumar</name>
    </author>
    <author>
      <name>Peter Stone</name>
    </author>
    <author>
      <name>Raymond J. Mooney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018 Camera Ready</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.10009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09942v1</id>
    <updated>2018-08-29T17:43:11Z</updated>
    <published>2018-08-29T17:43:11Z</published>
    <title>Neural Compositional Denotational Semantics for Question Answering</title>
    <summary>  Answering compositional questions requiring multi-step reasoning is
challenging. We introduce an end-to-end differentiable model for interpreting
questions about a knowledge graph (KG), which is inspired by formal approaches
to semantics. Each span of text is represented by a denotation in a KG and a
vector that captures ungrounded aspects of meaning. Learned composition modules
recursively combine constituent spans, culminating in a grounding for the
complete sentence which answers the question. For example, to interpret "not
green", the model represents "green" as a set of KG entities and "not" as a
trainable ungrounded vector---and then uses this vector to parameterize a
composition function that performs a complement operation. For each sentence,
we build a parse chart subsuming all possible parses, allowing the model to
jointly learn both the composition operators and output structure by gradient
descent from end-task supervision. The model learns a variety of challenging
semantic operators, such as quantifiers, disjunctions and composed relations,
and infers latent syntactic structure. It also generalizes well to longer
questions than seen in its training data, in contrast to RNN, its tree-based
variants, and semantic parsing baselines.
</summary>
    <author>
      <name>Nitish Gupta</name>
    </author>
    <author>
      <name>Mike Lewis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09940v1</id>
    <updated>2018-08-29T17:39:08Z</updated>
    <published>2018-08-29T17:39:08Z</published>
    <title>Deep Reinforcement Learning in Portfolio Management</title>
    <summary>  In this paper, we implement two state-of-art continuous reinforcement
learning algorithms, Deep Deterministic Policy Gradient (DDPG) and Proximal
Policy Optimization (PPO) in portfolio management. Both of them are widely-used
in game playing and robot control. What's more, PPO has appealing theoretical
propeties which is hopefully potential in portfolio management. We present the
performances of them under different settings, including different learning
rate, objective function, markets, feature combinations, in order to provide
insights for parameter tuning, features selection and data preparation.
</summary>
    <author>
      <name>Zhipeng Liang</name>
    </author>
    <author>
      <name>Kangkang Jiang</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Junhao Zhu</name>
    </author>
    <author>
      <name>Yanran Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09935v1</id>
    <updated>2018-08-29T17:29:12Z</updated>
    <published>2018-08-29T17:29:12Z</published>
    <title>Attention-based Neural Text Segmentation</title>
    <summary>  Text segmentation plays an important role in various Natural Language
Processing (NLP) tasks like summarization, context understanding, document
indexing and document noise removal. Previous methods for this task require
manual feature engineering, huge memory requirements and large execution times.
To the best of our knowledge, this paper is the first one to present a novel
supervised neural approach for text segmentation. Specifically, we propose an
attention-based bidirectional LSTM model where sentence embeddings are learned
using CNNs and the segments are predicted based on contextual information. This
model can automatically handle variable sized context information. Compared to
the existing competitive baselines, the proposed model shows a performance
improvement of ~7% in WinDiff score on three benchmark datasets.
</summary>
    <author>
      <name>Pinkesh Badjatiya</name>
    </author>
    <author>
      <name>Litton J Kurisinkel</name>
    </author>
    <author>
      <name>Manish Gupta</name>
    </author>
    <author>
      <name>Vasudeva Varma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09933v1</id>
    <updated>2018-08-29T17:21:17Z</updated>
    <published>2018-08-29T17:21:17Z</published>
    <title>Certified Mapper: Repeated testing for acyclicity and obstructions to
  the nerve lemma</title>
    <summary>  The Mapper algorithm does not include a check for whether the cover produced
conforms to the requirements of the nerve lemma. To perform a check for
obstructions to the nerve lemma, statistical considerations of multiple testing
quickly arise.
  In this paper, we propose several statistical approaches to finding
obstructions: through a persistent nerve lemma, through simulation testing, and
using a parametric refinement of simulation tests.
  We suggest Certified Mapper -- a method built from these approaches to
generate certificates of non-obstruction, or identify specific obstructions to
the nerve lemma -- and we give recommendations for which statistical approaches
are most appropriate for the task.
</summary>
    <author>
      <name>Mikael Vejdemo-Johansson</name>
    </author>
    <author>
      <name>Alisa Leshchenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, submitted to the proceedings of the Abel symposium</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12317v2</id>
    <updated>2018-08-29T16:53:44Z</updated>
    <published>2018-05-31T04:51:08Z</published>
    <title>Multiaccuracy: Black-Box Post-Processing for Fairness in Classification</title>
    <summary>  Prediction systems are successfully deployed in applications ranging from
disease diagnosis, to predicting credit worthiness, to image recognition. Even
when the overall accuracy is high, these systems may exhibit systematic biases
that harm specific subpopulations; such biases may arise inadvertently due to
underrepresentation in the data used to train a machine-learning model, or as
the result of intentional malicious discrimination. We develop a rigorous
framework of *multiaccuracy* auditing and post-processing to ensure accurate
predictions across *identifiable subgroups*.
  Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have
black-box access to a predictor and a relatively small set of labeled data for
auditing; importantly, this black-box framework allows for improved fairness
and accountability of predictions, even when the predictor is minimally
transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show
that if the initial model is accurate on an identifiable subgroup, then the
post-processed model will be also. We experimentally demonstrate the
effectiveness of the approach to improve the accuracy among minority subgroups
in diverse applications (image classification, finance, population health).
Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for
"black women") even when the sensitive features (e.g. "race", "gender") are not
given to the algorithm explicitly.
</summary>
    <author>
      <name>Michael P. Kim</name>
    </author>
    <author>
      <name>Amirata Ghorbani</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09907v1</id>
    <updated>2018-08-29T16:18:02Z</updated>
    <published>2018-08-29T16:18:02Z</published>
    <title>Dropout with Tabu Strategy for Regularizing Deep Neural Networks</title>
    <summary>  Dropout has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons in deep neural networks (DNN). It
randomly drops units with a probability $p$ during the training stage of DNN.
Dropout also provides a way of approximately combining exponentially many
different neural network architectures efficiently. In this work, we add a
diversification strategy into dropout, which aims at generating more different
neural network architectures in a proper times of iterations. The dropped units
in last forward propagation will be marked. Then the selected units for
dropping in the current FP will be kept if they have been marked in the last
forward propagation. We only mark the units from the last forward propagation.
We call this new technique Tabu Dropout. Tabu Dropout has no extra parameters
compared with the standard Dropout and also it is computationally cheap. The
experiments conducted on MNIST, Fashion-MNIST datasets show that Tabu Dropout
improves the performance of the standard dropout.
</summary>
    <author>
      <name>Zongjie Ma</name>
    </author>
    <author>
      <name>Abdul Sattar</name>
    </author>
    <author>
      <name>Jun Zhou</name>
    </author>
    <author>
      <name>Qingliang Chen</name>
    </author>
    <author>
      <name>Kaile Su</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09897v1</id>
    <updated>2018-08-29T15:57:27Z</updated>
    <published>2018-08-29T15:57:27Z</published>
    <title>Towards security defect prediction with AI</title>
    <summary>  In this study, we investigate the limits of the current state of the art AI
system for detecting buffer overflows and compare it with current static
analysis tools. To do so, we developed a code generator, s-bAbI, capable of
producing an arbitrarily large number of code samples of controlled complexity.
We found that the static analysis engines we examined have good precision, but
poor recall on this dataset, except for a sound static analyzer that has good
precision and recall. We found that the state of the art AI system, a memory
network modeled after Choi et al. [1], can achieve similar performance to the
static analysis engines, but requires an exhaustive amount of training data in
order to do so. Our work points towards future approaches that may solve these
problems; namely, using representations of code that can capture appropriate
scope information and using deep learning methods that are able to perform
arithmetic operations.
</summary>
    <author>
      <name>Carson D. Sestili</name>
    </author>
    <author>
      <name>William S. Snavely</name>
    </author>
    <author>
      <name>Nathan M. VanHoudnos</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10261v1</id>
    <updated>2018-08-29T15:24:33Z</updated>
    <published>2018-08-29T15:24:33Z</published>
    <title>Centroid estimation based on symmetric KL divergence for Multinomial
  text classification problem</title>
    <summary>  We define a new method to estimate centroid for text classification based on
the symmetric KL-divergence between the distribution of words in training
documents and their class centroids. Experiments on several standard data sets
indicate that the new method achieves substantial improvements over the
traditional classifiers.
</summary>
    <author>
      <name>Jiangning Chen</name>
    </author>
    <author>
      <name>Heinrich Matzinger</name>
    </author>
    <author>
      <name>Haoyan Zhai</name>
    </author>
    <author>
      <name>Mi Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04212v3</id>
    <updated>2018-08-29T15:16:08Z</updated>
    <published>2018-04-11T20:37:35Z</published>
    <title>Word2Vec applied to Recommendation: Hyperparameters Matter</title>
    <summary>  Skip-gram with negative sampling, a popular variant of Word2vec originally
designed and tuned to create word embeddings for Natural Language Processing,
has been used to create item embeddings with successful applications in
recommendation. While these fields do not share the same type of data, neither
evaluate on the same tasks, recommendation applications tend to use the same
already tuned hyperparameters values, even if optimal hyperparameters values
are often known to be data and task dependent. We thus investigate the marginal
importance of each hyperparameter in a recommendation setting through large
hyperparameter grid searches on various datasets. Results reveal that
optimizing neglected hyperparameters, namely negative sampling distribution,
number of epochs, subsampling parameter and window-size, significantly improves
performance on a recommendation task, and can increase it by an order of
magnitude. Importantly, we find that optimal hyperparameters configurations for
Natural Language Processing tasks and Recommendation tasks are noticeably
different.
</summary>
    <author>
      <name>Hugo Caselles-Dupré</name>
    </author>
    <author>
      <name>Florian Lesaint</name>
    </author>
    <author>
      <name>Jimena Royo-Letelier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published on the 12th ACM Conference on Recommender
  Systems, Vancouver, Canada, 2nd-7th October 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04212v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04212v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01495v1</id>
    <updated>2018-08-29T15:11:49Z</updated>
    <published>2018-08-29T15:11:49Z</published>
    <title>A Reinforcement Learning-driven Translation Model for Search-Oriented
  Conversational Systems</title>
    <summary>  Search-oriented conversational systems rely on information needs expressed in
natural language (NL). We focus here on the understanding of NL expressions for
building keyword-based queries. We propose a reinforcement-learning-driven
translation model framework able to 1) learn the translation from NL
expressions to queries in a supervised way, and, 2) to overcome the lack of
large-scale dataset by framing the translation model as a word selection
approach and injecting relevance feedback in the learning process. Experiments
are carried out on two TREC datasets and outline the effectiveness of our
approach.
</summary>
    <author>
      <name>Wafa Aissa</name>
    </author>
    <author>
      <name>Laure Soulier</name>
    </author>
    <author>
      <name>Ludovic Denoyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the author's pre-print version of the work. It is posted here
  for your personal use, not for redistribution. Please cite the definitive
  version which will be published in Proceedings of the 2018 EMNLP Workshop
  SCAI: The 2nd International Workshop on Search-Oriented Conversational AI -
  ISBN: 978-1-948087-75-9</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09856v1</id>
    <updated>2018-08-29T14:43:12Z</updated>
    <published>2018-08-29T14:43:12Z</published>
    <title>Application of Machine Learning in Rock Facies Classification with
  Physics-Motivated Feature Augmentation</title>
    <summary>  With recent progress in algorithms and the availability of massive amounts of
computation power, application of machine learning techniques is becoming a hot
topic in the oil and gas industry. One of the most promising aspects to apply
machine learning to the upstream field is the rock facies classification in
reservoir characterization, which is crucial in determining the net pay
thickness of reservoirs, thus a definitive factor in drilling decision making
process. For complex machine learning tasks like facies classification, feature
engineering is often critical. This paper shows the inclusion of
physics-motivated feature interaction in feature augmentation can further
improve the capability of machine learning in rock facies classification. We
demonstrate this approach with the SEG 2016 machine learning contest dataset
and the top winning algorithms. The improvement is roboust and can be $\sim5\%$
better than current existing best F-1 score, where F-1 is an evaluation metric
used to quantify average prediction accuracy.
</summary>
    <author>
      <name>Jie Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Corresponding author</arxiv:affiliation>
    </author>
    <author>
      <name>Yu Zeng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Corresponding author</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10259v1</id>
    <updated>2018-08-29T14:06:31Z</updated>
    <published>2018-08-29T14:06:31Z</published>
    <title>Analyze Unstructured Data Patterns for Conceptual Representation</title>
    <summary>  Online news media provides aggregated news and stories from different sources
all over the world and up-to-date news coverage. The main goal of this study is
to have a solution that considered as a homogeneous source for the news and to
represent the news in a new conceptual framework. Furthermore, the user can
easily find different updated news in a fast way through the designed
interface. The Mobile App implementation is based on modeling the multi-level
conceptual analysis discipline. Discovering main concepts of any domain is
captured from the hidden unstructured data that are analyzed by the proposed
solution. Concepts are discovered through analyzing data patterns to be
structured into a tree-based interface for easy navigation for the end user,
through the discovered news concepts. Our final experiment results showing that
analyzing the news before displaying to the end-user and restructuring the
final output in a conceptual multilevel structure, that producing new display
frame for the end user to find the related information to his interest.
</summary>
    <author>
      <name>Aboubakr Aqle</name>
    </author>
    <author>
      <name>Dena Al-Thani</name>
    </author>
    <author>
      <name>Ali Jaoua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 Figures, 4th Annual Conference on Computational Science &amp;
  Computational Intelligence (CSCI'17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04566v2</id>
    <updated>2018-08-29T13:55:56Z</updated>
    <published>2018-04-12T15:23:39Z</published>
    <title>Latent Geometry Inspired Graph Dissimilarities Enhance Affinity
  Propagation Community Detection in Complex Networks</title>
    <summary>  Affinity propagation is one of the most effective unsupervised pattern
recognition algorithms for data clustering in high-dimensional feature space.
However, the numerous attempts to test its performance for community detection
in complex networks have been attaining results very far from the state of the
art methods such as Infomap and Louvain. Yet, all these studies agreed that the
crucial problem is to convert the unweighted network topology in a
'smart-enough' node dissimilarity matrix that is able to properly address the
message passing procedure behind affinity propagation clustering. Here we
introduce a conceptual innovation and we discuss how to leverage network latent
geometry notions in order to design dissimilarity matrices for affinity
propagation community detection. Our results demonstrate that the latent
geometry inspired dissimilarity measures we design bring affinity propagation
to equal or outperform current state of the art methods for community
detection. These findings are solidly proven considering both synthetic
'realistic' networks (with known ground-truth communities) and real networks
(with community metadata), even when the data structure is corrupted by noise
artificially induced by missing or spurious connectivity.
</summary>
    <author>
      <name>Carlo Vittorio Cannistraci</name>
    </author>
    <author>
      <name>Alessandro Muscoloni</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04566v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04566v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09819v1</id>
    <updated>2018-08-29T13:41:33Z</updated>
    <published>2018-08-29T13:41:33Z</published>
    <title>Approximate Exploration through State Abstraction</title>
    <summary>  Although exploration in reinforcement learning is well understood from a
theoretical point of view, provably correct methods remain impractical. In this
paper we study the interplay between exploration and approximation, what we
call \emph{approximate exploration}. We first provide results when the
approximation is explicit, quantifying the performance of an exploration
algorithm, MBIE-EB \citep{strehl2008analysis}, when combined with state
aggregation. In particular, we show that this allows the agent to trade off
between learning speed and quality of the policy learned. We then turn to a
successful exploration scheme in practical, pseudo-count based exploration
bonuses \citep{bellemare2016unifying}. We show that choosing a density model
implicitly defines an abstraction and that the pseudo-count bonus incentivizes
the agent to explore using this abstraction. We find, however, that implicit
exploration may result in a mismatch between the approximated value function
and exploration bonus, leading to either under- or over-exploration.
</summary>
    <author>
      <name>Adrien Ali Taïga</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Marc G. Bellemare</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09744v1</id>
    <updated>2018-08-29T12:02:11Z</updated>
    <published>2018-08-29T12:02:11Z</published>
    <title>Rule induction for global explanation of trained models</title>
    <summary>  Understanding the behavior of a trained network and finding explanations for
its outputs is important for improving the network's performance and
generalization ability, and for ensuring trust in automated systems. Several
approaches have previously been proposed to identify and visualize the most
important features by analyzing a trained network. However, the relations
between different features and classes are lost in most cases. We propose a
technique to induce sets of if-then-else rules that capture these relations to
globally explain the predictions of a network. We first calculate the
importance of the features in the trained network. We then weigh the original
inputs with these feature importance scores, simplify the transformed input
space, and finally fit a rule induction model to explain the model predictions.
We find that the output rule-sets can explain the predictions of a neural
network trained for 4-class text classification from the 20 newsgroups dataset
to a macro-averaged F-score of 0.80. We make the code available at
https://github.com/clips/interpret_with_rules.
</summary>
    <author>
      <name>Madhumita Sushil</name>
    </author>
    <author>
      <name>Simon Šuster</name>
    </author>
    <author>
      <name>Walter Daelemans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the Workshop on 'Analyzing and interpreting neural
  networks for NLP' (BlackboxNLP), EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10260v1</id>
    <updated>2018-08-29T11:04:07Z</updated>
    <published>2018-08-29T11:04:07Z</published>
    <title>Understanding Latent Factors Using a GWAP</title>
    <summary>  Recommender systems relying on latent factor models often appear as black
boxes to their users. Semantic descriptions for the factors might help to
mitigate this problem. Achieving this automatically is, however, a
non-straightforward task due to the models' statistical nature. We present an
output-agreement game that represents factors by means of sample items and
motivates players to create such descriptions. A user study shows that the
collected output actually reflects real-world characteristics of the factors.
</summary>
    <author>
      <name>Johannes Kunkel</name>
    </author>
    <author>
      <name>Benedikt Loepp</name>
    </author>
    <author>
      <name>Jürgen Ziegler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Late-Breaking Results track part of the Twelfth
  ACM Conference on Recommender Systems (RecSys '18), Vancouver, BC, Canada,
  October 2-7, 2018, 2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01471v1</id>
    <updated>2018-08-29T09:21:22Z</updated>
    <published>2018-08-29T09:21:22Z</published>
    <title>Chest X-ray Inpainting with Deep Generative Models</title>
    <summary>  Generative adversarial networks have been successfully applied to inpainting
in natural images. However, the current state-of-the-art models have not yet
been widely adopted in the medical imaging domain. In this paper, we
investigate the performance of three recently published deep learning based
inpainting models: context encoders, semantic image inpainting, and the
contextual attention model, applied to chest x-rays, as the chest exam is the
most commonly performed radiological procedure. We train these generative
models on 1.2M 128 $\times$ 128 patches from 60K healthy x-rays, and learn to
predict the center 64 $\times$ 64 region in each patch. We test the models on
both the healthy and abnormal radiographs. We evaluate the results by visual
inspection and comparing the PSNR scores. The outputs of the models are in most
cases highly realistic. We show that the methods have potential to enhance and
detect abnormalities. In addition, we perform a 2AFC observer study and show
that an experienced human observer performs poorly in detecting inpainted
regions, particularly those generated by the contextual attention model.
</summary>
    <author>
      <name>Ecem Sogancioglu</name>
    </author>
    <author>
      <name>Shi Hu</name>
    </author>
    <author>
      <name>Davide Belli</name>
    </author>
    <author>
      <name>Bram van Ginneken</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09670v1</id>
    <updated>2018-08-29T07:58:28Z</updated>
    <published>2018-08-29T07:58:28Z</published>
    <title>Accelerated proximal boosting</title>
    <summary>  Gradient boosting is a prediction method that iteratively combines weak
learners to produce a complex and accurate model. From an optimization point of
view, the learning procedure of gradient boosting mimics a gradient descent on
a functional variable. This paper proposes to build upon the proximal point
algorithm when the empirical risk to minimize is not differentiable. In
addition, the novel boosting approach, called accelerated proximal boosting,
benefits from Nesterov's acceleration in the same way as gradient boosting
[Biau et al., 2018]. Advantages of leveraging proximal methods for boosting are
illustrated by numerical experiments on simulated and real-world data. In
particular, we exhibit a favorable comparison over gradient boosting regarding
convergence rate and prediction accuracy.
</summary>
    <author>
      <name>Erwan Fouillen</name>
    </author>
    <author>
      <name>Claire Boyer</name>
    </author>
    <author>
      <name>Maxime Sangnier</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09955v1</id>
    <updated>2018-08-29T07:52:42Z</updated>
    <published>2018-08-29T07:52:42Z</published>
    <title>QuasarNET: Human-level spectral classification and redshifting with Deep
  Neural Networks</title>
    <summary>  We introduce QuasarNET, a deep convolutional neural network that performs
classification and redshift estimation of astrophysical spectra with
human-expert accuracy. We pose these two tasks as a \emph{feature detection}
problem: presence or absence of spectral features determines the class, and
their wavelength determines the redshift, very much like human-experts proceed.
When ran on BOSS data to identify quasars through their emission lines,
QuasarNET defines a sample $99.51\pm0.03$\% pure and $99.52\pm0.03$\% complete,
well above the requirements of many analyses using these data. QuasarNET
significantly reduces the problem of line-confusion that induces catastrophic
redshift failures to below 0.2\%. We also extend QuasarNET to classify spectra
with broad absorption line (BAL) features, achieving an accuracy of
$98.0\pm0.4$\% for recognizing BAL and $97.0\pm0.2$\% for rejecting non-BAL
quasars. QuasarNET is trained on data of low signal-to-noise and medium
resolution, typical of current and future astrophysical surveys, and could be
easily applied to classify spectra from current and upcoming surveys such as
eBOSS, DESI and 4MOST.
</summary>
    <author>
      <name>Nicolas Busca</name>
    </author>
    <author>
      <name>Christophe Balland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to MMRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09663v1</id>
    <updated>2018-08-29T07:18:29Z</updated>
    <published>2018-08-29T07:18:29Z</published>
    <title>Wasserstein is all you need</title>
    <summary>  We propose a unified framework for building unsupervised representations of
individual objects or entities (and their compositions), by associating with
each object both a distributional as well as a point estimate (vector
embedding). This is made possible by the use of optimal transport, which allows
us to build these associated estimates while harnessing the underlying geometry
of the ground space. Our method gives a novel perspective for building rich and
powerful feature representations that simultaneously capture uncertainty (via a
distributional estimate) and interpretability (with the optimal transport map).
As a guiding example, we formulate unsupervised representations for text, in
particular for sentence representation and entailment detection. Empirical
results show strong advantages gained through the proposed framework. This
approach can be used for any unsupervised or supervised problem (on text or
other modalities) with a co-occurrence structure, such as any sequence data.
The key tools underlying the framework are Wasserstein distances and
Wasserstein barycenters (and, hence the title!).
</summary>
    <author>
      <name>Sidak Pal Singh</name>
    </author>
    <author>
      <name>Andreas Hug</name>
    </author>
    <author>
      <name>Aymeric Dieuleveut</name>
    </author>
    <author>
      <name>Martin Jaggi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05240v3</id>
    <updated>2018-08-29T07:04:22Z</updated>
    <published>2018-08-15T18:13:12Z</published>
    <title>Blended Coarse Gradient Descent for Full Quantization of Deep Neural
  Networks</title>
    <summary>  Quantized deep neural networks (QDNNs) are attractive due to their much lower
memory storage and faster inference speed than their regular full precision
counterparts. To maintain the same performance level especially at low
bit-widths, QDNNs must be retrained. Their training involves piecewise constant
activation functions and discrete weights, hence mathematical challenges arise.
We introduce the notion of coarse derivative and propose the blended coarse
gradient descent (BCGD) algorithm, for training fully quantized neural
networks. Coarse gradient is generally not a gradient of any function but an
artificial ascent direction. The weight update of BCGD goes by coarse gradient
correction of a weighted average of the full precision weights and their
quantization (the so-called blending), which yields sufficient descent in the
objective value and thus accelerates the training. Our experiments demonstrate
that this simple blending technique is very effective for quantization at
extremely low bit-width such as binarization. In full quantization of ResNet-18
for ImageNet classification task, BCGD gives 64.36% top-1 accuracy with binary
weights across all layers and 4-bit adaptive activation. If the weights in the
first and last layers are kept in full precision, this number increases to
65.46%. As theoretical justification, we provide the convergence analysis of
coarse gradient descent for a two-layer neural network model with Gaussian
input data, and prove that the expected coarse gradient correlates positively
with the underlying true gradient.
</summary>
    <author>
      <name>Penghang Yin</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Jiancheng Lyu</name>
    </author>
    <author>
      <name>Stanley Osher</name>
    </author>
    <author>
      <name>Yingyong Qi</name>
    </author>
    <author>
      <name>Jack Xin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05240v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05240v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09658v1</id>
    <updated>2018-08-29T06:49:49Z</updated>
    <published>2018-08-29T06:49:49Z</published>
    <title>APRIL: Interactively Learning to Summarise by Combining Active
  Preference Learning and Reinforcement Learning</title>
    <summary>  We propose a method to perform automatic document summarisation without using
reference summaries. Instead, our method interactively learns from users'
preferences. The merit of preference-based interactive summarisation is that
preferences are easier for users to provide than reference summaries. Existing
preference-based interactive learning methods suffer from high sample
complexity, i.e. they need to interact with the oracle for many rounds in order
to converge. In this work, we propose a new objective function, which enables
us to leverage active learning, preference learning and reinforcement learning
techniques in order to reduce the sample complexity. Both simulation and
real-user experiments suggest that our method significantly advances the state
of the art. Our source code is freely available at
https://github.com/UKPLab/emnlp2018-april.
</summary>
    <author>
      <name>Yang Gao</name>
    </author>
    <author>
      <name>Christian M. Meyer</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09645v1</id>
    <updated>2018-08-29T05:36:07Z</updated>
    <published>2018-08-29T05:36:07Z</published>
    <title>Diffusion Approximations for Online Principal Component Estimation and
  Global Convergence</title>
    <summary>  In this paper, we propose to adopt the diffusion approximation tools to study
the dynamics of Oja's iteration which is an online stochastic gradient descent
method for the principal component analysis. Oja's iteration maintains a
running estimate of the true principal component from streaming data and enjoys
less temporal and spatial complexities. We show that the Oja's iteration for
the top eigenvector generates a continuous-state discrete-time Markov chain
over the unit sphere. We characterize the Oja's iteration in three phases using
diffusion approximation and weak convergence tools. Our three-phase analysis
further provides a finite-sample error bound for the running estimate, which
matches the minimax information lower bound for principal component analysis
under the additional assumption of bounded samples.
</summary>
    <author>
      <name>Chris Junchi Li</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09642v1</id>
    <updated>2018-08-29T05:30:21Z</updated>
    <published>2018-08-29T05:30:21Z</published>
    <title>Online ICA: Understanding Global Dynamics of Nonconvex Optimization via
  Diffusion Processes</title>
    <summary>  Solving statistical learning problems often involves nonconvex optimization.
Despite the empirical success of nonconvex statistical optimization methods,
their global dynamics, especially convergence to the desirable local minima,
remain less well understood in theory. In this paper, we propose a new analytic
paradigm based on diffusion processes to characterize the global dynamics of
nonconvex statistical optimization. As a concrete example, we study stochastic
gradient descent (SGD) for the tensor decomposition formulation of independent
component analysis. In particular, we cast different phases of SGD into
diffusion processes, i.e., solutions to stochastic differential equations.
Initialized from an unstable equilibrium, the global dynamics of SGD transit
over three consecutive phases: (i) an unstable Ornstein-Uhlenbeck process
slowly departing from the initialization, (ii) the solution to an ordinary
differential equation, which quickly evolves towards the desirable local
minimum, and (iii) a stable Ornstein-Uhlenbeck process oscillating around the
desirable local minimum. Our proof techniques are based upon Stroock and
Varadhan's weak convergence of Markov chains to diffusion processes, which are
of independent interest.
</summary>
    <author>
      <name>Chris Junchi Li</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09634v1</id>
    <updated>2018-08-29T04:32:42Z</updated>
    <published>2018-08-29T04:32:42Z</published>
    <title>Voice Conversion Based on Cross-Domain Features Using Variational Auto
  Encoders</title>
    <summary>  An effective approach to non-parallel voice conversion (VC) is to utilize
deep neural networks (DNNs), specifically variational auto encoders (VAEs), to
model the latent structure of speech in an unsupervised manner. A previous
study has confirmed the ef- fectiveness of VAE using the STRAIGHT spectra for
VC. How- ever, VAE using other types of spectral features such as mel- cepstral
coefficients (MCCs), which are related to human per- ception and have been
widely used in VC, have not been prop- erly investigated. Instead of using one
specific type of spectral feature, it is expected that VAE may benefit from
using multi- ple types of spectral features simultaneously, thereby improving
the capability of VAE for VC. To this end, we propose a novel VAE framework
(called cross-domain VAE, CDVAE) for VC. Specifically, the proposed framework
utilizes both STRAIGHT spectra and MCCs by explicitly regularizing multiple
objectives in order to constrain the behavior of the learned encoder and de-
coder. Experimental results demonstrate that the proposed CD- VAE framework
outperforms the conventional VAE framework in terms of subjective tests.
</summary>
    <author>
      <name>Wen-Chin Huang</name>
    </author>
    <author>
      <name>Hsin-Te Hwang</name>
    </author>
    <author>
      <name>Yu-Huai Peng</name>
    </author>
    <author>
      <name>Yu Tsao</name>
    </author>
    <author>
      <name>Hsin-Min Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ISCSLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08270v2</id>
    <updated>2018-08-29T04:29:26Z</updated>
    <published>2018-08-24T19:22:12Z</published>
    <title>Building a Robust Text Classifier on a Test-Time Budget</title>
    <summary>  We propose a generic and interpretable learning framework for building robust
text classification model that achieves accuracy comparable to full models
under test-time budget constraints. Our approach learns a selector to identify
words that are relevant to the prediction tasks and passes them to the
classifier for processing. The selector is trained jointly with the classifier
and directly learns to incorporate with the classifier. We further propose a
data aggregation scheme to improve the robustness of the classifier. Our
learning framework is general and can be incorporated with any type of text
classification model. On real-world data, we show that the proposed approach
improves the performance of a given classifier and speeds up the model with a
mere loss in accuracy performance.
</summary>
    <author>
      <name>Md Rizwan Parvez</name>
    </author>
    <author>
      <name>Tolga Bolukbasi</name>
    </author>
    <author>
      <name>kai-Wei Chang</name>
    </author>
    <author>
      <name>Venkatesh Saligrama</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08270v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08270v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09633v1</id>
    <updated>2018-08-29T04:23:02Z</updated>
    <published>2018-08-29T04:23:02Z</published>
    <title>Improved Semantic-Aware Network Embedding with Fine-Grained Word
  Alignment</title>
    <summary>  Network embeddings, which learn low-dimensional representations for each
vertex in a large-scale network, have received considerable attention in recent
years. For a wide range of applications, vertices in a network are typically
accompanied by rich textual information such as user profiles, paper abstracts,
etc. We propose to incorporate semantic features into network embeddings by
matching important words between text sequences for all pairs of vertices. We
introduce a word-by-word alignment framework that measures the compatibility of
embeddings between word pairs, and then adaptively accumulates these alignment
features with a simple yet effective aggregation function. In experiments, we
evaluate the proposed framework on three real-world benchmarks for downstream
tasks, including link prediction and multi-label vertex classification. Results
demonstrate that our model outperforms state-of-the-art network embedding
methods by a large margin.
</summary>
    <author>
      <name>Dinghan Shen</name>
    </author>
    <author>
      <name>Xinyuan Zhang</name>
    </author>
    <author>
      <name>Ricardo Henao</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09484v2</id>
    <updated>2018-08-29T04:14:30Z</updated>
    <published>2018-05-24T02:22:08Z</published>
    <title>Multi-Level Deep Cascade Trees for Conversion Rate Prediction</title>
    <summary>  Developing effective and efficient recommendation methods is very challenging
for modern e-commerce platforms (e.g., Taobao). In this paper, we tackle this
problem by proposing multi-Level Deep Cascade Trees (ldcTree), which is a novel
decision tree ensemble approach. It leverages deep cascade structures by
stacking Gradient Boosting Decision Trees (GBDT) to effectively learn feature
representation. In addition, we propose to utilize the cross-entropy in each
tree of the preceding GBDT as the input feature representation for next level
GBDT, which has a clear explanation, i.e., a traversal from root to leaf nodes
in the next level GBDT corresponds to the combination of certain traversals in
the preceding GBDT. The deep cascade structure and the combination rule enable
the proposed ldcTree to have a stronger distributed feature representation
ability. Moreover, we propose an ensemble ldcTree to take full use of weak and
strong correlation features. Experimental results on off-line dataset and
online deployment demonstrate the effectiveness of the proposed methods.
</summary>
    <author>
      <name>Hong Wen</name>
    </author>
    <author>
      <name>Jing Zhang</name>
    </author>
    <author>
      <name>Quan Lin</name>
    </author>
    <author>
      <name>Keping Yang</name>
    </author>
    <author>
      <name>Taiwei Jin</name>
    </author>
    <author>
      <name>Fuyu Lv</name>
    </author>
    <author>
      <name>Xiaofeng Pan</name>
    </author>
    <author>
      <name>Pipei Huang</name>
    </author>
    <author>
      <name>Zheng-Jun Zha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, Submitted to CIKM'2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09484v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09484v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09617v1</id>
    <updated>2018-08-29T03:22:47Z</updated>
    <published>2018-08-29T03:22:47Z</published>
    <title>Elastic bands across the path: A new framework and methods to lower
  bound DTW</title>
    <summary>  There has been renewed recent interest in developing effective lower bounds
for Dynamic Time Warping (DTW) distance between time series. These have many
applications in time series indexing, clustering, forecasting, regression and
classification. One of the key time series classification algorithms, the
nearest neighbor algorithm with DTW distance (NN-DTW) is very expensive to
compute, due to the quadratic complexity of DTW. Lower bound search can speed
up NN-DTW substantially. An effective and tight lower bound quickly prunes off
unpromising nearest neighbor candidates from the search space and minimises the
number of the costly DTW computations. The speed up provided by lower bound
search becomes increasingly critical as training set size increases. Different
lower bounds provide different trade-offs between computation time and
tightness. Most existing lower bounds interact with DTW warping window sizes.
They are very tight and effective at smaller warping window sizes, but become
looser as the warping window increases, thus reducing the pruning effectiveness
for NN-DTW. In this work, we present a new class of lower bounds that are
tighter than the popular Keogh lower bound, while requiring similar computation
time. Our new lower bounds take advantage of the DTW boundary condition,
monotonicity and continuity constraints to create a tighter lower bound. Of
particular significance, they remain relatively tight even for large windows. A
single parameter to these new lower bounds controls the speed-tightness
trade-off. We demonstrate that these new lower bounds provide an exceptional
balance between computation time and tightness for the NN-DTW time series
classification task, resulting in greatly improved efficiency for NN-DTW lower
bound search.
</summary>
    <author>
      <name>Chang Wei Tan</name>
    </author>
    <author>
      <name>Francois Petitjean</name>
    </author>
    <author>
      <name>Geoffrey I. Webb</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09607v1</id>
    <updated>2018-08-29T02:24:50Z</updated>
    <published>2018-08-29T02:24:50Z</published>
    <title>Nonlinear regression based on a hybrid quantum computer</title>
    <summary>  Incorporating nonlinearity into quantum machine learning is essential for
learning a complicated input-output mapping. We here propose quantum algorithms
for nonlinear regression, where nonlinearity is introduced with feature maps
when loading classical data into quantum states. Our implementation is based on
a hybrid quantum computer, exploiting both discrete and continuous variables,
for their capacity to encode novel features and efficiency of processing
information. We propose encoding schemes that can realize well-known polynomial
and Gaussian kernel ridge regressions, with exponentially speed-up regarding to
the number of samples.
</summary>
    <author>
      <name>Dan-Bo Zhang</name>
    </author>
    <author>
      <name>Shi-Liang Zhu</name>
    </author>
    <author>
      <name>Z. D. Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03144v3</id>
    <updated>2018-08-29T02:21:07Z</updated>
    <published>2018-02-09T06:37:05Z</published>
    <title>Neural Dynamic Programming for Musical Self Similarity</title>
    <summary>  We present a neural sequence model designed specifically for symbolic music.
The model is based on a learned edit distance mechanism which generalises a
classic recursion from computer sci- ence, leading to a neural dynamic program.
Re- peated motifs are detected by learning the transfor- mations between them.
We represent the arising computational dependencies using a novel data
structure, the edit tree; this perspective suggests natural approximations
which afford the scaling up of our otherwise cubic time algorithm. We
demonstrate our model on real and synthetic data; in all cases it out-performs
a strong stacked long short-term memory benchmark.
</summary>
    <author>
      <name>Christian J. Walder</name>
    </author>
    <author>
      <name>Dongwoo Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1802.03144v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03144v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07226v2</id>
    <updated>2018-08-29T02:06:06Z</updated>
    <published>2018-08-22T05:43:16Z</published>
    <title>Mean-field approximation, convex hierarchies, and the optimality of
  correlation rounding: a unified perspective</title>
    <summary>  The free energy is a key quantity of interest in Ising models, but
unfortunately, computing it in general is computationally intractable. Two
popular (variational) approximation schemes for estimating the free energy of
general Ising models (in particular, even in regimes where correlation decay
does not hold) are: (i) the mean-field approximation with roots in statistical
physics, which estimates the free energy from below, and (ii) hierarchies of
convex relaxations with roots in theoretical computer science, which estimate
the free energy from above. We show, surprisingly, that the tight regime for
both methods to compute the free energy to leading order is identical.
  More precisely, we show that the mean-field approximation is within
$O((n\|J\|_{F})^{2/3})$ of the free energy, where $\|J\|_F$ denotes the
Frobenius norm of the interaction matrix of the Ising model. This
simultaneously subsumes both the breakthrough work of Basak and Mukherjee, who
showed the tight result that the mean-field approximation is within $o(n)$
whenever $\|J\|_{F} = o(\sqrt{n})$, as well as the work of Jain, Koehler, and
Mossel, who gave the previously best known non-asymptotic bound of
$O((n\|J\|_{F})^{2/3}\log^{1/3}(n\|J\|_{F}))$. We give a simple, algorithmic
proof of this result using a convex relaxation proposed by Risteski based on
the Sherali-Adams hierarchy, automatically giving sub-exponential time
approximation schemes for the free energy in this entire regime. Our
algorithmic result is tight under Gap-ETH.
  We furthermore combine our techniques with spin glass theory to prove (in a
strong sense) the optimality of correlation rounding, refuting a recent
conjecture of Allen, O'Donnell, and Zhou. Finally, we give the tight
generalization of all of these results to $k$-MRFs, capturing as a special case
previous work on approximating MAX-$k$-CSP.
</summary>
    <author>
      <name>Vishesh Jain</name>
    </author>
    <author>
      <name>Frederic Koehler</name>
    </author>
    <author>
      <name>Andrej Risteski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version: minor formatting changes, added grant acknowledgements</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07226v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07226v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09574v1</id>
    <updated>2018-08-28T23:03:55Z</updated>
    <published>2018-08-28T23:03:55Z</published>
    <title>Probabilistic Sparse Subspace Clustering Using Delayed Association</title>
    <summary>  Discovering and clustering subspaces in high-dimensional data is a
fundamental problem of machine learning with a wide range of applications in
data mining, computer vision, and pattern recognition. Earlier methods divided
the problem into two separate stages of finding the similarity matrix and
finding clusters. Similar to some recent works, we integrate these two steps
using a joint optimization approach. We make the following contributions: (i)
we estimate the reliability of the cluster assignment for each point before
assigning a point to a subspace. We group the data points into two groups of
"certain" and "uncertain", with the assignment of latter group delayed until
their subspace association certainty improves. (ii) We demonstrate that delayed
association is better suited for clustering subspaces that have ambiguities,
i.e. when subspaces intersect or data are contaminated with outliers/noise.
(iii) We demonstrate experimentally that such delayed probabilistic association
leads to a more accurate self-representation and final clusters. The proposed
method has higher accuracy both for points that exclusively lie in one
subspace, and those that are on the intersection of subspaces. (iv) We show
that delayed association leads to huge reduction of computational cost, since
it allows for incremental spectral clustering.
</summary>
    <author>
      <name>Maryam Jaberi</name>
    </author>
    <author>
      <name>Marianna Pensky</name>
    </author>
    <author>
      <name>Hassan Foroosh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICPR 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.09574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08012v4</id>
    <updated>2018-08-28T21:52:57Z</updated>
    <published>2018-02-22T12:39:59Z</published>
    <title>Learning Topic Models by Neighborhood Aggregation</title>
    <summary>  Topic models are frequently used in machine learning owing to their high
interpretability and modular structure. However, extending a topic model to
include a supervisory signal, to incorporate pre-trained word embedding vectors
and to include a nonlinear output function is not an easy task because one has
to resort to a highly intricate approximate inference procedure. The present
paper shows that topic modeling with pre-trained word embedding vectors can be
viewed as implementing a neighborhood aggregation algorithm where messages are
passed through a network defined over words. From the network view of topic
models, nodes correspond to words in a document and edges correspond to either
a relationship describing co-occurring words in a document or a relationship
describing the same word in the corpus. The network view allows us to extend
the model to include supervisory signals, incorporate pre-trained word
embedding vectors and include a nonlinear output function in a simple manner.
In experiments, we show that our approach outperforms the state-of-the-art
supervised Latent Dirichlet Allocation implementation in terms of both held-out
document classification tasks and topic coherence.
</summary>
    <author>
      <name>Ryohei Hisano</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08012v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08012v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09551v1</id>
    <updated>2018-08-28T21:44:26Z</updated>
    <published>2018-08-28T21:44:26Z</published>
    <title>Explaining Character-Aware Neural Networks for Word-Level Prediction: Do
  They Discover Linguistic Rules?</title>
    <summary>  Character-level features are currently used in different neural network-based
natural language processing algorithms. However, little is known about the
character-level patterns those models learn. Moreover, models are often
compared only quantitatively while a qualitative analysis is missing. In this
paper, we investigate which character-level patterns neural networks learn and
if those patterns coincide with manually-defined word segmentations and
annotations. To that end, we extend the contextual decomposition technique
(Murdoch et al. 2018) to convolutional neural networks which allows us to
compare convolutional neural networks and bidirectional long short-term memory
networks. We evaluate and compare these models for the task of morphological
tagging on three morphologically different languages and show that these models
implicitly discover understandable linguistic rules. Our implementation can be
found at https://github.com/FredericGodin/ContextualDecomposition-NLP .
</summary>
    <author>
      <name>Fréderic Godin</name>
    </author>
    <author>
      <name>Kris Demuynck</name>
    </author>
    <author>
      <name>Joni Dambre</name>
    </author>
    <author>
      <name>Wesley De Neve</name>
    </author>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08156v4</id>
    <updated>2018-08-28T20:33:50Z</updated>
    <published>2018-06-21T10:23:23Z</published>
    <title>Identifiability of Gaussian Structural Equation Models with Dependent
  Errors Having Equal Variances</title>
    <summary>  In this paper, we prove that some Gaussian structural equation models with
dependent errors having equal variances are identifiable from their
corresponding Gaussian distributions. Specifically, we prove identifiability
for the Gaussian structural equation models that can be represented as
Andersson-Madigan-Perlman chain graphs (Andersson et al., 2001). These chain
graphs were originally developed to represent independence models. However,
they are also suitable for representing causal models with additive noise
(Pe\~na, 2016. Our result implies then that these causal models can be
identified from observational data alone. Our result generalizes the result by
Peters and B\"uhlmann (2014), who considered independent errors having equal
variances. The suitability of the equal error variances assumption should be
assessed on a per domain basis.
</summary>
    <author>
      <name>Jose M. Peña</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7th Causal Inference Workshop at UAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08156v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08156v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09517v1</id>
    <updated>2018-08-28T20:01:11Z</updated>
    <published>2018-08-28T20:01:11Z</published>
    <title>Extracting Epistatic Interactions in Type 2 Diabetes Genome-Wide Data
  Using Stacked Autoencoder</title>
    <summary>  2 Diabetes is a leading worldwide public health concern, and its increasing
prevalence has significant health and economic importance in all nations. The
condition is a multifactorial disorder with a complex aetiology. The genetic
determinants remain largely elusive, with only a handful of identified
candidate genes. Genome wide association studies (GWAS) promised to
significantly enhance our understanding of genetic based determinants of common
complex diseases. To date, 83 single nucleotide polymorphisms (SNPs) for type 2
diabetes have been identified using GWAS. Standard statistical tests for single
and multi-locus analysis such as logistic regression, have demonstrated little
effect in understanding the genetic architecture of complex human diseases.
Logistic regression is modelled to capture linear interactions but neglects the
non-linear epistatic interactions present within genetic data. There is an
urgent need to detect epistatic interactions in complex diseases as this may
explain the remaining missing heritability in such diseases. In this paper, we
present a novel framework based on deep learning algorithms that deal with
non-linear epistatic interactions that exist in genome wide association data.
Logistic association analysis under an additive genetic model, adjusted for
genomic control inflation factor, is conducted to remove statistically
improbable SNPs to minimize computational overheads.
</summary>
    <author>
      <name>Basma Abdulaimma</name>
    </author>
    <author>
      <name>Paul Fergus</name>
    </author>
    <author>
      <name>Carl Chalmers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01494v1</id>
    <updated>2018-08-28T19:44:51Z</updated>
    <published>2018-08-28T19:44:51Z</published>
    <title>Interpretation of Natural Language Rules in Conversational Machine
  Reading</title>
    <summary>  Most work in machine reading focuses on question answering problems where the
answer is directly expressed in the text to read. However, many real-world
question answering problems require the reading of text not because it contains
the literal answer, but because it contains a recipe to derive an answer
together with the reader's background knowledge. One example is the task of
interpreting regulations to answer "Can I...?" or "Do I have to...?" questions
such as "I am working in Canada. Do I have to carry on paying UK National
Insurance?" after reading a UK government website about this topic. This task
requires both the interpretation of rules and the application of background
knowledge. It is further complicated due to the fact that, in practice, most
questions are underspecified, and a human assistant will regularly have to ask
clarification questions such as "How long have you been working abroad?" when
the answer cannot be directly derived from the question and text. In this
paper, we formalise this task and develop a crowd-sourcing strategy to collect
32k task instances based on real-world rules and crowd-generated questions and
scenarios. We analyse the challenges of this task and assess its difficulty by
evaluating the performance of rule-based and machine-learning baselines. We
observe promising results when no background knowledge is necessary, and
substantial room for improvement whenever background knowledge is needed.
</summary>
    <author>
      <name>Marzieh Saeidi</name>
    </author>
    <author>
      <name>Max Bartolo</name>
    </author>
    <author>
      <name>Patrick Lewis</name>
    </author>
    <author>
      <name>Sameer Singh</name>
    </author>
    <author>
      <name>Tim Rocktäschel</name>
    </author>
    <author>
      <name>Mike Sheldon</name>
    </author>
    <author>
      <name>Guillaume Bouchard</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09501v1</id>
    <updated>2018-08-28T19:15:12Z</updated>
    <published>2018-08-28T19:15:12Z</published>
    <title>Concentrated Differentially Private Gradient Descent with Adaptive
  per-Iteration Privacy Budget</title>
    <summary>  Iterative algorithms, like gradient descent, are common tools for solving a
variety of problems, such as model fitting. For this reason, there is interest
in creating differentially private versions of them. However, their conversion
to differentially private algorithms is often naive. For instance, a fixed
number of iterations are chosen, the privacy budget is split evenly among them,
and at each iteration, parameters are updated with a noisy gradient. In this
paper, we show that gradient-based algorithms can be improved by a more careful
allocation of privacy budget per iteration. Intuitively, at the beginning of
the optimization, gradients are expected to be large, so that they do not need
to be measured as accurately. However, as the parameters approach their optimal
values, the gradients decrease and hence need to be measured more accurately.
We add a basic line-search capability that helps the algorithm decide when more
accurate gradient measurements are necessary. Our gradient descent algorithm
works with the recently introduced zCDP version of differential privacy. It
outperforms prior algorithms for model fitting and is competitive with the
state-of-the-art for $(\epsilon,\delta)$-differential privacy, a strictly
weaker definition than zCDP.
</summary>
    <author>
      <name>Jaewoo Lee</name>
    </author>
    <author>
      <name>Daniel Kifer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09489v1</id>
    <updated>2018-08-28T18:47:20Z</updated>
    <published>2018-08-28T18:47:20Z</published>
    <title>Convergence of Krasulina Scheme</title>
    <summary>  Principal component analysis (PCA) is one of the most commonly used
statistical procedures with a wide range of applications. Consider the points
$X_1, X_2,..., X_n$ are vectors drawn i.i.d. from a distribution with mean zero
and covariance $\Sigma$, where $\Sigma$ is unknown. Let $A_n = X_nX_n^T$, then
$E[A_n] = \Sigma$. This paper consider the problem of finding the least
eigenvalue and eigenvector of matrix $\Sigma$. A classical such estimator are
due to Krasulina\cite{krasulina_method_1969}. We are going to state the
convergence proof of Krasulina for the least eigenvalue and corresponding
eigenvector, and then find their convergence rate.
</summary>
    <author>
      <name>Jiangning Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05689v2</id>
    <updated>2018-08-28T18:19:23Z</updated>
    <published>2018-08-16T22:02:15Z</published>
    <title>Graph Edit Distance Computation via Graph Neural Networks</title>
    <summary>  Graph similarity search is among the most important graph-based applications,
e.g. finding the chemical compounds that are most similar to a query compound.
Graph similarity/distance computation, such as Graph Edit Distance (GED) and
Maximum Common Subgraph (MCS), is the core operation of graph similarity search
and many other applications, which is usually very costly to compute. Inspired
by the recent success of neural network approaches to several graph
applications, such as node classification and graph classification, we propose
a novel neural network-based approach to address this challenging while
classical graph problem, with the hope to alleviate the computational burden
while preserving a good performance. Our model generalizes to unseen graphs,
and in the worst case runs in linear time with respect to the number of nodes
in two graphs. Taking GED computation as an example, experimental results on
three real graph datasets demonstrate the effectiveness and efficiency of our
approach. Specifically, our model achieves smaller error and great time
reduction compared against several approximate algorithms on GED computation.
To the best of our knowledge, we are among the first to adopt neural networks
to model the similarity between two graphs, and provide a new direction for
future research on graph similarity computation and graph similarity search.
</summary>
    <author>
      <name>Yunsheng Bai</name>
    </author>
    <author>
      <name>Hao Ding</name>
    </author>
    <author>
      <name>Song Bian</name>
    </author>
    <author>
      <name>Ting Chen</name>
    </author>
    <author>
      <name>Yizhou Sun</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05689v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05689v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09442v1</id>
    <updated>2018-08-28T17:59:08Z</updated>
    <published>2018-08-28T17:59:08Z</published>
    <title>Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning</title>
    <summary>  This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving
the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed
framework that extends the Dyna-Q algorithm to integrate planning for
task-completion dialogue policy learning. To obviate DDQ's high dependency on
the quality of simulated experiences, we incorporate an RNN-based discriminator
in D3Q to differentiate simulated experience from real user experience in order
to control the quality of training data. Experiments show that D3Q
significantly outperforms DDQ by controlling the quality of simulated
experience used for planning. The effectiveness and robustness of D3Q is
further demonstrated in a domain extension setting, where the agent's
capability of adapting to a changing environment is tested.
</summary>
    <author>
      <name>Shang-Yu Su</name>
    </author>
    <author>
      <name>Xiujun Li</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Jingjing Liu</name>
    </author>
    <author>
      <name>Yun-Nung Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07018v2</id>
    <updated>2018-08-28T17:50:05Z</updated>
    <published>2018-08-21T17:05:28Z</published>
    <title>Hypernetwork Knowledge Graph Embeddings</title>
    <summary>  Knowledge graphs are large graph-structured databases of facts, which
typically suffer from incompleteness. Link prediction is the task of inferring
missing relations (links) between entities (nodes) in a knowledge graph. We
propose to solve this task by using a hypernetwork architecture to generate
convolutional layer filters specific to each relation and apply those filters
to the subject entity embeddings. This architecture enables a trade-off between
non-linear expressiveness and the number of parameters to learn. Our model
simplifies the entity and relation embedding interactions introduced by the
predecessor convolutional model, while outperforming all previous approaches to
link prediction across all standard link prediction datasets.
</summary>
    <author>
      <name>Ivana Balazevic</name>
    </author>
    <author>
      <name>Carl Allen</name>
    </author>
    <author>
      <name>Timothy M. Hospedales</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08773v2</id>
    <updated>2018-08-28T17:30:39Z</updated>
    <published>2018-08-27T10:37:16Z</published>
    <title>Learning Multilingual Word Embeddings in Latent Metric Space: A
  Geometric Approach</title>
    <summary>  We propose a novel geometric approach for learning bilingual mappings given
monolingual embeddings and a bilingual dictionary. Our approach decouples
learning the transformation from the source language to the target language
into (a) learning rotations for language-specific embeddings to align them to a
common space, and (b) learning a similarity metric in the common space to model
similarities between the embeddings. We model the bilingual mapping problem as
an optimization problem on smooth Riemannian manifolds. We show that our
approach outperforms previous approaches on the bilingual lexicon induction and
cross-lingual word similarity tasks. We also generalize our framework to
represent multiple languages in a common latent space. In particular, the
latent space representations for several languages are learned jointly, given
bilingual dictionaries for multiple language pairs. We illustrate the
effectiveness of joint learning for multiple languages in zero-shot word
translation setting.
</summary>
    <author>
      <name>Pratik Jawanpuria</name>
    </author>
    <author>
      <name>Arjun Balgovind</name>
    </author>
    <author>
      <name>Anoop Kunchukuttan</name>
    </author>
    <author>
      <name>Bamdev Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08773v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08773v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09785v1</id>
    <updated>2018-08-28T15:53:14Z</updated>
    <published>2018-08-28T15:53:14Z</published>
    <title>Using Taste Groups for Collaborative Filtering</title>
    <summary>  Implicit feedback is the simplest form of user feedback that can be used for
item recommendation. It is easy to collect and domain independent. However,
there is a lack of negative examples. Existing works circumvent this problem by
making various assumptions regarding the unconsumed items, which fail to hold
when the user did not consume an item because she was unaware of it. In this
paper, we propose as a novel method for addressing the lack of negative
examples in implicit feedback. The motivation is that if there is a large group
of users who share the same taste and none of them consumed an item, then it is
highly likely that the item is irrelevant to this taste. We use Hierarchical
Latent Tree Analysis(HLTA) to identify taste-based user groups and make
recommendations for a user based on her memberships in the groups.
</summary>
    <author>
      <name>Farhan Khawar</name>
    </author>
    <author>
      <name>Nevin L. Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RecSys 2018 LBRS. arXiv admin note: substantial text overlap with
  arXiv:1704.01889</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09371v1</id>
    <updated>2018-08-28T15:40:47Z</updated>
    <published>2018-08-28T15:40:47Z</published>
    <title>Matrix Factorization Equals Efficient Co-occurrence Representation</title>
    <summary>  Matrix factorization is a simple and effective solution to the recommendation
problem. It has been extensively employed in the industry and has attracted
much attention from the academia. However, it is unclear what the
low-dimensional matrices represent. We show that matrix factorization can
actually be seen as simultaneously calculating the eigenvectors of the
user-user and item-item sample co-occurrence matrices. We then use insights
from random matrix theory (RMT) to show that picking the top eigenvectors
corresponds to removing sampling noise from user/item co-occurrence matrices.
Therefore, the low-dimension matrices represent a reduced noise user and item
co-occurrence space. We also analyze the structure of the top eigenvector and
show that it corresponds to global effects and removing it results in less
popular items being recommended. This increases the diversity of the items
recommended without affecting the accuracy.
</summary>
    <author>
      <name>Farhan Khawar</name>
    </author>
    <author>
      <name>Nevin L. Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RecSys 2018 LBRS</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01653v2</id>
    <updated>2018-08-28T15:34:03Z</updated>
    <published>2018-04-05T02:23:59Z</published>
    <title>Review of Deep Learning</title>
    <summary>  In recent years, China, the United States and other countries, Google and
other high-tech companies have increased investment in artificial intelligence.
Deep learning is one of the current artificial intelligence research's key
areas. This paper analyzes and summarizes the latest progress and future
research directions of deep learning. Firstly, three basic models of deep
learning are outlined, including multilayer perceptrons, convolutional neural
networks, and recurrent neural networks. On this basis, we further analyze the
emerging new models of convolution neural networks and recurrent neural
networks. This paper then summarizes deep learning's applications in many areas
of artificial intelligence, including speech processing, computer vision,
natural language processing and so on. Finally, this paper discusses the
existing problems of deep learning and gives the corresponding possible
solutions.
</summary>
    <author>
      <name>Rong Zhang</name>
    </author>
    <author>
      <name>Weiping Li</name>
    </author>
    <author>
      <name>Tong Mo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Chinese. Have been published in the journal "Information and
  Control"</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01653v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01653v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09347v1</id>
    <updated>2018-08-28T15:04:32Z</updated>
    <published>2018-08-28T15:04:32Z</published>
    <title>Joint Domain Alignment and Discriminative Feature Learning for
  Unsupervised Deep Domain Adaptation</title>
    <summary>  Recently, considerable effort has been devoted to deep domain adaptation in
computer vision and machine learning communities. However, most of existing
work only concentrates on learning shared feature representation by minimizing
the distribution discrepancy across different domains. Due to the fact that all
the domain alignment approaches can only reduce, but not remove the domain
shift. Target domain samples distributed near the edge of the clusters, or far
from their corresponding class centers are easily to be misclassified by the
hyperplane learned from the source domain. To alleviate this issue, we propose
to joint domain alignment and discriminative feature learning, which could
benefit both domain alignment and final classification. Specifically, an
instance-based discriminative feature learning method and a center-based
discriminative feature learning method are proposed, both of which guarantee
the domain invariant features with better intra-class compactness and
inter-class separability. Extensive experiments show that learning the
discriminative features in the shared feature space can significantly boost the
performance of deep domain adaptation methods.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Zhihong Chen</name>
    </author>
    <author>
      <name>Boyuan Jiang</name>
    </author>
    <author>
      <name>Xinyu Jin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09334v1</id>
    <updated>2018-08-28T14:47:33Z</updated>
    <published>2018-08-28T14:47:33Z</published>
    <title>A Discriminative Latent-Variable Model for Bilingual Lexicon Induction</title>
    <summary>  We introduce a novel discriminative latent-variable model for the task of
bilingual lexicon induction. Our model combines the bipartite matching
dictionary prior of Haghighi et al. (2008) with a state-of-the-art
embedding-based approach. To train the model, we derive an efficient Viterbi EM
algorithm. We provide empirical improvements on six language pairs under two
metrics and show that the prior theoretically and empirically helps to mitigate
the hubness problem. We also demonstrate how previous work may be viewed as a
similarly fashioned latent-variable model, albeit with a different prior.
</summary>
    <author>
      <name>Sebastian Ruder</name>
    </author>
    <author>
      <name>Ryan Cotterell</name>
    </author>
    <author>
      <name>Yova Kementchedjhieva</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07325v2</id>
    <updated>2018-08-28T13:35:25Z</updated>
    <published>2018-08-22T12:03:48Z</published>
    <title>An Attention-Gated Convolutional Neural Network for Sentence
  Classification</title>
    <summary>  The classification task of sentences is very challenging because of the
limited contextual information that sentences contain. In this paper, we
propose an Attention Gated Convolutional Neural Network (AGCNN) for sentence
classification, which generates attention weights from the feature's context
windows of different sizes by using specialized convolution encoders, to
enhance the influence of critical features in predicting the sentence's
category. Experimental results demonstrate that our model could achieve a
general accuracy improvement highest up to 3.1% (compared with standard CNN
models), and gain competitive results over the strong baseline methods on four
out of the six tasks. Besides, we propose an activation function named Natural
Logarithm rescaled Rectified Linear Unit (NLReLU). Experimental results show
that NLReLU could outperform ReLU and performs comparably to other well-known
activation functions on AGCNN.
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Lixin Ji</name>
    </author>
    <author>
      <name>Ruiyang Huang</name>
    </author>
    <author>
      <name>Tuosiyu Ming</name>
    </author>
    <author>
      <name>Chao Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures and 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07325v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07325v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08646v2</id>
    <updated>2018-08-28T13:30:10Z</updated>
    <published>2018-08-27T00:54:02Z</published>
    <title>The Disparate Effects of Strategic Manipulation</title>
    <summary>  When consequential decisions are informed by algorithmic input, individuals
may feel compelled to alter their behavior in order to gain a system's
approval. Previous models of agent responsiveness, termed "strategic
manipulation," have analyzed the interaction between a learner and agents in a
world where all agents are equally able to manipulate their features in an
attempt to "trick" a published classifier. In cases of real world
classification, however, an agent's ability to adapt to an algorithm, is not
simply a function of her personal interest in receiving a positive
classification, but is bound up in a complex web of social factors that affect
her ability to pursue certain action responses. In this paper, we adapt models
of strategic manipulation to better capture dynamics that may arise in a
setting of social inequality wherein candidate groups face different costs to
manipulation. We find that whenever one group's costs are higher than the
other's, the learner's equilibrium strategy exhibits an inequality-reinforcing
phenomenon wherein the learner erroneously admits some members of the
advantaged group, while erroneously excluding some members of the disadvantaged
group. We also consider the effects of potential interventions in which a
learner can subsidize members of the disadvantaged group, lowering their costs
in order to improve her own classification performance. Here we encounter a
paradoxical result: there exist cases in which providing a subsidy improves
only the learner's utility while actually making both candidate groups
worse-off--even the group receiving the subsidy. Our results reveal the
potentially adverse social ramifications of deploying tools that attempt to
evaluate an individual's "quality" when agents' capacities to adaptively
respond differ.
</summary>
    <author>
      <name>Lily Hu</name>
    </author>
    <author>
      <name>Nicole Immorlica</name>
    </author>
    <author>
      <name>Jennifer Wortman Vaughan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08646v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08646v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09271v1</id>
    <updated>2018-08-28T13:14:33Z</updated>
    <published>2018-08-28T13:14:33Z</published>
    <title>Distance Based Source Domain Selection for Sentiment Classification</title>
    <summary>  Automated sentiment classification (SC) on short text fragments has received
increasing attention in recent years. Performing SC on unseen domains with few
or no labeled samples can significantly affect the classification performance
due to different expression of sentiment in source and target domain. In this
study, we aim to mitigate this undesired impact by proposing a methodology
based on a predictive measure, which allows us to select an optimal source
domain from a set of candidates. The proposed measure is a linear combination
of well-known distance functions between probability distributions supported on
the source and target domains (e.g. Earth Mover's distance and Kullback-Leibler
divergence). The performance of the proposed methodology is validated through
an SC case study in which our numerical experiments suggest a significant
improvement in the cross domain classification error in comparison with a
random selected source domain for both a naive and adaptive learning setting.
In the case of more heterogeneous datasets, the predictability feature of the
proposed model can be utilized to further select a subset of candidate domains,
where the corresponding classifier outperforms the one trained on all available
source domains. This observation reinforces a hypothesis that our proposed
model may also be deployed as a means to filter out redundant information
during a training phase of SC.
</summary>
    <author>
      <name>Lex Razoux Schultz</name>
    </author>
    <author>
      <name>Marco Loog</name>
    </author>
    <author>
      <name>Peyman Mohajerin Esfahani</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09784v1</id>
    <updated>2018-08-28T13:07:34Z</updated>
    <published>2018-08-28T13:07:34Z</published>
    <title>Superhighway: Bypass Data Sparsity in Cross-Domain CF</title>
    <summary>  Cross-domain collaborative filtering (CF) aims to alleviate data sparsity in
single-domain CF by leveraging knowledge transferred from related domains. Many
traditional methods focus on enriching compared neighborhood relations in CF
directly to address the sparsity problem. In this paper, we propose
superhighway construction, an alternative explicit relation-enrichment
procedure, to improve recommendations by enhancing cross-domain connectivity.
Specifically, assuming partially overlapped items (users), superhighway
bypasses multi-hop inter-domain paths between cross-domain users (items,
respectively) with direct paths to enrich the cross-domain connectivity. The
experiments conducted on a real-world cross-region music dataset and a
cross-platform movie dataset show that the proposed superhighway construction
significantly improves recommendation performance in both target and source
domains.
</summary>
    <author>
      <name>Kwei-Herng Lai</name>
    </author>
    <author>
      <name>Ting-Hsiang Wang</name>
    </author>
    <author>
      <name>Heng-Yu Chi</name>
    </author>
    <author>
      <name>Yian Chen</name>
    </author>
    <author>
      <name>Ming-Feng Tsai</name>
    </author>
    <author>
      <name>Chuan-Ju Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03847v3</id>
    <updated>2018-08-28T11:22:49Z</updated>
    <published>2017-06-12T20:49:23Z</published>
    <title>Recurrent Neural Networks with Top-k Gains for Session-based
  Recommendations</title>
    <summary>  RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.
</summary>
    <author>
      <name>Balázs Hidasi</name>
    </author>
    <author>
      <name>Alexandros Karatzoglou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271761</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271761" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM'18, authors' version</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03847v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03847v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08568v2</id>
    <updated>2018-08-28T11:13:40Z</updated>
    <published>2018-06-22T09:22:42Z</published>
    <title>Continuous Learning in Single-Incremental-Task Scenarios</title>
    <summary>  It was recently shown that architectural, regularization and rehearsal
strategies can be used to train deep models sequentially on a number of
disjoint tasks without forgetting previously acquired knowledge. However, these
strategies are still unsatisfactory if the tasks are not disjoint but
constitute a single incremental task (e.g., class-incremental learning). In
this paper we point out the differences between multi-task and
single-incremental-task scenarios and show that well-known approaches such as
LWF, EWC and SI are not ideal for incremental task scenarios. A new approach,
denoted as AR1, combining architectural and regularization strategies is then
specifically proposed. AR1 overhead (in term of memory and computation) is very
small thus making it suitable for online learning. When tested on CORe50 and
iCIFAR-100, AR1 outperformed existing regularization strategies by a good
margin.
</summary>
    <author>
      <name>Davide Maltoni</name>
    </author>
    <author>
      <name>Vincenzo Lomonaco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 14 figures; v2: several typos and minor mistakes corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08568v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08568v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09222v1</id>
    <updated>2018-08-28T11:09:02Z</updated>
    <published>2018-08-28T11:09:02Z</published>
    <title>Making \emph{ordinary least squares} linear classfiers more robust</title>
    <summary>  In the field of statistics and machine learning, the sums-of-squares,
commonly referred to as \emph{ordinary least squares}, can be used as a
convenient choice of cost function because of its many nice analytical
properties, though not always the best choice. However, it has been long known
that \emph{ordinary least squares} is not robust to outliers. Several attempts
to resolve this problem led to the creation of alternative methods that, either
did not fully resolved the \emph{outlier problem} or were computationally
difficult. In this paper, we provide a very simple solution that can make
\emph{ordinary least squares} less sensitive to outliers in data
classification, by \emph{scaling the augmented input vector by its length}. We
show some mathematical expositions of the \emph{outlier problem} using some
approximations and geometrical techniques. We present numerical results to
support the efficacy of our method.
</summary>
    <author>
      <name>Babatunde M. Ayeni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages with 6 figures. Comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06935v2</id>
    <updated>2018-08-28T10:44:14Z</updated>
    <published>2018-08-21T14:43:13Z</published>
    <title>Smart energy models for atomistic simulations using a DFT-driven
  multifidelity approach</title>
    <summary>  The reliability of atomistic simulations depends on the quality of the
underlying energy models providing the source of physical information, for
instance for the calculation of migration barriers in atomistic Kinetic Monte
Carlo simulations. Accurate (high-fidelity) methods are often available, but
since they are usually computationally expensive, they must be replaced by less
accurate (low-fidelity) models that introduce some degrees of approximation.
Machine-learning techniques such as artificial neural networks are usually
employed to work around this limitation and extract the needed parameters from
large databases of high-fidelity data, but the latter are often computationally
expensive to produce. This work introduces an alternative method based on the
multifidelity approach, where correlations between high-fidelity and
low-fidelity outputs are exploited to make an educated guess of the
high-fidelity outcome based only on quick low-fidelity estimations, hence
without the need of running full expensive high-fidelity calculations. With
respect to neural networks, this approach is expected to require less training
data because of the lower amount of fitting parameters involved. The method is
tested on the prediction of ab initio formation and migration energies of
vacancy diffusion in iron-copper alloys, and compared with the neural networks
trained on the same database.
</summary>
    <author>
      <name>Luca Messina</name>
    </author>
    <author>
      <name>Alessio Quaglino</name>
    </author>
    <author>
      <name>Alexandra Goryaeva</name>
    </author>
    <author>
      <name>Mihai-Cosmin Marinica</name>
    </author>
    <author>
      <name>Christophe Domain</name>
    </author>
    <author>
      <name>Nicolas Castin</name>
    </author>
    <author>
      <name>Giovanni Bonny</name>
    </author>
    <author>
      <name>Rolf Krause</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the COSIRES 2018 conference, submitted for peer-review
  in Nuclear Instruments and Methods in Physics Research B: Beam Interactions
  with Materials and Atoms</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06935v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06935v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00177v2</id>
    <updated>2018-08-28T09:08:32Z</updated>
    <published>2018-08-01T06:02:36Z</published>
    <title>Learning Dexterous In-Hand Manipulation</title>
    <summary>  We use reinforcement learning (RL) to learn dexterous in-hand manipulation
policies which can perform vision-based object reorientation on a physical
Shadow Dexterous Hand. The training is performed in a simulated environment in
which we randomize many of the physical properties of the system like friction
coefficients and an object's appearance. Our policies transfer to the physical
robot despite being trained entirely in simulation. Our method does not rely on
any human demonstrations, but many behaviors found in human manipulation emerge
naturally, including finger gaiting, multi-finger coordination, and the
controlled use of gravity. Our results were obtained using the same distributed
RL system that was used to train OpenAI Five. We also include a video of our
results: https://youtu.be/jwSbzNHGflM
</summary>
    <author>
      <name> OpenAI</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Marcin Andrychowicz</name>
    </author>
    <author>
      <name>Bowen Baker</name>
    </author>
    <author>
      <name>Maciek Chociej</name>
    </author>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Bob McGrew</name>
    </author>
    <author>
      <name>Jakub Pachocki</name>
    </author>
    <author>
      <name>Arthur Petron</name>
    </author>
    <author>
      <name>Matthias Plappert</name>
    </author>
    <author>
      <name>Glenn Powell</name>
    </author>
    <author>
      <name>Alex Ray</name>
    </author>
    <author>
      <name>Jonas Schneider</name>
    </author>
    <author>
      <name>Szymon Sidor</name>
    </author>
    <author>
      <name>Josh Tobin</name>
    </author>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Lilian Weng</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures, minor typos fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00177v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00177v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09446v1</id>
    <updated>2018-08-28T08:30:12Z</updated>
    <published>2018-08-28T08:30:12Z</published>
    <title>A Particle Filter based Multi-Objective Optimization Algorithm: PFOPS</title>
    <summary>  This letter is concerned with a recently developed paradigm of
population-based optimization, termed particle filter optimization (PFO). In
contrast with the commonly used meta-heuristics based methods, the PFO paradigm
is attractive in terms of coherence in theory and easiness in mathematical
analysis and interpretation. However, current PFO algorithms only work for
single-objective optimization cases, while many real-life problems involve
multiple objectives to be optimized simultaneously. To this end, we make an
effort to extend the scope of application of the PFO paradigm to
multi-objective optimization (MOO) cases. An idea called path sampling is
adopted within the PFO scheme to balance the different objectives to be
optimized. The resulting algorithm is thus termed PFO with Path Sampling
(PFOPS). Experimental results show that the proposed algorithm works
consistently well for three different types of MOO problems, which are
characterized by an associated convex, concave and discontinuous Pareto front,
respectively.
</summary>
    <author>
      <name>Bin Liu</name>
    </author>
    <author>
      <name>Yaochu Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09144v1</id>
    <updated>2018-08-28T07:19:43Z</updated>
    <published>2018-08-28T07:19:43Z</published>
    <title>Weighted total variation based convex clustering</title>
    <summary>  Data clustering is a fundamental problem with a wide range of applications.
Standard methods, eg the $k$-means method, usually require solving a non-convex
optimization problem. Recently, total variation based convex relaxation to the
$k$-means model has emerged as an attractive alternative for data clustering.
However, the existing results on its exact clustering property, ie, the
condition imposed on data so that the method can provably give correct
identification of all cluster memberships, is only applicable to very specific
data and is also much more restrictive than that of some other methods. This
paper aims at the revisit of total variation based convex clustering, by
proposing a weighted sum-of-$\ell_1$-norm relating convex model. Its exact
clustering property established in this paper, in both deterministic and
probabilistic context, is applicable to general data and is much sharper than
the existing results. These results provided good insights to advance the
research on convex clustering. Moreover, the experiments also demonstrated that
the proposed convex model has better empirical performance when be compared to
standard clustering methods, and thus it can see its potential in practice.
</summary>
    <author>
      <name>Guodong Xu</name>
    </author>
    <author>
      <name>Yu Xia</name>
    </author>
    <author>
      <name>Hui Ji</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07187v2</id>
    <updated>2018-08-28T06:27:09Z</updated>
    <published>2018-08-22T02:18:40Z</published>
    <title>Neural Latent Extractive Document Summarization</title>
    <summary>  Extractive summarization models require sentence-level labels, which are
usually created heuristically (e.g., with rule-based methods) given that most
summarization datasets only have document-summary pairs. Since these labels
might be suboptimal, we propose a latent variable extractive model where
sentences are viewed as latent variables and sentences with activated variables
are used to infer gold summaries. During training the loss comes
\emph{directly} from gold summaries. Experiments on the CNN/Dailymail dataset
show that our model improves over a strong extractive baseline trained on
heuristically approximated labels and also performs competitively to several
recent models.
</summary>
    <author>
      <name>Xingxing Zhang</name>
    </author>
    <author>
      <name>Mirella Lapata</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07187v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07187v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06561v2</id>
    <updated>2018-08-28T06:21:23Z</updated>
    <published>2018-04-18T05:31:45Z</published>
    <title>A Mean Field View of the Landscape of Two-Layers Neural Networks</title>
    <summary>  Multi-layer neural networks are among the most powerful models in machine
learning, yet the fundamental reasons for this success defy mathematical
understanding. Learning a neural network requires to optimize a non-convex
high-dimensional objective (risk function), a problem which is usually attacked
using stochastic gradient descent (SGD). Does SGD converge to a global optimum
of the risk or only to a local optimum? In the first case, does this happen
because local minima are absent, or because SGD somehow avoids them? In the
second, why do local minima reached by SGD have good generalization properties?
  In this paper we consider a simple case, namely two-layers neural networks,
and prove that -in a suitable scaling limit- SGD dynamics is captured by a
certain non-linear partial differential equation (PDE) that we call
distributional dynamics (DD). We then consider several specific examples, and
show how DD can be used to prove convergence of SGD to networks with nearly
ideal generalization error. This description allows to 'average-out' some of
the complexities of the landscape of neural networks, and can be used to prove
a general convergence result for noisy SGD.
</summary>
    <author>
      <name>Song Mei</name>
    </author>
    <author>
      <name>Andrea Montanari</name>
    </author>
    <author>
      <name>Phan-Minh Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">103 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09127v1</id>
    <updated>2018-08-28T05:39:48Z</updated>
    <published>2018-08-28T05:39:48Z</published>
    <title>High-confidence error estimates for learned value functions</title>
    <summary>  Estimating the value function for a fixed policy is a fundamental problem in
reinforcement learning. Policy evaluation algorithms---to estimate value
functions---continue to be developed, to improve convergence rates, improve
stability and handle variability, particularly for off-policy learning. To
understand the properties of these algorithms, the experimenter needs
high-confidence estimates of the accuracy of the learned value functions. For
environments with small, finite state-spaces, like chains, the true value
function can be easily computed, to compute accuracy. For large, or continuous
state-spaces, however, this is no longer feasible. In this paper, we address
the largely open problem of how to obtain these high-confidence estimates, for
general state-spaces. We provide a high-confidence bound on an empirical
estimate of the value error to the true value error. We use this bound to
design an offline sampling algorithm, which stores the required quantities to
repeatedly compute value error estimates for any learned value function. We
provide experiments investigating the number of samples required by this
offline algorithm in simple benchmark reinforcement learning domains, and
highlight that there are still many open questions to be solved for this
important problem.
</summary>
    <author>
      <name>Touqir Sajed</name>
    </author>
    <author>
      <name>Wesley Chung</name>
    </author>
    <author>
      <name>Martha White</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at (UAI) Uncertainty in Artificial Intelligence 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09123v1</id>
    <updated>2018-08-28T05:28:35Z</updated>
    <published>2018-08-28T05:28:35Z</published>
    <title>Investigating Human + Machine Complementarity for Recidivism Predictions</title>
    <summary>  When might human input help (or not) when assessing risk in fairness-related
domains? Dressel and Farid asked Mechanical Turk workers to evaluate a subset
of individuals in the ProPublica COMPAS data set for risk of recidivism, and
concluded that COMPAS predictions were no more accurate or fair than
predictions made by humans. We delve deeper into this claim in this paper. We
construct a Human Risk Score based on the predictions made by multiple
Mechanical Turk workers on the same individual, study the agreement and
disagreement between COMPAS and Human Scores on subgroups of individuals, and
construct hybrid Human+AI models to predict recidivism. Our key finding is that
on this data set, human and COMPAS decision making differed, but not in ways
that could be leveraged to significantly improve ground truth prediction. We
present the results of our analyses and suggestions for how machine and human
input may have complementary strengths to address challenges in the fairness
domain.
</summary>
    <author>
      <name>Sarah Tan</name>
    </author>
    <author>
      <name>Julius Adebayo</name>
    </author>
    <author>
      <name>Kori Inkpen</name>
    </author>
    <author>
      <name>Ece Kamar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09111v1</id>
    <updated>2018-08-28T04:33:25Z</updated>
    <published>2018-08-28T04:33:25Z</published>
    <title>Unsupervised Learning of Syntactic Structure with Invertible Neural
  Projections</title>
    <summary>  Unsupervised learning of syntactic structure is typically performed using
generative models with discrete latent variables and multinomial parameters. In
most cases, these models have not leveraged continuous word representations. In
this work, we propose a novel generative model that jointly learns discrete
syntactic structure and continuous word representations in an unsupervised
fashion by cascading an invertible neural network with a structured generative
prior. We show that the invertibility condition allows for efficient exact
inference and marginal likelihood computation in our model so long as the prior
is well-behaved. In experiments we instantiate our approach with both Markov
and tree-structured priors, evaluating on two tasks: part-of-speech (POS)
induction, and unsupervised dependency parsing without gold POS annotation. On
the Penn Treebank, our Markov-structured model surpasses state-of-the-art
results on POS induction. Similarly, we find that our tree-structured model
achieves state-of-the-art performance on unsupervised dependency parsing for
the difficult training condition where neither gold POS annotation nor
punctuation-based constraints are available.
</summary>
    <author>
      <name>Junxian He</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <author>
      <name>Taylor Berg-Kirkpatrick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09105v1</id>
    <updated>2018-08-28T03:48:25Z</updated>
    <published>2018-08-28T03:48:25Z</published>
    <title>SOLAR: Deep Structured Latent Representations for Model-Based
  Reinforcement Learning</title>
    <summary>  Model-based reinforcement learning (RL) methods can be broadly categorized as
global model methods, which depend on learning models that provide sensible
predictions in a wide range of states, or local model methods, which
iteratively refit simple models that are used for policy improvement. While
predicting future states that will result from the current actions is
difficult, local model methods only attempt to understand system dynamics in
the neighborhood of the current policy, making it possible to produce local
improvements without ever learning to predict accurately far into the future.
The main idea in this paper is that we can learn representations that make it
easy to retrospectively infer simple dynamics given the data from the current
policy, thus enabling local models to be used for policy learning in complex
systems. To that end, we focus on learning representations with probabilistic
graphical model (PGM) structure, which allows us to devise an efficient local
model method that infers dynamics from real-world rollouts with the PGM as a
global prior. We compare our method to other model-based and model-free RL
methods on a suite of robotics tasks, including manipulation tasks on a real
Sawyer robotic arm directly from camera images. Videos of our results are
available at https://sites.google.com/view/solar-iclips
</summary>
    <author>
      <name>Marvin Zhang</name>
    </author>
    <author>
      <name>Sharad Vikram</name>
    </author>
    <author>
      <name>Laura Smith</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Matthew J. Johnson</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02920v2</id>
    <updated>2018-08-28T03:34:14Z</updated>
    <published>2018-02-08T15:28:46Z</published>
    <title>Spectral State Compression of Markov Processes</title>
    <summary>  Model reduction of the Markov process is a basic problem in modeling
state-transition systems. Motivated by the state aggregation approach rooted in
control theory, we study the statistical state compression of a finite-state
Markov chain from empirical trajectories. Through the lens of spectral
decomposition, we study the rank and features of Markov processes, as well as
properties like representability, aggregatability, and lumpability. We develop
a class of spectral state compression methods for three tasks: (1) estimate the
transition matrix of a low-rank Markov model, (2) estimate the leading subspace
spanned by Markov features, and (3) recover latent structures of the state
space like state aggregation and lumpable partition. The proposed methods
provide an unsupervised learning framework for identifying Markov features and
clustering states. We provide upper bounds for the estimation errors and nearly
matching minimax lower bounds. Numerical studies are performed on synthetic
data and a dataset of New York City taxi trips.
</summary>
    <author>
      <name>Anru Zhang</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02920v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02920v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02046v3</id>
    <updated>2018-08-28T02:09:13Z</updated>
    <published>2018-01-31T01:22:21Z</published>
    <title>Neural Network Detection of Data Sequences in Communication Systems</title>
    <summary>  We consider detection based on deep learning, and show it is possible to
train detectors that perform well without any knowledge of the underlying
channel models. Moreover, when the channel model is known, we demonstrate that
it is possible to train detectors that do not require channel state information
(CSI). In particular, a technique we call a sliding bidirectional recurrent
neural network (SBRNN) is proposed for detection where, after training, the
detector estimates the data in real-time as the signal stream arrives at the
receiver. We evaluate this algorithm, as well as other neural network (NN)
architectures, using the Poisson channel model, which is applicable to both
optical and molecular communication systems. In addition, we also evaluate the
performance of this detection method applied to data sent over a molecular
communication platform, where the channel model is difficult to model
analytically. We show that SBRNN is computationally efficient, and can perform
detection under various channel conditions without knowing the underlying
channel model. We also demonstrate that the bit error rate (BER) performance of
the proposed SBRNN detector is better than that of a Viterbi detector with
imperfect CSI as well as that of other NN detectors that have been previously
proposed. Finally, we show that the SBRNN can perform well in rapidly changing
channels, where the coherence time is on the order of a single symbol duration.
</summary>
    <author>
      <name>Nariman Farsad</name>
    </author>
    <author>
      <name>Andrea Goldsmith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE Transactions on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02046v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02046v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05697v2</id>
    <updated>2018-08-28T01:26:33Z</updated>
    <published>2018-08-16T22:46:40Z</published>
    <title>Deep Bayesian Active Learning for Natural Language Processing: Results
  of a Large-Scale Empirical Study</title>
    <summary>  Several recent papers investigate Active Learning (AL) for mitigating the
data dependence of deep learning for natural language processing. However, the
applicability of AL to real-world problems remains an open question. While in
supervised learning, practitioners can try many different methods, evaluating
each against a validation set before selecting a model, AL affords no such
luxury. Over the course of one AL run, an agent annotates its dataset
exhausting its labeling budget. Thus, given a new task, an active learner has
no opportunity to compare models and acquisition functions. This paper provides
a large scale empirical study of deep active learning, addressing multiple
tasks and, for each, multiple datasets, multiple models, and a full suite of
acquisition functions. We find that across all settings, Bayesian active
learning by disagreement, using uncertainty estimates provided either by
Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and
usually outperforms classic uncertainty sampling.
</summary>
    <author>
      <name>Aditya Siddhant</name>
    </author>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07036v3</id>
    <updated>2018-08-28T00:58:48Z</updated>
    <published>2018-08-21T17:46:12Z</published>
    <title>QuAC : Question Answering in Context</title>
    <summary>  We present QuAC, a dataset for Question Answering in Context that contains
14K information-seeking QA dialogs (100K questions in total). The dialogs
involve two crowd workers: (1) a student who poses a sequence of freeform
questions to learn as much as possible about a hidden Wikipedia text, and (2) a
teacher who answers the questions by providing short excerpts from the text.
QuAC introduces challenges not found in existing machine comprehension
datasets: its questions are often more open-ended, unanswerable, or only
meaningful within the dialog context, as we show in a detailed qualitative
evaluation. We also report results for a number of reference models, including
a recently state-of-the-art reading comprehension architecture extended to
model dialog context. Our best model underperforms humans by 20 F1, suggesting
that there is significant room for future work on this data. Dataset, baseline,
and leaderboard available at http://quac.ai.
</summary>
    <author>
      <name>Eunsol Choi</name>
    </author>
    <author>
      <name>He He</name>
    </author>
    <author>
      <name>Mohit Iyyer</name>
    </author>
    <author>
      <name>Mark Yatskar</name>
    </author>
    <author>
      <name>Wen-tau Yih</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP Camera Ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07036v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07036v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03740v2</id>
    <updated>2018-08-28T00:30:44Z</updated>
    <published>2018-04-10T22:27:21Z</published>
    <title>Multimodal Sparse Bayesian Dictionary Learning</title>
    <summary>  The purpose of this paper is to address the problem of learning dictionaries
for multimodal datasets, i.e. datasets collected from multiple data sources. We
present an algorithm called multimodal sparse Bayesian dictionary learning
(MSBDL). MSBDL leverages information from all available data modalities through
a joint sparsity constraint. The underlying framework offers a considerable
amount of flexibility to practitioners and addresses many of the shortcomings
of existing multimodal dictionary learning approaches. In particular, the
procedure includes the automatic tuning of hyperparameters and is unique in
that it allows the dictionaries for each data modality to have different
cardinality, a significant feature in cases when the dimensionality of data
differs across modalities. MSBDL is scalable and can be used in supervised
learning settings. Theoretical results relating to the convergence of MSBDL are
presented and the numerical results provide evidence of the superior
performance on synthetic and real datasets compared to existing methods.
</summary>
    <author>
      <name>Igor Fedorov</name>
    </author>
    <author>
      <name>Bhaskar D. Rao</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03740v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03740v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09062v1</id>
    <updated>2018-08-27T23:26:08Z</updated>
    <published>2018-08-27T23:26:08Z</published>
    <title>Cognitive Consistency Routing Algorithm of Capsule-network</title>
    <summary>  Artificial Neural Networks (ANNs) are computational models inspired by the
central nervous system (especially the brain) of animals and are used to
estimate or generate unknown approximation functions relied on large amounts of
inputs. Capsule Neural Network (Sabour S, et al.[2017]) is a novel structure of
Convolutional Neural Networks which simulates the visual processing system of
human brain. In this paper, we introduce psychological theories which called
Cognitive Consistency to optimize the routing algorithm of Capsnet to make it
more close to the work pattern of human brain. It has been shown in the
experiment that a progress had been made compared with the baseline.
</summary>
    <author>
      <name>Huayu Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09057v1</id>
    <updated>2018-08-27T23:02:18Z</updated>
    <published>2018-08-27T23:02:18Z</published>
    <title>Choosing How to Choose Papers</title>
    <summary>  It is common to see a handful of reviewers reject a highly novel paper,
because they view, say, extensive experiments as far more important than
novelty, whereas the community as a whole would have embraced the paper. More
generally, the disparate mapping of criteria scores to final recommendations by
different reviewers is a major source of inconsistency in peer review. In this
paper we present a framework --- based on $L(p,q)$-norm empirical risk
minimization --- for learning the community's aggregate mapping. We draw on
computational social choice to identify desirable values of $p$ and $q$;
specifically, we characterize $p=q=1$ as the only choice that satisfies three
natural axiomatic properties. Finally, we implement and apply our approach to
reviews from IJCAI 2017.
</summary>
    <author>
      <name>Ritesh Noothigattu</name>
    </author>
    <author>
      <name>Nihar B. Shah</name>
    </author>
    <author>
      <name>Ariel D. Procaccia</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09050v1</id>
    <updated>2018-08-27T22:26:00Z</updated>
    <published>2018-08-27T22:26:00Z</published>
    <title>Adversarial Feature Learning of Online Monitoring Data for Operation
  Reliability Assessment in Distribution Network</title>
    <summary>  With deployments of online monitoring systems in distribution networks,
massive amounts of data collected through them contain rich information on the
operating status of distribution networks. By leveraging the data, based on
bidirectional generative adversarial networks (BiGANs), we propose an
unsupervised approach for online distribution reliability assessment. It is
capable of discovering the latent structure and automatically learning the most
representative features of the spatio-temporal data in distribution networks in
an adversarial way and it does not rely on any assumptions of the input data.
Based on the extracted features, a statistical magnitude for them is calculated
to indicate the data behavior. Furthermore, distribution reliability states are
divided into different levels and we combine them with the calculated
confidence level $1-\alpha$, during which clear criteria is defined
empirically. Case studies on both synthetic data and real-world online
monitoring data show that our proposed approach is feasible for the assessment
of distribution operation reliability and outperforms other existed techniques.
</summary>
    <author>
      <name>Xin Shi</name>
    </author>
    <author>
      <name>Robert Qiu</name>
    </author>
    <author>
      <name>Tiebin Mi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07855v2</id>
    <updated>2018-08-27T22:20:26Z</updated>
    <published>2018-04-20T23:06:44Z</published>
    <title>Subgoal Discovery for Hierarchical Dialogue Policy Learning</title>
    <summary>  Developing agents to engage in complex goal-oriented dialogues is challenging
partly because the main learning signals are very sparse in long conversations.
In this paper, we propose a divide-and-conquer approach that discovers and
exploits the hidden structure of the task to enable efficient policy learning.
First, given successful example dialogues, we propose the Subgoal Discovery
Network (SDN) to divide a complex goal-oriented task into a set of simpler
subgoals in an unsupervised fashion. We then use these subgoals to learn a
multi-level policy by hierarchical reinforcement learning. We demonstrate our
method by building a dialogue agent for the composite task of travel planning.
Experiments with simulated and real users show that our approach performs
competitively against a state-of-the-art method that requires human-defined
subgoals. Moreover, we show that the learned subgoals are often human
comprehensible.
</summary>
    <author>
      <name>Da Tang</name>
    </author>
    <author>
      <name>Xiujun Li</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Chong Wang</name>
    </author>
    <author>
      <name>Lihong Li</name>
    </author>
    <author>
      <name>Tony Jebara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures, EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07855v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07855v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00972v1</id>
    <updated>2018-08-27T20:46:50Z</updated>
    <published>2018-08-27T20:46:50Z</published>
    <title>Migrating Knowledge between Physical Scenarios based on Artificial
  Neural Networks</title>
    <summary>  Deep learning is known to be data-hungry, which hinders its application in
many areas of science when datasets are small. Here, we propose to use transfer
learning methods to migrate knowledge between different physical scenarios and
significantly improve the prediction accuracy of artificial neural networks
trained on a small dataset. This method can help reduce the demand for
expensive data by making use of additional inexpensive data. First, we
demonstrate that in predicting the transmission from multilayer photonic film,
the relative error rate is reduced by 46.8% (26.5%) when the source data comes
from 10-layer (8-layer) films and the target data comes from 8-layer (10-layer)
films. Second, we show that the relative error rate is decreased by 22% when
knowledge is transferred between two very different physical scenarios:
transmission from multilayer films and scattering from multilayer
nanoparticles. Finally, we propose a multi-task learning method to improve the
performance of different physical scenarios simultaneously in which each task
only has a small dataset.
</summary>
    <author>
      <name>Yurui Qu</name>
    </author>
    <author>
      <name>Li Jing</name>
    </author>
    <author>
      <name>Yichen Shen</name>
    </author>
    <author>
      <name>Min Qiu</name>
    </author>
    <author>
      <name>Marin Soljacic</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04572v2</id>
    <updated>2018-08-27T19:52:28Z</updated>
    <published>2018-03-12T23:27:06Z</published>
    <title>COPA: Constrained PARAFAC2 for Sparse &amp; Large Datasets</title>
    <summary>  PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.
</summary>
    <author>
      <name>Ardavan Afshar</name>
    </author>
    <author>
      <name>Ioakeim Perros</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <author>
      <name>Elizabeth Searles</name>
    </author>
    <author>
      <name>Joyce Ho</name>
    </author>
    <author>
      <name>Jimeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09004v1</id>
    <updated>2018-08-27T19:15:15Z</updated>
    <published>2018-08-27T19:15:15Z</published>
    <title>Downstream Effects of Affirmative Action</title>
    <summary>  We study a two-stage model, in which students are 1) admitted to college on
the basis of an entrance exam which is a noisy signal about their
qualifications (type), and then 2) those students who were admitted to college
can be hired by an employer as a function of their college grades, which are an
independently drawn noisy signal of their type. Students are drawn from one of
two populations, which might have different type distributions. We assume that
the employer at the end of the pipeline is rational, in the sense that it
computes a posterior distribution on student type conditional on all
information that it has available (college admissions, grades, and group
membership), and makes a decision based on posterior expectation. We then study
what kinds of fairness goals can be achieved by the college by setting its
admissions rule and grading policy. For example, the college might have the
goal of guaranteeing equal opportunity across populations: that the probability
of passing through the pipeline and being hired by the employer should be
independent of group membership, conditioned on type. Alternately, the college
might have the goal of incentivizing the employer to have a group blind hiring
rule. We show that both goals can be achieved when the college does not report
grades. On the other hand, we show that under reasonable conditions, these
goals are impossible to achieve even in isolation when the college uses an
(even minimally) informative grading policy.
</summary>
    <author>
      <name>Sampath Kannan</name>
    </author>
    <author>
      <name>Aaron Roth</name>
    </author>
    <author>
      <name>Juba Ziani</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08994v1</id>
    <updated>2018-08-27T18:59:43Z</updated>
    <published>2018-08-27T18:59:43Z</published>
    <title>Data Poisoning Attacks against Online Learning</title>
    <summary>  We consider data poisoning attacks, a class of adversarial attacks on machine
learning where an adversary has the power to alter a small fraction of the
training data in order to make the trained classifier satisfy certain
objectives. While there has been much prior work on data poisoning, most of it
is in the offline setting, and attacks for online learning, where training data
arrives in a streaming manner, are not well understood.
  In this work, we initiate a systematic investigation of data poisoning
attacks for online learning. We formalize the problem into two settings, and we
propose a general attack strategy, formulated as an optimization problem, that
applies to both with some modifications. We propose three solution strategies,
and perform extensive experimental evaluation. Finally, we discuss the
implications of our findings for building successful defenses.
</summary>
    <author>
      <name>Yizhen Wang</name>
    </author>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04877v2</id>
    <updated>2018-08-27T18:45:48Z</updated>
    <published>2018-02-13T22:19:10Z</published>
    <title>Learning via social awareness: Improving a deep generative sketching
  model with facial feedback</title>
    <summary>  In the quest towards general artificial intelligence (AI), researchers have
explored developing loss functions that act as intrinsic motivators in the
absence of external rewards. This paper argues that such research has
overlooked an important and useful intrinsic motivator: social interaction. We
posit that making an AI agent aware of implicit social feedback from humans can
allow for faster learning of more generalizable and useful representations, and
could potentially impact AI safety. We collect social feedback in the form of
facial expression reactions to samples from Sketch RNN, an LSTM-based
variational autoencoder (VAE) designed to produce sketch drawings. We use a
Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small
group of viewers, by optimizing the model to produce sketches that it predicts
will lead to more positive facial expressions. We show in multiple independent
evaluations that the model trained with facial feedback produced sketches that
are more highly rated, and induce significantly more positive facial
expressions. Thus, we establish that implicit social feedback can improve the
output of a deep learning model.
</summary>
    <author>
      <name>Natasha Jaques</name>
    </author>
    <author>
      <name>Jennifer McCleary</name>
    </author>
    <author>
      <name>Jesse Engel</name>
    </author>
    <author>
      <name>David Ha</name>
    </author>
    <author>
      <name>Fred Bertsch</name>
    </author>
    <author>
      <name>Rosalind Picard</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06901v3</id>
    <updated>2018-08-27T18:00:08Z</updated>
    <published>2018-02-19T22:57:54Z</published>
    <title>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative
  Refinement</title>
    <summary>  We propose a conditional non-autoregressive neural sequence model based on
iterative refinement. The proposed model is designed based on the principles of
latent variable models and denoising autoencoders, and is generally applicable
to any sequence generation task. We extensively evaluate the proposed model on
machine translation (En-De and En-Ro) and image caption generation, and observe
that it significantly speeds up decoding while maintaining the generation
quality comparable to the autoregressive counterpart.
</summary>
    <author>
      <name>Jason Lee</name>
    </author>
    <author>
      <name>Elman Mansimov</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06901v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06901v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09270v1</id>
    <updated>2018-08-27T17:42:14Z</updated>
    <published>2018-08-27T17:42:14Z</published>
    <title>Models for Predicting Community-Specific Interest in News Articles</title>
    <summary>  In this work, we ask two questions: 1. Can we predict the type of community
interested in a news article using only features from the article content? and
2. How well do these models generalize over time? To answer these questions, we
compute well-studied content-based features on over 60K news articles from 4
communities on reddit.com. We train and test models over three different time
periods between 2015 and 2017 to demonstrate which features degrade in
performance the most due to concept drift. Our models can classify news
articles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0
ROC AUC. However, while we can predict the community-specific popularity of
news articles with high accuracy, practitioners should approach these models
carefully. Predictions are both community-pair dependent and feature group
dependent. Moreover, these feature groups generalize over time differently,
with some only degrading slightly over time, but others degrading greatly.
Therefore, we recommend that community-interest predictions are done in a
hierarchical structure, where multiple binary classifiers can be used to
separate community pairs, rather than a traditional multi-class model. Second,
these models should be retrained over time based on accuracy goals and the
availability of training data.
</summary>
    <author>
      <name>Benjamin D. Horne</name>
    </author>
    <author>
      <name>William Dron</name>
    </author>
    <author>
      <name>Sibel Adali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at IEEE MILCOM 2018 in Los Angeles, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08931v1</id>
    <updated>2018-08-27T17:13:38Z</updated>
    <published>2018-08-27T17:13:38Z</published>
    <title>Smoothed Dilated Convolutions for Improved Dense Prediction</title>
    <summary>  Dilated convolutions, also known as atrous convolutions, have been widely
explored in deep convolutional neural networks (DCNNs) for various tasks like
semantic image segmentation, object detection, audio generation, video
modeling, and machine translation. However, dilated convolutions suffer from
the gridding artifacts, which hampers the performance of DCNNs with dilated
convolutions. In this work, we propose two simple yet effective degridding
methods by studying a decomposition of dilated convolutions. Unlike existing
models, which explore solutions by focusing on a block of cascaded dilated
convolutional layers, our methods address the gridding artifacts by smoothing
the dilated convolution itself. By analyzing them in both the original
operation and the decomposition views, we further point out that the two
degridding approaches are intrinsically related and define separable and shared
(SS) operations, which generalize the proposed methods. We evaluate our methods
thoroughly on two datasets and visualize the smoothing effect through effective
receptive field analysis. Experimental results show that our methods yield
significant and consistent improvements on the performance of DCNNs with
dilated convolutions, while adding negligible amounts of extra training
parameters.
</summary>
    <author>
      <name>Zhengyang Wang</name>
    </author>
    <author>
      <name>Shuiwang Ji</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219944</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219944" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by KDD2018. Code is publicly available at
  https://github.com/divelab/dilated</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 24th ACM SIGKDD International Conference on
  Knowledge Discovery &amp; Data Mining (pp. 2486-2495). 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.08931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10144v4</id>
    <updated>2018-08-27T16:47:22Z</updated>
    <published>2017-11-28T06:39:31Z</published>
    <title>The game theoretic p-Laplacian and semi-supervised learning with few
  labels</title>
    <summary>  We study the game theoretic p-Laplacian for semi-supervised learning on
graphs, and show that it is well-posed in the limit of finite labeled data and
infinite unlabeled data. In particular, we show that the continuum limit of
graph-based semi-supervised learning with the game theoretic p-Laplacian is a
weighted version of the continuous p-Laplace equation. We also prove that
solutions to the graph p-Laplace equation are approximately Holder continuous
with high probability. Our proof uses the viscosity solution machinery and the
maximum principle on a graph.
</summary>
    <author>
      <name>Jeff Calder</name>
    </author>
    <link href="http://arxiv.org/abs/1711.10144v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10144v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35D40, 35J60, 65N06" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03703v2</id>
    <updated>2018-08-27T16:36:40Z</updated>
    <published>2018-08-10T20:46:32Z</published>
    <title>LemmaTag: Jointly Tagging and Lemmatizing for Morphologically-Rich
  Languages with BRNNs</title>
    <summary>  We present LemmaTag, a featureless neural network architecture that jointly
generates part-of-speech tags and lemmas for sentences by using bidirectional
RNNs with character-level and word-level embeddings. We demonstrate that both
tasks benefit from sharing the encoding part of the network, predicting tag
subcategories, and using the tagger output as an input to the lemmatizer. We
evaluate our model across several languages with complex morphology, which
surpasses state-of-the-art accuracy in both part-of-speech tagging and
lemmatization in Czech, German, and Arabic.
</summary>
    <author>
      <name>Daniel Kondratyuk</name>
    </author>
    <author>
      <name>Tomáš Gavenčiak</name>
    </author>
    <author>
      <name>Milan Straka</name>
    </author>
    <author>
      <name>Jan Hajič</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures. Submitted to EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03703v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03703v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08914v1</id>
    <updated>2018-08-27T16:34:51Z</updated>
    <published>2018-08-27T16:34:51Z</published>
    <title>Deep Learning for Stress Field Prediction Using Convolutional Neural
  Networks</title>
    <summary>  This research presents a deep learning based approach to predict stress
fields in the solid material elastic deformation using convolutional neural
networks (CNN). Two different architectures are proposed to solve the problem.
One is Feature Representation embedded Convolutional Neural Network (FR-CNN)
with a single input channel, and the other is Squeeze-and-Excitation Residual
network modules embedded Fully Convolutional Neural network (SE-Res-FCN) with
multiple input channels. Both the tow architectures are stable and converged
reliably in training and testing on GPUs. Accuracy analysis shows that
SE-Res-FCN has a significantly smaller mean squared error (MSE) and mean
absolute error (MAE) than FR-CNN. Mean relative error (MRE) of the SE-Res-FCN
model is about 0.25% with respect to the average ground truth. The validation
results indicate that the SE-Res-FCN model can accurately predict the stress
field. For stress field prediction, the hierarchical architecture becomes
deeper within certain limits, and then its prediction becomes more accurate.
Fully trained deep learning models have higher computational efficiency over
conventional FEM models, so they have great foreground and potential in
structural design and topology optimization.
</summary>
    <author>
      <name>Zhenguo Nie</name>
    </author>
    <author>
      <name>Haoliang Jiang</name>
    </author>
    <author>
      <name>Levent Burak Kara</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09889v1</id>
    <updated>2018-08-27T16:12:36Z</updated>
    <published>2018-08-27T16:12:36Z</published>
    <title>Zero-shot Transfer Learning for Semantic Parsing</title>
    <summary>  While neural networks have shown impressive performance on large datasets,
applying these models to tasks where little data is available remains a
challenging problem.
  In this paper we propose to use feature transfer in a zero-shot experimental
setting on the task of semantic parsing.
  We first introduce a new method for learning the shared space between
multiple domains based on the prediction of the domain label for each example.
  Our experiments support the superiority of this method in a zero-shot
experimental setting in terms of accuracy metrics compared to state-of-the-art
techniques.
  In the second part of this paper we study the impact of individual domains
and examples on semantic parsing performance.
  We use influence functions to this aim and investigate the sensitivity of
domain-label classification loss on each example.
  Our findings reveal that cross-domain adversarial attacks identify useful
examples for training even from the domains the least similar to the target
domain. Augmenting our training data with these influential examples further
boosts our accuracy at both the token and the sequence level.
</summary>
    <author>
      <name>Javid Dadashkarimi</name>
    </author>
    <author>
      <name>Alexander Fabbri</name>
    </author>
    <author>
      <name>Sekhar Tatikonda</name>
    </author>
    <author>
      <name>Dragomir R. Radev</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11247v3</id>
    <updated>2018-08-27T16:03:31Z</updated>
    <published>2017-11-30T06:23:52Z</published>
    <title>Provably noise-robust, regularised $k$-means clustering</title>
    <summary>  We consider the problem of clustering in the presence of noise. That is, when
on top of cluster structure, the data also contains a subset of
\emph{unstructured} points. Our goal is to detect the clusters despite the
presence of many unstructured points. Any algorithm that achieves this goal is
noise-robust. We consider a regularisation method which converts any
center-based clustering objective into a noise-robust one. We focus on the
$k$-means objective and we prove that the regularised version of $k$-means is
NP-Hard even for $k=1$. We consider two algorithms based on the convex (sdp and
lp) relaxation of the regularised objective and prove robustness guarantees for
both.
  The sdp and lp relaxation of the standard (non-regularised) $k$-means
objective has been previously studied by [ABC+15]. Under the stochastic ball
model of the data they show that the sdp-based algorithm recovers the
underlying structure as long as the balls are separated by $\delta &gt; 2\sqrt{2}
+ \epsilon$. We improve upon this result in two ways. First, we show recovery
even for $\delta &gt; 2 + \epsilon$. Second, our regularised algorithm recovers
the balls even in the presence of noise so long as the number of noisy points
is not too large. We complement our theoretical analysis with simulations and
analyse the effect of various parameters like regularization constant,
noise-level etc. on the performance of our algorithm. In the presence of noise,
our algorithm performs better than $k$-means++ on MNIST.
</summary>
    <author>
      <name>Shrinu Kushagra</name>
    </author>
    <author>
      <name>Yaoliang Yu</name>
    </author>
    <author>
      <name>Shai Ben-David</name>
    </author>
    <link href="http://arxiv.org/abs/1711.11247v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11247v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08740v2</id>
    <updated>2018-08-27T15:19:38Z</updated>
    <published>2018-03-23T11:13:29Z</published>
    <title>Speeding-up Object Detection Training for Robotics with FALKON</title>
    <summary>  Latest deep learning methods for object detection provide remarkable
performance, but have limits when used in robotic applications. One of the most
relevant issues is the long training time, which is due to the large size and
imbalance of the associated training sets, characterized by few positive and a
large number of negative examples (i.e. background). Proposed approaches are
based on end-to-end learning by back-propagation [22] or kernel methods trained
with Hard Negatives Mining on top of deep features [8]. These solutions are
effective, but prohibitively slow for on-line applications. In this paper we
propose a novel pipeline for object detection that overcomes this problem and
provides comparable performance, with a 60x training speedup. Our pipeline
combines (i) the Region Proposal Network and the deep feature extractor from
[22] to efficiently select candidate RoIs and encode them into powerful
representations, with (ii) the FALKON [23] algorithm, a novel kernel-based
method that allows fast training on large scale problems (millions of points).
We address the size and imbalance of training data by exploiting the stochastic
subsampling intrinsic into the method and a novel, fast, bootstrapping
approach. We assess the effectiveness of the approach on a standard Computer
Vision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a
real robotic scenario with the iCubWorld Transformations [18] dataset.
</summary>
    <author>
      <name>Elisa Maiettini</name>
    </author>
    <author>
      <name>Giulia Pasquale</name>
    </author>
    <author>
      <name>Lorenzo Rosasco</name>
    </author>
    <author>
      <name>Lorenzo Natale</name>
    </author>
    <link href="http://arxiv.org/abs/1803.08740v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08740v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08871v1</id>
    <updated>2018-08-27T14:57:17Z</updated>
    <published>2018-08-27T14:57:17Z</published>
    <title>BézierGAN: Automatic Generation of Smooth Curves from Interpretable
  Low-Dimensional Parameters</title>
    <summary>  Many real-world objects are designed by smooth curves, especially in the
domain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and
hydrodynamic shapes (e.g., hulls) are designed. To facilitate the design
process of those objects, we propose a deep learning based generative model
that can synthesize smooth curves. The model maps a low-dimensional latent
representation to a sequence of discrete points sampled from a rational
B\'ezier curve. We demonstrate the performance of our method in completing both
synthetic and real-world generative tasks. Results show that our method can
generate diverse and realistic curves, while preserving consistent shape
variation in the latent space, which is favorable for latent space design
optimization or design space exploration.
</summary>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Mark Fuge</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08866v1</id>
    <updated>2018-08-27T14:43:38Z</updated>
    <published>2018-08-27T14:43:38Z</published>
    <title>A Study of Reinforcement Learning for Neural Machine Translation</title>
    <summary>  Recent studies have shown that reinforcement learning (RL) is an effective
approach for improving the performance of neural machine translation (NMT)
system. However, due to its instability, successfully RL training is
challenging, especially in real-world systems where deep models and large
datasets are leveraged. In this paper, taking several large-scale translation
tasks as testbeds, we conduct a systematic study on how to train better NMT
models using reinforcement learning. We provide a comprehensive comparison of
several important factors (e.g., baseline reward, reward shaping) in RL
training. Furthermore, to fill in the gap that it remains unclear whether RL is
still beneficial when monolingual data is used, we propose a new method to
leverage RL to further boost the performance of NMT systems trained with
source/target monolingual data. By integrating all our findings, we obtain
competitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17
Chinese-English translation tasks, especially setting a state-of-the-art
performance on WMT17 Chinese-English translation task.
</summary>
    <author>
      <name>Lijun Wu</name>
    </author>
    <author>
      <name>Fei Tian</name>
    </author>
    <author>
      <name>Tao Qin</name>
    </author>
    <author>
      <name>Jianhuang Lai</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04143v3</id>
    <updated>2018-08-27T14:33:39Z</updated>
    <published>2017-12-12T06:33:20Z</published>
    <title>Benchmarking Single Image Dehazing and Beyond</title>
    <summary>  We present a comprehensive study and evaluation of existing single image
dehazing algorithms, using a new large-scale benchmark consisting of both
synthetic and real-world hazy images, called REalistic Single Image DEhazing
(RESIDE). RESIDE highlights diverse data sources and image contents, and is
divided into five subsets, each serving different training or evaluation
purposes. We further provide a rich variety of criteria for dehazing algorithm
evaluation, ranging from full-reference metrics, to no-reference metrics, to
subjective evaluation and the novel task-driven evaluation. Experiments on
RESIDE shed light on the comparisons and limitations of state-of-the-art
dehazing algorithms, and suggest promising future directions.
</summary>
    <author>
      <name>Boyi Li</name>
    </author>
    <author>
      <name>Wenqi Ren</name>
    </author>
    <author>
      <name>Dengpan Fu</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <author>
      <name>Dan Feng</name>
    </author>
    <author>
      <name>Wenjun Zeng</name>
    </author>
    <author>
      <name>Zhangyang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Image Processing(Accepted to TIP 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.04143v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04143v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08858v1</id>
    <updated>2018-08-27T14:17:08Z</updated>
    <published>2018-08-27T14:17:08Z</published>
    <title>Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and
  They Are Both Weakly Supervised</title>
    <summary>  We present a neural framework for opinion summarization from online product
reviews which is knowledge-lean and only requires light supervision (e.g., in
the form of product domain labels and user-provided ratings). Our method
combines two weakly supervised components to identify salient opinions and form
extractive summaries from multiple reviews: an aspect extractor trained under a
multi-task objective, and a sentiment predictor based on multiple instance
learning. We introduce an opinion summarization dataset that includes a
training set of product reviews from six diverse domains and human-annotated
development and test sets with gold standard aspect annotations, salience
labels, and opinion summaries. Automatic evaluation shows significant
improvements over baselines, and a large-scale study indicates that our opinion
summaries are preferred by human judges according to multiple criteria.
</summary>
    <author>
      <name>Stefanos Angelidis</name>
    </author>
    <author>
      <name>Mirella Lapata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In EMNLP 2018 (long paper). For supplementary material, see
  http://stangelid.github.io/supplemental.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07954v3</id>
    <updated>2018-08-27T13:53:56Z</updated>
    <published>2017-10-22T14:59:08Z</published>
    <title>Bayesian Cluster Enumeration Criterion for Unsupervised Learning</title>
    <summary>  We derive a new Bayesian Information Criterion (BIC) by formulating the
problem of estimating the number of clusters in an observed data set as
maximization of the posterior probability of the candidate models. Given that
some mild assumptions are satisfied, we provide a general BIC expression for a
broad class of data distributions. This serves as a starting point when
deriving the BIC for specific distributions. Along this line, we provide a
closed-form BIC expression for multivariate Gaussian distributed variables. We
show that incorporating the data structure of the clustering problem into the
derivation of the BIC results in an expression whose penalty term is different
from that of the original BIC. We propose a two-step cluster enumeration
algorithm. First, a model-based unsupervised learning algorithm partitions the
data according to a given set of candidate models. Subsequently, the number of
clusters is determined as the one associated with the model for which the
proposed BIC is maximal. The performance of the proposed two-step algorithm is
tested using synthetic and real data sets.
</summary>
    <author>
      <name>Freweyni K. Teklehaymanot</name>
    </author>
    <author>
      <name>Michael Muma</name>
    </author>
    <author>
      <name>Abdelhak M. Zoubir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2018.2866385</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2018.2866385" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.07954v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07954v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08836v1</id>
    <updated>2018-08-27T13:19:49Z</updated>
    <published>2018-08-27T13:19:49Z</published>
    <title>A strong baseline for question relevancy ranking</title>
    <summary>  The best systems at the SemEval-16 and SemEval-17 community question
answering shared tasks -- a task that amounts to question relevancy ranking --
involve complex pipelines and manual feature engineering. Despite this, many of
these still fail at beating the IR baseline, i.e., the rankings provided by
Google's search engine. We present a strong baseline for question relevancy
ranking by training a simple multi-task feed forward network on a bag of 14
distance measures for the input question pair. This baseline model, which is
fast to train and uses only language-independent features, outperforms the best
shared task systems on the task of retrieving relevant previously asked
questions.
</summary>
    <author>
      <name>Ana V. González-Garduño</name>
    </author>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08833v1</id>
    <updated>2018-08-27T13:08:43Z</updated>
    <published>2018-08-27T13:08:43Z</published>
    <title>Gradient-based Training of Slow Feature Analysis by Differentiable
  Approximate Whitening</title>
    <summary>  This paper proposes Power Slow Feature Analysis, a gradient-based method to
extract temporally-slow features from a high-dimensional input stream that
varies on a faster time-scale, and a variant of Slow Feature Analysis (SFA).
While displaying performance comparable to hierarchical extensions to the SFA
algorithm, such as Hierarchical Slow Feature Analysis, for a small number of
output-features, our algorithm allows end-to-end training of arbitrary
differentiable approximators (e.g., deep neural networks). We provide
experimental evidence that PowerSFA is able to extract meaningful and
informative low-dimensional features in the case of a) synthetic
low-dimensional data, b) visual data, and also for c) a general dataset for
which symmetric non-temporal relations between points can be defined.
</summary>
    <author>
      <name>Merlin Schüler</name>
    </author>
    <author>
      <name>Hlynur Davíð Hlynsson</name>
    </author>
    <author>
      <name>Laurenz Wiskott</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05784v2</id>
    <updated>2018-08-27T12:59:53Z</updated>
    <published>2018-08-17T07:54:35Z</published>
    <title>Multiview Boosting by Controlling the Diversity and the Accuracy of
  View-specific Voters</title>
    <summary>  In this paper we propose a boosting based multiview learning algorithm,
referred to as PB-MVBoost, which iteratively learns i) weights over
view-specific voters capturing view-specific information; and ii) weights over
views by optimizing a PAC-Bayes multiview C-Bound that takes into account the
accuracy of view-specific classifiers and the diversity between the views. We
derive a generalization bound for this strategy following the PAC-Bayes theory
which is a suitable tool to deal with models expressed as weighted combination
over a set of voters. Different experiments on three publicly available
datasets show the efficiency of the proposed approach with respect to
state-of-art models.
</summary>
    <author>
      <name>Anil Goyal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AMA, LHC</arxiv:affiliation>
    </author>
    <author>
      <name>Emilie Morvant</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LHC</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Germain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MODAL</arxiv:affiliation>
    </author>
    <author>
      <name>Massih-Reza Amini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AMA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1808.05784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08811v1</id>
    <updated>2018-08-27T12:15:14Z</updated>
    <published>2018-08-27T12:15:14Z</published>
    <title>Exponential inequalities for nonstationary Markov Chains</title>
    <summary>  Exponential inequalities are main tools in machine learning theory. To prove
exponential inequalities for non i.i.d random variables allows to extend many
learning techniques to these variables. Indeed, much work has been done both on
inequalities and learning theory for time series, in the past 15 years.
However, for the non independent case, almost all the results concern
stationary time series. This excludes many important applications: for example
any series with a periodic behaviour is non-stationary. In this paper, we
extend the basic tools of Dedecker and Fan (2015) to nonstationary Markov
chains. As an application, we provide a Bernstein-type inequality, and we
deduce risk bounds for the prediction of periodic autoregressive processes with
an unknown period.
</summary>
    <author>
      <name>Pierre Alquier</name>
    </author>
    <author>
      <name>Paul Doukhan</name>
    </author>
    <author>
      <name>Xiequan Fan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08798v1</id>
    <updated>2018-08-27T11:49:44Z</updated>
    <published>2018-08-27T11:49:44Z</published>
    <title>Beyond expectation: Deep joint mean and quantile regression for
  spatio-temporal problems</title>
    <summary>  Spatio-temporal problems are ubiquitous and of vital importance in many
research fields. Despite the potential already demonstrated by deep learning
methods in modeling spatio-temporal data, typical approaches tend to focus
solely on conditional expectations of the output variables being modeled. In
this paper, we propose a multi-output multi-quantile deep learning approach for
jointly modeling several conditional quantiles together with the conditional
expectation as a way to provide a more complete "picture" of the predictive
density in spatio-temporal problems. Using two large-scale datasets from the
transportation domain, we empirically demonstrate that, by approaching the
quantile regression problem from a multi-task learning perspective, it is
possible to solve the embarrassing quantile crossings problem, while
simultaneously significantly outperforming state-of-the-art quantile regression
methods. Moreover, we show that jointly modeling the mean and several
conditional quantiles not only provides a rich description about the predictive
density that can capture heteroscedastic properties at a neglectable
computational overhead, but also leads to improved predictions of the
conditional expectation due to the extra information and a regularization
effect induced by the added quantiles.
</summary>
    <author>
      <name>Filipe Rodrigues</name>
    </author>
    <author>
      <name>Francisco C. Pereira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00948v1</id>
    <updated>2018-08-27T11:44:48Z</updated>
    <published>2018-08-27T11:44:48Z</published>
    <title>Task adapted reconstruction for inverse problems</title>
    <summary>  The paper considers the problem of performing a task defined on a model
parameter that is only observed indirectly through noisy data in an ill-posed
inverse problem. A key aspect is to formalize the steps of reconstruction and
task as appropriate estimators (non-randomized decision rules) in statistical
estimation problems. The implementation makes use of (deep) neural networks to
provide a differentiable parametrization of the family of estimators for both
steps. These networks are combined and jointly trained against suitable
supervised training data in order to minimize a joint differentiable loss
function, resulting in an end-to-end task adapted reconstruction method. The
suggested framework is generic, yet adaptable, with a plug-and-play structure
for adjusting both the inverse problem and the task at hand. More precisely,
the data model (forward operator and statistical model of the noise) associated
with the inverse problem is exchangeable, e.g., by using neural network
architecture given by a learned iterative method. Furthermore, any task that is
encodable as a trainable neural network can be used. The approach is
demonstrated on joint tomographic image reconstruction, classification and
joint tomographic image reconstruction segmentation.
</summary>
    <author>
      <name>Jonas Adler</name>
    </author>
    <author>
      <name>Sebastian Lunz</name>
    </author>
    <author>
      <name>Olivier Verdier</name>
    </author>
    <author>
      <name>Carola-Bibiane Schönlieb</name>
    </author>
    <author>
      <name>Ozan Öktem</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08784v1</id>
    <updated>2018-08-27T11:12:14Z</updated>
    <published>2018-08-27T11:12:14Z</published>
    <title>Sparsity in Deep Neural Networks - An Empirical Investigation with
  TensorQuant</title>
    <summary>  Deep learning is finding its way into the embedded world with applications
such as autonomous driving, smart sensors and aug- mented reality. However, the
computation of deep neural networks is demanding in energy, compute power and
memory. Various approaches have been investigated to reduce the necessary
resources, one of which is to leverage the sparsity occurring in deep neural
networks due to the high levels of redundancy in the network parameters. It has
been shown that sparsity can be promoted specifically and the achieved sparsity
can be very high. But in many cases the methods are evaluated on rather small
topologies. It is not clear if the results transfer onto deeper topologies. In
this paper, the TensorQuant toolbox has been extended to offer a platform to
investigate sparsity, especially in deeper models. Several practical relevant
topologies for varying classification problem sizes are investigated to show
the differences in sparsity for activations, weights and gradients.
</summary>
    <author>
      <name>Dominik Marek Loroch</name>
    </author>
    <author>
      <name>Franz-Josef Pfreundt</name>
    </author>
    <author>
      <name>Norbert Wehn</name>
    </author>
    <author>
      <name>Janis Keuper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECML18, Decentralized Machine Learning at the Edge workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07607v2</id>
    <updated>2018-08-27T10:06:23Z</updated>
    <published>2016-07-26T09:26:20Z</published>
    <title>Adaptive Nonnegative Matrix Factorization and Measure Comparisons for
  Recommender Systems</title>
    <summary>  The Nonnegative Matrix Factorization (NMF) of the rating matrix has shown to
be an effective method to tackle the recommendation problem. In this paper we
propose new methods based on the NMF of the rating matrix and we compare them
with some classical algorithms such as the SVD and the regularized and
unregularized non-negative matrix factorization approach. In particular a new
algorithm is obtained changing adaptively the function to be minimized at each
step, realizing a sort of dynamic prior strategy. Another algorithm is obtained
modifying the function to be minimized in the NMF formulation by enforcing the
reconstruction of the unknown ratings toward a prior term. We then combine
different methods obtaining two mixed strategies which turn out to be very
effective in the reconstruction of missing observations. We perform a
thoughtful comparison of different methods on the basis of several evaluation
measures. We consider in particular rating, classification and ranking measures
showing that the algorithm obtaining the best score for a given measure is in
general the best also when different measures are considered, lowering the
interest in designing specific evaluation measures. The algorithms have been
tested on different datasets, in particular the 1M, and 10M MovieLens datasets
containing ratings on movies, the Jester dataset with ranting on jokes and
Amazon Fine Foods dataset with ratings on foods. The comparison of the
different algorithms, shows the good performance of methods employing both an
explicit and an implicit regularization scheme. Moreover we can get a boost by
mixed strategies combining a fast method with a more accurate one.
</summary>
    <author>
      <name>Gianna M. Del Corso</name>
    </author>
    <author>
      <name>Francesco Romani</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65F99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08766v1</id>
    <updated>2018-08-27T10:06:01Z</updated>
    <published>2018-08-27T10:06:01Z</published>
    <title>Learning behavioral context recognition with multi-stream temporal
  convolutional networks</title>
    <summary>  Smart devices of everyday use (such as smartphones and wearables) are
increasingly integrated with sensors that provide immense amounts of
information about a person's daily life such as behavior and context. The
automatic and unobtrusive sensing of behavioral context can help develop
solutions for assisted living, fitness tracking, sleep monitoring, and several
other fields. Towards addressing this issue, we raise the question: can a
machine learn to recognize a diverse set of contexts and activities in a
real-life through joint learning from raw multi-modal signals (e.g.
accelerometer, gyroscope and audio etc.)? In this paper, we propose a
multi-stream temporal convolutional network to address the problem of
multi-label behavioral context recognition. A four-stream network architecture
handles learning from each modality with a contextualization module which
incorporates extracted representations to infer a user's context. Our empirical
evaluation suggests that a deep convolutional network trained end-to-end
achieves an optimal recognition rate. Furthermore, the presented architecture
can be extended to include similar sensors for performance improvements and
handles missing modalities through multi-task learning without any manual
feature engineering on highly imbalanced and sparsely labeled dataset.
</summary>
    <author>
      <name>Aaqib Saeed</name>
    </author>
    <author>
      <name>Tanir Ozcelebi</name>
    </author>
    <author>
      <name>Stojan Trajanovski</name>
    </author>
    <author>
      <name>Johan Lukkien</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08765v1</id>
    <updated>2018-08-27T10:04:36Z</updated>
    <published>2018-08-27T10:04:36Z</published>
    <title>Identifiability of Low-Rank Sparse Component Analysis</title>
    <summary>  Sparse component analysis (SCA) is the following problem: Given an input
matrix $M$ and an integer $r$, find a dictionary $D$ with $r$ columns and a
sparse matrix $B$ with $r$ rows such that $M \approx DB$. A key issue in SCA is
identifiability, that is, characterizing the conditions under which $D$ and $B$
are essentially unique (that is, they are unique up to permutation and scaling
of the columns of $D$ and rows of $B$). Although SCA has been vastly
investigated in the last two decades, only a few works have tackled this issue
in the deterministic scenario, and no work provides reasonable bounds in the
minimum number of data points (that is, columns of $M$) that leads to
identifiability. In this work, we provide new results in the deterministic
scenario when the data has a low-rank structure, that is, when $D$ has rank
$r$, drastically improving with respect to previous results. In particular, we
show that if each column of $B$ contains at least $s$ zeros then
$\mathcal{O}(r^3/s^2)$ data points are sufficient to obtain an essentially
unique decomposition, as long as these data points are well spread among the
subspaces spanned by $r-1$ columns of $D$. This implies for example that for a
fixed proportion of zeros (constant and independent of $r$, e.g., 10\% of zero
entries in $B$), one only requires $O(r)$ data points to guarantee
identifiability.
</summary>
    <author>
      <name>Jérémy E. Cohen</name>
    </author>
    <author>
      <name>Nicolas Gillis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08762v1</id>
    <updated>2018-08-27T09:50:56Z</updated>
    <published>2018-08-27T09:50:56Z</published>
    <title>Natural Language Inference with Hierarchical BiLSTM Max Pooling
  Architecture</title>
    <summary>  Recurrent neural networks have proven to be very effective for natural
language inference tasks. We build on top of one such model, namely BiLSTM with
max pooling, and show that adding a hierarchy of BiLSTM and max pooling layers
yields state of the art results for the SNLI sentence encoding-based models and
the SciTail dataset, as well as provides strong results for the MultiNLI
dataset. We also show that our sentence embeddings can be utilized in a wide
variety of transfer learning tasks, outperforming InferSent on 7 out of 10 and
SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks.
Furthermore, our model beats the InferSent model in 8 out of 10 recently
published SentEval probing tasks designed to evaluate sentence embeddings'
ability to capture some of the important linguistic properties of sentences.
</summary>
    <author>
      <name>Aarne Talman</name>
    </author>
    <author>
      <name>Anssi Yli-Jyrä</name>
    </author>
    <author>
      <name>Jörg Tiedemann</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08755v1</id>
    <updated>2018-08-27T09:39:52Z</updated>
    <published>2018-08-27T09:39:52Z</published>
    <title>Learning from Positive and Unlabeled Data under the Selected At Random
  Assumption</title>
    <summary>  For many interesting tasks, such as medical diagnosis and web page
classification, a learner only has access to some positively labeled examples
and many unlabeled examples. Learning from this type of data requires making
assumptions about the true distribution of the classes and/or the mechanism
that was used to select the positive examples to be labeled. The commonly made
assumptions, separability of the classes and positive examples being selected
completely at random, are very strong. This paper proposes a weaker assumption
that assumes the positive examples to be selected at random, conditioned on
some of the attributes. To learn under this assumption, an EM method is
proposed. Experiments show that our method is not only very capable of learning
under this assumption, but it also outperforms the state of the art for
learning under the selected completely at random assumption.
</summary>
    <author>
      <name>Jessa Bekker</name>
    </author>
    <author>
      <name>Jesse Davis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08750v1</id>
    <updated>2018-08-27T09:17:57Z</updated>
    <published>2018-08-27T09:17:57Z</published>
    <title>Generalisation in humans and deep neural networks</title>
    <summary>  We compare the robustness of humans and current convolutional deep neural
networks (DNNs) on object recognition under twelve different types of image
degradations. First, using three well known DNNs (ResNet-152, VGG-19,
GoogLeNet) we find the human visual system to be more robust to nearly all of
the tested image manipulations, and we observe progressively diverging
classification error-patterns between humans and DNNs when the signal gets
weaker. Secondly, we show that DNNs trained directly on distorted images
consistently surpass human performance on the exact distortion types they were
trained on, yet they display extremely poor generalisation abilities when
tested on other distortion types. For example, training on salt-and-pepper
noise does not imply robustness on uniform white noise and vice versa. Thus,
changes in the noise distribution between training and testing constitutes a
crucial challenge to deep learning vision systems that can be systematically
addressed in a lifelong machine learning approach. Our new dataset consisting
of 83K carefully measured human psychophysical trials provide a useful
reference for lifelong robustness against image degradations set by the human
visual system.
</summary>
    <author>
      <name>Robert Geirhos</name>
    </author>
    <author>
      <name>Carlos R. Medina Temme</name>
    </author>
    <author>
      <name>Jonas Rauber</name>
    </author>
    <author>
      <name>Heiko H. Schuett</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <author>
      <name>Felix A. Wichmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to NIPS 2018. 26 pages, 14 figures, 1 table. Supersedes and
  greatly extends arXiv:1706.06969</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08720v1</id>
    <updated>2018-08-27T07:55:41Z</updated>
    <published>2018-08-27T07:55:41Z</published>
    <title>Predefined Sparseness in Recurrent Sequence Models</title>
    <summary>  Inducing sparseness while training neural networks has been shown to yield
models with a lower memory footprint but similar effectiveness to dense models.
However, sparseness is typically induced starting from a dense model, and thus
this advantage does not hold during training. We propose techniques to enforce
sparseness upfront in recurrent sequence models for NLP applications, to also
benefit training. First, in language modeling, we show how to increase hidden
state sizes in recurrent layers without increasing the number of parameters,
leading to more expressive models. Second, for sequence labeling, we show that
word embeddings with predefined sparseness lead to similar performance as dense
embeddings, at a fraction of the number of trainable parameters.
</summary>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <author>
      <name>Johannes Deleu</name>
    </author>
    <author>
      <name>Fréderic Godin</name>
    </author>
    <author>
      <name>Chris Develder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">the SIGNLL Conference on Computational Natural Language Learning
  (CoNLL, 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07000v4</id>
    <updated>2018-08-27T07:17:25Z</updated>
    <published>2018-06-19T01:28:58Z</published>
    <title>A Syntactically Constrained Bidirectional-Asynchronous Approach for
  Emotional Conversation Generation</title>
    <summary>  Traditional neural language models tend to generate generic replies with poor
logic and no emotion. In this paper, a syntactically constrained
bidirectional-asynchronous approach for emotional conversation generation
(E-SCBA) is proposed to address this issue. In our model, pre-generated emotion
keywords and topic keywords are asynchronously introduced into the process of
decoding. It is much different from most existing methods which generate
replies from the first word to the last. Through experiments, the results
indicate that our approach not only improves the diversity of replies, but
gains a boost on both logic and emotion compared with baselines.
</summary>
    <author>
      <name>Jingyuan Li</name>
    </author>
    <author>
      <name>Xiao Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07000v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07000v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08703v1</id>
    <updated>2018-08-27T06:51:07Z</updated>
    <published>2018-08-27T06:51:07Z</published>
    <title>Generating Text through Adversarial Training using Skip-Thought Vectors</title>
    <summary>  In the past few years, various advancements have been made in generative
models owing to the formulation of Generative Adversarial Networks (GANs). GANs
have been shown to perform exceedingly well on a wide variety of tasks
pertaining to image generation and style transfer. In the field of Natural
Language Processing, word embeddings such as word2vec and GLoVe are
state-of-the-art methods for applying neural network models on textual data.
Attempts have been made for utilizing GANs with word embeddings for text
generation. This work presents an approach to text generation using
Skip-Thought sentence embeddings in conjunction with GANs based on gradient
penalty functions and f-measures. The results of using sentence embeddings with
GANs for generating text conditioned on input information are comparable to the
approaches where word embeddings are used.
</summary>
    <author>
      <name>Afroz Ahamad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 1. 6 pages, 1 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12547v3</id>
    <updated>2018-08-27T05:35:36Z</updated>
    <published>2018-05-31T16:36:26Z</published>
    <title>Long-time predictive modeling of nonlinear dynamical systems using
  neural networks</title>
    <summary>  We study the use of feedforward neural networks (FNN) to develop models of
nonlinear dynamical systems from data. Emphasis is placed on predictions at
long times, with limited data availability. Inspired by global stability
analysis, and the observation of the strong correlation between the local error
and the maximum singular value of the Jacobian of the ANN, we introduce
Jacobian regularization in the loss function. This regularization suppresses
the sensitivity of the prediction to the local error and is shown to improve
accuracy and robustness. Comparison between the proposed approach and sparse
polynomial regression is presented in numerical examples ranging from simple
ODE systems to nonlinear PDE systems including vortex shedding behind a
cylinder, and instability-driven buoyant mixing flow. Furthermore, limitations
of feedforward neural networks are highlighted, especially when the training
data does not include a low dimensional attractor. Strategies of data
augmentation are presented as remedies to address these issues to a certain
extent.
</summary>
    <author>
      <name>Shaowu Pan</name>
    </author>
    <author>
      <name>Karthik Duraisamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.12547v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12547v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="37M99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01560v4</id>
    <updated>2018-08-27T05:30:05Z</updated>
    <published>2018-08-05T05:10:26Z</published>
    <title>Stock Price Correlation Coefficient Prediction with ARIMA-LSTM Hybrid
  Model</title>
    <summary>  Predicting the price correlation of two assets for future time periods is
important in portfolio optimization. We apply LSTM recurrent neural networks
(RNN) in predicting the stock price correlation coefficient of two individual
stocks. RNNs are competent in understanding temporal dependencies. The use of
LSTM cells further enhances its long term predictive properties. To encompass
both linearity and nonlinearity in the model, we adopt the ARIMA model as well.
The ARIMA model filters linear tendencies in the data and passes on the
residual value to the LSTM model. The ARIMA LSTM hybrid model is tested against
other traditional predictive financial models such as the full historical
model, constant correlation model, single index model and the multi group
model. In our empirical study, the predictive ability of the ARIMA-LSTM model
turned out superior to all other financial models by a significant scale. Our
work implies that it is worth considering the ARIMA LSTM model to forecast
correlation coefficient for portfolio optimization.
</summary>
    <author>
      <name>Hyeong Kyu Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I'd appreciate any kind of comments on my work. Feel free to email
  me!</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01560v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01560v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08716v2</id>
    <updated>2018-08-27T05:22:38Z</updated>
    <published>2018-07-23T16:50:31Z</published>
    <title>NullaNet: Training Deep Neural Networks for Reduced-Memory-Access
  Inference</title>
    <summary>  Deep neural networks have been successfully deployed in a wide variety of
applications including computer vision and speech recognition. However,
computational and storage complexity of these models has forced the majority of
computations to be performed on high-end computing platforms or on the cloud.
To cope with computational and storage complexity of these models, this paper
presents a training method that enables a radically different approach for
realization of deep neural networks through Boolean logic minimization. The
aforementioned realization completely removes the energy-hungry step of
accessing memory for obtaining model parameters, consumes about two orders of
magnitude fewer computing resources compared to realizations that use
floatingpoint operations, and has a substantially lower latency.
</summary>
    <author>
      <name>Mahdi Nazemi</name>
    </author>
    <author>
      <name>Ghasem Pasandi</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08716v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08716v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08149v2</id>
    <updated>2018-08-27T02:04:51Z</updated>
    <published>2018-08-24T14:17:01Z</published>
    <title>From Random to Supervised: A Novel Dropout Mechanism Integrated with
  Global Information</title>
    <summary>  Dropout is used to avoid overfitting by randomly dropping units from the
neural networks during training. Inspired by dropout, this paper presents
GI-Dropout, a novel dropout method integrating with global information to
improve neural networks for text classification. Unlike the traditional dropout
method in which the units are dropped randomly according to the same
probability, we aim to use explicit instructions based on global information of
the dataset to guide the training process. With GI-Dropout, the model is
supposed to pay more attention to inapparent features or patterns. Experiments
demonstrate the effectiveness of the dropout with global information on seven
text classification tasks, including sentiment analysis and topic
classification.
</summary>
    <author>
      <name>Hengru Xu</name>
    </author>
    <author>
      <name>Shen Li</name>
    </author>
    <author>
      <name>Renfen Hu</name>
    </author>
    <author>
      <name>Si Li</name>
    </author>
    <author>
      <name>Sheng Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08149v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08149v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00946v1</id>
    <updated>2018-08-26T23:09:03Z</updated>
    <published>2018-08-26T23:09:03Z</published>
    <title>Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing
  GANs</title>
    <summary>  We present a framework for translating unlabeled images from one domain into
analog images in another domain. We employ a progressively growing
skip-connected encoder-generator structure and train it with a GAN loss for
realistic output, a cycle consistency loss for maintaining same-domain
translation identity, and a semantic consistency loss that encourages the
network to keep the input semantic features in the output. We apply our
framework on the task of translating face images, and show that it is capable
of learning semantic mappings for face images with no supervised one-to-one
image mapping.
</summary>
    <author>
      <name>Jerry Li</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08640v1</id>
    <updated>2018-08-26T23:02:54Z</updated>
    <published>2018-08-26T23:02:54Z</published>
    <title>Detecting Outliers in Data with Correlated Measures</title>
    <summary>  Advances in sensor technology have enabled the collection of large-scale
datasets. Such datasets can be extremely noisy and often contain a significant
amount of outliers that result from sensor malfunction or human operation
faults. In order to utilize such data for real-world applications, it is
critical to detect outliers so that models built from these datasets will not
be skewed by outliers.
  In this paper, we propose a new outlier detection method that utilizes the
correlations in the data (e.g., taxi trip distance vs. trip time). Different
from existing outlier detection methods, we build a robust regression model
that explicitly models the outliers and detects outliers simultaneously with
the model fitting.
  We validate our approach on real-world datasets against methods specifically
designed for each dataset as well as the state of the art outlier detectors.
Our outlier detection method achieves better performances, demonstrating the
robustness and generality of our method. Last, we report interesting case
studies on some outliers that result from atypical events.
</summary>
    <author>
      <name>Yu-Hsuan Kuo</name>
    </author>
    <author>
      <name>Zhenhui Li</name>
    </author>
    <author>
      <name>Daniel Kifer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271798</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271798" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03441v4</id>
    <updated>2018-08-26T22:52:50Z</updated>
    <published>2017-09-11T15:38:49Z</published>
    <title>The Diverse Cohort Selection Problem</title>
    <summary>  How should a firm allocate its limited interviewing resources to select the
optimal cohort of new employees from a large set of job applicants? How should
that firm allocate cheap but noisy resume screenings and expensive but in-depth
in-person interviews? We view this problem through the lens of combinatorial
pure exploration (CPE) in the multi-armed bandit setting, where a central
learning agent performs costly exploration of a set of arms before selecting a
final subset with some combinatorial structure. We generalize a recent CPE
algorithm to the setting where arm pulls can have different costs, and return
different levels of information, and prove theoretical upper bounds for a
general class of arm-pulling strategies in this new setting. We then apply our
general algorithm to a real-world problem with combinatorial structure:
incorporating diversity into university admissions. We take real data from
admissions at one of the largest US-based computer science graduate programs
and show that a simulation of our algorithm produces more diverse student
cohorts at low cost to individual student quality, spending comparable budget
to the current admissions process at that university.
</summary>
    <author>
      <name>Candice Schumann</name>
    </author>
    <author>
      <name>Samsara N. Counts</name>
    </author>
    <author>
      <name>Jeffrey S. Foster</name>
    </author>
    <author>
      <name>John P. Dickerson</name>
    </author>
    <link href="http://arxiv.org/abs/1709.03441v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03441v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04523v3</id>
    <updated>2018-08-26T22:45:16Z</updated>
    <published>2018-08-14T05:01:55Z</published>
    <title>Adaptive Sampling for Convex Regression</title>
    <summary>  In this paper, we introduce the first principled adaptive-sampling procedure
for learning a convex function in the $L_\infty$ norm, a problem that arises
often in the behavioral and social sciences. We present a function-specific
measure of complexity and use it to prove that, for each convex function
$f_{\star}$, our algorithm nearly attains the information-theoretically
optimal, function-specific error rate. We also corroborate our theoretical
contributions with numerical experiments, finding that our method substantially
outperforms passive, uniform sampling for favorable synthetic and data-derived
functions in low-noise settings with large sampling budgets. Our results also
suggest an idealized "oracle strategy", which we use to gauge the potential
advance of any adaptive-sampling strategy over passive sampling, for any given
convex function.
</summary>
    <author>
      <name>Max Simchowitz</name>
    </author>
    <author>
      <name>Kevin Jamieson</name>
    </author>
    <author>
      <name>Jordan W. Suchow</name>
    </author>
    <author>
      <name>Thomas L. Griffiths</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04523v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04523v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06523v3</id>
    <updated>2018-08-26T21:35:36Z</updated>
    <published>2018-03-17T15:38:20Z</published>
    <title>Stochastic model-based minimization of weakly convex functions</title>
    <summary>  We consider a family of algorithms that successively sample and minimize
simple stochastic models of the objective function. We show that under
reasonable conditions on approximation quality and regularity of the models,
any such algorithm drives a natural stationarity measure to zero at the rate
$O(k^{-1/4})$. As a consequence, we obtain the first complexity guarantees for
the stochastic proximal point, proximal subgradient, and regularized
Gauss-Newton methods for minimizing compositions of convex functions with
smooth maps. The guiding principle, underlying the complexity guarantees, is
that all algorithms under consideration can be interpreted as approximate
descent methods on an implicit smoothing of the problem, given by the Moreau
envelope. Specializing to classical circumstances, we obtain the long-sought
convergence rate of the stochastic projected gradient method, without batching,
for minimizing a smooth function on a closed convex set.
</summary>
    <author>
      <name>Damek Davis</name>
    </author>
    <author>
      <name>Dmitriy Drusvyatskiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.06523v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06523v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65K05, 65K10, 90C15, 90C30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08627v1</id>
    <updated>2018-08-26T21:34:36Z</updated>
    <published>2018-08-26T21:34:36Z</published>
    <title>Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation</title>
    <summary>  As opposed to manual feature engineering which is tedious and difficult to
scale, network representation learning has attracted a surge of research
interests as it automates the process of feature learning on graphs. The
learned low-dimensional node vector representation is generalizable and eases
the knowledge discovery process on graphs by enabling various off-the-shelf
machine learning tools to be directly applied. Recent research has shown that
the past decade of network embedding approaches either explicitly factorize a
carefully designed matrix to obtain the low-dimensional node vector
representation or are closely related to implicit matrix factorization, with
the fundamental assumption that the factorized node connectivity matrix is
low-rank. Nonetheless, the global low-rank assumption does not necessarily hold
especially when the factorized matrix encodes complex node interactions, and
the resultant single low-rank embedding matrix is insufficient to capture all
the observed connectivity patterns. In this regard, we propose a novel
multi-level network embedding framework BoostNE, which can learn multiple
network embedding representations of different granularity from coarse to fine
without imposing the prevalent global low-rank assumption. The proposed BoostNE
method is also in line with the successful gradient boosting method in ensemble
learning as multiple weak embeddings lead to a stronger and more effective one.
We assess the effectiveness of the proposed BoostNE framework by comparing it
with existing state-of-the-art network embedding methods on various datasets,
and the experimental results corroborate the superiority of the proposed
BoostNE network embedding framework.
</summary>
    <author>
      <name>Jundong Li</name>
    </author>
    <author>
      <name>Liang Wu</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08952v1</id>
    <updated>2018-08-26T20:54:33Z</updated>
    <published>2018-08-26T20:54:33Z</published>
    <title>Deep Learning of Vortex Induced Vibrations</title>
    <summary>  Vortex induced vibrations of bluff bodies occur when the vortex shedding
frequency is close to the natural frequency of the structure. Of interest is
the prediction of the lift and drag forces on the structure given some limited
and scattered information on the velocity field. This is an inverse problem
that is not straightforward to solve using standard computational fluid
dynamics (CFD) methods, especially since no information is provided for the
pressure. An even greater challenge is to infer the lift and drag forces given
some dye or smoke visualizations of the flow field. Here we employ deep neural
networks that are extended to encode the incompressible Navier-Stokes equations
coupled with the structure's dynamic motion equation. In the first case, given
scattered data in space-time on the velocity field and the structure's motion,
we use four coupled deep neural networks to infer very accurately the
structural parameters, the entire time-dependent pressure field (with no prior
training data), and reconstruct the velocity vector field and the structure's
dynamic motion. In the second case, given scattered data in space-time on a
concentration field only, we use five coupled deep neural networks to infer
very accurately the vector velocity field and all other quantities of interest
as before. This new paradigm of inference in fluid mechanics for coupled
multi-physics problems enables velocity and pressure quantification from flow
snapshots in small subdomains and can be exploited for flow control
applications and also for system identification.
</summary>
    <author>
      <name>Maziar Raissi</name>
    </author>
    <author>
      <name>Zhicheng Wang</name>
    </author>
    <author>
      <name>Michael S. Triantafyllou</name>
    </author>
    <author>
      <name>George Em Karniadakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1808.04327</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01860v2</id>
    <updated>2018-08-26T20:50:58Z</updated>
    <published>2017-06-06T17:18:38Z</published>
    <title>Attributed Network Embedding for Learning in a Dynamic Environment</title>
    <summary>  Network embedding leverages the node proximity manifested to learn a
low-dimensional node vector representation for each node in the network. The
learned embeddings could advance various learning tasks such as node
classification, network clustering, and link prediction. Most, if not all, of
the existing works, are overwhelmingly performed in the context of plain and
static networks. Nonetheless, in reality, network structure often evolves over
time with addition/deletion of links and nodes. Also, a vast majority of
real-world networks are associated with a rich set of node attributes, and
their attribute values are also naturally changing, with the emerging of new
content patterns and the fading of old content patterns. These changing
characteristics motivate us to seek an effective embedding representation to
capture network and attribute evolving patterns, which is of fundamental
importance for learning in a dynamic environment. To our best knowledge, we are
the first to tackle this problem with the following two challenges: (1) the
inherently correlated network and node attributes could be noisy and
incomplete, it necessitates a robust consensus representation to capture their
individual properties and correlations; (2) the embedding learning needs to be
performed in an online fashion to adapt to the changes accordingly. In this
paper, we tackle this problem by proposing a novel dynamic attributed network
embedding framework - DANE. In particular, DANE first provides an offline
method for a consensus embedding and then leverages matrix perturbation theory
to maintain the freshness of the end embedding results in an online manner. We
perform extensive experiments on both synthetic and real attributed networks to
corroborate the effectiveness and efficiency of the proposed framework.
</summary>
    <author>
      <name>Jundong Li</name>
    </author>
    <author>
      <name>Harsh Dani</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <author>
      <name>Yi Chang</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3132847.3132919</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3132847.3132919" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.01860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07996v5</id>
    <updated>2018-08-26T20:43:57Z</updated>
    <published>2016-01-29T08:32:10Z</published>
    <title>Feature Selection: A Data Perspective</title>
    <summary>  Feature selection, as a data preprocessing strategy, has been proven to be
effective and efficient in preparing data (especially high-dimensional data)
for various data mining and machine learning problems. The objectives of
feature selection include: building simpler and more comprehensible models,
improving data mining performance, and preparing clean, understandable data.
The recent proliferation of big data has presented some substantial challenges
and opportunities to feature selection. In this survey, we provide a
comprehensive and structured overview of recent advances in feature selection
research. Motivated by current challenges and opportunities in the era of big
data, we revisit feature selection research from a data perspective and review
representative feature selection algorithms for conventional data, structured
data, heterogeneous data and streaming data. Methodologically, to emphasize the
differences and similarities of most existing feature selection algorithms for
conventional data, we categorize them into four main groups: similarity based,
information theoretical based, sparse learning based and statistical based
methods. To facilitate and promote the research in this community, we also
present an open-source feature selection repository that consists of most of
the popular feature selection algorithms
(\url{http://featureselection.asu.edu/}). Also, we use it as an example to show
how to evaluate feature selection algorithms. At the end of the survey, we
present a discussion about some open problems and challenges that require more
attention in future research.
</summary>
    <author>
      <name>Jundong Li</name>
    </author>
    <author>
      <name>Kewei Cheng</name>
    </author>
    <author>
      <name>Suhang Wang</name>
    </author>
    <author>
      <name>Fred Morstatter</name>
    </author>
    <author>
      <name>Robert P. Trevino</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3136625</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3136625" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Computing Surveys (CSUR), 50(6): 94:1-94:45, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.07996v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07996v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08619v1</id>
    <updated>2018-08-26T20:36:58Z</updated>
    <published>2018-08-26T20:36:58Z</published>
    <title>Discriminative but Not Discriminatory: A Comparison of Fairness
  Definitions under Different Worldviews</title>
    <summary>  We mathematically compare three competing definitions of group-level
nondiscrimination: demographic parity, equalized odds, and calibration. Using
the theoretical framework of Friedler et al., we study the properties of each
definition under various worldviews, which are assumptions about how, if at
all, the observed data is biased. We prove that different worldviews call for
different definitions of fairness, and we specify when it is appropriate to use
demographic parity and equalized odds. In addition, we argue that calibration
is unsuitable for the purpose of ensuring nondiscrimination. Finally, we define
a worldview that is more realistic than the previously considered ones, and we
introduce a new notion of fairness that is suitable for this worldview.
</summary>
    <author>
      <name>Samuel Yeom</name>
    </author>
    <author>
      <name>Michael Carl Tschantz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08618v1</id>
    <updated>2018-08-26T20:26:11Z</updated>
    <published>2018-08-26T20:26:11Z</published>
    <title>Deep Learning: Computational Aspects</title>
    <summary>  In this article we review computational aspects of Deep Learning (DL). Deep
learning uses network architectures consisting of hierarchical layers of latent
variables to construct predictors for high-dimensional input-output models.
Training a deep learning architecture is computationally intensive, and
efficient linear algebra libraries is the key for training and inference.
Stochastic gradient descent (SGD) optimization and batch sampling are used to
learn from massive data sets.
</summary>
    <author>
      <name>Nicholas Polson</name>
    </author>
    <author>
      <name>Vadim Sokolov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08473v2</id>
    <updated>2018-08-26T19:48:07Z</updated>
    <published>2017-10-23T19:37:00Z</published>
    <title>A Unified Framework for Long Range and Cold Start Forecasting of
  Seasonal Profiles in Time Series</title>
    <summary>  Providing long-range forecasts is a fundamental challenge in time series
modeling, which is only compounded by the challenge of having to form such
forecasts when a time series has never previously been observed. The latter
challenge is the time series version of the cold-start problem seen in
recommender systems which, to our knowledge, has not been addressed in previous
work. A similar problem occurs when a long range forecast is required after
only observing a small number of time points --- a warm start forecast. With
these aims in mind, we focus on forecasting seasonal profiles---or baseline
demand---for periods on the order of a year in three cases: the long range case
with multiple previously observed seasonal profiles, the cold start case with
no previous observed seasonal profiles, and the warm start case with only a
single partially observed profile. Classical time series approaches that
perform iterated step-ahead forecasts based on previous observations struggle
to provide accurate long range predictions; in settings with little to no
observed data, such approaches are simply not applicable. Instead, we present a
straightforward framework which combines ideas from high-dimensional regression
and matrix factorization on a carefully constructed data matrix. Key to our
formulation and resulting performance is leveraging (1) repeated patterns over
fixed periods of time and across series, and (2) metadata associated with the
individual series; without this additional data, the cold-start/warm-start
problems are nearly impossible to solve. We demonstrate that our framework can
accurately forecast an array of seasonal profiles on multiple large scale
datasets.
</summary>
    <author>
      <name>Christopher Xie</name>
    </author>
    <author>
      <name>Alex Tank</name>
    </author>
    <author>
      <name>Alec Greaves-Tunnell</name>
    </author>
    <author>
      <name>Emily Fox</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08473v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08473v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02365v4</id>
    <updated>2018-08-26T19:23:06Z</updated>
    <published>2016-11-08T02:20:46Z</published>
    <title>NonSTOP: A NonSTationary Online Prediction Method for Time Series</title>
    <summary>  We present online prediction methods for time series that let us explicitly
handle nonstationary artifacts (e.g. trend and seasonality) present in most
real time series. Specifically, we show that applying appropriate
transformations to such time series before prediction can lead to improved
theoretical and empirical prediction performance. Moreover, since these
transformations are usually unknown, we employ the learning with experts
setting to develop a fully online method (NonSTOP-NonSTationary Online
Prediction) for predicting nonstationary time series. This framework allows for
seasonality and/or other trends in univariate time series and cointegration in
multivariate time series. Our algorithms and regret analysis subsume recent
related work while significantly expanding the applicability of such methods.
For all the methods, we provide sub-linear regret bounds using relaxed
assumptions. The theoretical guarantees do not fully capture the benefits of
the transformations, thus we provide a data-dependent analysis of the
follow-the-leader algorithm that provides insight into the success of using
such transformations. We support all of our results with experiments on
simulated and real data.
</summary>
    <author>
      <name>Christopher Xie</name>
    </author>
    <author>
      <name>Avleen Bijral</name>
    </author>
    <author>
      <name>Juan Lavista Ferres</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02365v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02365v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08613v1</id>
    <updated>2018-08-26T19:19:24Z</updated>
    <published>2018-08-26T19:19:24Z</published>
    <title>Ensemble Learning Applied to Classify GPS Trajectories of Birds into
  Male or Female</title>
    <summary>  We describe our first-place solution to the Animal Behavior Challenge (ABC
2018) on predicting gender of bird from its GPS trajectory. The task consisted
in predicting the gender of shearwater based on how they navigate themselves
across a big ocean. The trajectories are collected from GPS loggers attached on
shearwaters' body, and represented as a variable-length sequence of GPS points
(latitude and longitude), and associated meta-information, such as the sun
azimuth, the sun elevation, the daytime, the elapsed time on each GPS location
after starting the trip, the local time (date is trimmed), and the indicator of
the day starting the from the trip. We used ensemble of several variants of
Gradient Boosting Classifier along with Gaussian Process Classifier and Support
Vector Classifier after extensive feature engineering and we ranked first out
of 74 registered teams. The variants of Gradient Boosting Classifier we tried
are CatBoost (Developed by Yandex), LightGBM (Developed by Microsoft), XGBoost
(Developed by Distributed Machine Learning Community). Our approach could
easily be adapted to other applications in which the goal is to predict a
classification output from a variable-length sequence.
</summary>
    <author>
      <name>Dewan Fayzur</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08609v1</id>
    <updated>2018-08-26T18:36:20Z</updated>
    <published>2018-08-26T18:36:20Z</published>
    <title>Adversarially Regularising Neural NLI Models to Integrate Logical
  Background Knowledge</title>
    <summary>  Adversarial examples are inputs to machine learning models designed to cause
the model to make a mistake. They are useful for understanding the shortcomings
of machine learning models, interpreting their results, and for regularisation.
In NLP, however, most example generation strategies produce input text by using
known, pre-specified semantic transformations, requiring significant manual
effort and in-depth understanding of the problem and domain. In this paper, we
investigate the problem of automatically generating adversarial examples that
violate a set of given First-Order Logic constraints in Natural Language
Inference (NLI). We reduce the problem of identifying such adversarial examples
to a combinatorial optimisation problem, by maximising a quantity measuring the
degree of violation of such constraints and by using a language model for
generating linguistically-plausible examples. Furthermore, we propose a method
for adversarially regularising neural NLI models for incorporating background
knowledge. Our results show that, while the proposed method does not always
improve results on the SNLI and MultiNLI datasets, it significantly and
consistently increases the predictive accuracy on adversarially-crafted
datasets -- up to a 79.6% relative improvement -- while drastically reducing
the number of background knowledge violations. Furthermore, we show that
adversarial examples transfer among model architectures, and that the proposed
adversarial training procedure improves the robustness of NLI models to
adversarial examples.
</summary>
    <author>
      <name>Pasquale Minervini</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the SIGNLL Conference on Computational Natural Language
  Learning (CoNLL 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08680v3</id>
    <updated>2018-08-26T16:43:36Z</updated>
    <published>2018-03-23T07:57:04Z</published>
    <title>Improving DNN Robustness to Adversarial Attacks using Jacobian
  Regularization</title>
    <summary>  Deep neural networks have lately shown tremendous performance in various
applications including vision and speech processing tasks. However, alongside
their ability to perform these tasks with such high accuracy, it has been shown
that they are highly susceptible to adversarial attacks: a small change in the
input would cause the network to err with high confidence. This phenomenon
exposes an inherent fault in these networks and their ability to generalize
well. For this reason, providing robustness to adversarial attacks is an
important challenge in networks training, which has led to extensive research.
In this work, we suggest a theoretically inspired novel approach to improve the
networks' robustness. Our method applies regularization using the Frobenius
norm of the Jacobian of the network, which is applied as post-processing, after
regular training has finished. We demonstrate empirically that it leads to
enhanced robustness results with a minimal change in the original network's
accuracy.
</summary>
    <author>
      <name>Daniel Jakubovitz</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 Conference Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.08680v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08680v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06474v3</id>
    <updated>2018-08-26T16:28:20Z</updated>
    <published>2018-08-17T11:44:34Z</published>
    <title>A study on speech enhancement using exponent-only floating point
  quantized neural network (EOFP-QNN)</title>
    <summary>  Numerous studies have investigated the effectiveness of neural network
quantization on pattern classification tasks. The present study, for the first
time, investigated the performance of speech enhancement (a regression task in
speech processing) using a novel exponent-only floating-point quantized neural
network (EOFP-QNN). The proposed EOFP-QNN consists of two stages:
mantissa-quantization and exponent-quantization. In the mantissa-quantization
stage, EOFP-QNN learns how to quantize the mantissa bits of the model
parameters while preserving the regression accuracy using the least mantissa
precision. In the exponent-quantization stage, the exponent part of the
parameters is further quantized without causing any additional performance
degradation. We evaluated the proposed EOFP quantization technique on two types
of neural networks, namely, bidirectional long short-term memory (BLSTM) and
fully convolutional neural network (FCN), on a speech enhancement task.
Experimental results showed that the model sizes can be significantly reduced
(the model sizes of the quantized BLSTM and FCN models were only 18.75% and
21.89%, respectively, compared to those of the original models) while
maintaining satisfactory speech-enhancement performance.
</summary>
    <author>
      <name>Yi-Te Hsu</name>
    </author>
    <author>
      <name>Yu-Chen Lin</name>
    </author>
    <author>
      <name>Szu-Wei Fu</name>
    </author>
    <author>
      <name>Yu Tsao</name>
    </author>
    <author>
      <name>Tei-Wei Kuo</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06474v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06474v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08951v1</id>
    <updated>2018-08-26T16:01:11Z</updated>
    <published>2018-08-26T16:01:11Z</published>
    <title>Water Disaggregation via Shape Features based Bayesian Discriminative
  Sparse Coding</title>
    <summary>  As the issue of freshwater shortage is increasing daily, it is critical to
take effective measures for water conservation. According to previous studies,
device level consumption could lead to significant freshwater conservation.
Existing water disaggregation methods focus on learning the signatures for
appliances; however, they are lack of the mechanism to accurately discriminate
parallel appliances' consumption. In this paper, we propose a Bayesian
Discriminative Sparse Coding model using Laplace Prior (BDSC-LP) to extensively
enhance the disaggregation performance. To derive discriminative basis
functions, shape features are presented to describe the low-sampling-rate water
consumption patterns. A Gibbs sampling based inference method is designed to
extend the discriminative capability of the disaggregation dictionaries.
Extensive experiments were performed to validate the effectiveness of the
proposed model using both real-world and synthetic datasets.
</summary>
    <author>
      <name>Bingsheng Wang</name>
    </author>
    <author>
      <name>Xuchao Zhang</name>
    </author>
    <author>
      <name>Chang-Tien Lu</name>
    </author>
    <author>
      <name>Feng Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08558v1</id>
    <updated>2018-08-26T14:25:52Z</updated>
    <published>2018-08-26T14:25:52Z</published>
    <title>Spectral-Pruning: Compressing deep neural network via spectral analysis</title>
    <summary>  The model size of deep neural network is getting larger and larger to realize
superior performance in complicated tasks. This makes it difficult to implement
deep neural network in small edge-computing devices. To overcome this problem,
model compression methods have been gathering much attention. However, there
have been only few theoretical back-grounds that explain what kind of quantity
determines the compression ability. To resolve this issue, we develop a new
theoretical frame-work for model compression, and propose a new method called
{\it Spectral-Pruning} based on the theory. Our theoretical analysis is based
on the observation such that the eigenvalues of the covariance matrix of the
output from nodes in the internal layers often shows rapid decay. We define
"degree of freedom" to quantify an intrinsic dimensionality of the model by
using the eigenvalue distribution and show that the compression ability is
essentially controlled by this quantity. Along with this, we give a
generalization error bound of the compressed model. Our proposed method is
applicable to wide range of models, unlike the existing methods, e.g., ones
possess complicated branches as implemented in SegNet and ResNet. Our method
makes use of both "input" and "output" in each layer and is easy to implement.
We apply our method to several datasets to justify our theoretical analyses and
show that the proposed method achieves the state-of-the-art performance.
</summary>
    <author>
      <name>Taiji Suzuki</name>
    </author>
    <author>
      <name>Hiroshi Abe</name>
    </author>
    <author>
      <name>Tomoya Murata</name>
    </author>
    <author>
      <name>Shingo Horiuchi</name>
    </author>
    <author>
      <name>Kotaro Ito</name>
    </author>
    <author>
      <name>Tokuma Wachi</name>
    </author>
    <author>
      <name>So Hirai</name>
    </author>
    <author>
      <name>Masatoshi Yukishima</name>
    </author>
    <author>
      <name>Tomoaki Nishimura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08531v1</id>
    <updated>2018-08-26T11:09:44Z</updated>
    <published>2018-08-26T11:09:44Z</published>
    <title>DeepTracker: Visualizing the Training Process of Convolutional Neural
  Networks</title>
    <summary>  Deep convolutional neural networks (CNNs) have achieved remarkable success in
various fields. However, training an excellent CNN is practically a
trial-and-error process that consumes a tremendous amount of time and computer
resources. To accelerate the training process and reduce the number of trials,
experts need to understand what has occurred in the training process and why
the resulting CNN behaves as such. However, current popular training platforms,
such as TensorFlow, only provide very little and general information, such as
training/validation errors, which is far from enough to serve this purpose. To
bridge this gap and help domain experts with their training tasks in a
practical environment, we propose a visual analytics system, DeepTracker, to
facilitate the exploration of the rich dynamics of CNN training processes and
to identify the unusual patterns that are hidden behind the huge amount of
training log. Specifically,we combine a hierarchical index mechanism and a set
of hierarchical small multiples to help experts explore the entire training log
from different levels of detail. We also introduce a novel cube-style
visualization to reveal the complex correlations among multiple types of
heterogeneous training data including neuron weights, validation images, and
training iterations. Three case studies are conducted to demonstrate how
DeepTracker provides its users with valuable knowledge in an industry-level CNN
training process, namely in our case, training ResNet-50 on the ImageNet
dataset. We show that our method can be easily applied to other
state-of-the-art "very deep" CNN models.
</summary>
    <author>
      <name>Dongyu Liu</name>
    </author>
    <author>
      <name>Weiwei Cui</name>
    </author>
    <author>
      <name>Kai Jin</name>
    </author>
    <author>
      <name>Yuxiao Guo</name>
    </author>
    <author>
      <name>Huamin Qu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ACM Transactions on Intelligent Systems and Technology
  (in press)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00831v3</id>
    <updated>2018-08-26T10:42:39Z</updated>
    <published>2017-05-02T07:36:35Z</published>
    <title>BLENDER: Enabling Local Search with a Hybrid Differential Privacy Model</title>
    <summary>  We propose a hybrid model of differential privacy that considers a
combination of regular and opt-in users who desire the differential privacy
guarantees of the local privacy model and the trusted curator model,
respectively. We demonstrate that within this model, it is possible to design a
new type of blended algorithm for the task of privately computing the head of a
search log. This blended approach provides significant improvements in the
utility of obtained data compared to related work while providing users with
their desired privacy guarantees. Specifically, on two large search click data
sets, comprising 1.75 and 16 GB respectively, our approach attains NDCG values
exceeding 95% across a range of privacy budget values.
</summary>
    <author>
      <name>Brendan Avent</name>
    </author>
    <author>
      <name>Aleksandra Korolova</name>
    </author>
    <author>
      <name>David Zeber</name>
    </author>
    <author>
      <name>Torgeir Hovden</name>
    </author>
    <author>
      <name>Benjamin Livshits</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 26th USENIX Security Symposium (USENIX Security
  17). August 16-18, 2017, Vancouver, BC. ISBN 978-1-931971-40-9</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00831v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00831v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05092v2</id>
    <updated>2018-08-26T07:34:07Z</updated>
    <published>2018-08-13T23:31:01Z</published>
    <title>ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary
  classifier variational autoencoder</title>
    <summary>  This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
</summary>
    <author>
      <name>Hirokazu Kameoka</name>
    </author>
    <author>
      <name>Takuhiro Kaneko</name>
    </author>
    <author>
      <name>Kou Tanaka</name>
    </author>
    <author>
      <name>Nobukatsu Hojo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1806.02169.
  arXiv admin note: substantial text overlap with arXiv:1806.02169</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05092v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05092v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00892v3</id>
    <updated>2018-08-26T07:29:03Z</updated>
    <published>2018-08-02T16:30:51Z</published>
    <title>Semi-blind source separation with multichannel variational autoencoder</title>
    <summary>  This paper proposes a multichannel source separation technique called the
multichannel variational autoencoder (MVAE) method, which uses a conditional
VAE (CVAE) to model and estimate the power spectrograms of the sources in a
mixture. By training the CVAE using the spectrograms of training examples with
source-class labels, we can use the trained decoder distribution as a universal
generative model capable of generating spectrograms conditioned on a specified
class label. By treating the latent space variables and the class label as the
unknown parameters of this generative model, we can develop a
convergence-guaranteed semi-blind source separation algorithm that consists of
iteratively estimating the power spectrograms of the underlying sources as well
as the separation matrices. In experimental evaluations, our MVAE produced
better separation performance than a baseline method.
</summary>
    <author>
      <name>Hirokazu Kameoka</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Shota Inoue</name>
    </author>
    <author>
      <name>Shoji Makino</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00892v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00892v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05512v2</id>
    <updated>2018-08-26T01:33:02Z</updated>
    <published>2018-06-14T12:55:35Z</published>
    <title>NetScore: Towards Universal Metrics for Large-scale Performance Analysis
  of Deep Neural Networks for Practical On-Device Edge Usage</title>
    <summary>  Much of the focus in the design of deep neural networks has been on improving
accuracy, leading to more powerful yet highly complex network architectures
that are difficult to deploy in practical scenarios, particularly on edge
devices such as mobile and other consumer devices given their high
computational and memory requirements. As a result, there has been a recent
interest in the design of quantitative metrics for evaluating deep neural
networks that accounts for more than just model accuracy as the sole indicator
of network performance. In this study, we continue the conversation towards
universal metrics for evaluating the performance of deep neural networks for
practical on-device edge usage. In particular, we propose a new balanced metric
called NetScore, which is designed specifically to provide a quantitative
assessment of the balance between accuracy, computational complexity, and
network architecture complexity of a deep neural network, which is important
for on-device edge operation. In what is one of the largest comparative
analysis between deep neural networks in literature, the NetScore metric, the
top-1 accuracy metric, and the popular information density metric were compared
across a diverse set of 60 different deep convolutional neural networks for
image classification on the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC 2012) dataset. The evaluation results across these three metrics for
this diverse set of networks are presented in this study to act as a reference
guide for practitioners in the field. The proposed NetScore metric, along with
the other tested metrics, are by no means perfect, but the hope is to push the
conversation towards better universal metrics for evaluating deep neural
networks for use in practical on-device edge scenarios to help guide
practitioners in model design for such scenarios.
</summary>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05512v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05512v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08493v1</id>
    <updated>2018-08-26T01:17:50Z</updated>
    <published>2018-08-26T01:17:50Z</published>
    <title>Contextual Parameter Generation for Universal Neural Machine Translation</title>
    <summary>  We propose a simple modification to existing neural machine translation (NMT)
models that enables using a single universal model to translate between
multiple languages while allowing for language specific parameterization, and
that can also be used for domain adaptation. Our approach requires no changes
to the model architecture of a standard NMT system, but instead introduces a
new component, the contextual parameter generator (CPG), that generates the
parameters of the system (e.g., weights in a neural network). This parameter
generator accepts source and target language embeddings as input, and generates
the parameters for the encoder and the decoder, respectively. The rest of the
model remains unchanged and is shared across all languages. We show how this
simple modification enables the system to use monolingual data for training and
also perform zero-shot translation. We further show it is able to surpass
state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and
that the learned language embeddings are able to uncover interesting
relationships between languages.
</summary>
    <author>
      <name>Emmanouil Antonios Platanios</name>
    </author>
    <author>
      <name>Mrinmaya Sachan</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <author>
      <name>Tom Mitchell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the proceedings of Empirical Methods in Natural Language
  Processing (EMNLP), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08090v4</id>
    <updated>2018-08-26T00:13:38Z</updated>
    <published>2018-05-21T14:38:31Z</published>
    <title>Graph Capsule Convolutional Neural Networks</title>
    <summary>  Graph Convolutional Neural Networks (GCNNs) are the most recent exciting
advancement in deep learning field and their applications are quickly spreading
in multi-cross-domains including bioinformatics, chemoinformatics, social
networks, natural language processing and computer vision. In this paper, we
expose and tackle some of the basic weaknesses of a GCNN model with a capsule
idea presented in \cite{hinton2011transforming} and propose our Graph Capsule
Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve
especially graph classification problem which current GCNN models find
challenging. Through extensive experiments, we show that our proposed Graph
Capsule Network can significantly outperforms both the existing state-of-art
deep learning methods and graph kernels on graph classification benchmark
datasets.
</summary>
    <author>
      <name>Saurabh Verma</name>
    </author>
    <author>
      <name>Zhi-Li Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Joint ICML and IJCAI Workshop on Computational Biology,
  Stockholm, Sweden, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08090v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08090v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00957v1</id>
    <updated>2018-08-25T23:18:52Z</updated>
    <published>2018-08-25T23:18:52Z</published>
    <title>Road User Abnormal Trajectory Detection using a Deep Autoencoder</title>
    <summary>  In this paper, we focus on the development of a method that detects abnormal
trajectories of road users at traffic intersections. The main difficulty with
this is the fact that there are very few abnormal data and the normal ones are
insufficient for the training of any kinds of machine learning model. To tackle
these problems, we proposed the solution of using a deep autoencoder network
trained solely through augmented data considered as normal. By generating
artificial abnormal trajectories, our method is tested on four different
outdoor urban users scenes and performs better compared to some classical
outlier detection methods.
</summary>
    <author>
      <name>Pankaj Raj Roy</name>
    </author>
    <author>
      <name>Guillaume-Alexandre Bilodeau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted for oral presentation at ISVC'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08478v1</id>
    <updated>2018-08-25T22:42:46Z</updated>
    <published>2018-08-25T22:42:46Z</published>
    <title>Network Inference from Temporal-Dependent Grouped Observations</title>
    <summary>  In social network analysis, the observed data is usually some social
behavior, such as the formation of groups, rather than an explicit network
structure. Zhao and Weko (2017) propose a model-based approach called the hub
model to infer implicit networks from grouped observations. The hub model
assumes independence between groups, which sometimes is not valid in practice.
In this article, we generalize the idea of the hub model into the case of
grouped observations with temporal dependence. As in the hub model, we assume
that the group at each time point is gathered by one leader. Unlike in the hub
model, the group leaders are not sampled independently but follow a Markov
chain, and other members in adjacent groups can also be correlated.
  An expectation-maximization (EM) algorithm is developed for this model and a
polynomial-time algorithm is proposed for the E-step. The performance of the
new model is evaluated under different simulation settings. We apply this model
to a data set of the Kibale Chimpanzee Project.
</summary>
    <author>
      <name>Yunpeng Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08362v2</id>
    <updated>2018-08-25T22:25:00Z</updated>
    <published>2018-07-22T20:37:45Z</published>
    <title>An Intersectional Definition of Fairness</title>
    <summary>  We introduce a measure of fairness for algorithms and data with regard to
multiple protected attributes. Our proposed definition, differential fairness,
is informed by the framework of intersectionality, which analyzes how
interlocking systems of power and oppression affect individuals along
overlapping dimensions including race, gender, sexual orientation, class, and
disability. We show that our criterion behaves sensibly for any subset of the
set of protected attributes, and we illustrate links to differential privacy. A
case study on census data demonstrates the utility of our approach.
</summary>
    <author>
      <name>James Foulds</name>
    </author>
    <author>
      <name>Shimei Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08362v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08362v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00961v1</id>
    <updated>2018-08-25T22:00:10Z</updated>
    <published>2018-08-25T22:00:10Z</published>
    <title>MSCE: An edge preserving robust loss function for improving
  super-resolution algorithms</title>
    <summary>  With the recent advancement in the deep learning technologies such as CNNs
and GANs, there is significant improvement in the quality of the images
reconstructed by deep learning based super-resolution (SR) techniques. In this
work, we propose a robust loss function based on the preservation of edges
obtained by the Canny operator. This loss function, when combined with the
existing loss function such as mean square error (MSE), gives better SR
reconstruction measured in terms of PSNR and SSIM. Our proposed loss function
guarantees improved performance on any existing algorithm using MSE loss
function, without any increase in the computational complexity during testing.
</summary>
    <author>
      <name>Ram Krishna Pandey</name>
    </author>
    <author>
      <name>Nabagata Saha</name>
    </author>
    <author>
      <name>Samarjit Karmakar</name>
    </author>
    <author>
      <name>A G Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICONIP-2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08469v1</id>
    <updated>2018-08-25T21:02:46Z</updated>
    <published>2018-08-25T21:02:46Z</published>
    <title>DNN: A Two-Scale Distributional Tale of Heterogeneous Treatment Effect
  Inference</title>
    <summary>  Heterogeneous treatment effects are the center of gravity in many modern
causal inference applications. In this paper, we investigate the estimation and
inference of heterogeneous treatment effects with precision in a general
nonparametric setting. To this end, we enhance the classical $k$-nearest
neighbor method with a simple algorithm, extend it to a distributional setting,
and suggest the two-scale distributional nearest neighbors (DNN) estimator with
reduced finite-sample bias. Our recipe is first to subsample the data and
average the 1-nearest neighbor estimators from each subsample. With
appropriately chosen subsampling scale, the resulting DNN estimator is proved
to be asymptotically unbiased and normal under mild regularity conditions. We
then proceed with combining DNN estimators with different subsampling scales to
further reduce bias. Our theoretical results on the advantages of the new
two-scale DNN framework are well supported by several Monte Carlo simulations.
The newly suggested method is also applied to a real-life data set to study the
heterogeneity of treatment effects of smoking on children's birth weights
across mothers' ages.
</summary>
    <author>
      <name>Yingying Fan</name>
    </author>
    <author>
      <name>Jinchi Lv</name>
    </author>
    <author>
      <name>Jingbo Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10253v2</id>
    <updated>2018-08-25T20:23:27Z</updated>
    <published>2018-04-26T19:28:02Z</published>
    <title>From Principal Subspaces to Principal Components with Linear
  Autoencoders</title>
    <summary>  The autoencoder is an effective unsupervised learning model which is widely
used in deep learning. It is well known that an autoencoder with a single
fully-connected hidden layer, a linear activation function and a squared error
cost function trains weights that span the same subspace as the one spanned by
the principal component loading vectors, but that they are not identical to the
loading vectors. In this paper, we show how to recover the loading vectors from
the autoencoder weights.
</summary>
    <author>
      <name>Elad Plaut</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03968v2</id>
    <updated>2018-08-25T20:06:48Z</updated>
    <published>2018-01-11T19:56:43Z</published>
    <title>The Complexity of Learning Acyclic Conditional Preference Networks</title>
    <summary>  Learning of user preferences, as represented by, for example, Conditional
Preference Networks (CP-nets), has become a core issue in AI research. Recent
studies investigate learning of CP-nets from randomly chosen examples or from
membership and equivalence queries. To assess the optimality of learning
algorithms as well as to better understand the combinatorial structure of
classes of CP-nets, it is helpful to calculate certain learning-theoretic
information complexity parameters. This article focuses on the frequently
studied case of learning from so-called swap examples, which express
preferences among objects that differ in only one attribute. It presents bounds
on or exact values of some well-studied information complexity parameters,
namely the VC dimension, the teaching dimension, and the recursive teaching
dimension, for classes of acyclic CP-nets. We further provide algorithms that
learn tree-structured and general acyclic CP-nets from membership queries.
Using our results on complexity parameters, we assess the optimality of our
algorithms as well as that of another query learning algorithm for acyclic
CP-nets presented in the literature. Our algorithms are near-optimal, and can,
under certain assumptions, be adapted to the case when the membership oracle is
faulty.
</summary>
    <author>
      <name>Eisa Alanazi</name>
    </author>
    <author>
      <name>Malek Mouhoub</name>
    </author>
    <author>
      <name>Sandra Zilles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">64 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03968v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03968v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03432v3</id>
    <updated>2018-08-25T19:32:18Z</updated>
    <published>2018-06-09T07:42:01Z</published>
    <title>Hierarchical Clustering with Prior Knowledge</title>
    <summary>  Hierarchical clustering is a class of algorithms that seeks to build a
hierarchy of clusters. It has been the dominant approach to constructing
embedded classification schemes since it outputs dendrograms, which capture the
hierarchical relationship among members at all levels of granularity,
simultaneously. Being greedy in the algorithmic sense, a hierarchical
clustering partitions data at every step solely based on a similarity /
dissimilarity measure. The clustering results oftentimes depend on not only the
distribution of the underlying data, but also the choice of dissimilarity
measure and the clustering algorithm. In this paper, we propose a method to
incorporate prior domain knowledge about entity relationship into the
hierarchical clustering. Specifically, we use a distance function in
ultrametric space to encode the external ontological information. We show that
popular linkage-based algorithms can faithfully recover the encoded structure.
Similar to some regularized machine learning techniques, we add this distance
as a penalty term to the original pairwise distance to regulate the final
structure of the dendrogram. As a case study, we applied this method on real
data in the building of a customer behavior based product taxonomy for an
Amazon service, leveraging the information from a larger Amazon-wide browse
structure. The method is useful when one wants to leverage the relational
information from external sources, or the data used to generate the distance
matrix is noisy and sparse. Our work falls in the category of semi-supervised
or constrained clustering.
</summary>
    <author>
      <name>Xiaofei Ma</name>
    </author>
    <author>
      <name>Satya Dhavala</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03432v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03432v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08460v1</id>
    <updated>2018-08-25T18:31:52Z</updated>
    <published>2018-08-25T18:31:52Z</published>
    <title>The Social Cost of Strategic Classification</title>
    <summary>  Consequential decision-making typically incentivizes individuals to behave
strategically, tailoring their behavior to the specifics of the decision rule.
A long line of work has therefore sought to counteract strategic behavior by
designing more conservative decision boundaries in an effort to increase
robustness to the effects of strategic covariate shift.
  We show that these efforts benefit the institutional decision maker at the
expense of the individuals being classified. Introducing a notion of social
burden, we prove that any increase in institutional utility necessarily leads
to a corresponding increase in social burden. Moreover, we show that the
negative externalities of strategic classification can disproportionately harm
disadvantaged groups in the population.
  Our results highlight that strategy-robustness must be weighed against
considerations of social welfare and fairness.
</summary>
    <author>
      <name>Smitha Milli</name>
    </author>
    <author>
      <name>John Miller</name>
    </author>
    <author>
      <name>Anca D. Dragan</name>
    </author>
    <author>
      <name>Moritz Hardt</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00459v2</id>
    <updated>2018-08-25T18:29:32Z</updated>
    <published>2017-10-02T02:17:09Z</published>
    <title>Deep Abstract Q-Networks</title>
    <summary>  We examine the problem of learning and planning on high-dimensional domains
with long horizons and sparse rewards. Recent approaches have shown great
successes in many Atari 2600 domains. However, domains with long horizons and
sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for
existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup,
and Singh 1999) have shown to be useful in tackling long-horizon problems. We
combine recent techniques of deep reinforcement learning with existing
model-based approaches using an expert-provided state abstraction. We construct
toy domains that elucidate the problem of long horizons, sparse rewards and
high-dimensional inputs, and show that our algorithm significantly outperforms
previous methods on these domains. Our abstraction-based approach outperforms
Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and
exhibits backtracking behavior that is absent from previous methods.
</summary>
    <author>
      <name>Melrose Roderick</name>
    </author>
    <author>
      <name>Christopher Grimm</name>
    </author>
    <author>
      <name>Stefanie Tellex</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00459v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00459v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08437v1</id>
    <updated>2018-08-25T15:10:59Z</updated>
    <published>2018-08-25T15:10:59Z</published>
    <title>Meta-Learning for Low-Resource Neural Machine Translation</title>
    <summary>  In this paper, we propose to extend the recently introduced model-agnostic
meta-learning algorithm (MAML) for low-resource neural machine translation
(NMT). We frame low-resource translation as a meta-learning problem, and we
learn to adapt to low-resource languages based on multilingual high-resource
language tasks. We use the universal lexical
representation~\citep{gu2018universal} to overcome the input-output mismatch
across different languages. We evaluate the proposed meta-learning strategy
using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt,
Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,
Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach
significantly outperforms the multilingual, transfer learning based
approach~\citep{zoph2016transfer} and enables us to train a competitive NMT
system with only a fraction of training examples. For instance, the proposed
approach can achieve as high as 22.04 BLEU on Romanian-English WMT'16 by seeing
only 16,000 translated words (~600 parallel sentences).
</summary>
    <author>
      <name>Jiatao Gu</name>
    </author>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Yun Chen</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a full paper at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08416v1</id>
    <updated>2018-08-25T12:06:17Z</updated>
    <published>2018-08-25T12:06:17Z</published>
    <title>Multiplayer bandits without observing collision information</title>
    <summary>  We study multiplayer stochastic multi-armed bandit problems in which the
players cannot communicate, and if two or more players pull the same arm, a
collision occurs and the involved players receive zero reward. We consider two
feedback models: a model in which the players can observe whether a collision
has occurred, and a more difficult setup when no collision information is
available. We give the first theoretical guarantees for the second model: an
algorithm with a logarithmic regret, and an algorithm with a square-root regret
type that does not depend on the gaps between the means. For the first model,
we give the first square-root regret bounds that do not depend on the gaps.
Building on these ideas, we also give an algorithm for reaching approximate
Nash equilibria quickly in stochastic anti-coordination games.
</summary>
    <author>
      <name>Gabor Lugosi</name>
    </author>
    <author>
      <name>Abbas Mehrabian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08414v1</id>
    <updated>2018-08-25T12:02:41Z</updated>
    <published>2018-08-25T12:02:41Z</published>
    <title>Unsupervised Hypergraph Feature Selection via a Novel Point-Weighting
  Framework and Low-Rank Representation</title>
    <summary>  Feature selection methods are widely used in order to solve the 'curse of
dimensionality' problem. Many proposed feature selection frameworks, treat all
data points equally; neglecting their different representation power and
importance. In this paper, we propose an unsupervised hypergraph feature
selection method via a novel point-weighting framework and low-rank
representation that captures the importance of different data points. We
introduce a novel soft hypergraph with low complexity to model data. Then, we
formulate the feature selection as an optimization problem to preserve local
relationships and also global structure of data. Our approach for global
structure preservation helps the framework overcome the problem of
unavailability of data labels in unsupervised learning. The proposed feature
selection method treats with different data points based on their importance in
defining data structure and representation power. Moreover, since the
robustness of feature selection methods against noise and outlier is of great
importance, we adopt low-rank representation in our model. Also, we provide an
efficient algorithm to solve the proposed optimization problem. The
computational cost of the proposed algorithm is lower than many
state-of-the-art methods which is of high importance in feature selection
tasks. We conducted comprehensive experiments with various evaluation methods
on different benchmark data sets. These experiments indicate significant
improvement, compared with state-of-the-art feature selection methods.
</summary>
    <author>
      <name>Ammar Gilani</name>
    </author>
    <author>
      <name>Maryam Amirmazlaghani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08366v1</id>
    <updated>2018-08-25T07:07:42Z</updated>
    <published>2018-08-25T07:07:42Z</published>
    <title>Relaxing the Identically Distributed Assumption in Gaussian
  Co-Clustering for High Dimensional Data</title>
    <summary>  A co-clustering model for continuous data that relaxes the identically
distributed assumption within blocks of traditional co-clustering is presented.
The proposed model, although allowing more flexibility, still maintains the
very high degree of parsimony achieved by traditional co-clustering. A
stochastic EM algorithm along with a Gibbs sampler is used for parameter
estimation and an ICL criterion is used for model selection. Simulated and real
datasets are used for illustration and comparison with traditional
co-clustering.
</summary>
    <author>
      <name>M. P. B. Gallaugher</name>
    </author>
    <author>
      <name>C. Biernacki</name>
    </author>
    <author>
      <name>P. D. McNicholas</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08361v1</id>
    <updated>2018-08-25T06:00:44Z</updated>
    <published>2018-08-25T06:00:44Z</published>
    <title>Data-dependent Learning of Symmetric/Antisymmetric Relations for
  Knowledge Base Completion</title>
    <summary>  Embedding-based methods for knowledge base completion (KBC) learn
representations of entities and relations in a vector space, along with the
scoring function to estimate the likelihood of relations between entities. The
learnable class of scoring functions is designed to be expressive enough to
cover a variety of real-world relations, but this expressive comes at the cost
of an increased number of parameters. In particular, parameters in these
methods are superfluous for relations that are either symmetric or
antisymmetric. To mitigate this problem, we propose a new L1 regularizer for
Complex Embeddings, which is one of the state-of-the-art embedding-based
methods for KBC. This regularizer promotes symmetry or antisymmetry of the
scoring function on a relation-by-relation basis, in accordance with the
observed data. Our empirical evaluation shows that the proposed method
outperforms the original Complex Embeddings and other baseline methods on the
FB15k dataset.
</summary>
    <author>
      <name>Hitoshi Manabe</name>
    </author>
    <author>
      <name>Katsuhiko Hayashi</name>
    </author>
    <author>
      <name>Masashi Shimbo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03467v4</id>
    <updated>2018-08-25T05:52:08Z</updated>
    <published>2018-03-09T11:12:01Z</published>
    <title>RippleNet: Propagating User Preferences on the Knowledge Graph for
  Recommender Systems</title>
    <summary>  To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple "ripples"
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.
</summary>
    <author>
      <name>Hongwei Wang</name>
    </author>
    <author>
      <name>Fuzheng Zhang</name>
    </author>
    <author>
      <name>Jialin Wang</name>
    </author>
    <author>
      <name>Miao Zhao</name>
    </author>
    <author>
      <name>Wenjie Li</name>
    </author>
    <author>
      <name>Xing Xie</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271739" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03467v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03467v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09184v3</id>
    <updated>2018-08-25T05:22:45Z</updated>
    <published>2018-02-26T07:01:24Z</published>
    <title>Variance Reduction Methods for Sublinear Reinforcement Learning</title>
    <summary>  This work considers the problem of provably optimal reinforcement learning
for episodic finite horizon MDPs, i.e. how an agent learns to maximize his/her
long term reward in an uncertain environment. The main contribution is in
providing a novel algorithm --- Variance-reduced Upper Confidence Q-learning
(vUCQ) --- which enjoys a regret bound of $\widetilde{O}(\sqrt{HSAT} + H^5SA)$,
where the $T$ is the number of time steps the agent acts in the MDP, $S$ is the
number of states, $A$ is the number of actions, and $H$ is the (episodic)
horizon time.
  This is the first regret bound that is both sub-linear in the model size and
asymptotically optimal. The algorithm is sub-linear in that the time to achieve
$\epsilon$-average regret for any constant $\epsilon$ is $O(SA)$, which is a
number of samples that is far less than that required to learn any non-trivial
estimate of the transition model (the transition model is specified by
$O(S^2A)$ parameters). The importance of sub-linear algorithms is largely the
motivation for algorithms such as $Q$-learning and other "model free"
approaches. vUCQ algorithm also enjoys minimax optimal regret in the long run,
matching the $\Omega(\sqrt{HSAT})$ lower bound.
  Variance-reduced Upper Confidence Q-learning (vUCQ) is a successive
refinement method in which the algorithm reduces the variance in $Q$-value
estimates and couples this estimation scheme with an upper confidence based
algorithm. Technically, the coupling of both of these techniques is what leads
to the algorithm enjoying both the sub-linear regret property and the
asymptotically optimal regret.
</summary>
    <author>
      <name>Sham Kakade</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <author>
      <name>Lin F. Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed a bug of a previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09184v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09184v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08344v1</id>
    <updated>2018-08-25T01:48:05Z</updated>
    <published>2018-08-25T01:48:05Z</published>
    <title>Multiobjective Optimization Training of PLDA for Speaker Verification</title>
    <summary>  Most current state-of-the-art text-independent speaker verification systems
take probabilistic linear discriminant analysis (PLDA) as their backend
classifiers. The model parameters of PLDA is often estimated by maximizing the
log-likelihood function. This training procedure focuses on increasing the
log-likelihood, while ignoring the distinction between speakers. In order to
better distinguish speakers, we propose a multiobjective optimization training
for PLDA. Experiment results show that the proposed method has more than 10%
relative performance improvement for both EER and the MinDCF on the NIST SRE
2014 i-vector challenge dataset.
</summary>
    <author>
      <name>Liang He</name>
    </author>
    <author>
      <name>Xianhong Chen</name>
    </author>
    <author>
      <name>Can Xu</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02957v2</id>
    <updated>2018-08-24T23:59:31Z</updated>
    <published>2018-06-08T03:24:50Z</published>
    <title>A Deep Neural Network Surrogate for High-Dimensional Random Partial
  Differential Equations</title>
    <summary>  Developing efficient numerical algorithms for the solution of high
dimensional random Partial Differential Equations (PDEs) has been a challenging
task due to the well-known curse of dimensionality. We present a new solution
framework for these problems based on a deep learning approach. Specifically,
the random PDE is approximated by a feed-forward fully-connected deep residual
network, with either strong or weak enforcement of initial and boundary
constraints. The framework is mesh-free, and can handle irregular computational
domains. Parameters of the approximating deep neural network are determined
iteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.
The satisfactory accuracy of the proposed frameworks is numerically
demonstrated on diffusion and heat conduction problems, in comparison with the
converged Monte Carlo-based finite element results.
</summary>
    <author>
      <name>Mohammad Amin Nabian</name>
    </author>
    <author>
      <name>Hadi Meidani</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02957v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02957v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08317v1</id>
    <updated>2018-08-24T21:36:05Z</updated>
    <published>2018-08-24T21:36:05Z</published>
    <title>To Cluster, or Not to Cluster: An Analysis of Clusterability Methods</title>
    <summary>  Clustering is an essential data mining tool that aims to discover inherent
cluster structure in data. For most applications, applying clustering is only
appropriate when cluster structure is present. As such, the study of
clusterability, which evaluates whether data possesses such structure, is an
integral part of cluster analysis. However, methods for evaluating
clusterability vary radically, making it challenging to select a suitable
measure. In this paper, we perform an extensive comparison of measures of
clusterability and provide guidelines that clustering users can reference to
select suitable measures for their applications.
</summary>
    <author>
      <name>A. Adolfsson</name>
    </author>
    <author>
      <name>M. Ackerman</name>
    </author>
    <author>
      <name>N. C. Brownstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 3 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08315v1</id>
    <updated>2018-08-24T21:28:36Z</updated>
    <published>2018-08-24T21:28:36Z</published>
    <title>A Deterministic Self-Organizing Map Approach and its Application on
  Satellite Data based Cloud Type Classification</title>
    <summary>  A self-organizing map (SOM) is a type of competitive artificial neural
network, which projects the high-dimensional input space of the training
samples into a low-dimensional space with the topology relations preserved.
This makes SOMs supportive of organizing and visualizing complex data sets and
have been pervasively used among numerous disciplines with different
applications. Notwithstanding its wide applications, the self-organizing map is
perplexed by its inherent randomness, which produces dissimilar SOM patterns
even when being trained on identical training samples with the same parameters
every time, and thus causes usability concerns for other domain practitioners
and precludes more potential users from exploring SOM based applications in a
broader spectrum. Motivated by this practical concern, we propose a
deterministic approach as a supplement to the standard self-organizing map. In
accordance with the theoretical design, the experimental results with satellite
cloud data demonstrate the effective and efficient organization as well as
simplification capabilities of the proposed approach.
</summary>
    <author>
      <name>Wenbin Zhang</name>
    </author>
    <author>
      <name>Jianwu Wang</name>
    </author>
    <author>
      <name>Daeho Jin</name>
    </author>
    <author>
      <name>Lazaros Oreopoulos</name>
    </author>
    <author>
      <name>Zhibo Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08311v1</id>
    <updated>2018-08-24T21:14:40Z</updated>
    <published>2018-08-24T21:14:40Z</published>
    <title>Voice Conversion with Conditional SampleRNN</title>
    <summary>  Here we present a novel approach to conditioning the SampleRNN generative
model for voice conversion (VC). Conventional methods for VC modify the
perceived speaker identity by converting between source and target acoustic
features. Our approach focuses on preserving voice content and depends on the
generative network to learn voice style. We first train a multi-speaker
SampleRNN model conditioned on linguistic features, pitch contour, and speaker
identity using a multi-speaker speech corpus. Voice-converted speech is
generated using linguistic features and pitch contour extracted from the source
speaker, and the target speaker identity. We demonstrate that our system is
capable of many-to-many voice conversion without requiring parallel data,
enabling broad applications. Subjective evaluation demonstrates that our
approach outperforms conventional VC methods.
</summary>
    <author>
      <name>Cong Zhou</name>
    </author>
    <author>
      <name>Michael Horgan</name>
    </author>
    <author>
      <name>Vivek Kumar</name>
    </author>
    <author>
      <name>Cristina Vasco</name>
    </author>
    <author>
      <name>Dan Darcy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Interspeech 2018, Hyderabad, India. This version matches
  the final version submitted to the conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08294v1</id>
    <updated>2018-08-24T20:13:49Z</updated>
    <published>2018-08-24T20:13:49Z</published>
    <title>Unknown Examples &amp; Machine Learning Model Generalization</title>
    <summary>  Over the past decades, researchers and ML practitioners have come up with
better and better ways to build, understand and improve the quality of ML
models, but mostly under the key assumption that the training data is
distributed identically to the testing data. In many real-world applications,
however, some potential training examples are unknown to the modeler, due to
sample selection bias or, more generally, covariate shift, i.e., a distribution
shift between the training and deployment stage. The resulting discrepancy
between training and testing distributions leads to poor generalization
performance of the ML model and hence biased predictions. We provide novel
algorithms that estimate the number and properties of these unknown training
examples---unknown unknowns. This information can then be used to correct the
training set, prior to seeing any test data. The key idea is to combine
species-estimation techniques with data-driven methods for estimating the
feature values for the unknown unknowns. Experiments on a variety of ML models
and datasets indicate that taking the unknown examples into account can yield a
more robust ML model that generalizes better.
</summary>
    <author>
      <name>Yeounoh Chung</name>
    </author>
    <author>
      <name>Peter J. Haas</name>
    </author>
    <author>
      <name>Eli Upfal</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08268v1</id>
    <updated>2018-08-24T19:07:10Z</updated>
    <published>2018-08-24T19:07:10Z</published>
    <title>Learning Models for Shared Control of Human-Machine Systems with Unknown
  Dynamics</title>
    <summary>  We present a novel approach to shared control of human-machine systems. Our
method assumes no a priori knowledge of the system dynamics. Instead, we learn
both the dynamics and information about the user's interaction from observation
through the use of the Koopman operator. Using the learned model, we define an
optimization problem to compute the optimal policy for a given task, and
compare the user input to the optimal input. We demonstrate the efficacy of our
approach with a user study. We also analyze the individual nature of the
learned models by comparing the effectiveness of our approach when the
demonstration data comes from a user's own interactions, from the interactions
of a group of users and from a domain expert. Positive results include
statistically significant improvements on task metrics when comparing a
user-only control paradigm with our shared control paradigm. Surprising results
include findings that suggest that individualizing the model based on a user's
own data does not effect the ability to learn a useful dynamic system. We
explore this tension as it relates to developing human-in-the-loop systems
further in the discussion.
</summary>
    <author>
      <name>Alexander Broad</name>
    </author>
    <author>
      <name>Todd Murphey</name>
    </author>
    <author>
      <name>Brenna Argall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Robotics: Science and Systems Proceedings, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05559v2</id>
    <updated>2018-08-24T18:16:03Z</updated>
    <published>2018-06-13T13:57:13Z</published>
    <title>Extracting Parallel Sentences with Bidirectional Recurrent Neural
  Networks to Improve Machine Translation</title>
    <summary>  Parallel sentence extraction is a task addressing the data sparsity problem
found in multilingual natural language processing applications. We propose a
bidirectional recurrent neural network based approach to extract parallel
sentences from collections of multilingual texts. Our experiments with noisy
parallel corpora show that we can achieve promising results against a
competitive baseline by removing the need of specific feature engineering or
additional external resources. To justify the utility of our approach, we
extract sentence pairs from Wikipedia articles to train machine translation
systems and show significant improvements in translation performance.
</summary>
    <author>
      <name>Francis Grégoire</name>
    </author>
    <author>
      <name>Philippe Langlais</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, COLING 2018. arXiv admin note: text overlap with
  arXiv:1709.09783</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05559v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05559v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09259v2</id>
    <updated>2018-08-24T18:00:21Z</updated>
    <published>2018-07-24T17:54:51Z</published>
    <title>Learning to Generate and Reconstruct 3D Meshes with only 2D Supervision</title>
    <summary>  We present a unified framework tackling two problems: class-specific 3D
reconstruction from a single image, and generation of new 3D shape samples.
These tasks have received considerable attention recently; however, existing
approaches rely on 3D supervision, annotation of 2D images with keypoints or
poses, and/or training with multiple views of each object instance. Our
framework is very general: it can be trained in similar settings to these
existing approaches, while also supporting weaker supervision scenarios.
Importantly, it can be trained purely from 2D images, without ground-truth pose
annotations, and with a single view per instance. We employ meshes as an output
representation, instead of voxels used in most prior work. This allows us to
exploit shading information during training, which previous 2D-supervised
methods cannot. Thus, our method can learn to generate and reconstruct concave
object classes. We evaluate our approach on synthetic data in various settings,
showing that (i) it learns to disentangle shape from pose; (ii) using shading
in the loss improves performance; (iii) our model is comparable or superior to
state-of-the-art voxel-based approaches on quantitative metrics, while
producing results that are visually more pleasing; (iv) it still performs well
when given supervision weaker than in prior works.
</summary>
    <author>
      <name>Paul Henderson</name>
    </author>
    <author>
      <name>Vittorio Ferrari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2018 (Oral). Differentiable renderer available at
  https://github.com/pmh47/dirt</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09259v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09259v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09064v3</id>
    <updated>2018-08-24T17:45:32Z</updated>
    <published>2018-02-25T19:06:06Z</published>
    <title>Time Series Analysis via Matrix Estimation</title>
    <summary>  We propose an algorithm to impute and forecast a time series by transforming
the observed time series into a matrix, utilizing matrix estimation to recover
missing values and de-noise observed entries, and performing linear regression
to make predictions. At the core of our analysis is a representation result,
which states that for a large model class, the transformed matrix obtained from
the time series via our algorithm is (approximately) low-rank. This, in effect,
generalizes the widely used Singular Spectrum Analysis (SSA) in literature, and
allows us to establish a rigorous link between time series analysis and matrix
estimation. The key is to construct a matrix with non-overlapping entries
rather than with the Hankel matrix as done in the literature, including in SSA.
We provide finite sample analysis for imputation and prediction leading to the
asymptotic consistency of our method. A salient feature of our algorithm is
that it is model agnostic both with respect to the underlying time dynamics as
well as the noise model in the observations. Being noise agnostic makes our
algorithm applicable to the setting where the state is hidden and we only have
access to its noisy observations a la a Hidden Markov Model, e.g., observing a
Poisson process with a time-varying parameter without knowing that the process
is Poisson, but still recovering the time-varying parameter accurately. As part
of the forecasting algorithm, an important task is to perform regression with
noisy observations of the features a la an error- in-variable regression. In
essence, our approach suggests a matrix estimation based method for such a
setting, which could be of interest in its own right. Through synthetic and
real-world datasets, we demonstrate that our algorithm outperforms standard
software packages (including R libraries) in the presence of missing data as
well as high levels of noise.
</summary>
    <author>
      <name>Anish Agarwal</name>
    </author>
    <author>
      <name>Muhammad Jehangir Amjad</name>
    </author>
    <author>
      <name>Devavrat Shah</name>
    </author>
    <author>
      <name>Dennis Shen</name>
    </author>
    <link href="http://arxiv.org/abs/1802.09064v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09064v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03963v3</id>
    <updated>2018-08-24T17:09:49Z</updated>
    <published>2018-05-10T13:24:34Z</published>
    <title>Monotone Learning with Rectified Wire Networks</title>
    <summary>  We introduce a new neural network model, together with a tractable and
monotone online learning algorithm. Our model describes feed-forward networks
for classification, with one output node for each class. The only nonlinear
operation is rectification using a ReLU function with a bias. However, there is
a rectifier on every edge rather than at the nodes of the network. There are
also weights, but these are positive, static, and associated with the nodes.
Our "rectified wire" networks are able to represent arbitrary Boolean
functions. Only the bias parameters, on the edges of the network, are learned.
Another departure in our approach, from standard neural networks, is that the
loss function is replaced by a constraint. This constraint is simply that the
value of the output node associated with the correct class should be zero. Our
model has the property that the exact norm-minimizing parameter update,
required to correctly classify a training item, is the solution to a quadratic
program that can be computed with a few passes through the network. We
demonstrate a training algorithm using this update, called sequential
deactivation (SDA), on MNIST and some synthetic datasets. Upon adopting a
natural choice for the nodal weights, SDA has no hyperparameters other than
those describing the network structure. Our experiments explore behavior with
respect to network size and depth in a family of sparse expander networks.
</summary>
    <author>
      <name>Veit Elser</name>
    </author>
    <author>
      <name>Dan Schmidt</name>
    </author>
    <author>
      <name>Jonathan Yedidia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 21 figures, new experimental results, various improvements</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03963v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03963v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08266v2</id>
    <updated>2018-08-24T16:27:28Z</updated>
    <published>2016-08-29T22:10:17Z</published>
    <title>Visualizing and Understanding Sum-Product Networks</title>
    <summary>  Sum-Product Networks (SPNs) are recently introduced deep tractable
probabilistic models by which several kinds of inference queries can be
answered exactly and in a tractable time. Up to now, they have been largely
used as black box density estimators, assessed only by comparing their
likelihood scores only. In this paper we explore and exploit the inner
representations learned by SPNs. We do this with a threefold aim: first we want
to get a better understanding of the inner workings of SPNs; secondly, we seek
additional ways to evaluate one SPN model and compare it against other
probabilistic models, providing diagnostic tools to practitioners; lastly, we
want to empirically evaluate how good and meaningful the extracted
representations are, as in a classic Representation Learning framework. In
order to do so we revise their interpretation as deep neural networks and we
propose to exploit several visualization techniques on their node activations
and network outputs under different types of inference queries. To investigate
these models as feature extractors, we plug some SPNs, learned in a greedy
unsupervised fashion on image datasets, in supervised classification learning
tasks. We extract several embedding types from node activations by filtering
nodes by their type, by their associated feature abstraction level and by their
scope. In a thorough empirical comparison we prove them to be competitive
against those generated from popular feature extractors as Restricted Boltzmann
Machines. Finally, we investigate embeddings generated from random
probabilistic marginal queries as means to compare other tractable
probabilistic models on a common ground, extending our experiments to Mixtures
of Trees.
</summary>
    <author>
      <name>Antonio Vergari</name>
    </author>
    <author>
      <name>Nicola Di Mauro</name>
    </author>
    <author>
      <name>Floriana Esposito</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10994-018-5760-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10994-018-5760-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning Journal paper (First Online), 24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08266v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08266v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08195v1</id>
    <updated>2018-08-24T16:12:21Z</updated>
    <published>2018-08-24T16:12:21Z</published>
    <title>GoT-WAVE: Temporal network alignment using graphlet-orbit transitions</title>
    <summary>  Global pairwise network alignment (GPNA) aims to find a one-to-one node
mapping between two networks that identifies conserved network regions. GPNA
algorithms optimize node conservation (NC) and edge conservation (EC). NC
quantifies topological similarity between nodes. Graphlet-based degree vectors
(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were
used as a dynamic NC measure within the first-ever algorithms for GPNA of
temporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger
networks. We recently developed a different graphlet-based measure of temporal
node similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead
of DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new
approach, GoT-WAVE.
  On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed
by 64%. On real networks, when optimizing only dynamic NC, each method is
superior ~50% of the time. While DynaWAVE benefits more from also optimizing
dynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a
promising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.
Future work on better incorporating dynamic EC may yield further improvements.
</summary>
    <author>
      <name>David Aparício</name>
    </author>
    <author>
      <name>Pedro Ribeiro</name>
    </author>
    <author>
      <name>Tijana Milenković</name>
    </author>
    <author>
      <name>Fernando Silva</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02162v3</id>
    <updated>2018-08-24T15:12:46Z</updated>
    <published>2017-12-06T12:44:27Z</published>
    <title>A trans-disciplinary review of deep learning research for water
  resources scientists</title>
    <summary>  Deep learning (DL), a new-generation of artificial neural network research,
has transformed industries, daily lives and various scientific disciplines in
recent years. DL represents significant progress in the ability of neural
networks to automatically engineer problem-relevant features and capture highly
complex data distributions. I argue that DL can help address several major new
and old challenges facing research in water sciences such as
inter-disciplinarity, data discoverability, hydrologic scaling, equifinality,
and needs for parameter regionalization. This review paper is intended to
provide water resources scientists and hydrologists in particular with a simple
technical overview, trans-disciplinary progress update, and a source of
inspiration about the relevance of DL to water. The review reveals that various
physical and geoscientific disciplines have utilized DL to address data
challenges, improve efficiency, and gain scientific insights. DL is especially
suited for information extraction from image-like data and sequential data.
Techniques and experiences presented in other disciplines are of high relevance
to water research. Meanwhile, less noticed is that DL may also serve as a
scientific exploratory tool. A new area termed 'AI neuroscience,' where
scientists interpret the decision process of deep networks and derive insights,
has been born. This budding sub-discipline has demonstrated methods including
correlation-based analysis, inversion of network-extracted features,
reduced-order approximations by interpretable models, and attribution of
network decisions to inputs. Moreover, DL can also use data to condition
neurons that mimic problem-specific fundamental organizing units, thus
revealing emergent behaviors of these units. Vast opportunities exist for DL to
propel advances in water sciences.
</summary>
    <author>
      <name>Chaopeng Shen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1029/2018WR022643</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1029/2018WR022643" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Water Resources Research, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.02162v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02162v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08166v1</id>
    <updated>2018-08-24T15:08:33Z</updated>
    <published>2018-08-24T15:08:33Z</published>
    <title>An Empirical Study of Rich Subgroup Fairness for Machine Learning</title>
    <summary>  Kearns et al. [2018] recently proposed a notion of rich subgroup fairness
intended to bridge the gap between statistical and individual notions of
fairness. Rich subgroup fairness picks a statistical fairness constraint (say,
equalizing false positive rates across protected groups), but then asks that
this constraint hold over an exponentially or infinitely large collection of
subgroups defined by a class of functions with bounded VC dimension. They give
an algorithm guaranteed to learn subject to this constraint, under the
condition that it has access to oracles for perfectly learning absent a
fairness constraint. In this paper, we undertake an extensive empirical
evaluation of the algorithm of Kearns et al. On four real datasets for which
fairness is a concern, we investigate the basic convergence of the algorithm
when instantiated with fast heuristics in place of learning oracles, measure
the tradeoffs between fairness and accuracy, and compare this approach with the
recent algorithm of Agarwal et al. [2018], which implements weaker and more
traditional marginal fairness constraints defined by individual protected
attributes. We find that in general, the Kearns et al. algorithm converges
quickly, large gains in fairness can be obtained with mild costs to accuracy,
and that optimizing accuracy subject only to marginal fairness leads to
classifiers with substantial subgroup unfairness. We also provide a number of
analyses and visualizations of the dynamics and behavior of the Kearns et al.
algorithm. Overall we find this algorithm to be effective on real data, and
rich subgroup fairness to be a viable notion in practice.
</summary>
    <author>
      <name>Michael Kearns</name>
    </author>
    <author>
      <name>Seth Neel</name>
    </author>
    <author>
      <name>Aaron Roth</name>
    </author>
    <author>
      <name>Zhiwei Steven Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08230v1</id>
    <updated>2018-08-24T13:57:40Z</updated>
    <published>2018-08-24T13:57:40Z</published>
    <title>Using Apple Machine Learning Algorithms to Detect and Subclassify
  Non-Small Cell Lung Cancer</title>
    <summary>  Lung cancer continues to be a major healthcare challenge with high morbidity
and mortality rates among both men and women worldwide. The majority of lung
cancer cases are of non-small cell lung cancer type. With the advent of
targeted cancer therapy, it is imperative not only to properly diagnose but
also sub-classify non-small cell lung cancer. In our study, we evaluated the
utility of using Apple Create ML module to detect and sub-classify non-small
cell carcinomas based on histopathological images. After module optimization,
the program detected 100% of non-small cell lung cancer images and successfully
subclassified the majority of the images. Trained modules, such as ours, can be
utilized in diagnostic smartphone-based applications, augmenting diagnostic
services in understaffed areas of the world.
</summary>
    <author>
      <name>Andrew A. Borkowski MD</name>
    </author>
    <author>
      <name>Catherine P. Wilson MT</name>
    </author>
    <author>
      <name>Steven A. Borkowski</name>
    </author>
    <author>
      <name>Lauren A. Deland RN</name>
    </author>
    <author>
      <name>Stephen M. Mastorides MD</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08097v1</id>
    <updated>2018-08-24T11:36:15Z</updated>
    <published>2018-08-24T11:36:15Z</published>
    <title>Memory Time Span in LSTMs for Multi-Speaker Source Separation</title>
    <summary>  With deep learning approaches becoming state-of-the-art in many speech (as
well as non-speech) related machine learning tasks, efforts are being taken to
delve into the neural networks which are often considered as a black box. In
this paper it is analyzed how recurrent neural network (RNNs) cope with
temporal dependencies by determining the relevant memory time span in a long
short-term memory (LSTM) cell. This is done by leaking the state variable with
a controlled lifetime and evaluating the task performance. This technique can
be used for any task to estimate the time span the LSTM exploits in that
specific scenario. The focus in this paper is on the task of separating
speakers from overlapping speech. We discern two effects: A long term effect,
probably due to speaker characterization and a short term effect, probably
exploiting phone-size formant tracks.
</summary>
    <author>
      <name>Jeroen Zegers</name>
    </author>
    <author>
      <name>Hugo Van hamme</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.08097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08095v1</id>
    <updated>2018-08-24T11:29:07Z</updated>
    <published>2018-08-24T11:29:07Z</published>
    <title>Multi-scenario deep learning for multi-speaker source separation</title>
    <summary>  Research in deep learning for multi-speaker source separation has received a
boost in the last years. However, most studies are restricted to mixtures of a
specific number of speakers, called a specific scenario. While some works
included experiments for different scenarios, research towards combining data
of different scenarios or creating a single model for multiple scenarios have
been very rare. In this work it is shown that data of a specific scenario is
relevant for solving another scenario. Furthermore, it is concluded that a
single model, trained on different scenarios is capable of matching performance
of scenario specific models.
</summary>
    <author>
      <name>Jeroen Zegers</name>
    </author>
    <author>
      <name>Hugo Van hamme</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.08095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00642v2</id>
    <updated>2018-08-24T11:16:30Z</updated>
    <published>2017-04-03T15:34:11Z</published>
    <title>Local nearest neighbour classification with applications to
  semi-supervised learning</title>
    <summary>  We derive a new asymptotic expansion for the global excess risk of a local
$k$-nearest neighbour classifier, where the choice of $k$ may depend upon the
test point. This expansion elucidates conditions under which the dominant
contribution to the excess risk comes from the locus of points at which each
class label is equally likely to occur, but we also show that if these
conditions are not satisfied, the dominant contribution may arise from the
tails of the marginal distribution of the features. Moreover, we prove that,
provided the $d$-dimensional marginal distribution of the features has a finite
$\rho$th moment for some $\rho &gt; 4$ (as well as other regularity conditions), a
local choice of $k$ can yield a rate of convergence of the excess risk of
$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard
$k$-nearest neighbour classifier, our theory would require $d \geq 5$ and $\rho
&gt; 4d/(d-4)$ finite moments to achieve this rate. Our results motivate a new
$k$-nearest neighbour classifier for semi-supervised learning problems, where
the unlabelled data are used to obtain an estimate of the marginal feature
density, and fewer neighbours are used for classification when this density
estimate is small. The potential improvements over the standard $k$-nearest
neighbour classifier are illustrated both through our theory and via a
simulation study.
</summary>
    <author>
      <name>Timothy I. Cannings</name>
    </author>
    <author>
      <name>Thomas B. Berrett</name>
    </author>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">59 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06262v2</id>
    <updated>2018-08-24T10:24:59Z</updated>
    <published>2018-04-16T15:25:14Z</published>
    <title>Analysis of Extremely Obese Individuals Using Deep Learning Stacked
  Autoencoders and Genome-Wide Genetic Data</title>
    <summary>  The aetiology of polygenic obesity is multifactorial, which indicates that
life-style and environmental factors may influence multiples genes to aggravate
this disorder. Several low-risk single nucleotide polymorphisms (SNPs) have
been associated with BMI. However, identified loci only explain a small
proportion of the variation ob-served for this phenotype. The linear nature of
genome wide association studies (GWAS) used to identify associations between
genetic variants and the phenotype have had limited success in explaining the
heritability variation of BMI and shown low predictive capacity in
classification studies. GWAS ignores the epistatic interactions that less
significant variants have on the phenotypic outcome. In this paper we utilise a
novel deep learning-based methodology to reduce the high dimensional space in
GWAS and find epistatic interactions between SNPs for classification purposes.
SNPs were filtered based on the effects associations have with BMI. Since
Bonferroni adjustment for multiple testing is highly conservative, an important
proportion of SNPs involved in SNP-SNP interactions are ignored. Therefore,
only SNPs with p-values &lt; 1x10-2 were considered for subsequent epistasis
analysis using stacked auto encoders (SAE). This allows the nonlinearity
present in SNP-SNP interactions to be discovered through progressively smaller
hidden layer units and to initialise a multi-layer feedforward artificial
neural network (ANN) classifier. The classifier is fine-tuned to classify
extremely obese and non-obese individuals. The best results were obtained with
2000 compressed units (SE=0.949153, SP=0.933014, Gini=0.949936,
Lo-gloss=0.1956, AUC=0.97497 and MSE=0.054057). Using 50 compressed units it
was possible to achieve (SE=0.785311, SP=0.799043, Gini=0.703566,
Logloss=0.476864, AUC=0.85178 and MSE=0.156315).
</summary>
    <author>
      <name>Casimiro A. Curbelo Montañez</name>
    </author>
    <author>
      <name>Paul Fergus</name>
    </author>
    <author>
      <name>Carl Chalmers</name>
    </author>
    <author>
      <name>Jade Hind</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, 13 equations, 2 tables, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06262v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06262v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03198v2</id>
    <updated>2018-08-24T10:18:40Z</updated>
    <published>2018-04-09T19:37:37Z</published>
    <title>Deep Learning Classification of Polygenic Obesity using Genome Wide
  Association Study SNPs</title>
    <summary>  In this paper, association results from genome-wide association studies
(GWAS) are combined with a deep learning framework to test the predictive
capacity of statistically significant single nucleotide polymorphism (SNPs)
associated with obesity phenotype. Our approach demonstrates the potential of
deep learning as a powerful framework for GWAS analysis that can capture
information about SNPs and the important interactions between them. Basic
statistical methods and techniques for the analysis of genetic SNP data from
population-based genome-wide studies have been considered. Statistical
association testing between individual SNPs and obesity was conducted under an
additive model using logistic regression. Four subsets of loci after
quality-control (QC) and association analysis were selected: P-values lower
than 1x10-5 (5 SNPs), 1x10-4 (32 SNPs), 1x10-3 (248 SNPs) and 1x10-2 (2465
SNPs). A deep learning classifier is initialised using these sets of SNPs and
fine-tuned to classify obese and non-obese observations. Using a deep learning
classifier model and genetic variants with P-value &lt; 1x10-2 (2465 SNPs) it was
possible to obtain results (SE=0.9604, SP=0.9712, Gini=0.9817, LogLoss=0.1150,
AUC=0.9908 and MSE=0.0300). As the P-value increased, an evident deterioration
in performance was observed. Results demonstrate that single SNP analysis fails
to capture the cumulative effect of less significant variants and their overall
contribution to the outcome in disease prediction, which is captured using a
deep learning framework.
</summary>
    <author>
      <name>Casimiro Adays Curbelo Montañez</name>
    </author>
    <author>
      <name>Paul Fergus</name>
    </author>
    <author>
      <name>Almudena Curbelo Montañez</name>
    </author>
    <author>
      <name>Carl Chalmers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 4 tables, 9 equations, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08068v1</id>
    <updated>2018-08-24T09:55:41Z</updated>
    <published>2018-08-24T09:55:41Z</published>
    <title>Self-Paced Multi-Task Clustering</title>
    <summary>  Multi-task clustering (MTC) has attracted a lot of research attentions in
machine learning due to its ability in utilizing the relationship among
different tasks. Despite the success of traditional MTC models, they are either
easy to stuck into local optima, or sensitive to outliers and noisy data. To
alleviate these problems, we propose a novel self-paced multi-task clustering
(SPMTC) paradigm. In detail, SPMTC progressively selects data examples to train
a series of MTC models with increasing complexity, thus highly decreases the
risk of trapping into poor local optima. Furthermore, to reduce the negative
influence of outliers and noisy data, we design a soft version of SPMTC to
further improve the clustering performance. The corresponding SPMTC framework
can be easily solved by an alternating optimization method. The proposed model
is guaranteed to converge and experiments on real data sets have demonstrated
its promising results compared with state-of-the-art multi-task clustering
methods.
</summary>
    <author>
      <name>Yazhou Ren</name>
    </author>
    <author>
      <name>Xiaofan Que</name>
    </author>
    <author>
      <name>Dezhong Yao</name>
    </author>
    <author>
      <name>Zenglin Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08065v1</id>
    <updated>2018-08-24T09:41:26Z</updated>
    <published>2018-08-24T09:41:26Z</published>
    <title>Towards Machine Learning-Based Optimal HAS</title>
    <summary>  Mobile video consumption is increasing and sophisticated video quality
adaptation strategies are required to deal with mobile throughput fluctuations.
These adaptation strategies have to keep the switching frequency low, the
average quality high and prevent stalling occurrences to ensure customer
satisfaction. This paper proposes a novel methodology for the design of machine
learning-based adaptation logics named HASBRAIN. Furthermore, the performance
of a trained neural network against two algorithms from the literature is
evaluated. We first use a modified existing optimization formulation to
calculate optimal adaptation paths with a minimum number of quality switches
for a wide range of videos and for challenging mobile throughput patterns.
Afterwards we use the resulting optimal adaptation paths to train and compare
different machine learning models. The evaluation shows that an artificial
neural network-based model can reach a high average quality with a low number
of switches in the mobile scenario. The proposed methodology is general enough
to be extended for further designs of machine learning-based algorithms and the
provided model can be deployed in on-demand streaming scenarios or be further
refined using reward-based mechanisms such as reinforcement learning. All
tools, models and datasets created during the work are provided as open-source
software.
</summary>
    <author>
      <name>Christian Sieber</name>
    </author>
    <author>
      <name>Korbinian Hagn</name>
    </author>
    <author>
      <name>Christian Moldovan</name>
    </author>
    <author>
      <name>Tobias Hoßfeld</name>
    </author>
    <author>
      <name>Wolfgang Kellerer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05833v2</id>
    <updated>2018-08-24T07:12:13Z</updated>
    <published>2018-06-15T07:24:36Z</published>
    <title>On the exact minimization of saturated loss functions for robust
  regression and subspace estimation</title>
    <summary>  This paper deals with robust regression and subspace estimation and more
precisely with the problem of minimizing a saturated loss function. In
particular, we focus on computational complexity issues and show that an exact
algorithm with polynomial time-complexity with respect to the number of data
can be devised for robust regression and subspace estimation. This result is
obtained by adopting a classification point of view and relating the problems
to the search for a linear model that can approximate the maximal number of
points with a given error. Approximate variants of the algorithms based on
ramdom sampling are also discussed and experiments show that it offers an
accuracy gain over the traditional RANSAC for a similar algorithmic simplicity.
</summary>
    <author>
      <name>Fabien Lauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ABC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters, Elsevier, 2018,
  \&amp;\#x3008;10.1016/j.patrec.2018.08.004\&amp;\#x3009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.05833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08023v1</id>
    <updated>2018-08-24T06:52:04Z</updated>
    <published>2018-08-24T06:52:04Z</published>
    <title>A Jointly Learned Context-Aware Place of Interest Embedding for Trip
  Recommendations</title>
    <summary>  Trip recommendation is an important location-based service that helps relieve
users from the time and efforts for trip planning. It aims to recommend a
sequence of places of interest (POIs) for a user to visit that maximizes the
user's satisfaction. When adding a POI to a recommended trip, it is essential
to understand the context of the recommendation, including the POI popularity,
other POIs co-occurring in the trip, and the preferences of the user. These
contextual factors are learned separately in existing studies, while in
reality, they impact jointly on a user's choice of a POI to visit. In this
study, we propose a POI embedding model to jointly learn the impact of these
contextual factors. We call the learned POI embedding a context-aware POI
embedding. To showcase the effectiveness of this embedding, we apply it to
generate trip recommendations given a user and a time budget. We propose two
trip recommendation algorithms based on our context-aware POI embedding. The
first algorithm finds the exact optimal trip by transforming and solving the
trip recommendation problem as an integer linear programming problem. To
achieve a high computation efficiency, the second algorithm finds a
heuristically optimal trip based on adaptive large neighborhood search. We
perform extensive experiments on real datasets. The results show that our
proposed algorithms consistently outperform state-of-the-art algorithms in trip
recommendation quality, with an advantage of up to 43% in F1-score.
</summary>
    <author>
      <name>Jiayuan He</name>
    </author>
    <author>
      <name>Jianzhong Qi</name>
    </author>
    <author>
      <name>Kotagiri Ramamohanarao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08013v1</id>
    <updated>2018-08-24T06:08:56Z</updated>
    <published>2018-08-24T06:08:56Z</published>
    <title>Reinforcement Learning for Relation Classification from Noisy Data</title>
    <summary>  Existing relation classification methods that rely on distant supervision
assume that a bag of sentences mentioning an entity pair are all describing a
relation for the entity pair. Such methods, performing classification at the
bag level, cannot identify the mapping between a relation and a sentence, and
largely suffers from the noisy labeling problem. In this paper, we propose a
novel model for relation classification at the sentence level from noisy data.
The model has two modules: an instance selector and a relation classifier. The
instance selector chooses high-quality sentences with reinforcement learning
and feeds the selected sentences into the relation classifier, and the relation
classifier makes sentence level prediction and provides rewards to the instance
selector. The two modules are trained jointly to optimize the instance
selection and relation classification processes. Experiment results show that
our model can deal with the noise of data effectively and obtains better
performance for relation classification at the sentence level.
</summary>
    <author>
      <name>Jun Feng</name>
    </author>
    <author>
      <name>Minlie Huang</name>
    </author>
    <author>
      <name>Li Zhao</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Xiaoyan Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01771v3</id>
    <updated>2018-08-24T04:22:19Z</updated>
    <published>2018-06-05T16:09:45Z</published>
    <title>Cycle-Consistent Adversarial Learning as Approximate Bayesian Inference</title>
    <summary>  We formalize the problem of learning interdomain correspondences in the
absence of paired data as Bayesian inference in a latent variable model (LVM),
where one seeks the underlying hidden representations of entities from one
domain as entities from the other domain. First, we introduce implicit latent
variable models, where the prior over hidden representations can be specified
flexibly as an implicit distribution. Next, we develop a new variational
inference (VI) algorithm for this model based on minimization of the symmetric
Kullback-Leibler (KL) divergence between a variational joint and the exact
joint distribution. Lastly, we demonstrate that the state-of-the-art
cycle-consistent adversarial learning (CYCLEGAN) models can be derived as a
special case within our proposed VI framework, thus establishing its connection
to approximate Bayesian inference methods.
</summary>
    <author>
      <name>Louis C. Tiao</name>
    </author>
    <author>
      <name>Edwin V. Bonilla</name>
    </author>
    <author>
      <name>Fabio Ramos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the ICML 2018 Workshop on Theoretical Foundations and
  Applications of Deep Generative Models. Stockholm, Sweden, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01771v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01771v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06670v2</id>
    <updated>2018-08-24T04:02:03Z</updated>
    <published>2018-08-20T19:52:51Z</published>
    <title>Learning deep representations by mutual information estimation and
  maximization</title>
    <summary>  Many popular representation-learning algorithms use training objectives
defined on the observed data space, which we call pixel-level. This may be
detrimental when only a small fraction of the bits of signal actually matter at
a semantic level. We hypothesize that representations should be learned and
evaluated more directly in terms of their information content and statistical
or structural constraints. To address the first quality, we consider learning
unsupervised representations by maximizing mutual information between part or
all of the input and a high-level feature vector. To address the second, we
control characteristics of the representation by matching to a prior
adversarially. Our method, which we call Deep INFOMAX (DIM), can be used to
learn representations with desired characteristics and which empirically
outperform a number of popular unsupervised learning methods on classification
tasks. DIM opens new avenues for unsupervised learn-ing of representations and
is an important step towards flexible formulations of representation learning
objectives catered towards specific end-goals.
</summary>
    <author>
      <name>R Devon Hjelm</name>
    </author>
    <author>
      <name>Alex Fedorov</name>
    </author>
    <author>
      <name>Samuel Lavoie-Marchildon</name>
    </author>
    <author>
      <name>Karan Grewal</name>
    </author>
    <author>
      <name>Adam Trischler</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05760v2</id>
    <updated>2018-08-24T03:26:42Z</updated>
    <published>2018-08-17T05:25:29Z</published>
    <title>Data Poisoning Attacks in Contextual Bandits</title>
    <summary>  We study offline data poisoning attacks in contextual bandits, a class of
reinforcement learning problems with important applications in online
recommendation and adaptive medical treatment, among others. We provide a
general attack framework based on convex optimization and show that by slightly
manipulating rewards in the data, an attacker can force the bandit algorithm to
pull a target arm for a target contextual vector. The target arm and target
contextual vector are both chosen by the attacker. That is, the attacker can
hijack the behavior of a contextual bandit. We also investigate the feasibility
and the side effects of such attacks, and identify future directions for
defense. Experiments on both synthetic and real-world data demonstrate the
efficiency of the attack algorithm.
</summary>
    <author>
      <name>Yuzhe Ma</name>
    </author>
    <author>
      <name>Kwang-Sung Jun</name>
    </author>
    <author>
      <name>Lihong Li</name>
    </author>
    <author>
      <name>Xiaojin Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GameSec 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05760v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05760v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07992v1</id>
    <updated>2018-08-24T03:15:15Z</updated>
    <published>2018-08-24T03:15:15Z</published>
    <title>Undersampling and Bagging of Decision Trees in the Analysis of
  Cardiorespiratory Behavior for the Prediction of Extubation Readiness in
  Extremely Preterm Infants</title>
    <summary>  Extremely preterm infants often require endotracheal intubation and
mechanical ventilation during the first days of life. Due to the detrimental
effects of prolonged invasive mechanical ventilation (IMV), clinicians aim to
extubate infants as soon as they deem them ready. Unfortunately, existing
strategies for prediction of extubation readiness vary across clinicians and
institutions, and lead to high reintubation rates. We present an approach using
Random Forest classifiers for the analysis of cardiorespiratory variability to
predict extubation readiness. We address the issue of data imbalance by
employing random undersampling of examples from the majority class before
training each Decision Tree in a bag. By incorporating clinical domain
knowledge, we further demonstrate that our classifier could have identified 71%
of infants who failed extubation, while maintaining a success detection rate of
78%.
</summary>
    <author>
      <name>Lara J. Kanbar</name>
    </author>
    <author>
      <name>Charles C. Onu</name>
    </author>
    <author>
      <name>Wissam Shalish</name>
    </author>
    <author>
      <name>Karen A. Brown</name>
    </author>
    <author>
      <name>Guilherme M. Sant'Anna</name>
    </author>
    <author>
      <name>Robert E. Kearney</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: 2018 40th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07991v1</id>
    <updated>2018-08-24T03:10:48Z</updated>
    <published>2018-08-24T03:10:48Z</published>
    <title>Predicting Extubation Readiness in Extreme Preterm Infants based on
  Patterns of Breathing</title>
    <summary>  Extremely preterm infants commonly require intubation and invasive mechanical
ventilation after birth. While the duration of mechanical ventilation should be
minimized in order to avoid complications, extubation failure is associated
with increases in morbidities and mortality. As part of a prospective
observational study aimed at developing an accurate predictor of extubation
readiness, Markov and semi-Markov chain models were applied to gain insight
into the respiratory patterns of these infants, with more robust time-series
modeling using semi-Markov models. This model revealed interesting similarities
and differences between newborns who succeeded extubation and those who failed.
The parameters of the model were further applied to predict extubation
readiness via generative (joint likelihood) and discriminative (support vector
machine) approaches. Results showed that up to 84\% of infants who failed
extubation could have been accurately identified prior to extubation.
</summary>
    <author>
      <name>Charles C. Onu</name>
    </author>
    <author>
      <name>Lara J. Kanbar</name>
    </author>
    <author>
      <name>Wissam Shalish</name>
    </author>
    <author>
      <name>Karen A. Brown</name>
    </author>
    <author>
      <name>Guilherme M. Sant'Anna</name>
    </author>
    <author>
      <name>Robert E. Kearney</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: 2017 IEEE Symposium Series on Computational
  Intelligence (SSCI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07632v2</id>
    <updated>2018-08-24T02:26:23Z</updated>
    <published>2018-08-23T04:44:25Z</published>
    <title>DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection
  with GAN</title>
    <summary>  Recently, the introduction of the generative adversarial network (GAN) and
its variants has enabled the generation of realistic synthetic samples, which
has been used for enlarging training sets. Previous work primarily focused on
data augmentation for semi-supervised and supervised tasks. In this paper, we
instead focus on unsupervised anomaly detection and propose a novel generative
data augmentation framework optimized for this task. In particular, we propose
to oversample infrequent normal samples - normal samples that occur with small
probability, e.g., rare normal events. We show that these samples are
responsible for false positives in anomaly detection. However, oversampling of
infrequent normal samples is challenging for real-world high-dimensional data
with multimodal distributions. To address this challenge, we propose to use a
GAN variant known as the adversarial autoencoder (AAE) to transform the
high-dimensional multimodal data distributions into low-dimensional unimodal
latent distributions with well-defined tail probability. Then, we
systematically oversample at the `edge' of the latent distributions to increase
the density of infrequent normal samples. We show that our oversampling
pipeline is a unified one: it is generally applicable to datasets with
different complex data distributions. To the best of our knowledge, our method
is the first data augmentation technique focused on improving performance in
unsupervised anomaly detection. We validate our method by demonstrating
consistent improvements across several real-world datasets.
</summary>
    <author>
      <name>Swee Kiat Lim</name>
    </author>
    <author>
      <name>Yi Loo</name>
    </author>
    <author>
      <name>Ngoc-Trung Tran</name>
    </author>
    <author>
      <name>Ngai-Man Cheung</name>
    </author>
    <author>
      <name>Gemma Roig</name>
    </author>
    <author>
      <name>Yuval Elovici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICDM 2018 (IEEE International
  Conference on Data Mining)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07632v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07632v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07983v1</id>
    <updated>2018-08-24T02:14:45Z</updated>
    <published>2018-08-24T02:14:45Z</published>
    <title>Analysis of Noise Contrastive Estimation from the Perspective of
  Asymptotic Variance</title>
    <summary>  There are many models, often called unnormalized models, whose normalizing
constants are not calculated in closed form. Maximum likelihood estimation is
not directly applicable to unnormalized models. Score matching, contrastive
divergence method, pseudo-likelihood, Monte Carlo maximum likelihood, and noise
contrastive estimation (NCE) are popular methods for estimating parameters of
such models. In this paper, we focus on NCE. The estimator derived from NCE is
consistent and asymptotically normal because it is an M-estimator. NCE
characteristically uses an auxiliary distribution to calculate the normalizing
constant in the same spirit of the importance sampling. In addition, there are
several candidates as objective functions of NCE.
  We focus on how to reduce asymptotic variance. First, we propose a method for
reducing asymptotic variance by estimating the parameters of the auxiliary
distribution. Then, we determine the form of the objective functions, where the
asymptotic variance takes the smallest values in the original estimator class
and the proposed estimator classes. We further analyze the robustness of the
estimator.
</summary>
    <author>
      <name>Masatoshi Uehara</name>
    </author>
    <author>
      <name>Takeru Matsuda</name>
    </author>
    <author>
      <name>Fumiyasu Komaki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07982v1</id>
    <updated>2018-08-24T02:14:43Z</updated>
    <published>2018-08-24T02:14:43Z</published>
    <title>Proximal Policy Optimization and its Dynamic Version for Sequence
  Generation</title>
    <summary>  In sequence generation task, many works use policy gradient for model
optimization to tackle the intractable backpropagation issue when maximizing
the non-differentiable evaluation metrics or fooling the discriminator in
adversarial learning. In this paper, we replace policy gradient with proximal
policy optimization (PPO), which is a proved more efficient reinforcement
learning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We
demonstrate the efficacy of PPO and PPO-dynamic on conditional sequence
generation tasks including synthetic experiment and chit-chat chatbot. The
results show that PPO and PPO-dynamic can beat policy gradient by stability and
performance.
</summary>
    <author>
      <name>Yi-Lin Tuan</name>
    </author>
    <author>
      <name>Jinzhi Zhang</name>
    </author>
    <author>
      <name>Yujia Li</name>
    </author>
    <author>
      <name>Hung-yi Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00411v2</id>
    <updated>2018-08-23T23:54:05Z</updated>
    <published>2018-02-01T17:39:15Z</published>
    <title>Dense 3D Object Reconstruction from a Single Depth View</title>
    <summary>  In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs
the complete 3D structure of a given object from a single arbitrary depth view
using generative adversarial networks. Unlike existing work which typically
requires multiple views of the same object or class labels to recover the full
3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation
of a depth view of the object as input, and is able to generate the complete 3D
occupancy grid with a high resolution of 256^3 by recovering the
occluded/missing regions. The key idea is to combine the generative
capabilities of autoencoders and the conditional Generative Adversarial
Networks (GAN) framework, to infer accurate and fine-grained 3D structures of
objects in high-dimensional voxel space. Extensive experiments on large
synthetic datasets and real-world Kinect datasets show that the proposed
3D-RecGAN++ significantly outperforms the state of the art in single view 3D
object reconstruction, and is able to reconstruct unseen types of objects.
</summary>
    <author>
      <name>Bo Yang</name>
    </author>
    <author>
      <name>Stefano Rosa</name>
    </author>
    <author>
      <name>Andrew Markham</name>
    </author>
    <author>
      <name>Niki Trigoni</name>
    </author>
    <author>
      <name>Hongkai Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TPAMI 2018. Code and data are available at:
  https://github.com/Yang7879/3D-RecGAN-extended. This article extends from
  arXiv:1708.07969</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00411v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00411v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00057v2</id>
    <updated>2018-08-23T21:14:20Z</updated>
    <published>2018-03-30T21:13:34Z</published>
    <title>Understanding Autoencoders with Information Theoretic Concepts</title>
    <summary>  Despite their great success in practical applications, there is still a lack
of theoretical and systematic methods to analyze deep neural networks. In this
paper, we illustrate an advanced information theoretic methodology to
understand the dynamics of learning and the design of autoencoders, a special
type of deep learning architectures that resembles a communication channel. By
generalizing the information plane to any cost function, and inspecting the
roles and dynamics of different layers using layer-wise information quantities,
we emphasize the role that mutual information plays in quantifying learning
from data. We further suggest and also experimentally validate, for mean square
error training, three fundamental properties regarding the layer-wise flow of
information and intrinsic dimensionality of the bottleneck layer, using
respectively the data processing inequality and the identification of a
bifurcation point in the information plane that is controlled by the given
data. Our observations have direct impact on the optimal design of
autoencoders, the design of alternative feedforward training methods, and even
in the problem of generalization.
</summary>
    <author>
      <name>Shujian Yu</name>
    </author>
    <author>
      <name>Jose C. Principe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">64 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07945v1</id>
    <updated>2018-08-23T20:58:08Z</updated>
    <published>2018-08-23T20:58:08Z</published>
    <title>Maximal Jacobian-based Saliency Map Attack</title>
    <summary>  The Jacobian-based Saliency Map Attack is a family of adversarial attack
methods for fooling classification models, such as deep neural networks for
image classification tasks. By saturating a few pixels in a given image to
their maximum or minimum values, JSMA can cause the model to misclassify the
resulting adversarial image as a specified erroneous target class. We propose
two variants of JSMA, one which removes the requirement to specify a target
class, and another that additionally does not need to specify whether to only
increase or decrease pixel intensities. Our experiments highlight the
competitive speeds and qualities of these variants when applied to datasets of
hand-written digits and natural scenes.
</summary>
    <author>
      <name>Rey Wiyatno</name>
    </author>
    <author>
      <name>Anqi Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of extended abstract for MAIS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07912v1</id>
    <updated>2018-08-23T19:19:16Z</updated>
    <published>2018-08-23T19:19:16Z</published>
    <title>Multivariate Extension of Matrix-based Renyi's α-order Entropy
  Functional</title>
    <summary>  The matrix-based Renyi's {\alpha}-order entropy functional was recently
introduced using the normalized eigenspectrum of an Hermitian matrix of the
projected data in the reproducing kernel Hilbert space (RKHS). However, the
current theory in the matrix-based Renyi's {\alpha}-order entropy functional
only defines the entropy of a single variable or mutual information between two
random variables. In information theory and machine learning communities, one
is also frequently interested in multivariate information quantities, such as
the multivariate joint entropy and different interactive quantities among
multiple variables. In this paper, we first define the matrix-based Renyi's
{\alpha}-order joint entropy among multiple variables. We then show how this
definition can ease the estimation of various information quantities that
measure the interactions among multiple variables, such as interactive
information and total correlation. We finally present an application to feature
selection to show how our definition provides a simple yet powerful way to
estimate a widely-acknowledged intractable quantity from data. A real example
on hyperspectral image (HSI) band selection is also provided.
</summary>
    <author>
      <name>Shujian Yu</name>
    </author>
    <author>
      <name>Luis Gonzalo Sanchez Giraldo</name>
    </author>
    <author>
      <name>Robert Jenssen</name>
    </author>
    <author>
      <name>Jose C. Principe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07910v1</id>
    <updated>2018-08-23T19:17:24Z</updated>
    <published>2018-08-23T19:17:24Z</published>
    <title>The Importance of Generation Order in Language Modeling</title>
    <summary>  Neural language models are a critical component of state-of-the-art systems
for machine translation, summarization, audio transcription, and other tasks.
These language models are almost universally autoregressive in nature,
generating sentences one token at a time from left to right. This paper studies
the influence of token generation order on model quality via a novel two-pass
language model that produces partially-filled sentence "templates" and then
fills in missing tokens. We compare various strategies for structuring these
two passes and observe a surprisingly large variation in model quality. We find
the most effective strategy generates function words in the first pass followed
by content words in the second. We believe these experimental results justify a
more extensive investigation of generation order for neural language models.
</summary>
    <author>
      <name>Nicolas Ford</name>
    </author>
    <author>
      <name>Daniel Duckworth</name>
    </author>
    <author>
      <name>Mohammad Norouzi</name>
    </author>
    <author>
      <name>George E. Dahl</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07903v1</id>
    <updated>2018-08-23T18:50:49Z</updated>
    <published>2018-08-23T18:50:49Z</published>
    <title>LIFT: Reinforcement Learning in Computer Systems by Learning From
  Demonstrations</title>
    <summary>  Reinforcement learning approaches have long appealed to the data management
community due to their ability to learn to control dynamic behavior from raw
system performance. Recent successes in combining deep neural networks with
reinforcement learning have sparked significant new interest in this domain.
However, practical solutions remain elusive due to large training data
requirements, algorithmic instability, and lack of standard tools. In this
work, we introduce LIFT, an end-to-end software stack for applying deep
reinforcement learning to data management tasks. While prior work has
frequently explored applications in simulations, LIFT centers on utilizing
human expertise to learn from demonstrations, thus lowering online training
times. We further introduce TensorForce, a TensorFlow library for applied deep
reinforcement learning exposing a unified declarative interface to common RL
algorithms, thus providing a backend to LIFT. We demonstrate the utility of
LIFT in two case studies in database compound indexing and resource management
in stream processing. Results show LIFT controllers initialized from
demonstrations can outperform human baselines and heuristics across latency
metrics and space usage by up to 70%.
</summary>
    <author>
      <name>Michael Schaarschmidt</name>
    </author>
    <author>
      <name>Alexander Kuhnle</name>
    </author>
    <author>
      <name>Ben Ellis</name>
    </author>
    <author>
      <name>Kai Fricke</name>
    </author>
    <author>
      <name>Felix Gessert</name>
    </author>
    <author>
      <name>Eiko Yoneki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07840v1</id>
    <updated>2018-08-23T16:55:53Z</updated>
    <published>2018-08-23T16:55:53Z</published>
    <title>Learning to Importance Sample in Primary Sample Space</title>
    <summary>  Importance sampling is one of the most widely used variance reduction
strategies in Monte Carlo rendering. In this paper, we propose a novel
importance sampling technique that uses a neural network to learn how to sample
from a desired density represented by a set of samples. Our approach considers
an existing Monte Carlo rendering algorithm as a black box. During a
scene-dependent training phase, we learn to generate samples with a desired
density in the primary sample space of the rendering algorithm using maximum
likelihood estimation. We leverage a recent neural network architecture that
was designed to represent real-valued non-volume preserving ('Real NVP')
transformations in high dimensional spaces. We use Real NVP to non-linearly
warp primary sample space and obtain desired densities. In addition, Real NVP
efficiently computes the determinant of the Jacobian of the warp, which is
required to implement the change of integration variables implied by the warp.
A main advantage of our approach is that it is agnostic of underlying light
transport effects, and can be combined with many existing rendering techniques
by treating them as a black box. We show that our approach leads to effective
variance reduction in several practical scenarios.
</summary>
    <author>
      <name>Quan Zheng</name>
    </author>
    <author>
      <name>Matthias Zwicker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to SIGGRAPH ASIA'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07804v1</id>
    <updated>2018-08-23T15:27:14Z</updated>
    <published>2018-08-23T15:27:14Z</published>
    <title>Transfer Learning for Estimating Causal Effects using Neural Networks</title>
    <summary>  We develop new algorithms for estimating heterogeneous treatment effects,
combining recent developments in transfer learning for neural networks with
insights from the causal inference literature. By taking advantage of transfer
learning, we are able to efficiently use different data sources that are
related to the same underlying causal mechanisms. We compare our algorithms
with those in the extant literature using extensive simulation studies based on
large-scale voter persuasion experiments and the MNIST database. Our methods
can perform an order of magnitude better than existing benchmarks while using a
fraction of the data.
</summary>
    <author>
      <name>Sören R. Künzel</name>
    </author>
    <author>
      <name>Bradly C. Stadie</name>
    </author>
    <author>
      <name>Nikita Vemuri</name>
    </author>
    <author>
      <name>Varsha Ramakrishnan</name>
    </author>
    <author>
      <name>Jasjeet S. Sekhon</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07801v1</id>
    <updated>2018-08-23T15:22:57Z</updated>
    <published>2018-08-23T15:22:57Z</published>
    <title>On a 'Two Truths' Phenomenon in Spectral Graph Clustering</title>
    <summary>  Clustering is concerned with coherently grouping observations without any
explicit concept of true groupings. Spectral graph clustering - clustering the
vertices of a graph based on their spectral embedding - is commonly approached
via K-means (or, more generally, Gaussian mixture model) clustering composed
with either Laplacian or Adjacency spectral embedding (LSE or ASE). Recent
theoretical results provide new understanding of the problem and solutions, and
lead us to a 'Two Truths' LSE vs. ASE spectral graph clustering phenomenon
convincingly illustrated here via a diffusion MRI connectome data set: the
different embedding methods yield different clustering results, with LSE
capturing left hemisphere/right hemisphere affinity structure and ASE capturing
gray matter/white matter core-periphery structure.
</summary>
    <author>
      <name>Carey E. Priebe</name>
    </author>
    <author>
      <name>Youngser Park</name>
    </author>
    <author>
      <name>Joshua T. Vogelstein</name>
    </author>
    <author>
      <name>John M. Conroy</name>
    </author>
    <author>
      <name>Vince Lyzinskic</name>
    </author>
    <author>
      <name>Minh Tang</name>
    </author>
    <author>
      <name>Avanti Athreya</name>
    </author>
    <author>
      <name>Joshua Cape</name>
    </author>
    <author>
      <name>Eric Bridgeford</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.01199v5</id>
    <updated>2018-08-23T14:57:12Z</updated>
    <published>2017-12-04T17:06:11Z</published>
    <title>tHoops: A Multi-Aspect Analytical Framework Spatio-Temporal Basketball
  Data</title>
    <summary>  During the past few years advancements in sports information systems and
technology has allowed us to collect a number of detailed spatio-temporal data
capturing various aspects of basketball. For example, shot charts, that is,
maps capturing locations of (made or missed) shots, and spatio-temporal
trajectories for all the players on the court can capture information about the
offensive and defensive tendencies and schemes of a team. Characterization of
these processes is important for player and team comparisons, pre-game
scouting, game preparation etc. Playing tendencies among teams have
traditionally been compared in a heuristic manner. Recently automated ways for
similar comparisons have appeared in the sports analytics literature. However,
these approaches are almost exclusively focused on the spatial distribution of
the underlying actions (usually shots taken), ignoring a multitude of other
parameters that can affect the action studied. In this work, we propose a
framework based on tensor decomposition for obtaining a set of prototype
spatio-temporal patterns based on the core spatiotemporal information and
contextual meta-data. The core of our framework is a 3D tensor X, whose
dimensions represent the entity under consideration (team, player, possession
etc.), the location on the court and time. We make use of the PARAFAC
decomposition and we decompose the tensor into several interpretable patterns,
that can be thought of as prototype patterns of the process examined (e.g.,
shot selection, offensive schemes etc.). We also introduce an approach for
choosing the number of components to be considered. Using the tensor
components, we can then express every entity as a weighted combination of these
components. The framework introduced in this paper can have further
applications in the work-flow of the basketball operations of a franchise,
which we also briefly discuss.
</summary>
    <author>
      <name>Evangelos Papalexakis</name>
    </author>
    <author>
      <name>Konstantinos Pelechrinis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.01199v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01199v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07784v1</id>
    <updated>2018-08-23T14:52:40Z</updated>
    <published>2018-08-23T14:52:40Z</published>
    <title>Time-Agnostic Prediction: Predicting Predictable Video Frames</title>
    <summary>  Prediction is arguably one of the most basic functions of an intelligent
system. In general, the problem of predicting events in the future or between
two waypoints is exceedingly difficult. However, most phenomena naturally pass
through relatively predictable bottlenecks---while we cannot predict the
precise trajectory of a robot arm between being at rest and holding an object
up, we can be certain that it must have picked the object up. To exploit this,
we decouple visual prediction from a rigid notion of time. While conventional
approaches predict frames at regularly spaced temporal intervals, our
time-agnostic predictors (TAP) are not tied to specific times so that they may
instead discover predictable "bottleneck" frames no matter when they occur. We
evaluate our approach for future and intermediate frame prediction across three
robotic manipulation tasks. Our predictions are not only of higher visual
quality, but also correspond to coherent semantic subgoals in temporally
extended tasks. Project website: goo.gl/tL6Jgr.
</summary>
    <author>
      <name>Dinesh Jayaraman</name>
    </author>
    <author>
      <name>Frederik Ebert</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, plus appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11139v2</id>
    <updated>2018-08-23T14:27:53Z</updated>
    <published>2017-11-29T22:43:20Z</published>
    <title>Easy High-Dimensional Likelihood-Free Inference</title>
    <summary>  We introduce a framework using Generative Adversarial Networks (GANs) for
likelihood--free inference (LFI) and Approximate Bayesian Computation (ABC)
where we replace the black-box simulator model with an approximator network and
generate a rich set of summary features in a data driven fashion. On benchmark
data sets, our approach improves on others with respect to scalability, ability
to handle high dimensional data and complex probability distributions.
</summary>
    <author>
      <name>Vinay Jethava</name>
    </author>
    <author>
      <name>Devdatt Dubhashi</name>
    </author>
    <link href="http://arxiv.org/abs/1711.11139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11297v2</id>
    <updated>2018-08-23T14:17:38Z</updated>
    <published>2018-04-30T16:19:51Z</published>
    <title>Sampling strategies in Siamese Networks for unsupervised speech
  representation learning</title>
    <summary>  Recent studies have investigated siamese network architectures for learning
invariant speech representations using same-different side information at the
word level. Here we investigate systematically an often ignored component of
siamese networks: the sampling procedure (how pairs of same vs. different
tokens are selected). We show that sampling strategies taking into account
Zipf's Law, the distribution of speakers and the proportions of same and
different pairs of words significantly impact the performance of the network.
In particular, we show that word frequency compression improves learning across
a large range of variations in number of training pairs. This effect does not
apply to the same extent to the fully unsupervised setting, where the pairs of
same-different words are obtained by spoken term discovery. We apply these
results to pairs of words discovered using an unsupervised algorithm and show
an improvement on state-of-the-art in unsupervised representation learning
using siamese networks.
</summary>
    <author>
      <name>Rachid Riad</name>
    </author>
    <author>
      <name>Corentin Dancette</name>
    </author>
    <author>
      <name>Julien Karadayi</name>
    </author>
    <author>
      <name>Neil Zeghidour</name>
    </author>
    <author>
      <name>Thomas Schatz</name>
    </author>
    <author>
      <name>Emmanuel Dupoux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference paper at Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.11297v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11297v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00953v1</id>
    <updated>2018-08-23T14:05:31Z</updated>
    <published>2018-08-23T14:05:31Z</published>
    <title>Deep Learning Based Vehicle Make-Model Classification</title>
    <summary>  This paper studies the problems of vehicle make &amp; model classification. Some
of the main challenges are reaching high classification accuracy and reducing
the annotation time of the images. To address these problems, we have created a
fine-grained database using online vehicle marketplaces of Turkey. A pipeline
is proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN
(Convolutional Neural Network) model to train on the database. In the pipeline,
we first detect the vehicles by following an algorithm which reduces the time
for annotation. Then, we feed them into the CNN model. It is reached
approximately 4% better classification accuracy result than using a
conventional CNN model. Next, we propose to use the detected vehicles as ground
truth bounding box (GTBB) of the images and feed them into an SSD model in
another pipeline. At this stage, it is reached reasonable classification
accuracy result without using perfectly shaped GTBB. Lastly, an application is
implemented in a use case by using our proposed pipelines. It detects the
unauthorized vehicles by comparing their license plate numbers and make &amp;
models. It is assumed that license plates are readable.
</summary>
    <author>
      <name>Burak Satar</name>
    </author>
    <author>
      <name>Ahmet Emir Dirik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. It is accepted by 27th International Conference on
  Artificial Neural Networks 2018. It hasn't been presented yet. Conference
  proceedings are published by Springer in Lecture Notes in Computer Science</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07769v1</id>
    <updated>2018-08-23T14:01:05Z</updated>
    <published>2018-08-23T14:01:05Z</published>
    <title>Topology and Prediction Focused Research on Graph Convolutional Neural
  Networks</title>
    <summary>  Important advances have been made using convolutional neural network (CNN)
approaches to solve complicated problems in areas that rely on grid structured
data such as image processing and object classification. Recently, research on
graph convolutional neural networks (GCNN) has increased dramatically as
researchers try to replicate the success of CNN for graph structured data.
Unfortunately, traditional CNN methods are not readily transferable to GCNN,
given the irregularity and geometric complexity of graphs. The emerging field
of GCNN is further complicated by research papers that differ greatly in their
scope, detail, and level of academic sophistication needed by the reader.
  The present paper provides a review of some basic properties of GCNN. As a
guide to the interested reader, recent examples of GCNN research are then
grouped according to techniques that attempt to uncover the underlying topology
of the graph model and those that seek to generalize traditional CNN methods on
graph data to improve prediction of class membership. Discrete Signal
Processing on Graphs (DSPg) is used as a theoretical framework to better
understand some of the performance gains and limitations of these recent GCNN
approaches. A brief discussion of Topology Adaptive Graph Convolutional
Networks (TAGCN) is presented as an approach motivated by DSPg and future
research directions using this approach are briefly discussed.
</summary>
    <author>
      <name>Matthew Baron</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07739v1</id>
    <updated>2018-08-23T13:24:17Z</updated>
    <published>2018-08-23T13:24:17Z</published>
    <title>Diversity-Driven Selection of Exploration Strategies in Multi-Armed
  Bandits</title>
    <summary>  We consider a scenario where an agent has multiple available strategies to
explore an unknown environment. For each new interaction with the environment,
the agent must select which exploration strategy to use. We provide a new
strategy-agnostic method that treat the situation as a Multi-Armed Bandits
problem where the reward signal is the diversity of effects that each strategy
produces. We test the method empirically on a simulated planar robotic arm, and
establish that the method is both able discriminate between strategies of
dissimilar quality, even when the differences are tenuous, and that the
resulting performance is competitive with the best fixed mixture of strategies.
</summary>
    <author>
      <name>Fabien C. Y. Benureau</name>
    </author>
    <author>
      <name>Pierre-Yves Oudeyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/devlrn.2015.7346130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/devlrn.2015.7346130" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 Joint IEEE International Conference on Development and
  Learning and Epigenetic Robotics (ICDL-EpiRob), pp. 135-142</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.07739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07724v1</id>
    <updated>2018-08-23T12:47:01Z</updated>
    <published>2018-08-23T12:47:01Z</published>
    <title>Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs</title>
    <summary>  This paper addresses the problem of mapping natural language text to
knowledge base entities. The mapping process is approached as a composition of
a phrase or a sentence into a point in a multi-dimensional entity space
obtained from a knowledge graph. The compositional model is an LSTM equipped
with a dynamic disambiguation mechanism on the input word embeddings (a
Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base
space is prepared by collecting random walks from a graph enhanced with textual
features, which act as a set of semantic bridges between text and knowledge
base entities. The ideas of this work are demonstrated on large-scale
text-to-entity mapping and entity classification tasks, with state of the art
results.
</summary>
    <author>
      <name>Dimitri Kartsaklis</name>
    </author>
    <author>
      <name>Mohammad Taher Pilehvar</name>
    </author>
    <author>
      <name>Nigel Collier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at EMNLP 2018 (main conference)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04708v4</id>
    <updated>2018-08-23T12:37:42Z</updated>
    <published>2017-12-13T11:17:37Z</published>
    <title>Differentiable lower bound for expected BLEU score</title>
    <summary>  In natural language processing tasks performance of the models is often
measured with some non-differentiable metric, such as BLEU score. To use
efficient gradient-based methods for optimization, it is a common workaround to
optimize some surrogate loss function. This approach is effective if
optimization of such loss also results in improving target metric. The
corresponding problem is referred to as loss-evaluation mismatch. In the
present work we propose a method for calculation of differentiable lower bound
of expected BLEU score that does not involve computationally expensive sampling
procedure such as the one required when using REINFORCE rule from reinforcement
learning (RL) framework.
</summary>
    <author>
      <name>Vlad Zhukov</name>
    </author>
    <author>
      <name>Eugene Golikov</name>
    </author>
    <author>
      <name>Maksim Kretov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at NIPS 2017 Workshop on Conversational AI: Today's
  Practice and Tomorrow's Potential</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.04708v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04708v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07713v1</id>
    <updated>2018-08-23T12:12:10Z</updated>
    <published>2018-08-23T12:12:10Z</published>
    <title>Adversarial Attacks on Deep-Learning Based Radio Signal Classification</title>
    <summary>  Deep learning (DL), despite its enormous success in many computer vision and
language processing applications, is exceedingly vulnerable to adversarial
attacks. We consider the use of DL for radio signal (modulation) classification
tasks, and present practical methods for the crafting of white-box and
universal black-box adversarial attacks in that application. We show that these
attacks can considerably reduce the classification performance, with extremely
small perturbations of the input. In particular, these attacks are
significantly more powerful than classical jamming attacks, which raises
significant security and robustness concerns in the use of DL-based algorithms
for the wireless physical layer.
</summary>
    <author>
      <name>Meysam Sadeghi</name>
    </author>
    <author>
      <name>Erik G. Larsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01610v4</id>
    <updated>2018-08-23T12:07:40Z</updated>
    <published>2018-06-05T11:16:42Z</published>
    <title>Training Generative Reversible Networks</title>
    <summary>  Generative models with an encoding component such as autoencoders currently
receive great interest. However, training of autoencoders is typically
complicated by the need to train a separate encoder and decoder model that have
to be enforced to be reciprocal to each other. To overcome this problem,
by-design reversible neural networks (RevNets) had been previously used as
generative models either directly optimizing the likelihood of the data under
the model or using an adversarial approach on the generated data. Here, we
instead investigate their performance using an adversary on the latent space in
the adversarial autoencoder framework. We investigate the generative
performance of RevNets on the CelebA dataset, showing that generative RevNets
can generate coherent faces with similar quality as Variational Autoencoders.
This first attempt to use RevNets inside the adversarial autoencoder framework
slightly underperformed relative to recent advanced generative models using an
autoencoder component on CelebA, but this gap may diminish with further
optimization of the training setup of generative RevNets. In addition to the
experiments on CelebA, we show a proof-of-principle experiment on the MNIST
dataset suggesting that adversary-free trained RevNets can discover meaningful
latent dimensions without pre-specifying the number of dimensions of the latent
sampling distribution. In summary, this study shows that RevNets can be
employed in different generative training settings.
  Source code for this study is at
https://github.com/robintibor/generative-reversible
</summary>
    <author>
      <name>Robin Tibor Schirrmeister</name>
    </author>
    <author>
      <name>Patryk Chrabąszcz</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <author>
      <name>Tonio Ball</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source code for this study is at
  https://github.com/robintibor/generative-reversible</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01610v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01610v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07560v2</id>
    <updated>2018-08-23T09:12:37Z</updated>
    <published>2018-07-19T17:57:16Z</published>
    <title>Compositional GAN: Learning Conditional Image Composition</title>
    <summary>  Generative Adversarial Networks (GANs) can produce images of surprising
complexity and realism, but are generally modeled to sample from a single
latent source ignoring the explicit spatial interaction between multiple
entities that could be present in a scene. Capturing such complex interactions
between different objects in the world, including their relative scaling,
spatial layout, occlusion, or viewpoint transformation is a challenging
problem. In this work, we propose to model object composition in a GAN
framework as a self-consistent composition-decomposition network. Our model is
conditioned on the object images from their marginal distributions to generate
a realistic image from their joint distribution by explicitly learning the
possible interactions. We evaluate our model through qualitative experiments
and user evaluations in both the scenarios when either paired or unpaired
examples for the individual object images and the joint scenes are given during
training. Our results reveal that the learned model captures potential
interactions between the two object domains given as input to output new
instances of composed scene at test time in a reasonable fashion.
</summary>
    <author>
      <name>Samaneh Azadi</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <author>
      <name>Sayna Ebrahimi</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1807.07560v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07560v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07647v1</id>
    <updated>2018-08-23T07:06:41Z</updated>
    <published>2018-08-23T07:06:41Z</published>
    <title>Machine Learning at the Edge: A Data-Driven Architecture with
  Applications to 5G Cellular Networks</title>
    <summary>  The fifth generation of cellular networks (5G) will rely on edge cloud
deployments to satisfy the ultra-low latency demand of future applications. In
this paper, we argue that an edge-based deployment can also be used as an
enabler of advanced Machine Learning (ML) applications in cellular networks,
thanks to the balance it strikes between a completely distributed and a
centralized approach. First, we will present an edge-controller-based
architecture for cellular networks. Second, by using real data from hundreds of
base stations of a major U.S. national operator, we will provide insights on
how to dynamically cluster the base stations under the domain of each
controller. Third, we will describe how these controllers can be used to run ML
algorithms to predict the number of users, and a use case in which these
predictions are used by a higher-layer application to route vehicular traffic
according to network Key Performance Indicators (KPIs). We show that prediction
accuracy improves when based on machine learning algorithms that exploit the
controllers' view with respect to when it is based only on the local data of
each single base station.
</summary>
    <author>
      <name>Michele Polese</name>
    </author>
    <author>
      <name>Rittwik Jana</name>
    </author>
    <author>
      <name>Velin Kounev</name>
    </author>
    <author>
      <name>Ke Zhang</name>
    </author>
    <author>
      <name>Supratim Deb</name>
    </author>
    <author>
      <name>Michele Zorzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures, 3 tables. Submitted to IEEE JSAC Special Issue
  on Special Issue on Artificial Intelligence and Machine Learning for
  Networking and Communications</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08111v1</id>
    <updated>2018-08-23T06:51:09Z</updated>
    <published>2018-08-23T06:51:09Z</published>
    <title>Multiclass Universum SVM</title>
    <summary>  We introduce Universum learning for multiclass problems and propose a novel
formulation for multiclass universum SVM (MU-SVM). We also propose an analytic
span bound for model selection with almost 2-4x faster computation times than
standard resampling techniques. We empirically demonstrate the efficacy of the
proposed MUSVM formulation on several real world datasets achieving &gt; 20%
improvement in test accuracies compared to multi-class SVM.
</summary>
    <author>
      <name>Sauptik Dhar</name>
    </author>
    <author>
      <name>Vladimir Cherkassky</name>
    </author>
    <author>
      <name>Mohak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages. arXiv admin note: text overlap with arXiv:1609.09162</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05385v2</id>
    <updated>2018-08-23T06:23:32Z</updated>
    <published>2018-08-16T09:25:50Z</published>
    <title>On the Decision Boundary of Deep Neural Networks</title>
    <summary>  While deep learning models and techniques have achieved great empirical
success, our understanding of the source of success in many aspects remains
very limited. In an attempt to bridge the gap, we investigate the decision
boundary of a production deep learning architecture with weak assumptions on
both the training data and the model. We demonstrate, both theoretically and
empirically, that the last weight layer of a neural network converges to a
linear SVM trained on the output of the last hidden layer, for both the binary
case and the multi-class case with the commonly used cross-entropy loss.
Furthermore, we show empirically that training a neural network as a whole,
instead of only fine-tuning the last weight layer, may result in better bias
constant for the last weight layer, which is important for generalization. In
addition to facilitating the understanding of deep learning, our result can be
helpful for solving a broad range of practical problems of deep learning, such
as catastrophic forgetting and adversarial attacking. The experiment codes are
available at https://github.com/lykaust15/NN_decision_boundary
</summary>
    <author>
      <name>Yu Li</name>
    </author>
    <author>
      <name>Lizhong Ding</name>
    </author>
    <author>
      <name>Xin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05385v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05385v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05581v3</id>
    <updated>2018-08-23T05:07:47Z</updated>
    <published>2017-09-16T23:16:44Z</published>
    <title>MultiNet: Multi-Modal Multi-Task Learning for Autonomous Driving</title>
    <summary>  Several deep learning approaches have been applied to the autonomous driving
task, many employing end-to-end deep neural networks. Autonomous driving is
complex, utilizing multiple behavioral modalities ranging from lane changing to
turning and stopping. However, most existing approaches do not factor in the
different behavioral modalities of the driving task into the training strategy.
This paper describes a technique for using Multi-Modal Multi-Task Learning,
which we denote as MultiNet which considers multiple behavioral modalities as
distinct modes of operation for an end-to-end autonomous deep neural network
utilizing the insertion of modal information as secondary input data. Using
labeled data from hours of driving our fleet of 1/10th scale model cars, we
trained different neural networks to imitate the steering angle and driving
speed of human control of a car. We show that in each case, MultiNet models
outperform networks trained on individual tasks, while using a fraction of the
number of parameters.
</summary>
    <author>
      <name>Sauhaarda Chowdhuri</name>
    </author>
    <author>
      <name>Tushar Pankaj</name>
    </author>
    <author>
      <name>Karl Zipser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05581v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05581v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08124v1</id>
    <updated>2018-08-23T05:01:05Z</updated>
    <published>2018-08-23T05:01:05Z</published>
    <title>Insect cyborgs: Biological feature generators improve machine learning
  accuracy on limited data</title>
    <summary>  Despite many successes, machine learning (ML) methods such as neural nets
often struggle to learn given small training sets. In contrast, biological
neural nets (BNNs) excel at fast learning. We can thus look to BNNs for tools
to improve performance of ML methods in this low-data regime.
  The insect olfactory network, though simple, can learn new odors very
rapidly. Its two key structures are a layer with competitive inhibition (the
Antennal Lobe, AL), followed by a high dimensional sparse plastic layer (the
Mushroom Body, MB). This AL-MB network can rapidly learn not only odors but
also handwritten digits, better in fact than standard ML methods in the
few-shot regime.
  In this work, we deploy the AL-MB network as an automatic feature generator,
using its Readout Neurons as additional features for standard ML classifiers.
We hypothesize that the AL-MB structure has a strong intrinsic clustering
ability; and that its Readout Neurons, used as input features, will boost the
performance of ML methods.
  We find that these "insect cyborgs", ie classifiers that are part-moth and
part-ML method, deliver significantly better performance than baseline ML
methods alone on a generic (non-spatial) 85-feature, 10-class task derived from
the MNIST dataset. Accuracy improves by an average of 6% to 33% for N &lt; 15
training samples per class, and by 6% to 10% for N &gt; 15. Remarkably, these
moth-generated features increase ML accuracy even when the ML method's baseline
accuracy already exceeds the AL-MB's own limited capacity.
  The two structures in the AL-MB, a competitive inhibition layer and a
high-dimensional sparse layer with Hebbian plasticity, are novel in the context
of artificial NNs but endemic in BNNs. We believe they can be deployed either
prepended as feature generators or inserted as layers into deep NNs, to
potentially improve ML performance.
</summary>
    <author>
      <name>Charles B Delahunt</name>
    </author>
    <author>
      <name>J Nathan Kutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07220v2</id>
    <updated>2018-08-23T02:21:56Z</updated>
    <published>2018-08-22T05:14:41Z</published>
    <title>Approximating Poker Probabilities with Deep Learning</title>
    <summary>  Many poker systems, whether created with heuristics or machine learning, rely
on the probability of winning as a key input. However calculating the precise
probability using combinatorics is an intractable problem, so instead we
approximate it. Monte Carlo simulation is an effective technique that can be
used to approximate the probability that a player will win and/or tie a hand.
However, without the use of a memory-intensive lookup table or a supercomputer,
it becomes infeasible to run millions of times when training an agent with
self-play. To combat the space-time tradeoff, we use deep learning to
approximate the probabilities obtained from the Monte Carlo simulation with
high accuracy. The learned model proves to be a lightweight alternative to
Monte Carlo simulation, which ultimately allows us to use the probabilities as
inputs during self-play efficiently. The source code and optimized neural
network can be found at
https://github.com/brandinho/Poker-Probability-Approximation
</summary>
    <author>
      <name>Brandon Da Silva</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06914v2</id>
    <updated>2018-08-23T01:45:09Z</updated>
    <published>2018-08-19T11:01:48Z</published>
    <title>Segmentation of Microscopy Data for finding Nuclei in Divergent Images</title>
    <summary>  Every year millions of people die due to disease of Cancer. Due to its
invasive nature it is very complex to cure even in primary stages. Hence, only
method to survive this disease completely is via forecasting by analyzing the
early mutation in cells of the patient biopsy. Cell Segmentation can be used to
find cell which have left their nuclei. This enables faster cure and high rate
of survival. Cell counting is a hard, yet tedious task that would greatly
benefit from automation. To accomplish this task, segmentation of cells need to
be accurate. In this paper, we have improved the learning of training data by
our network. It can annotate precise masks on test data. we examine the
strength of activation functions in medical image segmentation task by
improving learning rates by our proposed Carving Technique. Identifying the
cells nuclei is the starting point for most analyses, identifying nuclei allows
researchers to identify each individual cell in a sample, and by measuring how
cells react to various treatments, the researcher can understand the underlying
biological processes at work. Experimental results shows the efficiency of the
proposed work.
</summary>
    <author>
      <name>Shivam Singh</name>
    </author>
    <author>
      <name>Stuti Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1807.04459, arXiv:1802.10548, arXiv:1807.10165 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06914v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06914v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00373v2</id>
    <updated>2018-08-23T01:39:44Z</updated>
    <published>2018-07-01T19:09:15Z</published>
    <title>New Heuristics for Parallel and Scalable Bayesian Optimization</title>
    <summary>  Bayesian optimization has emerged as a strong candidate tool for global
optimization of functions with expensive evaluation costs. However, due to the
dynamic nature of research in Bayesian approaches, and the evolution of
computing technology, using Bayesian optimization in a parallel computing
environment remains a challenge for the non-expert. In this report, I review
the state-of-the-art in parallel and scalable Bayesian optimization methods. In
addition, I propose practical ways to avoid a few of the pitfalls of Bayesian
optimization, such as oversampling of edge parameters and over-exploitation of
high performance parameters. Finally, I provide relatively simple, heuristic
algorithms, along with their open source software implementations, that can be
immediately and easily deployed in any computing environment.
</summary>
    <author>
      <name>Ran Rubin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00373v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00373v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08121v1</id>
    <updated>2018-08-23T00:38:14Z</updated>
    <published>2018-08-23T00:38:14Z</published>
    <title>An Improvement of Data Classification Using Random Multimodel Deep
  Learning (RMDL)</title>
    <summary>  The exponential growth in the number of complex datasets every year requires
more enhancement in machine learning methods to provide robust and accurate
data classification. Lately, deep learning approaches have achieved surpassing
results in comparison to previous machine learning algorithms. However, finding
the suitable structure for these models has been a challenge for researchers.
This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,
deep learning approach for classification. RMDL solves the problem of finding
the best deep learning structure and architecture while simultaneously
improving robustness and accuracy through ensembles of deep learning
architectures. In short, RMDL trains multiple randomly generated models of Deep
Neural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural
Network (RNN) in parallel and combines their results to produce better result
of any of those models individually. In this paper, we describe RMDL model and
compare the results for image and text classification as well as face
recognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for
image classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text
classification. Lastly, we used ORL dataset to compare the model performance on
face recognition task.
</summary>
    <author>
      <name>Mojtaba Heidarysafa</name>
    </author>
    <author>
      <name>Kamran Kowsari</name>
    </author>
    <author>
      <name>Donald E. Brown</name>
    </author>
    <author>
      <name>Kiana Jafari Meimandi</name>
    </author>
    <author>
      <name>Laura E. Barnes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18178/ijmlc.2018.8.4.703</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18178/ijmlc.2018.8.4.703" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in International Journal of Machine Learning and Computing
  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07593v1</id>
    <updated>2018-08-23T00:13:18Z</updated>
    <published>2018-08-23T00:13:18Z</published>
    <title>Pathologies in information bottleneck for deterministic supervised
  learning</title>
    <summary>  Information bottleneck (IB) is a method for extracting information from one
random variable $X$ that is relevant for predicting another random variable
$Y$. To do so, IB identifies an intermediate "bottleneck" variable $T$ that has
low mutual information $I(X;T)$ and high mutual information $I(Y;T)$. The "IB
curve" characterizes the set of bottleneck variables that achieve maximal
$I(Y;T)$ for a given $I(X;T)$, and is typically explored by optimizing the "IB
Lagrangian", $I(Y;T) - \beta I(X;T)$. Recently, there has been interest in
applying IB to supervised learning, particularly for classification problems
that use neural networks. In most classification problems, the output class $Y$
is a deterministic function of the input $X$, which we refer to as
"deterministic supervised learning". We demonstrate three pathologies that
arise when IB is used in any scenario where $Y$ is a deterministic function of
$X$: (1) the IB curve cannot be recovered by optimizing the IB Lagrangian for
different values of $\beta$; (2) there are "uninteresting" solutions at all
points of the IB curve; and (3) for classifiers that achieve low error rates,
the activity of different hidden layers will not exhibit a strict trade-off
between compression and prediction, contrary to a recent proposal. To address
problem (1), we propose a functional that, unlike the IB Lagrangian, can
recover the IB curve in all cases. We finish by demonstrating these issues on
the MNIST dataset.
</summary>
    <author>
      <name>Artemy Kolchinsky</name>
    </author>
    <author>
      <name>Brendan D. Tracey</name>
    </author>
    <author>
      <name>Steven Van Kuyk</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06234v3</id>
    <updated>2018-08-22T23:44:34Z</updated>
    <published>2018-04-13T23:09:12Z</published>
    <title>Clustering Analysis on Locally Asymptotically Self-similar Processes
  with Known Number of Clusters</title>
    <summary>  We study the problems of clustering locally asymptotically self-similar
stochastic processes, when the true number of clusters is priorly known. A new
covariance-based dissimilarity measure is introduced, from which the so-called
approximately asymptotically consistent clustering algorithms are obtained. In
a simulation study, clustering data sampled from multifractional Brownian
motions is performed to illustrate the approximated asymptotic consistency of
the proposed algorithms.
</summary>
    <author>
      <name>Qidi Peng</name>
    </author>
    <author>
      <name>Nan Rao</name>
    </author>
    <author>
      <name>Ran Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1804.06234v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06234v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07, 60G10, 62M10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01346v2</id>
    <updated>2018-08-22T23:42:08Z</updated>
    <published>2018-08-03T20:32:32Z</published>
    <title>Deep convolutional recurrent autoencoders for learning low-dimensional
  feature dynamics of fluid systems</title>
    <summary>  Model reduction of high-dimensional dynamical systems alleviates
computational burdens faced in various tasks from design optimization to model
predictive control. One popular model reduction approach is based on projecting
the governing equations onto a subspace spanned by basis functions obtained
from the compression of a dataset of solution snapshots. However, this method
is intrusive since the projection requires access to the system operators.
Further, some systems may require special treatment of nonlinearities to ensure
computational efficiency or additional modeling to preserve stability. In this
work we propose a deep learning-based strategy for nonlinear model reduction
that is inspired by projection-based model reduction where the idea is to
identify some optimal low-dimensional representation and evolve it in time. Our
approach constructs a modular model consisting of a deep convolutional
autoencoder and a modified LSTM network. The deep convolutional autoencoder
returns a low-dimensional representation in terms of coordinates on some
expressive nonlinear data-supporting manifold. The dynamics on this manifold
are then modeled by the modified LSTM network in a computationally efficient
manner. An offline unsupervised training strategy that exploits the model
modularity is also developed. We demonstrate our model on three illustrative
examples each highlighting the model's performance in prediction tasks for
fluid systems with large parameter-variations and its stability in long-term
prediction.
</summary>
    <author>
      <name>Francisco J. Gonzalez</name>
    </author>
    <author>
      <name>Maciej Balajewicz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01346v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01346v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07576v1</id>
    <updated>2018-08-22T22:06:26Z</updated>
    <published>2018-08-22T22:06:26Z</published>
    <title>Cooperative SGD: A unified Framework for the Design and Analysis of
  Communication-Efficient SGD Algorithms</title>
    <summary>  State-of-the-art distributed machine learning suffers from significant delays
due to frequent communication and synchronizing between worker nodes. Emerging
communication-efficient SGD algorithms that limit synchronization between
locally trained models have been shown to be effective in speeding-up
distributed SGD. However, a rigorous convergence analysis and comparative study
of different communication-reduction strategies remains a largely open problem.
This paper presents a new framework called Coooperative SGD that subsumes
existing communication-efficient SGD algorithms such as federated-averaging,
elastic-averaging and decentralized SGD. By analyzing Cooperative SGD, we
provide novel convergence guarantees for existing algorithms. Moreover this
framework enables us to design new communication-efficient SGD algorithms that
strike the best balance between reducing communication overhead and achieving
fast error convergence.
</summary>
    <author>
      <name>Jianyu Wang</name>
    </author>
    <author>
      <name>Gauri Joshi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07573v1</id>
    <updated>2018-08-22T21:50:56Z</updated>
    <published>2018-08-22T21:50:56Z</published>
    <title>Approximation Trees: Statistical Stability in Model Distillation</title>
    <summary>  This paper examines the stability of learned explanations for black-box
predictions via model distillation with decision trees. One approach to
intelligibility in machine learning is to use an understandable `student' model
to mimic the output of an accurate `teacher'. Here, we consider the use of
regression trees as a student model, in which nodes of the tree can be used as
`explanations' for particular predictions, and the whole structure of the tree
can be used as a global representation of the resulting function. However,
individual trees are sensitive to the particular data sets used to train them,
and an interpretation of a student model may be suspect if small changes in the
training data have a large effect on it. In this context, access to outcomes
from a teacher helps to stabilize the greedy splitting strategy by generating a
much larger corpus of training examples than was originally available. We
develop tests to ensure that enough examples are generated at each split so
that the same splitting rule would be chosen with high probability were the
tree to be re trained. Further, we develop a stopping rule to indicate how deep
the tree should be built based on recent results on the variability of Random
Forests when these are used as the teacher. We provide concrete examples of
these procedures on the CAD-MDD and COMPAS data sets.
</summary>
    <author>
      <name>Yichen Zhou</name>
    </author>
    <author>
      <name>Zhengze Zhou</name>
    </author>
    <author>
      <name>Giles Hooker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper supercedes arXiv:1610.09036</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07569v1</id>
    <updated>2018-08-22T21:26:06Z</updated>
    <published>2018-08-22T21:26:06Z</published>
    <title>Robust Counterfactual Inferences using Feature Learning and their
  Applications</title>
    <summary>  In a wide variety of applications, including personalization, we want to
measure the difference in outcome due to an intervention and thus have to deal
with counterfactual inference. The feedback from a customer in any of these
situations is only 'bandit feedback' - that is, a partial feedback based on
whether we chose to intervene or not. Typically randomized experiments are
carried out to understand whether an intervention is overall better than no
intervention. Here we present a feature learning algorithm to learn from a
randomized experiment where the intervention in consideration is most effective
and where it is least effective rather than only focusing on the overall
impact, thus adding a context to our learning mechanism and extract more
information. From the randomized experiment, we learn the feature
representations which divide the population into subpopulations where we
observe statistically significant difference in average customer feedback
between those who were subjected to the intervention and those who were not,
with a level of significance l, where l is a configurable parameter in our
model. We use this information to derive the value of the intervention in
consideration for each instance in the population. With experiments, we show
that using this additional learning, in future interventions, the context for
each instance could be leveraged to decide whether to intervene or not.
</summary>
    <author>
      <name>Abhimanyu Mitra</name>
    </author>
    <author>
      <name>Kannan Achan</name>
    </author>
    <author>
      <name>Sushant Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07510v1</id>
    <updated>2018-08-22T18:25:53Z</updated>
    <published>2018-08-22T18:25:53Z</published>
    <title>XPCA: Extending PCA for a Combination of Discrete and Continuous
  Variables</title>
    <summary>  Principal component analysis (PCA) is arguably the most popular tool in
multivariate exploratory data analysis. In this paper, we consider the question
of how to handle heterogeneous variables that include continuous, binary, and
ordinal. In the probabilistic interpretation of low-rank PCA, the data has a
normal multivariate distribution and, therefore, normal marginal distributions
for each column. If some marginals are continuous but not normal, the
semiparametric copula-based principal component analysis (COCA) method is an
alternative to PCA that combines a Gaussian copula with nonparametric
marginals. If some marginals are discrete or semi-continuous, we propose a new
extended PCA (XPCA) method that also uses a Gaussian copula and nonparametric
marginals and accounts for discrete variables in the likelihood calculation by
integrating over appropriate intervals. Like PCA, the factors produced by XPCA
can be used to find latent structure in data, build predictive models, and
perform dimensionality reduction. We present the new model, its induced
likelihood function, and a fitting algorithm which can be applied in the
presence of missing data. We demonstrate how to use XPCA to produce an
estimated full conditional distribution for each data point, and use this to
produce to provide estimates for missing data that are automatically range
respecting. We compare the methods as applied to simulated and real-world data
sets that have a mixture of discrete and continuous variables.
</summary>
    <author>
      <name>Clifford Anderson-Bergman</name>
    </author>
    <author>
      <name>Tamara G. Kolda</name>
    </author>
    <author>
      <name>Kina Kincher-Winoto</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07452v1</id>
    <updated>2018-08-22T17:36:08Z</updated>
    <published>2018-08-22T17:36:08Z</published>
    <title>Generalized Canonical Polyadic Tensor Decomposition</title>
    <summary>  Tensor decomposition is a fundamental unsupervised machine learning method in
data science, with applications including network analysis and sensor data
processing. This work develops a generalized canonical polyadic (GCP) low-rank
tensor decomposition that allows other loss functions besides squared error.
For instance, we can use logistic loss or Kullback-Leibler divergence, enabling
tensor decomposition for binary or count data. We present a variety
statistically-motivated loss functions for various scenarios. We provide a
generalized framework for computing gradients and handling missing data that
enables the use of standard optimization methods for fitting the model. We
demonstrate the flexibility of GCP on several real-world examples including
interactions in a social network, neural activity in a mouse, and monthly
rainfall measurements in India.
</summary>
    <author>
      <name>David Hong</name>
    </author>
    <author>
      <name>Tamara G. Kolda</name>
    </author>
    <author>
      <name>Jed A. Duersch</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07440v1</id>
    <updated>2018-08-22T17:03:10Z</updated>
    <published>2018-08-22T17:03:10Z</published>
    <title>3D Topology Optimization using Convolutional Neural Networks</title>
    <summary>  Topology optimization is computationally demanding that requires the assembly
and solution to a finite element problem for each material distribution
hypothesis. As a complementary alternative to the traditional physics-based
topology optimization, we explore a data-driven approach that can quickly
generate accurate solutions. To this end, we propose a deep learning approach
based on a 3D encoder-decoder Convolutional Neural Network architecture for
accelerating 3D topology optimization and to determine the optimal
computational strategy for its deployment. Analysis of iteration-wise progress
of the Solid Isotropic Material with Penalization process is used as a
guideline to study how the earlier steps of the conventional topology
optimization can be used as input for our approach to predict the final
optimized output structure directly from this input. We conduct a comparative
study between multiple strategies for training the neural network and assess
the effect of using various input combinations for the CNN to finalize the
strategy with the highest accuracy in predictions for practical deployment. For
the best performing network, we achieved about 40% reduction in overall
computation time while also attaining structural accuracies in the order of
96%.
</summary>
    <author>
      <name>Saurabh Banga</name>
    </author>
    <author>
      <name>Harsh Gehani</name>
    </author>
    <author>
      <name>Sanket Bhilare</name>
    </author>
    <author>
      <name>Sagar Patel</name>
    </author>
    <author>
      <name>Levent Kara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is under review in 'Special issue on Computer-Aided Design
  on Advances in Generative Design', 16 Pages, 7 tables, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04572v3</id>
    <updated>2018-08-22T14:48:43Z</updated>
    <published>2018-08-14T08:01:07Z</published>
    <title>Small Sample Learning in Big Data Era</title>
    <summary>  As a promising area in artificial intelligence, a new learning paradigm,
called Small Sample Learning (SSL), has been attracting prominent research
attention in the recent years. In this paper, we aim to present a survey to
comprehensively introduce the current techniques proposed on this topic.
Specifically, current SSL techniques can be mainly divided into two categories.
The first category of SSL approaches can be called "concept learning", which
emphasizes learning new concepts from only few related observations. The
purpose is mainly to simulate human learning behaviors like recognition,
generation, imagination, synthesis and analysis. The second category is called
"experience learning", which usually co-exists with the large sample learning
manner of conventional machine learning. This category mainly focuses on
learning with insufficient samples, and can also be called small data learning
in some literatures. More extensive surveys on both categories of SSL
techniques are introduced and some neuroscience evidences are provided to
clarify the rationality of the entire SSL regime, and the relationship with
human learning process. Some discussions on the main challenges and possible
future research directions along this line are also presented.
</summary>
    <author>
      <name>Jun Shu</name>
    </author>
    <author>
      <name>Zongben Xu</name>
    </author>
    <author>
      <name>Deyu Meng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">76 pages, 15 figures, survey of small sample learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04572v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04572v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07390v1</id>
    <updated>2018-08-22T14:45:46Z</updated>
    <published>2018-08-22T14:45:46Z</published>
    <title>An Explicit Neural Network Construction for Piecewise Constant Function
  Approximation</title>
    <summary>  We present an explicit construction for feedforward neural network (FNN),
which provides a piecewise constant approximation for multivariate functions.
The proposed FNN has two hidden layers, where the weights and thresholds are
explicitly defined and do not require numerical optimization for training.
Unlike most of the existing work on explicit FNN construction, the proposed FNN
does not rely on tensor structure in multiple dimensions. Instead, it
automatically creates Voronoi tessellation of the domain, based on the given
data of the target function, and piecewise constant approximation of the
function. This makes the construction more practical for applications. We
present both theoretical analysis and numerical examples to demonstrate its
properties.
</summary>
    <author>
      <name>Kailiang Wu</name>
    </author>
    <author>
      <name>Dongbin Xiu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07384v1</id>
    <updated>2018-08-22T14:38:45Z</updated>
    <published>2018-08-22T14:38:45Z</published>
    <title>A Note on Inexact Condition for Cubic Regularized Newton's Method</title>
    <summary>  This note considers the inexact cubic-regularized Newton's method (CR), which
has been shown in \cite{Cartis2011a} to achieve the same order-level
convergence rate to a secondary stationary point as the exact CR
\citep{Nesterov2006}. However, the inexactness condition in \cite{Cartis2011a}
is not implementable due to its dependence on future iterates variable. This
note fixes such an issue by proving the same convergence rate for nonconvex
optimization under an inexact adaptive condition that depends on only the
current iterate. Our proof controls the sufficient decrease of the function
value over the total iterations rather than each iteration as used in the
previous studies, which can be of independent interest in other contexts.
</summary>
    <author>
      <name>Zhe Wang</name>
    </author>
    <author>
      <name>Yi Zhou</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <author>
      <name>Guanghui Lan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07383v1</id>
    <updated>2018-08-22T14:30:03Z</updated>
    <published>2018-08-22T14:30:03Z</published>
    <title>Dynamic Self-Attention : Computing Attention over Words Dynamically for
  Sentence Embedding</title>
    <summary>  In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention
mechanism for sentence embedding. We design DSA by modifying dynamic routing in
capsule network (Sabouretal.,2017) for natural language processing. DSA attends
to informative words with a dynamic weight vector. We achieve new
state-of-the-art results among sentence encoding methods in Stanford Natural
Language Inference (SNLI) dataset with the least number of parameters, while
showing comparative results in Stanford Sentiment Treebank (SST) dataset.
</summary>
    <author>
      <name>Deunsol Yoon</name>
    </author>
    <author>
      <name>Dongbok Lee</name>
    </author>
    <author>
      <name>SangKeun Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07382v1</id>
    <updated>2018-08-22T14:28:57Z</updated>
    <published>2018-08-22T14:28:57Z</published>
    <title>Convergence of Cubic Regularization for Nonconvex Optimization under KL
  Property</title>
    <summary>  Cubic-regularized Newton's method (CR) is a popular algorithm that guarantees
to produce a second-order stationary solution for solving nonconvex
optimization problems. However, existing understandings of the convergence rate
of CR are conditioned on special types of geometrical properties of the
objective function. In this paper, we explore the asymptotic convergence rate
of CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of
nonconvex objective functions. In specific, we characterize the asymptotic
convergence rate of various types of optimality measures for CR including
function value gap, variable distance gap, gradient norm and least eigenvalue
of the Hessian matrix. Our results fully characterize the diverse convergence
behaviors of these optimality measures in the full parameter regime of the KL
property. Moreover, we show that the obtained asymptotic convergence rates of
CR are order-wise faster than those of first-order gradient descent algorithms
under the KL property.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <author>
      <name>Zhe Wang</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07475v1</id>
    <updated>2018-08-22T13:38:19Z</updated>
    <published>2018-08-22T13:38:19Z</published>
    <title>Capsule Networks for Protein Structure Classification and Prediction</title>
    <summary>  Capsule Networks have great potential to tackle problems in structural
biology because of their attention to hierarchical relationships. This paper
describes the implementation and application of a Capsule Network architecture
to the classification of RAS protein family structures on GPU-based
computational resources. The proposed Capsule Network trained on 2D and 3D
structural encodings can successfully classify HRAS and KRAS structures. The
Capsule Network can also classify a protein-based dataset derived from a
PSI-BLAST search on sequences of KRAS and HRAS mutations. Our results show an
accuracy improvement compared to traditional convolutional networks, while
improving interpretability through visualization of activation vectors.
</summary>
    <author>
      <name>Dan Rosa de Jesus</name>
    </author>
    <author>
      <name>Julian Cuevas</name>
    </author>
    <author>
      <name>Wilson Rivera</name>
    </author>
    <author>
      <name>Silvia Crivelli</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09900v2</id>
    <updated>2018-08-22T13:19:33Z</updated>
    <published>2018-02-13T08:44:58Z</published>
    <title>Query-Free Attacks on Industry-Grade Face Recognition Systems under
  Resource Constraints</title>
    <summary>  To launch black-box attacks against a Deep Neural Network (DNN) based Face
Recognition (FR) system, one needs to build \textit{substitute} models to
simulate the target model, so the adversarial examples discovered from
substitute models could also mislead the target model. Such
\textit{transferability} is achieved in recent studies through querying the
target model to obtain data for training the substitute models. A real-world
target, likes the FR system of law enforcement, however, is less accessible to
the adversary. To attack such a system, a substitute model with similar quality
as the target model is needed to identify their common defects. This is hard
since the adversary often does not have the enough resources to train such a
powerful model (hundreds of millions of images and rooms of GPUs are needed to
train a commercial FR system).
  We found in our research, however, that a resource-constrained adversary
could still effectively approximate the target model's capability to recognize
\textit{specific} individuals, by training \textit{biased} substitute models on
additional images of those victims whose identities the attacker want to cover
or impersonate. This is made possible by a new property we discovered, called
\textit{Nearly Local Linearity} (NLL), which models the observation that an
ideal DNN model produces the image representations (embeddings) whose distances
among themselves truthfully describe the human perception of the differences
among the input images. By simulating this property around the victim's images,
we significantly improve the transferability of black-box impersonation attacks
by nearly 50\%. Particularly, we successfully attacked a commercial system
trained over 20 million images, using 4 million images and 1/5 of the training
time but achieving 62\% transferability in an impersonation attack and 89\% in
a dodging attack.
</summary>
    <author>
      <name>Di Tang</name>
    </author>
    <author>
      <name>XiaoFeng Wang</name>
    </author>
    <author>
      <name>Kehuan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.09900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11470v2</id>
    <updated>2018-08-22T12:53:06Z</updated>
    <published>2018-07-30T17:59:28Z</published>
    <title>Deep Encoder-Decoder Models for Unsupervised Learning of Controllable
  Speech Synthesis</title>
    <summary>  Generating versatile and appropriate synthetic speech requires control over
the output expression separate from the spoken text. Important non-textual
speech variation is seldom annotated, in which case output control must be
learned in an unsupervised fashion. In this paper, we perform an in-depth study
of methods for unsupervised learning of control in statistical speech
synthesis. For example, we show that popular unsupervised training heuristics
can be interpreted as variational inference in certain autoencoder models. We
additionally connect these models to VQ-VAEs, another, recently-proposed class
of deep variational autoencoders, which we show can be derived from a very
similar mathematical argument. The implications of these new probabilistic
interpretations are discussed. We illustrate the utility of the various
approaches with an application to acoustic modelling for emotional speech
synthesis, where the unsupervised methods for learning expression control
(without access to emotional labels) are found to give results that in many
aspects match or surpass the previous best supervised approach.
</summary>
    <author>
      <name>Gustav Eje Henter</name>
    </author>
    <author>
      <name>Jaime Lorenzo-Trueba</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05527v2</id>
    <updated>2018-08-22T12:50:41Z</updated>
    <published>2018-08-16T15:01:01Z</published>
    <title>Deep Learning for Energy Markets</title>
    <summary>  Deep Learning (DL) provides a methodology to predict extreme loads observed
in energy grids. Forecasting energy loads and prices is challenging due to
sharp peaks and troughs that arise from intraday system constraints due to
supply and demand fluctuations. We propose deep spatio-temporal models and
extreme value theory (DL-EVT) to capture the tail behavior of load spikes. Deep
architectures, such as ReLU and LSTM can model generation trends and temporal
dependencies while EVT captures highly volatile load spikes. To illustrate our
methodology, we use hourly price and demand data from the PJM interconnection
for 4719 nodes and we develop a deep predictor. DL-EVT outperforms traditional
Fourier and time series methods, both in-and out-of-sample, by capturing the
nonlinearities in prices. Finally, we conclude with directions for future
research.
</summary>
    <author>
      <name>Michael Polson</name>
    </author>
    <author>
      <name>Vadim Sokolov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05527v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05527v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07314v1</id>
    <updated>2018-08-22T11:16:59Z</updated>
    <published>2018-08-22T11:16:59Z</published>
    <title>Predicting Musical Sophistication from Music Listening Behaviors: A
  Preliminary Study</title>
    <summary>  Psychological models are increasingly being used to explain online behavioral
traces. Aside from the commonly used personality traits as a general user
model, more domain dependent models are gaining attention. The use of domain
dependent psychological models allows for more fine-grained identification of
behaviors and provide a deeper understanding behind the occurrence of those
behaviors. Understanding behaviors based on psychological models can provide an
advantage over data-driven approaches. For example, relying on psychological
models allow for ways to personalize when data is scarce. In this preliminary
work we look at the relation between users' musical sophistication and their
online music listening behaviors and to what extent we can successfully predict
musical sophistication. An analysis of data from a study with 61 participants
shows that listening behaviors can successfully be used to infer users' musical
sophistication.
</summary>
    <author>
      <name>Bruce Ferwerda</name>
    </author>
    <author>
      <name>Mark Graus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Late-Breaking Results track part of the Twelfth ACM Conference on
  Recommender Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07292v1</id>
    <updated>2018-08-22T09:28:43Z</updated>
    <published>2018-08-22T09:28:43Z</published>
    <title>k-meansNet: When k-means Meets Differentiable Programming</title>
    <summary>  In this paper, we study how to make clustering benefiting from differentiable
programming whose basic idea is treating the neural network as a language
instead of a machine learning method. To this end, we recast the vanilla
$k$-means as a novel feedforward neural network in an elegant way. Our
contribution is two-fold. On the one hand, the proposed \textit{k}-meansNet is
a neural network implementation of the vanilla \textit{k}-means, which enjoys
four advantages highly desired, i.e., robustness to initialization, fast
inference speed, the capability of handling new coming data, and provable
convergence. On the other hand, this work may provide novel insights into
differentiable programming. More specifically, most existing differentiable
programming works unroll an \textbf{optimizer} as a \textbf{recurrent neural
network}, namely, the neural network is employed to solve an existing
optimization problem. In contrast, we reformulate the \textbf{objective
function} of \textit{k}-means as a \textbf{feedforward neural network}, namely,
we employ the neural network to describe a problem. In such a way, we advance
the boundary of differentiable programming by treating the neural network as
from an alternative optimization approach to the problem formulation. Extensive
experimental studies show that our method achieves promising performance
comparing with 12 clustering methods on some challenging datasets.
</summary>
    <author>
      <name>Xi Peng</name>
    </author>
    <author>
      <name>Joey Tianyi Zhou</name>
    </author>
    <author>
      <name>Hongyuan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07288v1</id>
    <updated>2018-08-22T09:15:25Z</updated>
    <published>2018-08-22T09:15:25Z</published>
    <title>Clustering and Labelling Auction Fraud Data</title>
    <summary>  Although shill bidding is a common auction fraud, it is however very tough to
detect. Due to the unavailability and lack of training data, in this study, we
build a high-quality labeled shill bidding dataset based on recently collected
auctions from eBay. Labeling shill biding instances with multidimensional
features is a critical phase for the fraud classification task. For this
purpose, we introduce a new approach to systematically label the fraud data
with the help of the hierarchical clustering CURE that returns remarkable
results as illustrated in the experiments.
</summary>
    <author>
      <name>Ahmad Alzahrani</name>
    </author>
    <author>
      <name>Samira Sadaoui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.6084/m9.figshare.6993308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.6084/m9.figshare.6993308" rel="related"/>
    <link href="http://arxiv.org/abs/1808.07288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07285v1</id>
    <updated>2018-08-22T09:02:53Z</updated>
    <published>2018-08-22T09:02:53Z</published>
    <title>DeepCorr: Strong Flow Correlation Attacks on Tor Using Deep Learning</title>
    <summary>  Flow correlation is the core technique used in a multitude of deanonymization
attacks on Tor. Despite the importance of flow correlation attacks on Tor,
existing flow correlation techniques are considered to be ineffective and
unreliable in linking Tor flows when applied at a large scale, i.e., they
impose high rates of false positive error rates or require impractically long
flow observations to be able to make reliable correlations. In this paper, we
show that, unfortunately, flow correlation attacks can be conducted on Tor
traffic with drastically higher accuracies than before by leveraging emerging
learning mechanisms. We particularly design a system, called DeepCorr, that
outperforms the state-of-the-art by significant margins in correlating Tor
connections. DeepCorr leverages an advanced deep learning architecture to learn
a flow correlation function tailored to Tor's complex network this is in
contrast to previous works' use of generic statistical correlation metrics to
correlated Tor flows. We show that with moderate learning, DeepCorr can
correlate Tor connections (and therefore break its anonymity) with accuracies
significantly higher than existing algorithms, and using substantially shorter
lengths of flow observations. For instance, by collecting only about 900
packets of each target Tor flow (roughly 900KB of Tor data), DeepCorr provides
a flow correlation accuracy of 96% compared to 4% by the state-of-the-art
system of RAPTOR using the same exact setting.
  We hope that our work demonstrates the escalating threat of flow correlation
attacks on Tor given recent advances in learning algorithms, calling for the
timely deployment of effective countermeasures by the Tor community.
</summary>
    <author>
      <name>Milad Nasr</name>
    </author>
    <author>
      <name>Alireza Bahramali</name>
    </author>
    <author>
      <name>Amir Houmansadr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3243734.3243824</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3243734.3243824" rel="related"/>
    <link href="http://arxiv.org/abs/1808.07285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07272v1</id>
    <updated>2018-08-22T08:29:38Z</updated>
    <published>2018-08-22T08:29:38Z</published>
    <title>Deep Adaptive Temporal Pooling for Activity Recognition</title>
    <summary>  Deep neural networks have recently achieved competitive accuracy for human
activity recognition. However, there is room for improvement, especially in
modeling long-term temporal importance and determining the activity relevance
of different temporal segments in a video. To address this problem, we propose
a learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP).
DATP applies a self-attention mechanism to adaptively pool the classification
scores of different video segments. Specifically, using frame-level features,
DATP regresses importance of different temporal segments and generates weights
for them. Remarkably, DATP is trained using only the video-level label. There
is no need of additional supervision except video-level activity class label.
We conduct extensive experiments to investigate various input features and
different weight models. Experimental results show that DATP can learn to
assign large weights to key video segments. More importantly, DATP can improve
training of frame-level feature extractor. This is because relevant temporal
segments are assigned large weights during back-propagation. Overall, we
achieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.
</summary>
    <author>
      <name>Sibo Song</name>
    </author>
    <author>
      <name>Ngai-Man Cheung</name>
    </author>
    <author>
      <name>Vijay Chandrasekhar</name>
    </author>
    <author>
      <name>Bappaditya Mandal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240508.3240713</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240508.3240713" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM Multimedia 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07270v1</id>
    <updated>2018-08-22T08:29:16Z</updated>
    <published>2018-08-22T08:29:16Z</published>
    <title>Learning to Support: Exploiting Structure Information in Support Sets
  for One-Shot Learning</title>
    <summary>  Deep Learning shows very good performance when trained on large labeled data
sets. The problem of training a deep net on a few or one sample per class
requires a different learning approach which can generalize to unseen classes
using only a few representatives of these classes. This problem has previously
been approached by meta-learning. Here we propose a novel meta-learner which
shows state-of-the-art performance on common benchmarks for one/few shot
classification. Our model features three novel components: First is a
feed-forward embedding that takes random class support samples (after a
customary CNN embedding) and transfers them to a better class representation in
terms of a classification problem. Second is a novel attention mechanism,
inspired by competitive learning, which causes class representatives to compete
with each other to become a temporary class prototype with respect to the query
point. This mechanism allows switching between representatives depending on the
position of the query point. Once a prototype is chosen for each class, the
predicated label is computed using a simple attention mechanism over prototypes
of all considered classes. The third feature is the ability of our meta-learner
to incorporate deeper CNN embedding, enabling larger capacity. Finally, to ease
the training procedure and reduce overfitting, we averages the top $t$ models
(evaluated on the validation) over the optimization trajectory. We show that
this approach can be viewed as an approximation to an ensemble, which saves the
factor of $t$ in training and test times and the factor of of $t$ in the
storage of the final model.
</summary>
    <author>
      <name>Jinchao Liu</name>
    </author>
    <author>
      <name>Stuart J. Gibson</name>
    </author>
    <author>
      <name>Margarita Osadchy</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07260v1</id>
    <updated>2018-08-22T07:55:29Z</updated>
    <published>2018-08-22T07:55:29Z</published>
    <title>On an improvement of LASSO by scaling</title>
    <summary>  A sparse modeling is a major topic in machine learning and statistics. LASSO
(Least Absolute Shrinkage and Selection Operator) is a popular sparse modeling
method while it has been known to yield unexpected large bias especially at a
sparse representation. There have been several studies for improving this
problem such as the introduction of non-convex regularization terms. The
important point is that this bias problem directly affects model selection in
applications since a sparse representation cannot be selected by a prediction
error based model selection even if it is a good representation. In this
article, we considered to improve this problem by introducing a scaling that
expands LASSO estimator to compensate excessive shrinkage, thus a large bias in
LASSO estimator. We here gave an empirical value for the amount of scaling.
There are two advantages of this scaling method as follows. Since the proposed
scaling value is calculated by using LASSO estimator, we only need LASSO
estimator that is obtained by a fast and stable optimization procedure such as
LARS (Least Angle Regression) under LASSO modification or coordinate descent.
And, the simplicity of our scaling method enables us to derive SURE (Stein's
Unbiased Risk Estimate) under the modified LASSO estimator with scaling. Our
scaling method together with model selection based on SURE is fully empirical
and do not need additional hyper-parameters. In a simple numerical example, we
verified that our scaling method actually improves LASSO and the SURE based
model selection criterion can stably choose an appropriate sparse model.
</summary>
    <author>
      <name>Katsuyuki Hagiwara</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07258v1</id>
    <updated>2018-08-22T07:51:22Z</updated>
    <published>2018-08-22T07:51:22Z</published>
    <title>Escaping from Collapsing Modes in a Constrained Space</title>
    <summary>  Generative adversarial networks (GANs) often suffer from unpredictable
mode-collapsing during training. We study the issue of mode collapse of
Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of
the state-of-the-art generative models. Despite its potential of generating
high-quality images, we find that BEGAN tends to collapse at some modes after a
period of training. We propose a new model, called \emph{BEGAN with a
Constrained Space} (BEGAN-CS), which includes a latent-space constraint in the
loss function. We show that BEGAN-CS can significantly improve training
stability and suppress mode collapse without either increasing the model
complexity or degrading the image quality. Further, we visualize the
distribution of latent vectors to elucidate the effect of latent-space
constraint. The experimental results show that our method has additional
advantages of being able to train on small datasets and to generate images
similar to a given real image yet with variations of designated attributes
on-the-fly.
</summary>
    <author>
      <name>Chia-Che Chang</name>
    </author>
    <author>
      <name>Chieh Hubert Lin</name>
    </author>
    <author>
      <name>Che-Rung Lee</name>
    </author>
    <author>
      <name>Da-Cheng Juan</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Hwann-Tzong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06464v2</id>
    <updated>2018-08-22T07:28:28Z</updated>
    <published>2017-11-17T09:29:52Z</published>
    <title>A unified deep artificial neural network approach to partial
  differential equations in complex geometries</title>
    <summary>  In this paper we use deep feedforward artificial neural networks to
approximate solutions to partial differential equations in complex geometries.
We show how to modify the backpropagation algorithm to compute the partial
derivatives of the network output with respect to the space variables which is
needed to approximate the differential operator. The method is based on an
ansatz for the solution which requires nothing but feedforward neural networks
and an unconstrained gradient based optimization method such as gradient
descent or a quasi-Newton method.
  We show an example where classical mesh based methods cannot be used and
neural networks can be seen as an attractive alternative. Finally, we highlight
the benefits of deep compared to shallow neural networks and device some other
convergence enhancing techniques.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nyström</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2018.06.056</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2018.06.056" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06464v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06464v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07251v1</id>
    <updated>2018-08-22T07:27:26Z</updated>
    <published>2018-08-22T07:27:26Z</published>
    <title>Genie: An Open Box Counterfactual Policy Estimator for Optimizing
  Sponsored Search Marketplace</title>
    <summary>  In this paper, we propose an offline counterfactual policy estimation
framework called Genie to optimize Sponsored Search Marketplace. Genie employs
an open box simulation engine with click calibration model to compute the KPI
impact of any modification to the system. From the experimental results on Bing
traffic, we showed that Genie performs better than existing observational
approaches that employs randomized experiments for traffic slices that have
frequent policy updates. We also show that Genie can be used to tune completely
new policies efficiently without creating risky randomized experiments due to
cold start problem. As time of today, Genie hosts more than 10000 optimization
jobs yearly which runs more than 30 Million processing node hours of big data
jobs for Bing Ads. For the last 3 years, Genie has been proven to be the one of
the major platforms to optimize Bing Ads Marketplace due to its reliability
under frequent policy changes and its efficiency to minimize risks in real
experiments.
</summary>
    <author>
      <name>Murat Ali Bayir</name>
    </author>
    <author>
      <name>Mingsen Xu</name>
    </author>
    <author>
      <name>Yaojia Zhu</name>
    </author>
    <author>
      <name>Yifan Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07249v1</id>
    <updated>2018-08-22T07:20:04Z</updated>
    <published>2018-08-22T07:20:04Z</published>
    <title>Analysis of Network Lasso For Semi-Supervised Regression</title>
    <summary>  We characterize the statistical properties of network Lasso for
semi-supervised regression problems involving network- structured data. This
characterization is based on the con- nectivity properties of the empirical
graph which encodes the similarities between individual data points. Loosely
speaking, network Lasso is accurate if the available label informa- tion is
well connected with the boundaries between clusters of the network-structure
datasets. We make this property precise using the notion of network flows. In
particular, the existence of a sufficiently large network flow over the
empirical graph implies a network compatibility condition which, in turn, en-
sures accuracy of network Lasso.
</summary>
    <author>
      <name>Alexander Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07243v1</id>
    <updated>2018-08-22T06:55:44Z</updated>
    <published>2018-08-22T06:55:44Z</published>
    <title>Controversy Rules - Discovering Regions Where Classifiers (Dis-)Agree
  Exceptionally</title>
    <summary>  Finding regions for which there is higher controversy among different
classifiers is insightful with regards to the domain and our models. Such
evaluation can falsify assumptions, assert some, or also, bring to the
attention unknown phenomena. The present work describes an algorithm, which is
based on the Exceptional Model Mining framework, and enables that kind of
investigations. We explore several public datasets and show the usefulness of
this approach in classification tasks. We show in this paper a few interesting
observations about those well explored datasets, some of which are general
knowledge, and other that as far as we know, were not reported before.
</summary>
    <author>
      <name>Oren Zeev-Ben-Mordehai</name>
    </author>
    <author>
      <name>Wouter Duivesteijn</name>
    </author>
    <author>
      <name>Mykola Pechenizkiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08280v1</id>
    <updated>2018-08-22T06:08:45Z</updated>
    <published>2018-08-22T06:08:45Z</published>
    <title>Deep multiscale convolutional feature learning for weakly supervised
  localization of chest pathologies in X-ray images</title>
    <summary>  Localization of chest pathologies in chest X-ray images is a challenging task
because of their varying sizes and appearances. We propose a novel weakly
supervised method to localize chest pathologies using class aware deep
multiscale feature learning. Our method leverages intermediate feature maps
from CNN layers at different stages of a deep network during the training of a
classification model using image level annotations of pathologies. During the
training phase, a set of \emph{layer relevance weights} are learned for each
pathology class and the CNN is optimized to perform pathology classification by
convex combination of feature maps from both shallow and deep layers using the
learned weights. During the test phase, to localize the predicted pathology,
the multiscale attention map is obtained by convex combination of class
activation maps from each stage using the \emph{layer relevance weights}
learned during the training phase. We have validated our method using 112000
X-ray images and compared with the state-of-the-art localization methods. We
experimentally demonstrate that the proposed weakly supervised method can
improve the localization performance of small pathologies such as nodule and
mass while giving comparable performance for bigger pathologies e.g.,
Cardiomegaly
</summary>
    <author>
      <name>Suman Sedai</name>
    </author>
    <author>
      <name>Dwarikanath Mahapatra</name>
    </author>
    <author>
      <name>Zongyuan Ge</name>
    </author>
    <author>
      <name>Rajib Chakravorty</name>
    </author>
    <author>
      <name>Rahil Garnavi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06314v2</id>
    <updated>2018-08-22T05:25:38Z</updated>
    <published>2018-08-20T05:47:41Z</published>
    <title>A General Framework of Multi-Armed Bandit Processes by Arm Switch
  Restrictions</title>
    <summary>  This paper proposes a general framework of multi-armed bandit (MAB) processes
by introducing a type of restrictions on the switches among arms evolving in
continuous time.
  The Gittins index process is constructed for any single arm subject to the
restrictions on switches and then the optimality of the corresponding Gittins
index rule is established. The Gittins indices defined in this paper are
consistent with the ones for MAB processes in continuous time, integer time,
semi-Markovian setting as well as general discrete time setting, so that the
new theory covers the classical models as special cases and also applies to
many other situations that have not yet been touched in the literature. While
the proof of the optimality of Gittins index policies benefits from ideas in
the existing theory of MAB processes in continuous time, new techniques are
introduced which drastically simplify the proof.
</summary>
    <author>
      <name>Wenqing Bao</name>
    </author>
    <author>
      <name>Xiaoqiang Cai</name>
    </author>
    <author>
      <name>Xianyi Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C39, 93E20, 93E35, 49L20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07217v1</id>
    <updated>2018-08-22T04:50:55Z</updated>
    <published>2018-08-22T04:50:55Z</published>
    <title>Don't Use Large Mini-Batches, Use Local SGD</title>
    <summary>  Mini-batch stochastic gradient methods are the current state of the art for
large-scale distributed training of neural networks and other machine learning
models. However, they fail to adapt to a changing communication vs computation
trade-off in a system, such as when scaling to a large number of workers or
devices. More so, the fixed requirement of communication bandwidth for gradient
exchange severely limits the scalability to multi-node training e.g. in
datacenters, and even more so for training on decentralized networks such as
mobile devices. We argue that variants of local SGD, which perform several
update steps on a local model before communicating to other nodes, offer
significantly improved overall performance and communication efficiency, as
well as adaptivity to the underlying system resources. Furthermore, we present
a new hierarchical extension of local SGD, and demonstrate that it can
efficiently adapt to several levels of computation costs in a heterogeneous
distributed system.
</summary>
    <author>
      <name>Tao Lin</name>
    </author>
    <author>
      <name>Sebastian U. Stich</name>
    </author>
    <author>
      <name>Martin Jaggi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07216v1</id>
    <updated>2018-08-22T04:24:30Z</updated>
    <published>2018-08-22T04:24:30Z</published>
    <title>Model Interpretation: A Unified Derivative-based Framework for
  Nonparametric Regression and Supervised Machine Learning</title>
    <summary>  Interpreting a nonparametric regression model with many predictors is known
to be a challenging problem. There has been renewed interest in this topic due
to the extensive use of machine learning algorithms and the difficulty in
understanding and explaining their input-output relationships. This paper
develops a unified framework using a derivative-based approach for existing
tools in the literature, including the partial-dependence plots, marginal plots
and accumulated effects plots. It proposes a new interpretation technique
called the accumulated total derivative effects plot and demonstrates how its
components can be used to develop extensive insights in complex regression
models with correlated predictors. The techniques are illustrated through
simulation results.
</summary>
    <author>
      <name>Xiaoyu Liu</name>
    </author>
    <author>
      <name>Jie Chen</name>
    </author>
    <author>
      <name>Vijayan Nair</name>
    </author>
    <author>
      <name>Agus Sudjianto</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07209v1</id>
    <updated>2018-08-22T03:55:33Z</updated>
    <published>2018-08-22T03:55:33Z</published>
    <title>Coarse-to-Fine Annotation Enrichment for Semantic Segmentation Learning</title>
    <summary>  Rich high-quality annotated data is critical for semantic segmentation
learning, yet acquiring dense and pixel-wise ground-truth is both labor- and
time-consuming. Coarse annotations (e.g., scribbles, coarse polygons) offer an
economical alternative, with which training phase could hardly generate
satisfactory performance unfortunately. In order to generate high-quality
annotated data with a low time cost for accurate segmentation, in this paper,
we propose a novel annotation enrichment strategy, which expands existing
coarse annotations of training data to a finer scale. Extensive experiments on
the Cityscapes and PASCAL VOC 2012 benchmarks have shown that the neural
networks trained with the enriched annotations from our framework yield a
significant improvement over that trained with the original coarse labels. It
is highly competitive to the performance obtained by using human annotated
dense annotations. The proposed method also outperforms among other
state-of-the-art weakly-supervised segmentation methods.
</summary>
    <author>
      <name>Yadan Luo</name>
    </author>
    <author>
      <name>Ziwei Wang</name>
    </author>
    <author>
      <name>Zi Huang</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Cong Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM 2018 International Conference on Information and Knowledge
  Management</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06661v2</id>
    <updated>2018-08-22T01:32:59Z</updated>
    <published>2018-08-20T19:24:45Z</published>
    <title>A Hybrid Differential Evolution Approach to Designing Deep Convolutional
  Neural Networks for Image Classification</title>
    <summary>  Convolutional Neural Networks (CNNs) have demonstrated their superiority in
image classification, and evolutionary computation (EC) methods have recently
been surging to automatically design the architectures of CNNs to save the
tedious work of manually designing CNNs. In this paper, a new hybrid
differential evolution (DE) algorithm with a newly added crossover operator is
proposed to evolve the architectures of CNNs of any lengths, which is named
DECNN. There are three new ideas in the proposed DECNN method. Firstly, an
existing effective encoding scheme is refined to cater for variable-length CNN
architectures; Secondly, the new mutation and crossover operators are developed
for variable-length DE to optimise the hyperparameters of CNNs; Finally, the
new second crossover is introduced to evolve the depth of the CNN
architectures. The proposed algorithm is tested on six widely-used benchmark
datasets and the results are compared to 12 state-of-the-art methods, which
shows the proposed method is vigorously competitive to the state-of-the-art
algorithms. Furthermore, the proposed method is also compared with a method
using particle swarm optimisation with a similar encoding strategy named IPPSO,
and the proposed DECNN outperforms IPPSO in terms of the accuracy.
</summary>
    <author>
      <name>Bin Wang</name>
    </author>
    <author>
      <name>Yanan Sun</name>
    </author>
    <author>
      <name>Bing Xue</name>
    </author>
    <author>
      <name>Mengjie Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by The Australasian Joint Conference on Artificial
  Intelligence 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06661v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06661v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07172v1</id>
    <updated>2018-08-22T01:04:07Z</updated>
    <published>2018-08-22T01:04:07Z</published>
    <title>Fisher Information and Natural Gradient Learning of Random Deep Networks</title>
    <summary>  A deep neural network is a hierarchical nonlinear model transforming input
signals to output signals. Its input-output relation is considered to be
stochastic, being described for a given input by a parameterized conditional
probability distribution of outputs. The space of parameters consisting of
weights and biases is a Riemannian manifold, where the metric is defined by the
Fisher information matrix. The natural gradient method uses the steepest
descent direction in a Riemannian manifold, so it is effective in learning,
avoiding plateaus. It requires inversion of the Fisher information matrix,
however, which is practically impossible when the matrix has a huge number of
dimensions. Many methods for approximating the natural gradient have therefore
been introduced. The present paper uses statistical neurodynamical method to
reveal the properties of the Fisher information matrix in a net of random
connections under the mean field approximation. We prove that the Fisher
information matrix is unit-wise block diagonal supplemented by small order
terms of off-block-diagonal elements, which provides a justification for the
quasi-diagonal natural gradient method by Y. Ollivier. A unitwise
block-diagonal Fisher metrix reduces to the tensor product of the Fisher
information matrices of single units. We further prove that the Fisher
information matrix of a single unit has a simple reduced form, a sum of a
diagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the
inverse of Fisher information explicitly. We then have an explicit form of the
natural gradient, without relying on the numerical matrix inversion, which
drastically speeds up stochastic gradient learning.
</summary>
    <author>
      <name>Shun-ichi Amari</name>
    </author>
    <author>
      <name>Ryo Karakida</name>
    </author>
    <author>
      <name>Masafumi Oizumi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07169v1</id>
    <updated>2018-08-22T00:57:41Z</updated>
    <published>2018-08-22T00:57:41Z</published>
    <title>Statistical Neurodynamics of Deep Networks: Geometry of Signal Spaces</title>
    <summary>  Statistical neurodynamics studies macroscopic behaviors of randomly connected
neural networks. We consider a deep layered feedforward network where input
signals are processed layer by layer. The manifold of input signals is embedded
in a higher dimensional manifold of the next layer as a curved submanifold,
provided the number of neurons is larger than that of inputs. We show
geometrical features of the embedded manifold, proving that the manifold
enlarges or shrinks locally isotropically so that it is always embedded
conformally. We study the curvature of the embedded manifold. The scalar
curvature converges to a constant or diverges to infinity slowly. The distance
between two signals also changes, converging eventually to a stable fixed
value, provided both the number of neurons in a layer and the number of layers
tend to infinity. This causes a problem, since when we consider a curve in the
input space, it is mapped as a continuous curve of fractal nature, but our
theory contradictorily suggests that the curve eventually converges to a
discrete set of equally spaced points. In reality, the numbers of neurons and
layers are finite and thus, it is expected that the finite size effect causes
the discrepancies between our theory and reality. We need to further study the
discrepancies to understand their implications on information processing.
</summary>
    <author>
      <name>Shun-ichi Amari</name>
    </author>
    <author>
      <name>Ryo Karakida</name>
    </author>
    <author>
      <name>Masafumi Oizumi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07168v1</id>
    <updated>2018-08-22T00:51:57Z</updated>
    <published>2018-08-22T00:51:57Z</published>
    <title>On Deep Neural Networks for Detecting Heart Disease</title>
    <summary>  Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as "at risk." Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5's design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
</summary>
    <author>
      <name>Nathalie-Sofia Tomov</name>
    </author>
    <author>
      <name>Stanimire Tomov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04577v3</id>
    <updated>2018-08-21T22:41:34Z</updated>
    <published>2018-04-12T15:46:12Z</published>
    <title>Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and
  Some New Implementations</title>
    <summary>  In this paper we discuss policy iteration methods for approximate solution of
a finite-state discounted Markov decision problem, with a focus on
feature-based aggregation methods and their connection with deep reinforcement
learning schemes. We introduce features of the states of the original problem,
and we formulate a smaller "aggregate" Markov decision problem, whose states
relate to the features. We discuss properties and possible implementations of
this type of aggregation, including a new approach to approximate policy
iteration. In this approach the policy improvement operation combines
feature-based aggregation with feature construction using deep neural networks
or other calculations. We argue that the cost function of a policy may be
approximated much more accurately by the nonlinear function of the features
provided by aggregation, than by the linear function of the features provided
by neural network-based reinforcement learning, thereby potentially leading to
more effective policy improvement.
</summary>
    <author>
      <name>Dimitri P. Bertsekas</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04577v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04577v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="49, 90, 93" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05236v3</id>
    <updated>2018-08-21T22:16:21Z</updated>
    <published>2018-01-16T13:06:12Z</published>
    <title>MORF: A Framework for Predictive Modeling and Replication At Scale With
  Privacy-Restricted MOOC Data</title>
    <summary>  Big data repositories from online learning platforms such as Massive Open
Online Courses (MOOCs) represent an unprecedented opportunity to advance
research on education at scale and impact a global population of learners. To
date, such research has been hindered by poor reproducibility and a lack of
replication, largely due to three types of barriers: experimental, inferential,
and data. We present a novel system for large-scale computational research, the
MOOC Replication Framework (MORF), to jointly address these barriers. We
discuss MORF's architecture, an open-source platform-as-a-service (PaaS) which
includes a simple, flexible software API providing for multiple modes of
research (predictive modeling or production rule analysis) integrated with a
high-performance computing environment. All experiments conducted on MORF use
executable Docker containers which ensure complete reproducibility while
allowing for the use of any software or language which can be installed in the
linux-based Docker container. Each experimental artifact is assigned a DOI and
made publicly available. MORF has the potential to accelerate and democratize
research on its massive data repository, which currently includes over 200
MOOCs, as demonstrated by initial research conducted on the platform. We also
highlight ways in which MORF represents a solution template to a more general
class of problems faced by computational researchers in other domains.
</summary>
    <author>
      <name>Josh Gardner</name>
    </author>
    <author>
      <name>Christopher Brooks</name>
    </author>
    <author>
      <name>Juan Miguel L. Andres</name>
    </author>
    <author>
      <name>Ryan Baker</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05236v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05236v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04295v2</id>
    <updated>2018-08-21T21:01:43Z</updated>
    <published>2018-08-13T15:40:41Z</published>
    <title>Understanding training and generalization in deep learning by Fourier
  analysis</title>
    <summary>  Background: It is still an open research area to theoretically understand why
Deep Neural Networks (DNNs)---equipped with many more parameters than training
data and trained by (stochastic) gradient-based methods---often achieve
remarkably low generalization error. Contribution: We study DNN training by
Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic)
gradient-based methods endows low-frequency components of the target function
with a higher priority during the training; ii) Small initialization leads to
good generalization ability of DNN while preserving the DNN's ability of
fitting any function. These results are further confirmed by experiments of
DNNs fitting the following datasets, i.e., natural images, one-dimensional
functions and MNIST dataset.
</summary>
    <author>
      <name>Zhiqin John Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04295v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04295v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32, 68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07089v1</id>
    <updated>2018-08-21T19:14:16Z</updated>
    <published>2018-08-21T19:14:16Z</published>
    <title>CoBaR: Confidence-Based Recommender</title>
    <summary>  Neighborhood-based collaborative filtering algorithms usually adopt a fixed
neighborhood size for every user or item, although groups of users or items may
have different lengths depending on users' preferences. In this paper, we
propose an extension to a non-personalized recommender based on confidence
intervals and hierarchical clustering to generate groups of users with optimal
sizes. The evaluation shows that the proposed technique outperformed the
traditional recommender algorithms in four publicly available datasets.
</summary>
    <author>
      <name>Fernando S. Aguiar Neto</name>
    </author>
    <author>
      <name>Arthur F. da Costa</name>
    </author>
    <author>
      <name>Marcelo G. Manzato</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11374v2</id>
    <updated>2018-08-21T18:40:09Z</updated>
    <published>2018-07-24T17:58:56Z</published>
    <title>Weakly-Supervised Deep Learning of Heat Transport via Physics Informed
  Loss</title>
    <summary>  In typical machine learning tasks and applications, it is necessary to obtain
or create large labeled datasets in order to to achieve high performance.
Unfortunately, large labeled datasets are not always available and can be
expensive to source, creating a bottleneck towards more widely applicable
machine learning. The paradigm of weak supervision offers an alternative that
allows for integration of domain-specific knowledge by enforcing constraints
that a correct solution to the learning problem will obey over the output
space. In this work, we explore the application of this paradigm to 2-D
physical systems governed by non-linear differential equations. We demonstrate
that knowledge of the partial differential equations governing a system can be
encoded into the loss function of a neural network via an appropriately chosen
convolutional kernel. We demonstrate this by showing that the steady-state
solution to the 2-D heat equation can be learned directly from initial
conditions by a convolutional neural network, in the absence of labeled
training data. We also extend recent work in the progressive growing of fully
convolutional networks to achieve high accuracy (&lt; 1.5% error) at multiple
scales of the heat-flow problem, including at the very large scale (1024x1024).
Finally, we demonstrate that this method can be used to speed up exact
calculation of the solution to the differential equations via finite
difference.
</summary>
    <author>
      <name>Rishi Sharma</name>
    </author>
    <author>
      <name>Amir Barati Farimani</name>
    </author>
    <author>
      <name>Joe Gomes</name>
    </author>
    <author>
      <name>Peter Eastman</name>
    </author>
    <author>
      <name>Vijay Pande</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01251v2</id>
    <updated>2018-08-21T17:53:43Z</updated>
    <published>2018-07-03T15:50:41Z</published>
    <title>Training behavior of deep neural network in frequency domain</title>
    <summary>  Why deep neural networks (DNNs) capable of overfitting often generalize well
in practice is a mystery in deep learning. Existing works indicate that this
observation holds for both complicated real datasets and simple datasets of
one-dimensional (1-d) functions. In this work, for natural images and
low-frequency dominant 1-d functions, we empirically found that a DNN with
common settings first quickly captures the dominant low-frequency components,
and then relatively slowly captures high-frequency ones. We call this
phenomenon Frequency Principle (F-Principle). F-Principle can be observed over
various DNN setups of different activation functions, layer structures and
training algorithms in our experiments. F-Principle can be used to understand
(i) the behavior of DNN training in the information plane and (ii) why DNNs
often generalize well albeit its ability of overfitting. This F-Principle
potentially can provide insights into understanding the general principle
underlying DNN optimization and generalization for real datasets.
</summary>
    <author>
      <name>Zhi-Qin J. Xu</name>
    </author>
    <author>
      <name>Yaoyu Zhang</name>
    </author>
    <author>
      <name>Yanyang Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07042v1</id>
    <updated>2018-08-21T17:52:02Z</updated>
    <published>2018-08-21T17:52:02Z</published>
    <title>CoQA: A Conversational Question Answering Challenge</title>
    <summary>  Humans gather information by engaging in conversations involving a series of
interconnected questions and answers. For machines to assist in information
gathering, it is therefore essential to enable them to answer conversational
questions. We introduce CoQA, a novel dataset for building Conversational
Question Answering systems. Our dataset contains 127k questions with answers,
obtained from 8k conversations about text passages from seven diverse domains.
The questions are conversational, and the answers are free-form text with their
corresponding evidence highlighted in the passage. We analyze CoQA in depth and
show that conversational questions have challenging phenomena not present in
existing reading comprehension datasets, e.g., coreference and pragmatic
reasoning. We evaluate strong conversational and reading comprehension models
on CoQA. The best system obtains an F1 score of 65.1%, which is 23.7 points
behind human performance (88.8%), indicating there is ample room for
improvement. We launch CoQA as a challenge to the community at
http://stanfordnlp.github.io/coqa/
</summary>
    <author>
      <name>Siva Reddy</name>
    </author>
    <author>
      <name>Danqi Chen</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04926v2</id>
    <updated>2018-08-21T16:48:54Z</updated>
    <published>2018-08-14T23:59:26Z</published>
    <title>How Much Reading Does Reading Comprehension Require? A Critical
  Investigation of Popular Benchmarks</title>
    <summary>  Many recent papers address reading comprehension, where examples consist of
(question, passage, answer) tuples. Presumably, a model must combine
information from both questions and passages to predict corresponding answers.
However, despite intense interest in the topic, with hundreds of published
papers vying for leaderboard dominance, basic questions about the difficulty of
many popular benchmarks remain unanswered. In this paper, we establish sensible
baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding
that question- and passage-only models often perform surprisingly well. On $14$
out of $20$ bAbI tasks, passage-only models achieve greater than $50\%$
accuracy, sometimes matching the full model. Interestingly, while CBT provides
$20$-sentence stories only the last is needed for comparably accurate
prediction. By comparison, SQuAD and CNN appear better-constructed.
</summary>
    <author>
      <name>Divyansh Kaushik</name>
    </author>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04926v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04926v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06996v1</id>
    <updated>2018-08-21T16:16:46Z</updated>
    <published>2018-08-21T16:16:46Z</published>
    <title>Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models
  and Phase Retrieval</title>
    <summary>  We study the fundamental tradeoffs between statistical accuracy and
computational tractability in the analysis of high dimensional heterogeneous
data. As examples, we study sparse Gaussian mixture model, mixture of sparse
linear regressions, and sparse phase retrieval model. For these models, we
exploit an oracle-based computational model to establish conjecture-free
computationally feasible minimax lower bounds, which quantify the minimum
signal strength required for the existence of any algorithm that is both
computationally tractable and statistically accurate. Our analysis shows that
there exist significant gaps between computationally feasible minimax risks and
classical ones. These gaps quantify the statistical price we must pay to
achieve computational tractability in the presence of data heterogeneity. Our
results cover the problems of detection, estimation, support recovery, and
clustering, and moreover, resolve several conjectures of Azizyan et al. (2013,
2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our
results reveal a new but counter-intuitive phenomenon in heterogeneous data
analysis that more data might lead to less computation complexity.
</summary>
    <author>
      <name>Jianqing Fan</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Zhuoran Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">75 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06992v1</id>
    <updated>2018-08-21T16:14:55Z</updated>
    <published>2018-08-21T16:14:55Z</published>
    <title>Optimizing the Union of Intersections LASSO ($UoI_{LASSO}$) and Vector
  Autoregressive ($UoI_{VAR}$) Algorithms for Improved Statistical Estimation
  at Scale</title>
    <summary>  The analysis of scientific data of increasing size and complexity requires
statistical machine learning methods that are both interpretable and
predictive. Union of Intersections (UoI), a recently developed framework, is a
two-step approach that separates model selection and model estimation. A linear
regression algorithm based on UoI, $UoI_{LASSO}$, simultaneously achieves low
false positives and low false negative feature selection as well as low bias
and low variance estimates. Together, these qualities make the results both
predictive and interpretable. In this paper, we optimize the $UoI_{LASSO}$
algorithm for single-node execution on NERSC's Cori Knights Landing, a Xeon Phi
based supercomputer. We then scale $UoI_{LASSO}$ to execute on cores ranging
from 68-278,528 cores on a range of dataset sizes demonstrating the weak and
strong scaling of the implementation. We also implement a variant of
$UoI_{LASSO}$, $UoI_{VAR}$ for vector autoregressive models, to analyze high
dimensional time-series data. We perform single node optimization and
multi-node scaling experiments for $UoI_{VAR}$ to demonstrate the effectiveness
of the algorithm for weak and strong scaling. Our implementations enable to use
estimate the largest VAR model (1000 nodes) we are aware of, and apply it to
large neurophysiology data 192 nodes).
</summary>
    <author>
      <name>Mahesh Balasubramanian</name>
    </author>
    <author>
      <name>Trevor Ruiz</name>
    </author>
    <author>
      <name>Brandon Cook</name>
    </author>
    <author>
      <name>Sharmodeep Bhattacharyya</name>
    </author>
    <author>
      <name> Prabhat</name>
    </author>
    <author>
      <name>Aviral Shrivastava</name>
    </author>
    <author>
      <name>Kristofer Bouchard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00939v2</id>
    <updated>2018-08-21T16:00:02Z</updated>
    <published>2018-07-02T04:21:10Z</published>
    <title>Mining Illegal Insider Trading of Stocks: A Proactive Approach</title>
    <summary>  Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
</summary>
    <author>
      <name>Sheikh Rabiul Islam</name>
    </author>
    <author>
      <name>Sheikh Khaled Ghafoor</name>
    </author>
    <author>
      <name>William Eberle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Previous version (A Deep Learning Based Illegal Insider-Trading
  Detection and Prediction Technique in Stock Market ) was accepted for ICDATA,
  2018. But we didn't publish it there as the work was not adequately complete
  and the writing style wasn't good enough. We revamped paper and extended to a
  new perspective for IEEE Big Data, 2018 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00939v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00939v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05965v2</id>
    <updated>2018-08-21T15:38:25Z</updated>
    <published>2018-08-17T18:11:37Z</published>
    <title>On Geometric Analysis of Affine Sparse Subspace Clustering</title>
    <summary>  Sparse subspace clustering (SSC) is a state-of-the-art method for segmenting
a set of data points drawn from a union of subspaces into their respective
subspaces. It is now well understood that SSC produces subspace-preserving data
affinity under broad geometric conditions but suffers from a connectivity
issue. In this paper, we develop a novel geometric analysis for a variant of
SSC, named affine SSC (ASSC), for the problem of clustering data from a union
of affine subspaces. Our contributions include a new concept called affine
independence for capturing the arrangement of a collection of affine subspaces.
Under the affine independence assumption, we show that ASSC is guaranteed to
produce subspace-preserving affinity. Moreover, inspired by the phenomenon that
the $\ell_1$ regularization no longer induces sparsity when the solution is
nonnegative, we further show that subspace-preserving recovery can be achieved
under much weaker conditions for all data points other than the extreme points
of samples from each subspace. In addition, we confirm a curious observation
that the affinity produced by ASSC may be subspace-dense---which could
guarantee the subspace-preserving affinity of ASSC to produce correct
clustering under rather weak conditions. We validate the theoretical findings
on carefully designed synthetic data and evaluate the performance of ASSC on
several real data sets.
</summary>
    <author>
      <name>Chun-Guang Li</name>
    </author>
    <author>
      <name>Chong You</name>
    </author>
    <author>
      <name>René Vidal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures, 2 tables. To appear on IEEE Journal of Selected
  Topics in Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05965v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05965v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06934v1</id>
    <updated>2018-08-21T14:41:56Z</updated>
    <published>2018-08-21T14:41:56Z</published>
    <title>Backpropagation and Biological Plausibility</title>
    <summary>  By and large, Backpropagation (BP) is regarded as one of the most important
neural computation algorithms at the basis of the progress in machine learning,
including the recent advances in deep learning. However, its computational
structure has been the source of many debates on its arguable biological
plausibility. In this paper, it is shown that when framing supervised learning
in the Lagrangian framework, while one can see a natural emergence of
Backpropagation, biologically plausible local algorithms can also be devised
that are based on the search for saddle points in the learning adjoint space
composed of weights, neural outputs, and Lagrangian multipliers. This might
open the doors to a truly novel class of learning algorithms where, because of
the introduction of the notion of support neurons, the optimization scheme also
plays a fundamental role in the construction of the architecture.
</summary>
    <author>
      <name>Alessandro Betti</name>
    </author>
    <author>
      <name>Marco Gori</name>
    </author>
    <author>
      <name>Giuseppe Marra</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02833v3</id>
    <updated>2018-08-21T14:31:18Z</updated>
    <published>2015-10-09T21:51:09Z</published>
    <title>On the Definiteness of Earth Mover's Distance and Its Relation to Set
  Intersection</title>
    <summary>  Positive definite kernels are an important tool in machine learning that
enable efficient solutions to otherwise difficult or intractable problems by
implicitly linearizing the problem geometry. In this paper we develop a
set-theoretic interpretation of the Earth Mover's Distance (EMD) and propose
Earth Mover's Intersection (EMI), a positive definite analog to EMD for sets of
different sizes. We provide conditions under which EMD or certain
approximations to EMD are negative definite. We also present a
positive-definite-preserving transformation that can be applied to any kernel
and can also be used to derive positive definite EMD-based kernels and show
that the Jaccard index is simply the result of this transformation. Finally, we
evaluate kernels based on EMI and the proposed transformation versus EMD in
various computer vision tasks and show that EMD is generally inferior even with
indefinite kernel techniques.
</summary>
    <author>
      <name>Andrew Gardner</name>
    </author>
    <author>
      <name>Christian A. Duncan</name>
    </author>
    <author>
      <name>Jinko Kanno</name>
    </author>
    <author>
      <name>Rastko R. Selmic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCYB.2017.2761798</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCYB.2017.2761798" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Major revision based on referee comments. Includes significant
  reorganization of content, new title, new propositions, revised proofs of
  previous propositions, and additional experiments with new data, kernels, and
  indefinite kernel techniques</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02833v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02833v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06918v1</id>
    <updated>2018-08-21T14:25:09Z</updated>
    <published>2018-08-21T14:25:09Z</published>
    <title>On a New Improvement-Based Acquisition Function for Bayesian
  Optimization</title>
    <summary>  Bayesian optimization (BO) is a popular algorithm for solving challenging
optimization tasks. It is designed for problems where the objective function is
expensive to evaluate, perhaps not available in exact form, without gradient
information and possibly returning noisy values. Different versions of the
algorithm vary in the choice of the acquisition function, which recommends the
point to query the objective at next. Initially, researchers focused on
improvement-based acquisitions, while recently the attention has shifted to
more computationally expensive information-theoretical measures. In this paper
we present two major contributions to the literature. First, we propose a new
improvement-based acquisition function that recommends query points where the
improvement is expected to be high with high confidence. The proposed algorithm
is evaluated on a large set of benchmark functions from the global optimization
literature, where it turns out to perform at least as well as current
state-of-the-art acquisition functions, and often better. This suggests that it
is a powerful default choice for BO. The novel policy is then compared to
widely used global optimization solvers in order to confirm that BO methods
reduce the computational costs of the optimization by keeping the number of
function evaluations small. The second main contribution represents an
application to precision medicine, where the interest lies in the estimation of
parameters of a partial differential equations model of the human pulmonary
blood circulation system. Once inferred, these parameters can help clinicians
in diagnosing a patient with pulmonary hypertension without going through the
standard invasive procedure of right heart catheterization, which can lead to
side effects and complications (e.g. severe pain, internal bleeding,
thrombosis).
</summary>
    <author>
      <name>Umberto Noè</name>
    </author>
    <author>
      <name>Dirk Husmeier</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01183v3</id>
    <updated>2018-08-21T14:18:47Z</updated>
    <published>2018-07-03T13:39:19Z</published>
    <title>Quantified Markov Logic Networks</title>
    <summary>  Markov Logic Networks (MLNs) are well-suited for expressing statistics such
as "with high probability a smoker knows another smoker" but not for expressing
statements such as "there is a smoker who knows most other smokers", which is
necessary for modeling, e.g. influencers in social networks. To overcome this
shortcoming, we study quantified MLNs which generalize MLNs by introducing
statistical universal quantifiers, allowing to express also the latter type of
statistics in a principled way. Our main technical contribution is to show that
the standard reasoning tasks in quantified MLNs, maximum a posteriori and
marginal inference, can be reduced to their respective MLN counterparts in
polynomial time.
</summary>
    <author>
      <name>Víctor Gutiérrez-Basulto</name>
    </author>
    <author>
      <name>Jean Christoph Jung</name>
    </author>
    <author>
      <name>Ondrej Kuzelka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted at the 16th International Conference on Principles of
  Knowledge Representation and Reasoning (KR 2018). This work was also
  presented in the Eighth International Workshop on Statistical Relational AI
  (StarAI 2018) under the title "Markov Logic Networks with Statistical
  Quantifiers"</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01183v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01183v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06910v1</id>
    <updated>2018-08-21T14:08:44Z</updated>
    <published>2018-08-21T14:08:44Z</published>
    <title>Scalable Population Synthesis with Deep Generative Modeling</title>
    <summary>  Population synthesis is concerned with the generation of synthetic yet
realistic representations of populations. It is a fundamental problem in the
modeling of transport where the synthetic populations of micro agents represent
a key input to most agent-based models. In this paper, a new methodological
framework for how to grow pools of micro agents is presented. This is
accomplished by adopting a deep generative modeling approach from machine
learning based on a Variational Autoencoder (VAE) framework. Compared to the
previous population synthesis approaches based on Iterative Proportional
Fitting (IPF), Markov Chain Monte Carlo (MCMC) sampling or traditional
generative models, the proposed method allows unparalleled scalability with
respect to the number and types of attributes. In contrast to the approaches
that rely on approximating the joint distribution in the observed data space,
VAE learns its compressed latent representation. The advantage of the
compressed representation is that it avoids the problem of the generated
samples being trapped in local minima when the number of attributes becomes
large. The problem is illustrated using the Danish National Travel Survey data,
where the Gibbs sampler fails to generate a population with 21 attributes
(corresponding to the 121-dimensional joint distribution). At the same time,
VAE shows acceptable performance when 47 attributes (corresponding to the
357-dimensional joint distribution) are used. Moreover, VAE allows for growing
agents that are virtually different from those in the original data but have
similar statistical properties and correlation structure. The presented
approach will help modelers to generate better and richer populations with a
high level of detail, including smaller zones, personal details and travel
preferences.
</summary>
    <author>
      <name>Stanislav S. Borysov</name>
    </author>
    <author>
      <name>Jeppe Rich</name>
    </author>
    <author>
      <name>Francisco C. Pereira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07663v4</id>
    <updated>2018-08-21T13:24:17Z</updated>
    <published>2017-05-22T11:05:06Z</published>
    <title>LOGAN: Membership Inference Attacks Against Generative Models</title>
    <summary>  Generative models estimate the underlying distribution of a dataset to
generate realistic samples according to that distribution. In this paper, we
present the first membership inference attacks against generative models: given
a data point, the adversary determines whether or not it was used to train the
model. Our attacks leverage Generative Adversarial Networks (GANs), which
combine a discriminative and a generative model, to detect overfitting and
recognize inputs that were part of training datasets, using the discriminator's
capacity to learn statistical differences in distributions.
  We present attacks based on both white-box and black-box access to the target
model, against several state-of-the-art generative models, over datasets of
complex representations of faces (LFW), objects (CIFAR-10), and medical images
(Diabetic Retinopathy). We also discuss the sensitivity of the attacks to
different training parameters, and their robustness against mitigation
strategies, finding that defenses are either ineffective or lead to
significantly worse performances of the generative models in terms of training
stability and/or sample quality.
</summary>
    <author>
      <name>Jamie Hayes</name>
    </author>
    <author>
      <name>Luca Melis</name>
    </author>
    <author>
      <name>George Danezis</name>
    </author>
    <author>
      <name>Emiliano De Cristofaro</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings on Privacy Enhancing Technologies (PoPETs), Vol. 2019,
  Issue 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.07663v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07663v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06865v1</id>
    <updated>2018-08-21T12:17:08Z</updated>
    <published>2018-08-21T12:17:08Z</published>
    <title>Machine Learning for Spatiotemporal Sequence Forecasting: A Survey</title>
    <summary>  Spatiotemporal systems are common in the real-world. Forecasting the
multi-step future of these spatiotemporal systems based on the past
observations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant
and challenging problem. Although lots of real-world problems can be viewed as
STSF and many research works have proposed machine learning based methods for
them, no existing work has summarized and compared these methods from a unified
perspective. This survey aims to provide a systematic review of machine
learning for STSF. In this survey, we define the STSF problem and classify it
into three subcategories: Trajectory Forecasting of Moving Point Cloud
(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).
We then introduce the two major challenges of STSF: 1) how to learn a model for
multi-step forecasting and 2) how to adequately model the spatial and temporal
structures. After that, we review the existing works for solving these
challenges, including the general learning strategies for multi-step
forecasting, the classical machine learning based methods for STSF, and the
deep learning based methods for STSF. We also compare these methods and point
out some potential research directions.
</summary>
    <author>
      <name>Xingjian Shi</name>
    </author>
    <author>
      <name>Dit-Yan Yeung</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09597v2</id>
    <updated>2018-08-21T11:28:14Z</updated>
    <published>2018-07-23T06:29:43Z</published>
    <title>Acoustic-to-Word Recognition with Sequence-to-Sequence Models</title>
    <summary>  Acoustic-to-Word recognition provides a straightforward solution to
end-to-end speech recognition without needing external decoding, language model
re-scoring or lexicon. While character-based models offer a natural solution to
the out-of-vocabulary problem, word models can be simpler to decode and may
also be able to directly recognize semantically meaningful units. We present
effective methods to train Sequence-to-Sequence models for direct word-level
recognition (and character-level recognition) and show an absolute improvement
of 4.4-5.0\% in Word Error Rate on the Switchboard corpus compared to prior
work. In addition to these promising results, word-based models are more
interpretable than character models, which have to be composed into words using
a separate decoding step. We analyze the encoder hidden states and the
attention behavior, and show that location-aware attention naturally represents
words as a single speech-word-vector, despite spanning multiple frames in the
input. We finally show that the Acoustic-to-Word model also learns to segment
speech into words with a mean standard deviation of 3 frames as compared with
human annotated forced-alignments for the Switchboard corpus.
</summary>
    <author>
      <name>Shruti Palaskar</name>
    </author>
    <author>
      <name>Florian Metze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, Under Review at SLT 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09597v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09597v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06846v1</id>
    <updated>2018-08-21T11:19:06Z</updated>
    <published>2018-08-21T11:19:06Z</published>
    <title>Search for Common Minima in Joint Optimization of Multiple Cost
  Functions</title>
    <summary>  We present a novel optimization method, named the Combined Optimization
Method (COM), for the joint optimization of two or more cost functions. Unlike
the conventional joint optimization schemes, which try to find minima in a
weighted sum of cost functions, the COM explores search space for common minima
shared by all the cost functions. Given a set of multiple cost functions that
have qualitatively different distributions of local minima with each other, the
proposed method finds the common minima with a high success rate without the
help of any metaheuristics. As a demonstration, we apply the COM to the crystal
structure prediction in materials science. By introducing the concept of data
assimilation, i.e., adopting the theoretical potential energy of the crystal
and the crystallinity, which characterizes the agreement with the theoretical
and experimental X-ray diffraction patterns, as cost functions, we show that
the correct crystal structures of Si diamond, low quartz, and low cristobalite
can be predicted with significantly higher success rates than the previous
methods.
</summary>
    <author>
      <name>Daiki Adachi</name>
    </author>
    <author>
      <name>Naoto Tsujimoto</name>
    </author>
    <author>
      <name>Ryosuke Akashi</name>
    </author>
    <author>
      <name>Synge Todo</name>
    </author>
    <author>
      <name>Shinji Tsuneyuki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06809v1</id>
    <updated>2018-08-21T09:08:56Z</updated>
    <published>2018-08-21T09:08:56Z</published>
    <title>Are You Tampering With My Data?</title>
    <summary>  We propose a novel approach towards adversarial attacks on neural networks
(NN), focusing on tampering the data used for training instead of generating
attacks on trained models. Our network-agnostic method creates a backdoor
during training which can be exploited at test time to force a neural network
to exhibit abnormal behaviour. We demonstrate on two widely used datasets
(CIFAR-10 and SVHN) that a universal modification of just one pixel per image
for all the images of a class in the training set is enough to corrupt the
training procedure of several state-of-the-art deep neural networks causing the
networks to misclassify any images to which the modification is applied. Our
aim is to bring to the attention of the machine learning community, the
possibility that even learning-based methods that are personally trained on
public datasets can be subject to attacks by a skillful adversary.
</summary>
    <author>
      <name>Michele Alberti</name>
    </author>
    <author>
      <name>Vinaychandran Pondenkandath</name>
    </author>
    <author>
      <name>Marcel Würsch</name>
    </author>
    <author>
      <name>Manuel Bouillon</name>
    </author>
    <author>
      <name>Mathias Seuret</name>
    </author>
    <author>
      <name>Rolf Ingold</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Conference on Computer Vision (ECCV 2018), Workshop on
  Objectionable Content and Misinformation</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.06809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06797v1</id>
    <updated>2018-08-21T08:40:16Z</updated>
    <published>2018-08-21T08:40:16Z</published>
    <title>zoNNscan : a boundary-entropy index for zone inspection of neural models</title>
    <summary>  The training of deep neural network classifiers results in decision
boundaries which geometry is still not well understood. This is in direct
relation with classification problems such as so called adversarial examples.
We introduce zoNNscan, an index that is intended to inform on the boundary
uncertainty (in terms of the presence of other classes) around one given input
datapoint. It is based on confidence entropy, and is implemented through
sampling in the multidimensional ball surrounding that input. We detail the
zoNNscan index, give an algorithm for approximating it, and finally illustrate
its benefits on four applications, including two important problems for the
adoption of deep networks in critical systems: adversarial examples and corner
case inputs. We highlight that zoNNscan exhibits significantly higher values
than for standard inputs in those two problem classes.
</summary>
    <author>
      <name>Adel Jaouen</name>
    </author>
    <author>
      <name>Erwan Le Merrer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06774v1</id>
    <updated>2018-08-21T05:50:33Z</updated>
    <published>2018-08-21T05:50:33Z</published>
    <title>Multi-task multiple kernel machines for personalized pain recognition
  from functional near-infrared spectroscopy brain signals</title>
    <summary>  Currently there is no validated objective measure of pain. Recent
neuroimaging studies have explored the feasibility of using functional
near-infrared spectroscopy (fNIRS) to measure alterations in brain function in
evoked and ongoing pain. In this study, we applied multi-task machine learning
methods to derive a practical algorithm for pain detection derived from fNIRS
signals in healthy volunteers exposed to a painful stimulus. Especially, we
employed multi-task multiple kernel learning to account for the inter-subject
variability in pain response. Our results support the use of fNIRS and machine
learning techniques in developing objective pain detection, and also highlight
the importance of adopting personalized analysis in the process.
</summary>
    <author>
      <name>Daniel Lopez-Martinez</name>
    </author>
    <author>
      <name>Ke Peng</name>
    </author>
    <author>
      <name>Sarah C. Steele</name>
    </author>
    <author>
      <name>Arielle J. Lee</name>
    </author>
    <author>
      <name>David Borsook</name>
    </author>
    <author>
      <name>Rosalind Picard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Pattern Recognition (ICPR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09702v3</id>
    <updated>2018-08-21T05:41:09Z</updated>
    <published>2018-03-26T16:29:03Z</published>
    <title>HAMLET: Interpretable Human And Machine co-LEarning Technique</title>
    <summary>  Efficient label acquisition processes are key to obtaining robust
classifiers. However, data labeling is often challenging and subject to high
levels of label noise. This can arise even when classification targets are well
defined, if instances to be labeled are more difficult than the prototypes used
to define the class, leading to disagreements among the expert community. Here,
we enable efficient training of deep neural networks. From low-confidence
labels, we iteratively improve their quality by simultaneous learning of
machines and experts. We call it Human And Machine co-LEarning Technique
(HAMLET). Throughout the process, experts become more consistent, while the
algorithm provides them with explainable feedback for confirmation. HAMLET uses
a neural embedding function and a memory module filled with diverse reference
embeddings from different classes. Its output includes classification labels
and highly relevant reference embeddings as explanation. We took the study of
brain monitoring at intensive care unit (ICU) as an application of HAMLET on
continuous electroencephalography (cEEG) data. Although cEEG monitoring yields
large volumes of data, labeling costs and difficulty make it hard to build a
classifier. Additionally, while experts agree on the labels of clear-cut
examples of cEEG patterns, labeling many real-world cEEG data can be extremely
challenging. Thus, a large minority of sequences might be mislabeled. HAMLET
has shown significant performance gain against deep learning and other
baselines, increasing accuracy from 7.03% to 68.75% on challenging inputs.
Besides improved performance, clinical experts confirmed the interpretability
of those reference embeddings in helping explaining the classification results
by HAMLET.
</summary>
    <author>
      <name>Olivier Deiss</name>
    </author>
    <author>
      <name>Siddharth Biswal</name>
    </author>
    <author>
      <name>Jing Jin</name>
    </author>
    <author>
      <name>Haoqi Sun</name>
    </author>
    <author>
      <name>M. Brandon Westover</name>
    </author>
    <author>
      <name>Jimeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Removed KDD template</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09702v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09702v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03782v2</id>
    <updated>2018-08-21T05:38:27Z</updated>
    <published>2018-04-11T02:10:55Z</published>
    <title>CoT: Cooperative Training for Generative Modeling of Discrete Data</title>
    <summary>  We propose Cooperative Training (CoT) for training generative models that
measure a tractable density for discrete data. CoT coordinately trains a
generator $G$ and an auxiliary predictive mediator $M$. The training target of
$M$ is to estimate a mixture density of the learned distribution $G$ and the
target distribution $P$, and that of $G$ is to minimize the Jensen-Shannon
divergence estimated through $M$. CoT achieves independent success without the
necessity of pre-training via Maximum Likelihood Estimation or involving
high-variance algorithms like REINFORCE. This low-variance algorithm is
theoretically proved to be unbiased for both generative and predictive tasks.
We also theoretically and empirically show the superiority of CoT over most
previous algorithms in terms of generative quality and diversity, predictive
generalization ability and computational cost.
</summary>
    <author>
      <name>Sidi Lu</name>
    </author>
    <author>
      <name>Lantao Yu</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Yong Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03782v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03782v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04129v2</id>
    <updated>2018-08-21T04:37:56Z</updated>
    <published>2017-12-12T04:57:40Z</published>
    <title>Outlier Detection by Consistent Data Selection Method</title>
    <summary>  Often the challenge associated with tasks like fraud and spam detection[1] is
the lack of all likely patterns needed to train suitable supervised learning
models. In order to overcome this limitation, such tasks are attempted as
outlier or anomaly detection tasks. We also hypothesize that out- liers have
behavioral patterns that change over time. Limited data and continuously
changing patterns makes learning significantly difficult. In this work we are
proposing an approach that detects outliers in large data sets by relying on
data points that are consistent. The primary contribution of this work is that
it will quickly help retrieve samples for both consistent and non-outlier data
sets and is also mindful of new outlier patterns. No prior knowledge of each
set is required to extract the samples. The method consists of two phases, in
the first phase, consistent data points (non- outliers) are retrieved by an
ensemble method of unsupervised clustering techniques and in the second phase a
one class classifier trained on the consistent data point set is ap- plied on
the remaining sample set to identify the outliers. The approach is tested on
three publicly available data sets and the performance scores are competitive.
</summary>
    <author>
      <name>Utkarsh Porwal</name>
    </author>
    <author>
      <name>Smruthi Mukund</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04129v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04129v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07412v1</id>
    <updated>2018-08-21T03:40:21Z</updated>
    <published>2018-08-21T03:40:21Z</published>
    <title>Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation
  using Deep Neural Networks</title>
    <summary>  Statically estimating the number of processor clock cycles it takes to
execute a basic block of assembly instructions in steady state (throughput) is
important for compiler backend optimizations such as register allocation,
instruction selection and instruction scheduling. This is complicated specially
in modern x86-64 Complex Instruction Set Computer (CISC) machines with
sophisticated processor microarchitectures. Traditionally, compiler writers
invest time experimenting and referring to processor manuals to analytically
model modern processors with incomplete specifications. This is tedious, error
prone and should be done for each processor generation. We present Ithemal, the
first automatically learnt estimator to statically predict throughput of a set
of basic block instructions using machine learning. Ithemal uses a novel
Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven
approach for throughput estimation. We show that Ithemal is accurate than
state-of-the-art hand written tools used in compiler backends and static
machine code analyzers. In particular, our model has a worst case average error
of 10.53% on actual throughput values when compared to best case average errors
of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's
machine code analyzer when compared on three different microarchitectures,
while predicting throughput values at a faster rate than aforementioned tools.
We also show that Ithemal is portable, learning throughput estimation for Intel
Nehalem, Haswell and Skylake microarchitectures without requiring changes to
its structure.
</summary>
    <author>
      <name>Charith Mendis</name>
    </author>
    <author>
      <name>Saman Amarasinghe</name>
    </author>
    <author>
      <name>Michael Carbin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06266v2</id>
    <updated>2018-08-21T03:20:24Z</updated>
    <published>2018-02-17T17:50:19Z</published>
    <title>An analysis of training and generalization errors in shallow and deep
  networks</title>
    <summary>  An open problem around deep networks is the apparent absence of over-fitting
despite large over-parametrization which allows perfect fitting of the training
data. In this paper, we analyze this phenomenon in the case of regression
problems when each unit evaluates a trigonometric polynomial. It is well
understood that a trigonometric monomial can be synthesized with a good degree
of approximation by neural networks with fixed weights and thresholds.
Approximation by trigonometric polynomials serves as a `role model' for every
other approximation process, including that by neural and RBF networks. In this
paper, we argue that the maximum loss functional is necessary to measure the
generalization error. We give estimates on exactly how many parameters ensure
both zero training error as well as a good generalization error. We prove that
a solution of a regularization problem is guaranteed to yield a good training
error as well as a good generalization error and estimate how much error to
expect at which test data.
</summary>
    <author>
      <name>Hrushikesh Mhaskar</name>
    </author>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06266v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06266v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02538v4</id>
    <updated>2018-08-21T02:54:23Z</updated>
    <published>2017-09-08T04:53:51Z</published>
    <title>DeepFense: Online Accelerated Defense Against Adversarial Deep Learning</title>
    <summary>  Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
</summary>
    <author>
      <name>Bita Darvish Rouhani</name>
    </author>
    <author>
      <name>Mohammad Samragh</name>
    </author>
    <author>
      <name>Mojan Javaheripi</name>
    </author>
    <author>
      <name>Tara Javidi</name>
    </author>
    <author>
      <name>Farinaz Koushanfar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Adding hardware acceleration for real-time execution of defender
  modules</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02538v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02538v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06733v1</id>
    <updated>2018-08-21T01:39:17Z</updated>
    <published>2018-08-21T01:39:17Z</published>
    <title>Wrapped Loss Function for Regularizing Nonconforming Residual
  Distributions</title>
    <summary>  Multi-output is essential in machine learning that it might suffer from
nonconforming residual distributions, i.e., the multi-output residual
distributions are not conforming to the expected distribution. In this paper we
propose "Wrapped Loss Function" to wrap the original loss function to alleviate
the problem. This wrapped loss function acts just like original loss function
that its gradient can be used for backpropagation optimization. Empirical
evaluations show wrapped loss function has advanced properties of faster
convergence, better accuracy and improving imbalanced data.
</summary>
    <author>
      <name>Chun Ting Liu</name>
    </author>
    <author>
      <name>Ming Chuan Yang</name>
    </author>
    <author>
      <name>Meng Chang Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06725v1</id>
    <updated>2018-08-21T00:13:12Z</updated>
    <published>2018-08-21T00:13:12Z</published>
    <title>Learning to Exploit Invariances in Clinical Time-Series Data using
  Sequence Transformer Networks</title>
    <summary>  Recently, researchers have started applying convolutional neural networks
(CNNs) with one-dimensional convolutions to clinical tasks involving
time-series data. This is due, in part, to their computational efficiency,
relative to recurrent neural networks and their ability to efficiently exploit
certain temporal invariances, (e.g., phase invariance). However, it is
well-established that clinical data may exhibit many other types of invariances
(e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping)
may successfully transform and align inputs, their use often requires one to
identify the types of invariances in advance. In contrast, we propose the use
of Sequence Transformer Networks, an end-to-end trainable architecture that
learns to identify and account for invariances in clinical time-series data.
Applied to the task of predicting in-hospital mortality, our proposed approach
achieves an improvement in the area under the receiver operating characteristic
curve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our
results suggest that a variety of valuable invariances can be learned directly
from the data.
</summary>
    <author>
      <name>Jeeheh Oh</name>
    </author>
    <author>
      <name>Jiaxuan Wang</name>
    </author>
    <author>
      <name>Jenna Wiens</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00805v4</id>
    <updated>2018-08-21T00:02:44Z</updated>
    <published>2017-04-03T20:50:29Z</published>
    <title>On the Properties of the Softmax Function with Application in Game
  Theory and Reinforcement Learning</title>
    <summary>  In this paper, we utilize results from convex analysis and monotone operator
theory to derive additional properties of the softmax function that have not
yet been covered in the existing literature. In particular, we show that the
softmax function is the monotone gradient map of the log-sum-exp function. By
exploiting this connection, we show that the inverse temperature parameter
determines the Lipschitz and co-coercivity properties of the softmax function.
We then demonstrate the usefulness of these properties through an application
in game-theoretic reinforcement learning.
</summary>
    <author>
      <name>Bolin Gao</name>
    </author>
    <author>
      <name>Lacra Pavel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures. Comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00805v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00805v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06719v1</id>
    <updated>2018-08-20T23:19:48Z</updated>
    <published>2018-08-20T23:19:48Z</published>
    <title>Fast Spectrogram Inversion using Multi-head Convolutional Neural
  Networks</title>
    <summary>  We propose the multi-head convolutional neural network (MCNN) architecture
for waveform synthesis from spectrograms. Nonlinear interpolation in MCNN is
employed with transposed convolution layers in parallel heads. MCNN achieves
more than an order of magnitude higher compute intensity than commonly-used
iterative algorithms like Griffin-Lim, yielding efficient utilization for
modern multi-core processors, and very fast (more than 300x real-time) waveform
synthesis. For training of MCNN, we use a large-scale speech recognition
dataset and losses defined on waveforms that are related to perceptual audio
quality. We demonstrate that MCNN constitutes a very promising approach for
high-quality speech synthesis, without any iterative algorithms or
autoregression in computations.
</summary>
    <author>
      <name>Sercan O. Arik</name>
    </author>
    <author>
      <name>Heewoo Jun</name>
    </author>
    <author>
      <name>Gregory Diamos</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08774v2</id>
    <updated>2018-08-20T22:52:03Z</updated>
    <published>2018-04-23T23:01:50Z</published>
    <title>Neural-Brane: Neural Bayesian Personalized Ranking for Attributed
  Network Embedding</title>
    <summary>  Network embedding methodologies, which learn a distributed vector
representation for each vertex in a network, have attracted considerable
interest in recent years. Existing works have demonstrated that vertex
representation learned through an embedding method provides superior
performance in many real-world applications, such as node classification, link
prediction, and community detection. However, most of the existing methods for
network embedding only utilize topological information of a vertex, ignoring a
rich set of nodal attributes (such as, user profiles of an online social
network, or textual contents of a citation network), which is abundant in all
real-life networks. A joint network embedding that takes into account both
attributional and relational information entails a complete network information
and could further enrich the learned vector representations. In this work, we
present Neural-Brane, a novel Neural Bayesian Personalized Ranking based
Attributed Network Embedding. For a given network, Neural-Brane extracts latent
feature representation of its vertices using a designed neural network model
that unifies network topological information and nodal attributes; Besides, it
utilizes Bayesian personalized ranking objective, which exploits the proximity
ordering between a similar node-pair and a dissimilar node-pair. We evaluate
the quality of vertex embedding produced by Neural-Brane by solving the node
classification and clustering tasks on four real-world datasets. Experimental
results demonstrate the superiority of our proposed method over the
state-of-the-art existing methods.
</summary>
    <author>
      <name>Vachik S. Dave</name>
    </author>
    <author>
      <name>Baichuan Zhang</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Mohammad Al Hasan</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07049v1</id>
    <updated>2018-08-20T21:49:13Z</updated>
    <published>2018-08-20T21:49:13Z</published>
    <title>Catastrophic Importance of Catastrophic Forgetting</title>
    <summary>  This paper describes some of the possibilities of artificial neural networks
that open up after solving the problem of catastrophic forgetting. A simple
model and reinforcement learning applications of existing methods are also
proposed.
</summary>
    <author>
      <name>Albert Ierusalem</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06686v1</id>
    <updated>2018-08-20T20:47:56Z</updated>
    <published>2018-08-20T20:47:56Z</published>
    <title>Deep Multimodal Image-Repurposing Detection</title>
    <summary>  Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor's
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
</summary>
    <author>
      <name>Ekraam Sabir</name>
    </author>
    <author>
      <name>Wael AbdAlmageed</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Prem Natarajan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240508.3240707</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240508.3240707" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published at ACM Multimeda 2018 (orals)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06684v1</id>
    <updated>2018-08-20T20:39:24Z</updated>
    <published>2018-08-20T20:39:24Z</published>
    <title>Use Of Vapnik-Chervonenkis Dimension in Model Selection</title>
    <summary>  In this dissertation, I derive a new method to estimate the
Vapnik-Chervonenkis Dimension (VCD) for the class of linear functions. This
method is inspired by the technique developed by Vapnik et al. Vapnik et al.
(1994). My contribution rests on the approximation of the expected maximum
difference between two empirical Losses (EMDBTEL). In fact, I use a
cross-validated form of the error to compute the EMDBTEL, and I make the bound
on the EMDBTEL tighter by minimizing a constant in of its right upper bound. I
also derive two bounds for the true unknown risk using the additive (ERM1) and
the multiplicative (ERM2) Chernoff bounds. These bounds depend on the estimated
VCD and the empirical risk. These bounds can be used to perform model selection
and to declare with high probability, the chosen model will perform better
without making strong assumptions about the data generating process (DG).
  I measure the accuracy of my technique on simulated datasets and also on
three real datasets. The model selection provided by VCD was always as good as
if not better than the other methods under reasonable conditions.
</summary>
    <author>
      <name>Merlin Mpoudeu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">131 pages, 34 figures, and 44 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06675v1</id>
    <updated>2018-08-20T19:58:35Z</updated>
    <published>2018-08-20T19:58:35Z</published>
    <title>Class2Str: End to End Latent Hierarchy Learning</title>
    <summary>  Deep neural networks for image classification typically consists of a
convolutional feature extractor followed by a fully connected classifier
network. The predicted and the ground truth labels are represented as one hot
vectors. Such a representation assumes that all classes are equally dissimilar.
However, classes have visual similarities and often form a hierarchy. Learning
this latent hierarchy explicitly in the architecture could provide invaluable
insights. We propose an alternate architecture to the classifier network called
the Latent Hierarchy (LH) Classifier and an end to end learned Class2Str
mapping which discovers a latent hierarchy of the classes. We show that for
some of the best performing architectures on CIFAR and Imagenet datasets, the
proposed replacement and training by LH classifier recovers the accuracy, with
a fraction of the number of parameters in the classifier part. Compared to the
previous work of HDCNN, which also learns a 2 level hierarchy, we are able to
learn a hierarchy at an arbitrary number of levels as well as obtain an
accuracy improvement on the Imagenet classification task over them. We also
verify that many visually similar classes are grouped together, under the
learnt hierarchy.
</summary>
    <author>
      <name>Soham Saha</name>
    </author>
    <author>
      <name>Girish Varma</name>
    </author>
    <author>
      <name>C. V. Jawahar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, ICPR 2018, Beijing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06671v1</id>
    <updated>2018-08-20T19:53:19Z</updated>
    <published>2018-08-20T19:53:19Z</published>
    <title>Adversarial Sampling for Active Learning</title>
    <summary>  This paper describes ASAL a new active learning strategy that uses
uncertainty sampling, adversarial sample generation and sample matching.
Compared to traditional pool-based uncertainty sampling strategies, ASAL
synthesizes uncertain samples instead of performing an exhaustive search in
each active learning cycle. Then, the sample matching efficiently selects
similar samples from the pool. We present a comprehensive set of experiments on
MNIST and CIFAR-10 and show that ASAL outperforms similar methods and clearly
exceeds passive learning. To the best of our knowledge this is the first
pool-based adversarial active learning technique and the first that is applied
for multi-label classification using deep convolutional classifiers.
</summary>
    <author>
      <name>Christoph Mayer</name>
    </author>
    <author>
      <name>Radu Timofte</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06664v1</id>
    <updated>2018-08-20T19:29:57Z</updated>
    <published>2018-08-20T19:29:57Z</published>
    <title>Out-of-Distribution Detection using Multiple Semantic Label
  Representations</title>
    <summary>  Deep Neural Networks are powerful models that attained remarkable results on
a variety of tasks. These models are shown to be extremely efficient when
training and test data are drawn from the same distribution. However, it is not
clear how a network will act when it is fed with an out-of-distribution
example. In this work, we consider the problem of out-of-distribution detection
in neural networks. We propose to use multiple semantic dense representations
instead of sparse representation as the target label. Specifically, we propose
to use several word representations obtained from different corpora or
architectures as target labels. We evaluated the proposed model on computer
vision, and speech commands detection tasks and compared it to previous
methods. Results suggest that our method compares favorably with previous work.
Besides, we present the efficiency of our approach for detecting wrongly
classified and adversarial examples.
</summary>
    <author>
      <name>Gabi Shalev</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <author>
      <name>Joseph Keshet</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06350v2</id>
    <updated>2018-08-20T19:26:49Z</updated>
    <published>2018-05-16T14:43:33Z</published>
    <title>Approximating the Void: Learning Stochastic Channel Models from
  Observation with Variational Generative Adversarial Networks</title>
    <summary>  Channel modeling is a critical topic when considering designing, learning, or
evaluating the performance of any communications system. Most prior work in
designing or learning new modulation schemes has focused on using highly
simplified analytic channel models such as additive white Gaussian noise
(AWGN), Rayleigh fading channels or similar. Recently, we proposed the usage of
a generative adversarial networks (GANs) to jointly approximate a wireless
channel response model (e.g. from real black box measurements) and optimize for
an efficient modulation scheme over it using machine learning. This approach
worked to some degree, but was unable to produce accurate probability
distribution functions (PDFs) representing the stochastic channel response. In
this paper, we focus specifically on the problem of accurately learning a
channel PDF using a variational GAN, introducing an architecture and loss
function which can accurately capture stochastic behavior. We illustrate where
our prior method failed and share results capturing the performance of such as
system over a range of realistic channel distributions.
</summary>
    <author>
      <name>Timothy J. O'Shea</name>
    </author>
    <author>
      <name>Tamoghna Roy</name>
    </author>
    <author>
      <name>Nathan West</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06350v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06350v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06651v1</id>
    <updated>2018-08-20T18:49:32Z</updated>
    <published>2018-08-20T18:49:32Z</published>
    <title>Privacy Amplification by Iteration</title>
    <summary>  Many commonly used learning algorithms work by iteratively updating an
intermediate solution using one or a few data points in each iteration.
Analysis of differential privacy for such algorithms often involves ensuring
privacy of each step and then reasoning about the cumulative privacy cost of
the algorithm. This is enabled by composition theorems for differential privacy
that allow releasing of all the intermediate results. In this work, we
demonstrate that for contractive iterations, not releasing the intermediate
results strongly amplifies the privacy guarantees. We describe several
applications of this new analysis technique to solving convex optimization
problems via noisy stochastic gradient descent. For example, we demonstrate
that a relatively small number of non-private data points from the same
distribution can be used to close the gap between private and non-private
convex optimization. In addition, we demonstrate that we can achieve guarantees
similar to those obtainable using the privacy-amplification-by-sampling
technique in several natural settings where that technique cannot be applied.
</summary>
    <author>
      <name>Vitaly Feldman</name>
    </author>
    <author>
      <name>Ilya Mironov</name>
    </author>
    <author>
      <name>Kunal Talwar</name>
    </author>
    <author>
      <name>Abhradeep Thakurta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract appears in Foundations of Computer Science (FOCS)
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06649v1</id>
    <updated>2018-08-20T18:43:52Z</updated>
    <published>2018-08-20T18:43:52Z</published>
    <title>Inverse Problems in Asteroseismology</title>
    <summary>  Asteroseismology allows us to probe the internal structure of stars through
their global modes of oscillation. Thanks to missions such as the NASA Kepler
space observatory, we now have high-quality asteroseismic data for nearly 100
solar-type stars. In this thesis, new techniques to measure the ages, masses,
and radii of stars are presented, as well as a way to infer their internal
structure.
</summary>
    <author>
      <name>Earl Patrick Bellinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">256 pages, 68 figures, Ph.D. dissertation in Computer Science at the
  Georg-August-University School of Science (GAUSS) G\"ottingen undertaken in
  the context of the International Max Planck Research School for Solar System
  Science under the supervision of Dr. ir. Saskia Hekker (Group Leader, SAGE
  Group) and Prof. Dr. Sarbani Basu (Chair, Department of Astronomy, Yale
  University)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06645v1</id>
    <updated>2018-08-20T18:39:04Z</updated>
    <published>2018-08-20T18:39:04Z</published>
    <title>Stochastic Combinatorial Ensembles for Defending Against Adversarial
  Examples</title>
    <summary>  Many deep learning algorithms can be easily fooled with simple adversarial
examples. To address the limitations of existing defenses, we devised a
probabilistic framework that can generate an exponentially large ensemble of
models from a single model with just a linear cost. This framework takes
advantage of neural network depth and stochastically decides whether or not to
insert noise removal operators such as VAEs between layers. We show empirically
the important role that model gradients have when it comes to determining
transferability of adversarial examples, and take advantage of this result to
demonstrate that it is possible to train models with limited adversarial attack
transferability. Additionally, we propose a detection method based on metric
learning in order to detect adversarial examples that have no hope of being
cleaned of maliciously engineered noise.
</summary>
    <author>
      <name>George A. Adam</name>
    </author>
    <author>
      <name>Petr Smirnov</name>
    </author>
    <author>
      <name>Anna Goldenberg</name>
    </author>
    <author>
      <name>David Duvenaud</name>
    </author>
    <author>
      <name>Benjamin Haibe-Kains</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06601v1</id>
    <updated>2018-08-20T17:58:42Z</updated>
    <published>2018-08-20T17:58:42Z</published>
    <title>Video-to-Video Synthesis</title>
    <summary>  We study the problem of video-to-video synthesis, whose goal is to learn a
mapping function from an input source video (e.g., a sequence of semantic
segmentation masks) to an output photorealistic video that precisely depicts
the content of the source video. While its image counterpart, the
image-to-image synthesis problem, is a popular topic, the video-to-video
synthesis problem is less explored in the literature. Without understanding
temporal dynamics, directly applying existing image synthesis approaches to an
input video often results in temporally incoherent videos of low visual
quality. In this paper, we propose a novel video-to-video synthesis approach
under the generative adversarial learning framework. Through carefully-designed
generator and discriminator architectures, coupled with a spatio-temporal
adversarial objective, we achieve high-resolution, photorealistic, temporally
coherent video results on a diverse set of input formats including segmentation
masks, sketches, and poses. Experiments on multiple benchmarks show the
advantage of our method compared to strong baselines. In particular, our model
is capable of synthesizing 2K resolution videos of street scenes up to 30
seconds long, which significantly advances the state-of-the-art of video
synthesis. Finally, we apply our approach to future video prediction,
outperforming several state-of-the-art competing systems.
</summary>
    <author>
      <name>Ting-Chun Wang</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <author>
      <name>Jun-Yan Zhu</name>
    </author>
    <author>
      <name>Guilin Liu</name>
    </author>
    <author>
      <name>Andrew Tao</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code, models, and more results are available at
  https://github.com/NVIDIA/vid2vid</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11585v2</id>
    <updated>2018-08-20T17:55:56Z</updated>
    <published>2017-11-30T18:57:21Z</published>
    <title>High-Resolution Image Synthesis and Semantic Manipulation with
  Conditional GANs</title>
    <summary>  We present a new method for synthesizing high-resolution photo-realistic
images from semantic label maps using conditional generative adversarial
networks (conditional GANs). Conditional GANs have enabled a variety of
applications, but the results are often limited to low-resolution and still far
from realistic. In this work, we generate 2048x1024 visually appealing results
with a novel adversarial loss, as well as new multi-scale generator and
discriminator architectures. Furthermore, we extend our framework to
interactive visual manipulation with two additional features. First, we
incorporate object instance segmentation information, which enables object
manipulations such as removing/adding objects and changing the object category.
Second, we propose a method to generate diverse results given the same input,
allowing users to edit the object appearance interactively. Human opinion
studies demonstrate that our method significantly outperforms existing methods,
advancing both the quality and the resolution of deep image synthesis and
editing.
</summary>
    <author>
      <name>Ting-Chun Wang</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <author>
      <name>Jun-Yan Zhu</name>
    </author>
    <author>
      <name>Andrew Tao</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2: CVPR camera ready, adding more results for edge-to-photo examples</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11585v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11585v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06581v1</id>
    <updated>2018-08-20T17:41:39Z</updated>
    <published>2018-08-20T17:41:39Z</published>
    <title>The Deconfounded Recommender: A Causal Inference Approach to
  Recommendation</title>
    <summary>  The goal of a recommender system is to show its users items that they will
like. In forming its prediction, the recommender system tries to answer: "what
would the rating be if we 'forced' the user to watch the movie?" This is a
question about an intervention in the world, a causal question, and so
traditional recommender systems are doing causal inference from observational
data. This paper develops a causal inference approach to recommendation.
Traditional recommenders are likely biased by unobserved confounders, variables
that affect both the "treatment assignments" (which movies the users watch) and
the "outcomes" (how they rate them). We develop the deconfounded recommender, a
strategy to leverage classical recommendation models for causal predictions.
The deconfounded recommender uses Poisson factorization on which movies users
watched to infer latent confounders in the data; it then augments common
recommendation models to correct for potential confounding bias. The
deconfounded recommender improves recommendation and it enjoys stable
performance against interventions on test sets.
</summary>
    <author>
      <name>Yixin Wang</name>
    </author>
    <author>
      <name>Dawen Liang</name>
    </author>
    <author>
      <name>Laurent Charlin</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06573v1</id>
    <updated>2018-08-20T17:18:25Z</updated>
    <published>2018-08-20T17:18:25Z</published>
    <title>A Semi-Supervised and Inductive Embedding Model for Churn Prediction of
  Large-Scale Mobile Games</title>
    <summary>  Mobile gaming has emerged as a promising market with billion-dollar revenues.
A variety of mobile game platforms and services have been developed around the
world. One critical challenge for these platforms and services is to understand
user churn behavior in mobile games. Successful churn prediction will benefit
many stakeholders such as game developers and platform operators. In this
paper, we present the first large-scale churn prediction solution for mobile
games. In view of the common limitations of the state-of-the-art methods built
upon traditional machine learning models, we devise a novel semi-supervised and
inductive embedding model that jointly learns the prediction function and the
embedding function for user-app relationships. We model these two functions by
deep neural networks with a unique edge embedding technique that is able to
capture both contextual information and relationship dynamics. We also design a
novel attributed random walk technique that takes into consideration both
topological adjacency and attributes similarities. To evaluate the performance
of our solution, we collect the real-world data from a commercial mobile gaming
platform that includes tens of thousands of games and hundreds of millions of
user-app interactions. The experimental results with this data demonstrate the
superiority of our proposed model against existing state-of-the-art methods.
</summary>
    <author>
      <name>Xi Liu</name>
    </author>
    <author>
      <name>Muhe Xie</name>
    </author>
    <author>
      <name>Xidao Wen</name>
    </author>
    <author>
      <name>Rui Chen</name>
    </author>
    <author>
      <name>Yong Ge</name>
    </author>
    <author>
      <name>Nick Duffield</name>
    </author>
    <author>
      <name>Na Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in ICDM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01204v2</id>
    <updated>2018-08-20T16:43:28Z</updated>
    <published>2018-08-03T14:28:12Z</published>
    <title>Learning Overparameterized Neural Networks via Stochastic Gradient
  Descent on Structured Data</title>
    <summary>  Neural networks have many successful applications, while much less
theoretical understanding has been gained. Towards bridging this gap, we study
the problem of learning a two-layer overparameterized ReLU neural network for
multi-class classification via stochastic gradient descent (SGD) from random
initialization. In the overparameterized setting, when the data comes from
mixtures of well-separated distributions, we prove that SGD learns a network
with a small generalization error, albeit the network has enough capacity to
fit arbitrary labels. Furthermore, the analysis provides interesting insights
into several aspects of learning neural networks and can be verified based on
empirical studies on synthetic data and on the MNIST dataset.
</summary>
    <author>
      <name>Yuanzhi Li</name>
    </author>
    <author>
      <name>Yingyu Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06560v1</id>
    <updated>2018-08-20T16:40:35Z</updated>
    <published>2018-08-20T16:40:35Z</published>
    <title>Multi-View Graph Embedding Using Randomized Shortest Paths</title>
    <summary>  Real-world data sets often provide multiple types of information about the
same set of entities. This data is well represented by multi-view graphs, which
consist of several distinct sets of edges over the same nodes. These can be
used to analyze how entities interact from different viewpoints. Combining
multiple views improves the quality of inferences drawn from the underlying
data, which has increased interest in developing efficient multi-view graph
embedding methods. We propose an algorithm, C-RSP, that generates a common (C)
embedding of a multi-view graph using Randomized Shortest Paths (RSP). This
algorithm generates a dissimilarity measure between nodes by minimizing the
expected cost of a random walk between any two nodes across all views of a
multi-view graph, in doing so encoding both the local and global structure of
the graph. We test C-RSP on both real and synthetic data and show that it
outperforms benchmark algorithms at embedding and clustering tasks while
remaining computationally efficient.
</summary>
    <author>
      <name>Anuththari Gamage</name>
    </author>
    <author>
      <name>Brian Rappaport</name>
    </author>
    <author>
      <name>Shuchin Aeron</name>
    </author>
    <author>
      <name>Xiaozhe Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06556v1</id>
    <updated>2018-08-20T16:31:30Z</updated>
    <published>2018-08-20T16:31:30Z</published>
    <title>Triangle Lasso for Simultaneous Clustering and Optimization in Graph
  Datasets</title>
    <summary>  Recently, network lasso has drawn many attentions due to its remarkable
performance on simultaneous clustering and optimization. However, it usually
suffers from the imperfect data (noise, missing values etc), and yields
sub-optimal solutions. The reason is that it finds the similar instances
according to their features directly, which is usually impacted by the
imperfect data, and thus returns sub-optimal results. In this paper, we propose
triangle lasso to avoid its disadvantage. Triangle lasso finds the similar
instances according to their neighbours. If two instances have many common
neighbours, they tend to become similar. Although some instances are profiled
by the imperfect data, it is still able to find the similar counterparts.
Furthermore, we develop an efficient algorithm based on Alternating Direction
Method of Multipliers (ADMM) to obtain a moderately accurate solution. In
addition, we present a dual method to obtain the accurate solution with the low
additional time consumption. We demonstrate through extensive numerical
experiments that triangle lasso is robust to the imperfect data. It usually
yields a better performance than the state-of-the-art method when performing
data analysis tasks in practical scenarios.
</summary>
    <author>
      <name>Yawei Zhao</name>
    </author>
    <author>
      <name>Kai Xu</name>
    </author>
    <author>
      <name>Xinwang Liu</name>
    </author>
    <author>
      <name>En Zhu</name>
    </author>
    <author>
      <name>Xinzhong Zhu</name>
    </author>
    <author>
      <name>Jianping Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Knowledge and Data Engineering, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06942v1</id>
    <updated>2018-08-20T15:20:46Z</updated>
    <published>2018-08-20T15:20:46Z</published>
    <title>PACO: Signal Restoration via PAtch COnsensus</title>
    <summary>  Many signal processing algorithms operate by breaking the target signal into
possibly overlapping segments (typically called windows or patches), processing
them separately, and then stitching them back into place to produce a unified
output. In most cases where pach overlapping occurs, the final value of those
samples that are estimated by more than one patch is resolved by averaging
those estimates; this includes many recent image processing algorithms. In
other cases, typically frequency-based restoration methods, the average is
implicitly weighted by some window function such as Hanning, Blackman, etc.
which is applied prior to the Fourier/DCT transform in order to avoid Gibbs
oscillations in the processed patches. Such averaging may incidentally help in
covering up artifacts in the restoration process, but more often will simply
degrade the overall result, posing an upper limit to the size of the patches
that can be used. In order to avoid such drawbacks, we propose a new
methodology where the different estimates of any given sample are forced to be
identical. We show that, together, these consensus constraints constitute a
non-empty convex feasible set, provide a general formulation of the resulting
constrained optimization problem which can be applied to a wide variety of
signal restoration tasks, and propose an efficient algorithm for finding the
corresponding solutions. Finally, we describe in detail the application of the
proposed methodology to three different signal processing problems, in some
cases surpassing the state of the art by a significant margin.
</summary>
    <author>
      <name>Ignacio Francisco Ramírez Paulino</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06508v1</id>
    <updated>2018-08-20T15:15:32Z</updated>
    <published>2018-08-20T15:15:32Z</published>
    <title>Life-Long Disentangled Representation Learning with Cross-Domain Latent
  Homologies</title>
    <summary>  Intelligent behaviour in the real-world requires the ability to acquire new
knowledge from an ongoing sequence of experiences while preserving and reusing
past knowledge. We propose a novel algorithm for unsupervised representation
learning from piece-wise stationary visual data: Variational Autoencoder with
Shared Embeddings (VASE). Based on the Minimum Description Length principle,
VASE automatically detects shifts in the data distribution and allocates spare
representational capacity to new knowledge, while simultaneously protecting
previously learnt representations from catastrophic forgetting. Our approach
encourages the learnt representations to be disentangled, which imparts a
number of desirable properties: VASE can deal sensibly with ambiguous inputs,
it can enhance its own representations through imagination-based exploration,
and most importantly, it exhibits semantically meaningful sharing of latents
between different datasets. Compared to baselines with entangled
representations, our approach is able to reason beyond surface-level statistics
and perform semantically meaningful cross-domain inference.
</summary>
    <author>
      <name>Alessandro Achille</name>
    </author>
    <author>
      <name>Tom Eccles</name>
    </author>
    <author>
      <name>Loic Matthey</name>
    </author>
    <author>
      <name>Christopher P. Burgess</name>
    </author>
    <author>
      <name>Nick Watters</name>
    </author>
    <author>
      <name>Alexander Lerchner</name>
    </author>
    <author>
      <name>Irina Higgins</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06453v1</id>
    <updated>2018-08-20T13:58:55Z</updated>
    <published>2018-08-20T13:58:55Z</published>
    <title>Towards Fine Grained Network Flow Prediction</title>
    <summary>  One main challenge for the design of networks is that traffic load is not
generally known in advance. This makes it hard to adequately devote resources
such as to best prevent or mitigate bottlenecks. While several authors have
shown how to predict traffic in a coarse grained manner by aggregating flows,
fine grained prediction of traffic at the level of individual flows, including
bursty traffic, is widely considered to be impossible. This paper shows, to the
best of our knowledge, the first approach to fine grained per flow traffic
prediction. In short, we introduce the Frequency-based Kernel Kalman Filter
(FKKF), which predicts individual flows' behavior based on measurements. Our
FKKF relies on the well known Kalman Filter in combination with a kernel to
support the prediction of non linear functions. Furthermore we change the
operating space from time to frequency space. In this space, into which we
transform the input data via a Short-Time Fourier Transform (STFT), the peak
structures of flows can be predicted after gleaning their key characteristics,
with a Principal Component Analysis (PCA), from past and ongoing flows that
stem from the same socket-to-socket connection. We demonstrate the
effectiveness of our approach on popular benchmark traces from a university
data center. Our approach predicts traffic on average across 17 out of 20
groups of flows with an average prediction error of 6.43% around 0.49 (average)
seconds in advance, whilst existing coarse grained approaches exhibit
prediction errors of 77% at best.
</summary>
    <author>
      <name>Patrick Jahnke</name>
    </author>
    <author>
      <name>Emmanuel Stapf</name>
    </author>
    <author>
      <name>Jonas Mieseler</name>
    </author>
    <author>
      <name>Gerhard Neumann</name>
    </author>
    <author>
      <name>Patrick Eugster</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06452v1</id>
    <updated>2018-08-20T13:54:03Z</updated>
    <published>2018-08-20T13:54:03Z</published>
    <title>Reproducible evaluation of classification methods in Alzheimer's
  disease: framework and application to MRI and PET data</title>
    <summary>  A large number of papers have introduced novel machine learning and feature
extraction methods for automatic classification of AD. However, they are
difficult to reproduce because key components of the validation are often not
readily available. These components include selected participants and input
data, image preprocessing and cross-validation procedures. The performance of
the different approaches is also difficult to compare objectively. In
particular, it is often difficult to assess which part of the method provides a
real improvement, if any. We propose a framework for reproducible and objective
classification experiments in AD using three publicly available datasets (ADNI,
AIBL and OASIS). The framework comprises: i) automatic conversion of the three
datasets into BIDS format, ii) a modular set of preprocessing pipelines,
feature extraction and classification methods, together with an evaluation
framework, that provide a baseline for benchmarking the different components.
We demonstrate the use of the framework for a large-scale evaluation on 1960
participants using T1 MRI and FDG PET data. In this evaluation, we assess the
influence of different modalities, preprocessing, feature types, classifiers,
training set sizes and datasets. Performances were in line with the
state-of-the-art. FDG PET outperformed T1 MRI for all classification tasks. No
difference in performance was found for the use of different atlases, image
smoothing, partial volume correction of FDG PET images, or feature type. Linear
SVM and L2-logistic regression resulted in similar performance and both
outperformed random forests. The classification performance increased along
with the number of subjects used for training. Classifiers trained on ADNI
generalized well to AIBL and OASIS. All the code of the framework and the
experiments is publicly available at:
https://gitlab.icm-institute.org/aramislab/AD-ML.
</summary>
    <author>
      <name>Jorge Samper-González</name>
    </author>
    <author>
      <name>Ninon Burgos</name>
    </author>
    <author>
      <name>Simona Bottani</name>
    </author>
    <author>
      <name>Sabrina Fontanella</name>
    </author>
    <author>
      <name>Pascal Lu</name>
    </author>
    <author>
      <name>Arnaud Marcoux</name>
    </author>
    <author>
      <name>Alexandre Routier</name>
    </author>
    <author>
      <name>Jérémy Guillon</name>
    </author>
    <author>
      <name>Michael Bacci</name>
    </author>
    <author>
      <name>Junhao Wen</name>
    </author>
    <author>
      <name>Anne Bertrand</name>
    </author>
    <author>
      <name>Hugo Bertin</name>
    </author>
    <author>
      <name>Marie-Odile Habert</name>
    </author>
    <author>
      <name>Stanley Durrleman</name>
    </author>
    <author>
      <name>Theodoros Evgeniou</name>
    </author>
    <author>
      <name>Olivier Colliot</name>
    </author>
    <author>
      <name>for the Alzheimer's Disease Neuroimaging Initiative</name>
    </author>
    <author>
      <name>the Australian Imaging Biomarkers</name>
    </author>
    <author>
      <name>Lifestyle flagship study of ageing</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neuroimage.2018.08.042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neuroimage.2018.08.042" rel="related"/>
    <link href="http://arxiv.org/abs/1808.06452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09477v3</id>
    <updated>2018-08-20T13:50:41Z</updated>
    <published>2017-08-30T21:19:30Z</published>
    <title>A Compressive Sensing Approach to Community Detection with Applications</title>
    <summary>  The community detection problem for graphs asks one to partition the n
vertices V of a graph G into k communities, or clusters, such that there are
many intracluster edges and few intercluster edges. Of course this is
equivalent to finding a permutation matrix P such that, if A denotes the
adjacency matrix of G, then PAP^T is approximately block diagonal. As there are
k^n possible partitions of n vertices into k subsets, directly determining the
optimal clustering is clearly infeasible. Instead one seeks to solve a more
tractable approximation to the clustering problem. In this paper we reformulate
the community detection problem via sparse solution of a linear system
associated with the Laplacian of a graph G and then develop a two-stage
approach based on a thresholding technique and a compressive sensing algorithm
to find a sparse solution which corresponds to the community containing a
vertex of interest in G. Crucially, our approach results in an algorithm which
is able to find a single cluster of size n_0 in O(nlog(n)n_0) operations and
all k clusters in fewer than O(n^2ln(n)) operations. This is a marked
improvement over the classic spectral clustering algorithm, which is unable to
find a single cluster at a time and takes approximately O(n^3) operations to
find all k clusters. Moreover, we are able to provide robust guarantees of
success for the case where G is drawn at random from the Stochastic Block
Model, a popular model for graphs with clusters. Extensive numerical results
are also provided, showing the efficacy of our algorithm on both synthetic and
real-world data sets.
</summary>
    <author>
      <name>Ming-Jun Lai</name>
    </author>
    <author>
      <name>Daniel Mckenzie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 10 figures Version 2, disabled 'showkeys' package. Note
  that there is an error in the proof of Lemma 5.1. A correct version of this
  lemma, as well as a greatly improved version of the central algorithm of this
  paper, is available at: arXiv:1808.05780</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.09477v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09477v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06444v1</id>
    <updated>2018-08-20T13:36:07Z</updated>
    <published>2018-08-20T13:36:07Z</published>
    <title>Synthetic Patient Generation: A Deep Learning Approach Using Variational
  Autoencoders</title>
    <summary>  Artificial Intelligence in healthcare is a new and exciting frontier and the
possibilities are endless. With deep learning approaches beating human
performances in many areas, the logical next step is to attempt their
application in the health space. For these and other Machine Learning
approaches to produce good results and have their potential realized, the need
for, and importance of, large amounts of accurate data is second to none. This
is a challenge faced by many industries and more so in the healthcare space. We
present an approach of using Variational Autoencoders (VAE's) as an approach to
generating more data for training deeper networks, as well as uncovering
underlying patterns in diagnoses and the patients suffering from them. By
training a VAE, on available data, it was able to learn the latent distribution
of the patient features given the diagnosis. It is then possible, after
training, to sample from the learnt latent distribution to generate new
accurate patient records given the patient diagnosis.
</summary>
    <author>
      <name>Ally Salim Jr</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T00" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01085v5</id>
    <updated>2018-08-20T13:05:59Z</updated>
    <published>2018-07-03T11:19:17Z</published>
    <title>One-Class Kernel Spectral Regression for Outlier Detection</title>
    <summary>  The paper introduces a new efficient nonlinear one-class classifier
formulated as the Rayleigh quotient criterion optimisation. The method,
operating in a reproducing kernel Hilbert subspace, minimises the scatter of
target distribution along an optimal projection direction while at the same
time keeping projections of positive observations distant from the mean of the
negative class. We provide a graph embedding view of the problem which can then
be solved efficiently using the spectral regression approach. In this sense,
unlike previous similar methods which often require costly eigen-computations
of dense matrices, the proposed approach casts the problem under consideration
into a regression framework which is computationally more efficient. In
particular, it is shown that the dominant complexity of the proposed method is
the complexity of computing the kernel matrix. Additional appealing
characteristics of the proposed one-class classifier are: 1-the ability to be
trained in an incremental fashion (allowing for application in streaming data
scenarios while also reducing the computational complexity in a non-streaming
operation mode); 2-being unsupervised, but providing the option for refining
the solution using negative training examples, when available; Last but not
least, 3-the use of the kernel trick which facilitates a nonlinear mapping of
the data into a high-dimensional feature space to seek better solutions.
</summary>
    <author>
      <name>Shervin Rahimzadeh Arashloo</name>
    </author>
    <author>
      <name>Josef Kittler</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01085v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01085v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06396v1</id>
    <updated>2018-08-20T11:39:09Z</updated>
    <published>2018-08-20T11:39:09Z</published>
    <title>DeeSIL: Deep-Shallow Incremental Learning</title>
    <summary>  Incremental Learning (IL) is an interesting AI problem when the algorithm is
assumed to work on a budget. This is especially true when IL is modeled using a
deep learning approach, where two com- plex challenges arise due to limited
memory, which induces catastrophic forgetting and delays related to the
retraining needed in order to incorpo- rate new classes. Here we introduce
DeeSIL, an adaptation of a known transfer learning scheme that combines a fixed
deep representation used as feature extractor and learning independent shallow
classifiers to in- crease recognition capacity. This scheme tackles the two
aforementioned challenges since it works well with a limited memory budget and
each new concept can be added within a minute. Moreover, since no deep re-
training is needed when the model is incremented, DeeSIL can integrate larger
amounts of initial data that provide more transferable features. Performance is
evaluated on ImageNet LSVRC 2012 against three state of the art algorithms.
Results show that, at scale, DeeSIL performance is 23 and 33 points higher than
the best baseline when using the same and more initial data respectively.
</summary>
    <author>
      <name>Eden Belouadah</name>
    </author>
    <author>
      <name>Adrian Popescu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06394v1</id>
    <updated>2018-08-20T11:38:46Z</updated>
    <published>2018-08-20T11:38:46Z</published>
    <title>Faster Support Vector Machines</title>
    <summary>  The time complexity of support vector machines (SVMs) prohibits training on
huge data sets with millions of samples. Recently, multilevel approaches to
train SVMs have been developed to allow for time efficient training on huge
data sets. While regular SVMs perform the entire training in one - time
consuming - optimization step, multilevel SVMs first build a hierarchy of
problems decreasing in size that resemble the original problem and then train
an SVM model for each hierarchy level benefiting from the solved models of
previous levels. We present a faster multilevel support vector machine that
uses a label propagation algorithm to construct the problem hierarchy.
Extensive experiments show that our new algorithm achieves speed-ups up to two
orders of magnitude while having similar or better classification quality over
state-of-the-art algorithms.
</summary>
    <author>
      <name>Sebastian Schlag</name>
    </author>
    <author>
      <name>Matthias Schmitt</name>
    </author>
    <author>
      <name>Christian Schulz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11156v3</id>
    <updated>2018-08-20T10:32:55Z</updated>
    <published>2018-07-30T03:14:50Z</published>
    <title>Transformationally Identical and Invariant Convolutional Neural Networks
  by Combining Symmetric Operations or Input Vectors</title>
    <summary>  Transformationally invariant processors constructed by transformed input
vectors or operators have been suggested and applied to many applications. In
this study, transformationally identical processing based on combining results
of all sub-processes with corresponding transformations at one of the
processing steps or at the beginning step were found to be equivalent for a
given condition. This property can be applied to most convolutional neural
network (CNN) systems. Specifically, a transformationally identical CNN can be
constructed by arranging internally symmetric operations in parallel with the
same transformation family that includes a flatten layer with weights sharing
among their corresponding transformation elements. Other transformationally
identical CNNs can be constructed by averaging transformed input vectors of the
family at the input layer followed by an ordinary CNN process or by a set of
symmetric operations. Interestingly, we found that both types of
transformationally identical CNN systems are mathematically equivalent by
either applying an averaging operation to corresponding elements of all
sub-channels before the activation function or without using a non-linear
activation function.
</summary>
    <author>
      <name>ShihChung B. Lo</name>
    </author>
    <author>
      <name>Matthew T. Freedman</name>
    </author>
    <author>
      <name>Seong K. Mun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11156v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11156v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06362v1</id>
    <updated>2018-08-20T09:41:42Z</updated>
    <published>2018-08-20T09:41:42Z</published>
    <title>Towards Anticipation of Architectural Smells using Link Prediction
  Techniques</title>
    <summary>  Software systems naturally evolve, and this evolution often brings design
problems that cause system degradation. Architectural smells are typical
symptoms of such problems, and several of these smells are related to undesired
dependencies among modules. The early detection of these smells is important
for developers, because they can plan ahead for maintenance or refactoring
efforts, thus preventing system degradation. Existing tools for identifying
architectural smells can detect the smells once they exist in the source code.
This means that their undesired dependencies are already created. In this work,
we explore a forward-looking approach that is able to infer groups of likely
module dependencies that can anticipate architectural smells in a future system
version. Our approach considers the current module structure as a network,
along with information from previous versions, and applies link prediction
techniques (from the field of social network analysis). In particular, we focus
on dependency-related smells, such as Cyclic Dependency and Hublike Dependency,
which fit well with the link prediction model. An initial evaluation with two
open-source projects shows that, under certain considerations, the predictions
of our approach are satisfactory. Furthermore, the approach can be extended to
other types of dependency-based smells or metrics.
</summary>
    <author>
      <name>J. Andrés Díaz-Pace</name>
    </author>
    <author>
      <name>Antonela Tommasel</name>
    </author>
    <author>
      <name>Daniela Godoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at SCAM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06356v1</id>
    <updated>2018-08-20T09:28:24Z</updated>
    <published>2018-08-20T09:28:24Z</published>
    <title>Causal Discovery by Telling Apart Parents and Children</title>
    <summary>  We consider the problem of inferring the directed, causal graph from
observational data, assuming no hidden confounders. We take an information
theoretic approach, and make three main contributions.
  First, we show how through algorithmic information theory we can obtain SCI,
a highly robust, effective and computationally efficient test for conditional
independence---and show it outperforms the state of the art when applied in
constraint-based inference methods such as stable PC.
  Second, building upon on SCI, we show how to tell apart the parents and
children of a given node based on the algorithmic Markov condition. We give the
Climb algorithm to efficiently discover the directed, causal Markov
blanket---and show it is at least as accurate as inferring the global network,
while being much more efficient.
  Last, but not least, we detail how we can use the Climb score to direct those
edges that state of the art causal discovery algorithms based on PC or GES
leave undirected---and show this improves their precision, recall and F1 scores
by up to 20%.
</summary>
    <author>
      <name>Alexander Marx</name>
    </author>
    <author>
      <name>Jilles Vreeken</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06940v1</id>
    <updated>2018-08-20T09:25:30Z</updated>
    <published>2018-08-20T09:25:30Z</published>
    <title>End to End Vehicle Lateral Control Using a Single Fisheye Camera</title>
    <summary>  Convolutional neural networks are commonly used to control the steering angle
for autonomous cars. Most of the time, multiple long range cameras are used to
generate lateral failure cases. In this paper we present a novel model to
generate this data and label augmentation using only one short range fisheye
camera. We present our simulator and how it can be used as a consistent metric
for lateral end-to-end control evaluation. Experiments are conducted on a
custom dataset corresponding to more than 10000 km and 200 hours of open road
driving. Finally we evaluate this model on real world driving scenarios, open
road and a custom test track with challenging obstacle avoidance and sharp
turns. In our simulator based on real-world videos, the final model was capable
of more than 99% autonomy on urban road
</summary>
    <author>
      <name>Marin Toromanoff</name>
    </author>
    <author>
      <name>Emilie Wirbel</name>
    </author>
    <author>
      <name>Frédéric Wilhelm</name>
    </author>
    <author>
      <name>Camilo Vejarano</name>
    </author>
    <author>
      <name>Xavier Perrotton</name>
    </author>
    <author>
      <name>Fabien Moutarde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages paper accepted at IROS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06352v1</id>
    <updated>2018-08-20T09:06:21Z</updated>
    <published>2018-08-20T09:06:21Z</published>
    <title>Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality</title>
    <summary>  Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.
</summary>
    <author>
      <name>Sajad Saeedi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Bodin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Informatics, University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Harry Wagstaff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Informatics, University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Andy Nisbet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Luigi Nardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Electrical Engineering - Computer Systems, Stanford University, USA</arxiv:affiliation>
    </author>
    <author>
      <name>John Mawer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Melot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Oscar Palomar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Emanuele Vespa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Tom Spink</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Informatics, University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Cosmin Gorgovan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Webb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>James Clarkson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Erik Tomusk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Informatics, University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Thomas Debrunner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Kuba Kaszyk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Informatics, University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Pablo Gonzalez-de-Aledo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Andrey Rodchenko</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Graham Riley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Christos Kotselidis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Björn Franke</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Informatics, University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Michael F. P. O'Boyle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Informatics, University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew J. Davison</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Paul H. J. Kelly</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing, Imperial College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Mikel Luján</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Steve Furber</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Computer Science, University of Manchester, UK</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JPROC.2018.2856739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JPROC.2018.2856739" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06347v1</id>
    <updated>2018-08-20T08:46:33Z</updated>
    <published>2018-08-20T08:46:33Z</published>
    <title>A Distribution Similarity Based Regularizer for Learning Bayesian
  Networks</title>
    <summary>  Probabilistic graphical models compactly represent joint distributions by
decomposing them into factors over subsets of random variables. In Bayesian
networks, the factors are conditional probability distributions. For many
problems, common information exists among those factors. Adding similarity
restrictions can be viewed as imposing prior knowledge for model
regularization. With proper restrictions, learned models usually generalize
better. In this work, we study methods that exploit such high-level
similarities to regularize the learning process and apply them to the task of
modeling the wave propagation in inhomogeneous media. We propose a novel
distribution-based penalization approach that encourages similar conditional
probability distribution rather than force the parameters to be similar
explicitly. We show in experiment that our proposed algorithm solves the
modeling wave propagation problem, which other baseline methods are not able to
solve.
</summary>
    <author>
      <name>Weirui Kong</name>
    </author>
    <author>
      <name>Wenyi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06324v1</id>
    <updated>2018-08-20T06:41:01Z</updated>
    <published>2018-08-20T06:41:01Z</published>
    <title>PAC-learning is Undecidable</title>
    <summary>  The problem of attempting to learn the mapping between data and labels is the
crux of any machine learning task. It is, therefore, of interest to the machine
learning community on practical as well as theoretical counts to consider the
existence of a test or criterion for deciding the feasibility of attempting to
learn. We investigate the existence of such a criterion in the setting of
PAC-learning, basing the feasibility solely on whether the mapping to be learnt
lends itself to approximation by a given class of hypothesis functions. We show
that no such criterion exists, exposing a fundamental limitation in the
decidability of learning. In other words, we prove that testing for
PAC-learnability is undecidable in the Turing sense. We also briefly discuss
some of the probable implications of this result to the current practice of
machine learning.
</summary>
    <author>
      <name>Sairaam Venkatraman</name>
    </author>
    <author>
      <name>S Balasubramanian</name>
    </author>
    <author>
      <name>R Raghunatha Sarma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09781v1</id>
    <updated>2018-08-20T04:28:05Z</updated>
    <published>2018-08-20T04:28:05Z</published>
    <title>Self-Attentive Sequential Recommendation</title>
    <summary>  Sequential dynamics are a key feature of many modern recommender systems,
which seek to capture the `context' of users' activities on the basis of
actions they have performed recently. To capture such patterns, two approaches
have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs).
Markov Chains assume that a user's next action can be predicted on the basis of
just their last (or last few) actions, while RNNs in principle allow for
longer-term semantics to be uncovered. Generally speaking, MC-based methods
perform best in extremely sparse datasets, where model parsimony is critical,
while RNNs perform better in denser datasets where higher model complexity is
affordable. The goal of our work is to balance these two goals, by proposing a
self-attention based sequential model (SASRec) that allows us to capture
long-term semantics (like an RNN), but, using an attention mechanism, makes its
predictions based on relatively few actions (like an MC). At each time step,
SASRec seeks to identify which items are `relevant' from a user's action
history, and use them to predict the next item. Extensive empirical studies
show that our method outperforms various state-of-the-art sequential models
(including MC/CNN/RNN-based approaches) on both sparse and dense datasets.
Moreover, the model is an order of magnitude more efficient than comparable
CNN/RNN-based models. Visualizations on attention weights also show how our
model adaptively handles datasets with various density, and uncovers meaningful
patterns in activity sequences.
</summary>
    <author>
      <name>Wang-Cheng Kang</name>
    </author>
    <author>
      <name>Julian McAuley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICDM'18 as a long paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03277v2</id>
    <updated>2018-08-20T01:02:04Z</updated>
    <published>2018-08-09T17:10:21Z</published>
    <title>VerIDeep: Verifying Integrity of Deep Neural Networks through
  Sensitive-Sample Fingerprinting</title>
    <summary>  Deep learning has become popular, and numerous cloud-based services are
provided to help customers develop and deploy deep learning applications.
Meanwhile, various attack techniques have also been discovered to stealthily
compromise the model's integrity. When a cloud customer deploys a deep learning
model in the cloud and serves it to end-users, it is important for him to be
able to verify that the deployed model has not been tampered with, and the
model's integrity is protected.
  We propose a new low-cost and self-served methodology for customers to verify
that the model deployed in the cloud is intact, while having only black-box
access (e.g., via APIs) to the deployed model. Customers can detect arbitrary
changes to their deep learning models. Specifically, we define
\texttt{Sensitive-Sample} fingerprints, which are a small set of transformed
inputs that make the model outputs sensitive to the model's parameters. Even
small weight changes can be clearly reflected in the model outputs, and
observed by the customer. Our experiments on different types of model integrity
attacks show that we can detect model integrity breaches with high accuracy
($&gt;$99\%) and low overhead ($&lt;$10 black-box model accesses).
</summary>
    <author>
      <name>Zecheng He</name>
    </author>
    <author>
      <name>Tianwei Zhang</name>
    </author>
    <author>
      <name>Ruby B. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03277v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03277v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03698v2</id>
    <updated>2018-08-20T00:48:14Z</updated>
    <published>2018-08-10T20:37:52Z</published>
    <title>BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear
  Regressions</title>
    <summary>  In this paper we introduce a new machine learning (ML) model for nonlinear
regression called Boosting Smooth Transition Regression Trees (BooST). The main
advantage of the BooST model is that it estimates the derivatives (partial
effects) of very general nonlinear models, providing more interpretation about
the mapping between the covariates and the dependent variable than other tree
based models, such as Random Forests. We provide some asymptotic theory that
shows consistency of the partial derivative estimates and we present some
examples on both simulated and real data.
</summary>
    <author>
      <name>Yuri Fonseca</name>
    </author>
    <author>
      <name>Marcelo Medeiros</name>
    </author>
    <author>
      <name>Gabriel Vasconcelos</name>
    </author>
    <author>
      <name>Alvaro Veiga</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03698v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03698v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06275v1</id>
    <updated>2018-08-20T00:24:41Z</updated>
    <published>2018-08-20T00:24:41Z</published>
    <title>Applying Machine Learning To Maize Traits Prediction</title>
    <summary>  Heterosis is the improved or increased function of any biological quality in
a hybrid offspring. We have studied yet the largest maize SNP dataset for
traits prediction. We develop linear and non-linear models which consider
relationships between different hybrids as well as other effect. Specially
designed model proved to be efficient and robust in prediction maize's traits.
</summary>
    <author>
      <name>Binbin Shi</name>
    </author>
    <author>
      <name>Xupeng Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00309v2</id>
    <updated>2018-08-19T20:21:33Z</updated>
    <published>2017-07-02T15:36:07Z</published>
    <title>Variance Regularizing Adversarial Learning</title>
    <summary>  We introduce a novel approach for training adversarial models by replacing
the discriminator score with a bi-modal Gaussian distribution over the
real/fake indicator variables. In order to do this, we train the Gaussian
classifier to match the target bi-modal distribution implicitly through
meta-adversarial training. We hypothesize that this approach ensures a non-zero
gradient to the generator, even in the limit of a perfect classifier. We test
our method against standard benchmark image datasets as well as show the
classifier output distribution is smooth and has overlap between the real and
fake modes.
</summary>
    <author>
      <name>Karan Grewal</name>
    </author>
    <author>
      <name>R Devon Hjelm</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Method is out of date and some results are incorrect</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09001v3</id>
    <updated>2018-08-19T17:00:10Z</updated>
    <published>2018-05-23T08:08:23Z</published>
    <title>One-to-one Mapping between Stimulus and Neural State: Memory and
  Classification</title>
    <summary>  Synaptic strength can be seen as probability to propagate impulse, and
function could exist from propagation activity to synaptic strength. If the
function satisfies constraints such as continuity and monotonicity, neural
network under external stimulus would always go to fixed point, and there could
be one-one-one mapping between external stimulus and synaptic strength at fixed
point. In other words, neural network "memorizes" external stimulus in its
synapses. A biological classifier is proposed to utilize this mapping.
</summary>
    <author>
      <name>Sizhong Lan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09001v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09001v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06206v1</id>
    <updated>2018-08-19T13:14:01Z</updated>
    <published>2018-08-19T13:14:01Z</published>
    <title>TLR: Transfer Latent Representation for Unsupervised Domain Adaptation</title>
    <summary>  Domain adaptation refers to the process of learning prediction models in a
target domain by making use of data from a source domain. Many classic methods
solve the domain adaptation problem by establishing a common latent space,
which may cause the loss of many important properties across both domains. In
this manuscript, we develop a novel method, transfer latent representation
(TLR), to learn a better latent space. Specifically, we design an objective
function based on a simple linear autoencoder to derive the latent
representations of both domains. The encoder in the autoencoder aims to project
the data of both domains into a robust latent space. Besides, the decoder
imposes an additional constraint to reconstruct the original data, which can
preserve the common properties of both domains and reduce the noise that causes
domain shift. Experiments on cross-domain tasks demonstrate the advantages of
TLR over competing methods.
</summary>
    <author>
      <name>Pan Xiao</name>
    </author>
    <author>
      <name>Bo Du</name>
    </author>
    <author>
      <name>Jia Wu</name>
    </author>
    <author>
      <name>Lefei Zhang</name>
    </author>
    <author>
      <name>Ruimin Hu</name>
    </author>
    <author>
      <name>Xuelong Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06191v1</id>
    <updated>2018-08-19T09:41:21Z</updated>
    <published>2018-08-19T09:41:21Z</published>
    <title>Fourier analysis perspective for sufficient dimension reduction problem</title>
    <summary>  A theory of sufficient dimension reduction (SDR) is developed from an
optimizational perspective. In our formulation of the problem, instead of
dealing with raw data, we assume that our ground truth includes a mapping
${\mathbf f}: {\mathbb R}^n\rightarrow {\mathbb R}^m$ and a probability
distribution function $p$ over ${\mathbb R}^n$, both given analytically. We
formulate SDR as a problem of finding a function ${\mathbf g}: {\mathbb
R}^k\rightarrow {\mathbb R}^m$ and a matrix $P\in {\mathbb R}^{k\times n}$ such
that ${\mathbb E}_{{\mathbf x}\sim p({\mathbf x})} \left|{\mathbf f}({\mathbf
x}) - {\mathbf g}(P{\mathbf x})\right|^2$ is minimal. It turns out that the
latter problem allows a reformulation in the dual space, i.e. instead of
searching for ${\mathbf g}(P{\mathbf x})$ we suggest searching for its Fourier
transform. First, we characterize all tempered distributions that can serve as
the Fourier transform of such functions. The reformulation in the dual space
can be interpreted as a problem of finding a $k$-dimensional linear subspace
$S$ and a tempered distribution ${\mathbf t}$ supported in $S$ such that
${\mathbf t}$ is "close" in a certain sense to the Fourier transform of
${\mathbf f}$.
  Instead of optimizing over generalized functions with a $k$-dimensional
support, we suggest minimizing over ordinary functions but with an additional
term $R$ that penalizes a strong distortion of the support from any
$k$-dimensional linear subspace. For a specific case of $R$, we develop an
algorithm that can be formulated for functions given in the initial form as
well as for their Fourier transforms. Eventually, we report results of
numerical experiments with a discretized version of the latter algorithm.
</summary>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06170v1</id>
    <updated>2018-08-19T06:21:58Z</updated>
    <published>2018-08-19T06:21:58Z</published>
    <title>Linked Recurrent Neural Networks</title>
    <summary>  Recurrent Neural Networks (RNNs) have been proven to be effective in modeling
sequential data and they have been applied to boost a variety of tasks such as
document classification, speech recognition and machine translation. Most of
existing RNN models have been designed for sequences assumed to be identically
and independently distributed (i.i.d). However, in many real-world
applications, sequences are naturally linked. For example, web documents are
connected by hyperlinks; and genes interact with each other. On the one hand,
linked sequences are inherently not i.i.d., which poses tremendous challenges
to existing RNN models. On the other hand, linked sequences offer link
information in addition to the sequential information, which enables
unprecedented opportunities to build advanced RNN models. In this paper, we
study the problem of RNN for linked sequences. In particular, we introduce a
principled approach to capture link information and propose a linked Recurrent
Neural Network (LinkedRNN), which models sequential and link information
coherently. We conduct experiments on real-world datasets from multiple domains
and the experimental results validate the effectiveness of the proposed
framework.
</summary>
    <author>
      <name>Zhiwei Wang</name>
    </author>
    <author>
      <name>Yao Ma</name>
    </author>
    <author>
      <name>Dawei Yin</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06762v2</id>
    <updated>2018-08-19T05:57:41Z</updated>
    <published>2018-03-18T23:06:20Z</published>
    <title>Towards an Efficient Anomaly-Based Intrusion Detection for
  Software-Defined Networks</title>
    <summary>  Software-defined networking (SDN) is a new paradigm that allows developing
more flexible network applications. SDN controller, which represents a
centralized controlling point, is responsible for running various network
applications as well as maintaining different network services and
functionalities. Choosing an efficient intrusion detection system helps in
reducing the overhead of the running controller and creates a more secure
network. In this study, we investigate the performance of the well-known
anomaly-based intrusion detection approaches in terms of accuracy, false alarm
rate, precision, recall, f1-measure, area under ROC curve, execution time and
Mc Nemar's test. Precisely, we focus on supervised machine-learning approaches
where we use the following classifiers: Decision Trees (DT), Extreme Learning
Machine (ELM), Naive Bayes (NB), Linear Discriminant Analysis (LDA), Neural
Networks (NN), Support Vector Machines (SVM), Random Forest (RT), K
Nearest-Neighbour (KNN), AdaBoost, RUSBoost, LogitBoost and BaggingTrees where
we employ the well-known NSL-KDD benchmark dataset to compare the performance
of each one of these classifiers.
</summary>
    <author>
      <name>Majd Latah</name>
    </author>
    <author>
      <name>Levent Toker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript has been accepted for publication in IET Networks,
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.06762v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06762v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00560v2</id>
    <updated>2018-08-19T03:20:07Z</updated>
    <published>2018-02-02T05:09:10Z</published>
    <title>Interpretable Deep Convolutional Neural Networks via Meta-learning</title>
    <summary>  Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.
</summary>
    <author>
      <name>Xuan Liu</name>
    </author>
    <author>
      <name>Xiaoguang Wang</name>
    </author>
    <author>
      <name>Stan Matwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures, 2018 International Joint Conference on Neural
  Networks, in press</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00560v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00560v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07752v1</id>
    <updated>2018-08-19T03:19:40Z</updated>
    <published>2018-08-19T03:19:40Z</published>
    <title>Prediction of Reynolds Stresses in High-Mach-Number Turbulent Boundary
  Layers using Physics-Informed Machine Learning</title>
    <summary>  Modeled Reynolds stress is a major source of model-form uncertainties in
Reynolds-averaged Navier-Stokes (RANS) simulations. Recently, a
physics-informed machine-learning (PIML) approach has been proposed for
reconstructing the discrepancies in RANS-modeled Reynolds stresses. The merits
of the PIML framework has been demonstrated in several canonical incompressible
flows. However, its performance on high-Mach-number flows is still not clear.
In this work we use the PIML approach to predict the discrepancies in RANS
modeled Reynolds stresses in high-Mach-number flat-plate turbulent boundary
layers by using an existing DNS database. Specifically, the discrepancy
function is first constructed using a DNS training flow and then used to
correct RANS-predicted Reynolds stresses under flow conditions different from
the DNS. The machine-learning technique is shown to significantly improve
RANS-modeled turbulent normal stresses, the turbulent kinetic energy, and the
Reynolds-stress anisotropy. Improvements are consistently observed when
different training datasets are used. Moreover, a high-dimensional
visualization technique and distance metrics are used to provide a priori
assessment of prediction confidence based only on RANS simulations. This study
demonstrates that the PIML approach is a computationally affordable technique
for improving the accuracy of RANS-modeled Reynolds stresses for
high-Mach-number turbulent flows when there is a lack of experiments and
high-fidelity simulations.
</summary>
    <author>
      <name>Jian-Xun Wang</name>
    </author>
    <author>
      <name>Junji Huang</name>
    </author>
    <author>
      <name>Lian Duan</name>
    </author>
    <author>
      <name>Heng Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06152v1</id>
    <updated>2018-08-19T02:16:40Z</updated>
    <published>2018-08-19T02:16:40Z</published>
    <title>On Design of Problem Token Questions in Quality of Experience Surveys</title>
    <summary>  User surveys for Quality of Experience (QoE) are a critical source of
information. In addition to the common "star rating" used to estimate Mean
Opinion Score (MOS), more detailed survey questions (problem tokens) about
specific areas provide valuable insight into the factors impacting QoE. This
paper explores two aspects of the problem token questionnaire design. First, we
study the bias introduced by fixed question order, and second, we study the
challenge of selecting a subset of questions to keep the token set small. Based
on 900,000 calls gathered using a randomized controlled experiment from a live
system, we find that the order bias can be significantly reduced by randomizing
the display order of tokens. The difference in response rate varies based on
token position and display design. It is worth noting that the users respond to
the randomized-order variant at levels that are comparable to the fixed-order
variant. The effective selection of a subset of token questions is achieved by
extracting tokens that provide the highest information gain over user ratings.
This selection is known to be in the class of NP-hard problems. We apply a
well-known greedy submodular maximization method on our dataset to capture 94%
of the information using just 30% of the questions.
</summary>
    <author>
      <name>Jayant Gupchup</name>
    </author>
    <author>
      <name>Ebrahim Beyrami</name>
    </author>
    <author>
      <name>Martin Ellis</name>
    </author>
    <author>
      <name>Yasaman Hosseinkashi</name>
    </author>
    <author>
      <name>Sam Johnson</name>
    </author>
    <author>
      <name>Ross Cutler</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08402v3</id>
    <updated>2018-08-18T19:43:52Z</updated>
    <published>2018-05-22T05:29:13Z</published>
    <title>Adapted Deep Embeddings: A Synthesis of Methods for $k$-Shot Inductive
  Transfer Learning</title>
    <summary>  The focus in machine learning has branched beyond training classifiers on a
single task to investigating how previously acquired knowledge in a source
domain can be leveraged to facilitate learning in a related target domain,
known as inductive transfer learning. Three active lines of research have
independently explored transfer learning using neural networks. In weight
transfer, a model trained on the source domain is used as an initialization
point for a network to be trained on the target domain. In deep metric
learning, the source domain is used to construct an embedding that captures
class structure in both the source and target domains. In few-shot learning,
the focus is on generalizing well in the target domain based on a limited
number of labeled examples. We compare state-of-the-art methods from these
three paradigms and also explore hybrid adapted-embedding methods that use
limited target-domain data to fine tune embeddings constructed from
source-domain data. We conduct a systematic comparison of methods in a variety
of domains, varying the number of labeled instances available in the target
domain ($k$), as well as the number of target-domain classes. We reach three
principal conclusions: (1) Deep embeddings are far superior, compared to weight
transfer, as a starting point for inter-domain transfer or model re-use (2) Our
hybrid methods robustly outperform every few-shot learning and every deep
metric learning method previously proposed, with a mean error reduction of 30%
over state-of-the-art. (3) Among loss functions for discovering embeddings, the
histogram loss (Ustinova &amp; Lempitsky, 2016) is most robust. We hope our results
will motivate a unification of research in weight transfer, deep metric
learning, and few-shot learning.
</summary>
    <author>
      <name>Tyler R. Scott</name>
    </author>
    <author>
      <name>Karl Ridgeway</name>
    </author>
    <author>
      <name>Michael C. Mozer</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08402v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08402v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06117v1</id>
    <updated>2018-08-18T19:07:42Z</updated>
    <published>2018-08-18T19:07:42Z</published>
    <title>Effect of secular trend in drug effectiveness study in real world data</title>
    <summary>  We discovered secular trend bias in a drug effectiveness study for a recently
approved drug. We compared treatment outcomes between patients who received the
newly approved drug and patients exposed to the standard treatment. All
patients diagnosed after the new drug's approval date were considered. We built
a machine learning causal inference model to determine patient subpopulations
likely to respond better to the newly approved drug. After identifying the
presence of secular trend bias in our data, we attempted to adjust for the bias
in two different ways. First, we matched patients on the number of days from
the new drug's approval date that the patient's treatment (new or standard)
began. Second, we included a covariate in the model for the number of days
between the date of approval of the new drug and the treatment (new or
standard) start date. Neither approach completely mitigated the bias. Residual
bias we attribute to differences in patient disease severity or other
unmeasured patient characteristics. Had we not identified the secular trend
bias in our data, the causal inference model would have been interpreted
without consideration for this underlying bias. Being aware of, testing for,
and handling potential bias in the data is essential to diminish the
uncertainty in AI modeling.
</summary>
    <author>
      <name>Sharon Hensley Alford</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Watson</arxiv:affiliation>
    </author>
    <author>
      <name>Piyush Madan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Shilpa Mahatma</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Italo Buleje</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Yanyan Han</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Fang Lu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 7th Causal Inference Workshop at Uncertainty in
  Artificial Intelligence (UAI) Conference 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06107v1</id>
    <updated>2018-08-18T17:49:36Z</updated>
    <published>2018-08-18T17:49:36Z</published>
    <title>Exact Passive-Aggressive Algorithms for Learning to Rank Using Interval
  Labels</title>
    <summary>  In this paper, we propose exact passive-aggressive (PA) online algorithms for
learning to rank. The proposed algorithms can be used even when we have
interval labels instead of actual labels for examples. The proposed algorithms
solve a convex optimization problem at every trial. We find exact solution to
those optimization problems to determine the updated parameters. We propose
support class algorithm (SCA) which finds the active constraints using the KKT
conditions of the optimization problems. These active constrains form support
set which determines the set of thresholds that need to be updated. We derive
update rules for PA, PA-I and PA-II. We show that the proposed algorithms
maintain the ordering of the thresholds after every trial. We provide the
mistake bounds of the proposed algorithms in both ideal and general settings.
We also show experimentally that the proposed algorithms successfully learn
accurate classifiers using interval labels as well as exact labels. Proposed
algorithms also do well compared to other approaches.
</summary>
    <author>
      <name>Naresh Manwani</name>
    </author>
    <author>
      <name>Mohit Chandra</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03314v2</id>
    <updated>2018-08-18T16:05:02Z</updated>
    <published>2018-08-09T19:31:42Z</published>
    <title>Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term
  Memory (LSTM) Network</title>
    <summary>  Because of their effectiveness in broad practical applications, LSTM networks
have received a wealth of coverage in scientific journals, technical blogs, and
implementation guides. However, in most articles, the inference formulas for
the LSTM network and its parent, RNN, are stated axiomatically, while the
training formulas are omitted altogether. In addition, the technique of
"unrolling" an RNN is routinely presented without justification throughout the
literature. The goal of this paper is to explain the essential RNN and LSTM
fundamentals in a single document. Drawing from concepts in signal processing,
we formally derive the canonical RNN formulation from differential equations.
We then propose and prove a precise statement, which yields the RNN unrolling
technique. We also review the difficulties with training the standard RNN and
address them by transforming the RNN into the "Vanilla LSTM" network through a
series of logical arguments. We provide all equations pertaining to the LSTM
system together with detailed descriptions of its constituent entities. Albeit
unconventional, our choice of notation and the method for presenting the LSTM
system emphasizes ease of understanding. As part of the analysis, we identify
new opportunities to enrich the LSTM system and incorporate these extensions
into the Vanilla LSTM network, producing the most general LSTM variant to date.
The target reader has already been exposed to RNNs and LSTM networks through
numerous available resources and is open to an alternative pedagogical
approach. A Machine Learning practitioner seeking guidance for implementing our
new augmented LSTM model in software for experimentation and research will find
the insights and derivations in this tutorial valuable as well.
</summary>
    <author>
      <name>Alex Sherstinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 10 figures, 65 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01316v3</id>
    <updated>2018-08-18T15:02:51Z</updated>
    <published>2018-03-04T08:26:43Z</published>
    <title>On Cognitive Preferences and the Plausibility of Rule-based Models</title>
    <summary>  It is conventional wisdom in machine learning and data mining that logical
models such as rule sets are more interpretable than other models, and that
among such rule-based models, simpler models are more interpretable than more
complex ones. In this position paper, we question this latter assumption by
focusing on one particular aspect of interpretability, namely the plausibility
of models. Roughly speaking, we equate the plausibility of a model with the
likeliness that a user accepts it as an explanation for a prediction. In
particular, we argue that, all other things being equal, longer explanations
may be more convincing than shorter ones, and that the predominant bias for
shorter models, which is typically necessary for learning powerful
discriminative models, may not be suitable when it comes to user acceptance of
the learned models. To that end, we first recapitulate evidence for and against
this postulate, and then report the results of an evaluation in a
crowd-sourcing study based on about 3.000 judgments. The results do not reveal
a strong preference for simple rules, whereas we can observe a weak preference
for longer rules in some domains. We then relate these results to well-known
cognitive biases such as the conjunction fallacy, the representative heuristic,
or the recogition heuristic, and investigate their relation to rule length and
plausibility.
</summary>
    <author>
      <name>Johannes Fürnkranz</name>
    </author>
    <author>
      <name>Tomáš Kliegr</name>
    </author>
    <author>
      <name>Heiko Paulheim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">V3: Complete rewrite of section on interpretability to clarify focus
  on plausibility</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01316v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01316v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06603v1</id>
    <updated>2018-08-18T14:43:20Z</updated>
    <published>2018-08-18T14:43:20Z</published>
    <title>Network-based Biased Tree Ensembles (NetBiTE) for Drug Sensitivity
  Prediction and Drug Sensitivity Biomarker Identification in Cancer</title>
    <summary>  We present the Network-based Biased Tree Ensembles (NetBiTE) method for drug
sensitivity prediction and drug sensitivity biomarker identification in cancer
using a combination of prior knowledge and gene expression data. Our devised
method consists of a biased tree ensemble that is built according to a
probabilistic bias weight distribution. The bias weight distribution is
obtained from the assignment of high weights to the drug targets and
propagating the assigned weights over a protein-protein interaction network
such as STRING. The propagation of weights, defines neighborhoods of influence
around the drug targets and as such simulates the spread of perturbations
within the cell, following drug administration. Using a synthetic dataset, we
showcase how application of biased tree ensembles (BiTE) results in significant
accuracy gains at a much lower computational cost compared to the unbiased
random forests (RF) algorithm. We then apply NetBiTE to the Genomics of Drug
Sensitivity in Cancer (GDSC) dataset and demonstrate that NetBiTE outperforms
RF in predicting IC50 drug sensitivity, only for drugs that target membrane
receptor pathways (MRPs): RTK, EGFR and IGFR signaling pathways. We propose
based on the NetBiTE results, that for drugs that inhibit MRPs, the expression
of target genes prior to drug administration is a biomarker for IC50 drug
sensitivity following drug administration. We further verify and reinforce this
proposition through control studies on, PI3K/MTOR signaling pathway inhibitors,
a drug category that does not target MRPs, and through assignment of dummy
targets to MRP inhibiting drugs and investigating the variation in NetBiTE
accuracy.
</summary>
    <author>
      <name>Ali Oskooei</name>
    </author>
    <author>
      <name>Matteo Manica</name>
    </author>
    <author>
      <name>Roland Mathis</name>
    </author>
    <author>
      <name>Maria Rodriguez Martinez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 5 figures, 3 supplementary figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06088v1</id>
    <updated>2018-08-18T14:30:57Z</updated>
    <published>2018-08-18T14:30:57Z</published>
    <title>Tangent-Normal Adversarial Regularization for Semi-supervised Learning</title>
    <summary>  The ever-increasing size of modern datasets combined with the difficulty of
obtaining label information has made semi-supervised learning of significant
practical importance in modern machine learning applications. Compared with
supervised learning, the key difficulty in semi-supervised learning is how to
make full use of the unlabeled data. In order to utilize manifold information
provided by unlabeled data, we propose a novel regularization called the
tangent-normal adversarial regularization, which is composed by two parts. The
two terms complement with each other and jointly enforce the smoothness along
two different directions that are crucial for semi-supervised learning. One is
applied along the tangent space of the data manifold, aiming to enforce local
invariance of the classifier on the manifold, while the other is performed on
the normal space orthogonal to the tangent space, intending to impose
robustness on the classifier against the noise causing the observed data
deviating from the underlying data manifold. Both of the two regularizers are
achieved by the strategy of virtual adversarial training. Our method has
achieved state-of-the-art performance on semi-supervised learning tasks on both
artificial dataset and FashionMNIST dataset.
</summary>
    <author>
      <name>Bing Yu</name>
    </author>
    <author>
      <name>Jingfeng Wu</name>
    </author>
    <author>
      <name>Zhanxing Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06622v2</id>
    <updated>2018-08-18T14:13:37Z</updated>
    <published>2018-03-18T07:51:19Z</published>
    <title>Learning recurrent dynamics in spiking networks</title>
    <summary>  Spiking activity of neurons engaged in learning and performing a task show
complex spatiotemporal dynamics. While the output of recurrent network models
can learn to perform various tasks, the possible range of recurrent dynamics
that emerge after learning remains unknown. Here we show that modifying the
recurrent connectivity with a recursive least squares algorithm provides
sufficient flexibility for synaptic and spiking rate dynamics of spiking
networks to produce a wide range of spatiotemporal activity. We apply the
training method to learn arbitrary firing patterns, stabilize irregular spiking
activity of a balanced network, and reproduce the heterogeneous spiking rate
patterns of cortical neurons engaged in motor planning and movement. We
identify sufficient conditions for successful learning, characterize two types
of learning errors, and assess the network capacity. Our findings show that
synaptically-coupled recurrent spiking networks possess a vast computational
capability that can support the diverse activity patterns in the brain.
</summary>
    <author>
      <name>Christopher Kim</name>
    </author>
    <author>
      <name>Carson Chow</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06622v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06622v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06079v1</id>
    <updated>2018-08-18T12:34:17Z</updated>
    <published>2018-08-18T12:34:17Z</published>
    <title>Community detection in networks with unobserved edges</title>
    <summary>  We develop a Bayesian hierarchical model to identify communities in networks
for which we do not observe the edges directly, but instead observe a series of
interdependent signals for each of the nodes. Fitting the model provides an
end-to-end community detection algorithm that does not extract information as a
sequence of point estimates but propagates uncertainties from the raw data to
the community labels. Our approach naturally supports multiscale community
detection as well as the selection of an optimal scale using model comparison.
We study the properties of the algorithm using synthetic data and apply it to
daily returns of constituents of the S&amp;P100 index as well as climate data from
US cities.
</summary>
    <author>
      <name>Till Hoffmann</name>
    </author>
    <author>
      <name>Leto Peel</name>
    </author>
    <author>
      <name>Renaud Lambiotte</name>
    </author>
    <author>
      <name>Nick S. Jones</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07379v1</id>
    <updated>2018-08-18T10:42:52Z</updated>
    <published>2018-08-18T10:42:52Z</published>
    <title>Privacy Mining from IoT-based Smart Homes</title>
    <summary>  Recently, a wide range of smart devices are deployed in a variety of
environments to improve the quality of human life. One of the important
IoT-based applications is smart homes for healthcare, especially for elders.
IoT-based smart homes enable elders' health to be properly monitored and taken
care of. However, elders' privacy might be disclosed from smart homes due to
non-fully protected network communication or other reasons. To demonstrate how
serious this issue is, we introduce in this paper a Privacy Mining Approach
(PMA) to mine privacy from smart homes by conducting a series of deductions and
analyses on sensor datasets generated by smart homes. The experimental results
demonstrate that PMA is able to deduce a global sensor topology for a smart
home and disclose elders' privacy in terms of their house layouts.
</summary>
    <author>
      <name>Ming-Chang Lee</name>
    </author>
    <author>
      <name>Jia-Chun Lin</name>
    </author>
    <author>
      <name>Olaf Owe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper, which has 11 pages and 7 figures, has been accepted BWCCA
  2018 on 13th August 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00877v2</id>
    <updated>2018-08-18T04:19:15Z</updated>
    <published>2018-06-03T21:07:34Z</published>
    <title>Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual
  Optimization</title>
    <summary>  Despite the success of single-agent reinforcement learning, multi-agent
reinforcement learning (MARL) remains challenging due to complex interactions
between agents. Motivated by decentralized applications such as sensor
networks, swarm robotics, and power grids, we study policy evaluation in MARL,
where agents with jointly observed state-action pairs and private local rewards
collaborate to learn the value of a given policy.
  In this paper, we propose a double averaging scheme, where each agent
iteratively performs averaging over both space and time to incorporate
neighboring gradient information and local reward information, respectively. We
prove that the proposed algorithm converges to the optimal solution at a global
geometric rate. In particular, such an algorithm is built upon a primal-dual
reformulation of the mean squared projected Bellman error minimization problem,
which gives rise to a decentralized convex-concave saddle-point problem. To the
best of our knowledge, the proposed double averaging primal-dual optimization
algorithm is the first to achieve fast finite-time convergence on decentralized
convex-concave saddle-point problems.
</summary>
    <author>
      <name>Hoi-To Wai</name>
    </author>
    <author>
      <name>Zhuoran Yang</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Mingyi Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures; updated with improved convergence results</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10079v3</id>
    <updated>2018-08-17T23:51:52Z</updated>
    <published>2018-07-26T11:54:48Z</published>
    <title>Automatic Detection of Node-Replication Attack in Vehicular Ad-hoc
  Networks</title>
    <summary>  Recent advances in smart cities applications enforce security threads such as
node replication attacks. Such attack is take place when the attacker plants a
replicated network node within the network. Vehicular Ad hoc networks are
connecting sensors that have limited resources and required the response time
to be as low as possible. In this type networks, traditional detection
algorithms of node replication attacks are not efficient. In this paper, we
propose an initial idea to apply a newly adapted statistical methodology that
can detect node replication attacks with high performance as compared to
state-of-the-art techniques. We provide a sufficient description of this
methodology and a road-map for testing and experiment its performance.
</summary>
    <author>
      <name>Mohammed GH. I. AL Zamil</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10079v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10079v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10102v2</id>
    <updated>2018-08-17T21:38:55Z</updated>
    <published>2017-05-29T10:29:23Z</published>
    <title>Structural Conditions for Projection-Cost Preservation via Randomized
  Matrix Multiplication</title>
    <summary>  Projection-cost preservation is a low-rank approximation guarantee which
ensures that the cost of any rank-$k$ projection can be preserved using a
smaller sketch of the original data matrix. We present a general structural
result outlining four sufficient conditions to achieve projection-cost
preservation. These conditions can be satisfied using tools from the Randomized
Linear Algebra literature.
</summary>
    <author>
      <name>Agniva Chowdhury</name>
    </author>
    <author>
      <name>Jiasen Yang</name>
    </author>
    <author>
      <name>Petros Drineas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.10102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05979v1</id>
    <updated>2018-08-17T20:12:26Z</updated>
    <published>2018-08-17T20:12:26Z</published>
    <title>Optimizing Deep Neural Network Architecture: A Tabu Search Based
  Approach</title>
    <summary>  The performance of Feedforward neural network (FNN) fully de-pends upon the
selection of architecture and training algorithm. FNN architecture can be
tweaked using several parameters, such as the number of hidden layers, number
of hidden neurons at each hidden layer and number of connections between
layers. There may be exponential combinations for these architectural
attributes which may be unmanageable manually, so it requires an algorithm
which can automatically design an optimal architecture with high generalization
ability. Numerous optimization algorithms have been utilized for FNN
architecture determination. This paper proposes a new methodology which can
work on the estimation of hidden layers and their respective neurons for FNN.
This work combines the advantages of Tabu search (TS) and Gradient descent with
momentum backpropagation (GDM) training algorithm to demonstrate how Tabu
search can automatically select the best architecture from the populated
architectures based on minimum testing error criteria. The proposed approach
has been tested on four classification benchmark dataset of different size.
</summary>
    <author>
      <name>Tarun Kumar Gupta</name>
    </author>
    <author>
      <name>Khalid Raza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, 2 algorithms, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01960v2</id>
    <updated>2018-08-17T17:48:37Z</updated>
    <published>2018-03-05T23:46:27Z</published>
    <title>Thermodynamics of Restricted Boltzmann Machines and related learning
  dynamics</title>
    <summary>  We investigate the thermodynamic properties of a Restricted Boltzmann Machine
(RBM), a simple energy-based generative model used in the context of
unsupervised learning. Assuming the information content of this model to be
mainly reflected by the spectral properties of its weight matrix $W$, we try to
make a realistic analysis by averaging over an appropriate statistical ensemble
of RBMs.
  First, a phase diagram is derived. Otherwise similar to that of the
Sherrington- Kirkpatrick (SK) model with ferromagnetic couplings, the RBM's
phase diagram presents a ferromagnetic phase which may or may not be of
compositional type depending on the kurtosis of the distribution of the
components of the singular vectors of $W$.
  Subsequently, the learning dynamics of the RBM is studied in the
thermodynamic limit. A "typical" learning trajectory is shown to solve an
effective dynamical equation, based on the aforementioned ensemble average and
explicitly involving order parameters obtained from the thermodynamic analysis.
In particular, this let us show how the evolution of the dominant singular
values of $W$, and thus of the unstable modes, is driven by the input data. At
the beginning of the training, in which the RBM is found to operate in the
linear regime, the unstable modes reflect the dominant covariance modes of the
data. In the non-linear regime, instead, the selected modes interact and
eventually impose a matching of the order parameters to their empirical
counterparts estimated from the data.
  Finally, we illustrate our considerations by performing experiments on both
artificial and real data, showing in particular how the RBM operates in the
ferromagnetic compositional phase.
</summary>
    <author>
      <name>Aurélien Decelle</name>
    </author>
    <author>
      <name>Giancarlo Fissore</name>
    </author>
    <author>
      <name>Cyril Furtlehner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10955-018-2105-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10955-018-2105-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Decelle, A., Fissore, G. &amp; Furtlehner, C. J Stat Phys (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.01960v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01960v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05933v1</id>
    <updated>2018-08-17T17:20:06Z</updated>
    <published>2018-08-17T17:20:06Z</published>
    <title>Decentralized Dictionary Learning Over Time-Varying Digraphs</title>
    <summary>  This paper studies Dictionary Learning problems wherein the learning task is
distributed over a multi-agent network, modeled as a time-varying directed
graph. This formulation is relevant, for instance, in Big Data scenarios where
massive amounts of data are collected/stored in different locations (e.g.,
sensors, clouds) and aggregating and/or processing all data in a fusion center
might be inefficient or unfeasible, due to resource limitations, communication
overheads or privacy issues. We develop a unified decentralized algorithmic
framework for this class of nonconvex problems, and we establish its asymptotic
convergence to stationary solutions. The new method hinges on Successive Convex
Approximation techniques, coupled with a decentralized tracking mechanism
aiming at locally estimating the gradient of the smooth part of the
sum-utility. To the best of our knowledge, this is the first provably
convergent decentralized algorithm for Dictionary Learning and, more generally,
bi-convex problems over (time-varying) (di)graphs.
</summary>
    <author>
      <name>Amir Daneshmand</name>
    </author>
    <author>
      <name>Ying Sun</name>
    </author>
    <author>
      <name>Gesualdo Scutari</name>
    </author>
    <author>
      <name>Francisco Facchinei</name>
    </author>
    <author>
      <name>Brian M. Sadler</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08340v2</id>
    <updated>2018-08-17T17:13:01Z</updated>
    <published>2018-05-22T01:08:40Z</published>
    <title>Reducing Parameter Space for Neural Network Training</title>
    <summary>  For neural networks (NNs) with rectified linear unit (ReLU) or binary
activation functions, we show that their training can be accomplished in a
reduced parameter space. Specifically, the weights in each neuron can be
trained on the unit sphere, as opposed to the entire space, and the threshold
can be trained in a bounded interval, as opposed to the real line. We show that
the NNs in the reduced parameter space are mathematically equivalent to the
standard NNs with parameters in the whole space. The reduced parameter space
shall facilitate the optimization procedure for the network training, as the
search space becomes (much) smaller. We demonstrate the improved training
performance using numerical examples.
</summary>
    <author>
      <name>Tong Qin</name>
    </author>
    <author>
      <name>Ling Zhou</name>
    </author>
    <author>
      <name>Dongbin Xiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10692v2</id>
    <updated>2018-08-17T17:10:17Z</updated>
    <published>2018-06-10T13:04:53Z</published>
    <title>ActiveRemediation: The Search for Lead Pipes in Flint, Michigan</title>
    <summary>  We detail our ongoing work in Flint, Michigan to detect pipes made of lead
and other hazardous metals. After elevated levels of lead were detected in
residents' drinking water, followed by an increase in blood lead levels in area
children, the state and federal governments directed over $125 million to
replace water service lines, the pipes connecting each home to the water
system. In the absence of accurate records, and with the high cost of
determining buried pipe materials, we put forth a number of predictive and
procedural tools to aid in the search and removal of lead infrastructure.
Alongside these statistical and machine learning approaches, we describe our
interactions with government officials in recommending homes for both
inspection and replacement, with a focus on the statistical model that adapts
to incoming information. Finally, in light of discussions about increased
spending on infrastructure development by the federal government, we explore
how our approach generalizes beyond Flint to other municipalities nationwide.
</summary>
    <author>
      <name>Jacob Abernethy</name>
    </author>
    <author>
      <name>Alex Chojnacki</name>
    </author>
    <author>
      <name>Arya Farahi</name>
    </author>
    <author>
      <name>Eric Schwartz</name>
    </author>
    <author>
      <name>Jared Webb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219896</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219896" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, To appear in KDD 2018, For associated
  promotional video, see https://www.youtube.com/watch?v=YbIn_axYu9E</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05924v1</id>
    <updated>2018-08-17T16:42:09Z</updated>
    <published>2018-08-17T16:42:09Z</published>
    <title>Randomized Least Squares Regression: Combining Model- and
  Algorithm-Induced Uncertainties</title>
    <summary>  We analyze the uncertainties in the minimum norm solution of full-rank
regression problems, arising from Gaussian linear models, computed by
randomized (row-wise sampling and, more generally, sketching) algorithms. From
a deterministic perspective, our structural perturbation bounds imply that
least squares problems are less sensitive to multiplicative perturbations than
to additive perturbations. From a probabilistic perspective, our expressions
for the total expectation and variance with regard to both model- and
algorithm-induced uncertainties, are exact, hold for general sketching
matrices, and make no assumptions on the rank of the sketched matrix. The
relative differences between the total bias and variance on the one hand, and
the model bias and variance on the other hand, are governed by two factors: (i)
the expected rank deficiency of the sketched matrix, and (ii) the expected
difference between projectors associated with the original and the sketched
problems. A simple example, based on uniform sampling with replacement,
illustrates the statistical quantities.
</summary>
    <author>
      <name>Jocelyn T. Chi</name>
    </author>
    <author>
      <name>Ilse C. F. Ipsen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04044v3</id>
    <updated>2018-08-17T16:28:56Z</updated>
    <published>2017-11-10T23:42:42Z</published>
    <title>Kernelized Hashcode Representations for Biomedical Relation Extraction</title>
    <summary>  Kernel methods have produced state-of-the-art results for a number of NLP
tasks such as relation extraction, but suffer from poor scalability due to the
high cost of computing kernel similarities between discrete natural language
structures. A recently proposed technique, kernelized locality-sensitive
hashing (KLSH), can significantly reduce the computational cost, but is only
applicable to classifiers operating on kNN graphs. Here we propose to use
random subspaces of KLSH codes for efficiently constructing an explicit
representation of NLP structures suitable for general classification methods.
Further, we propose an approach for optimizing the KLSH model for
classification problems by maximizing a variational lower bound on mutual
information between the KLSH codes (feature vectors) and the class labels. We
evaluate the proposed approach on biomedical relation extraction datasets, and
observe significant and robust improvements in accuracy w.r.t. state-of-the-art
classifiers, along with drastic (orders-of-magnitude) speedup compared to
conventional kernel methods.
</summary>
    <author>
      <name>Sahil Garg</name>
    </author>
    <author>
      <name>Aram Galstyan</name>
    </author>
    <author>
      <name>Greg Ver Steeg</name>
    </author>
    <author>
      <name>Irina Rish</name>
    </author>
    <author>
      <name>Guillermo Cecchi</name>
    </author>
    <author>
      <name>Shuyang Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04044v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04044v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05917v1</id>
    <updated>2018-08-17T16:22:09Z</updated>
    <published>2018-08-17T16:22:09Z</published>
    <title>A bagging and importance sampling approach to Support Vector Machines</title>
    <summary>  An importance sampling and bagging approach to solving the support vector
machine (SVM) problem in the context of large databases is presented and
evaluated. Our algorithm builds on the nearest neighbors ideas presented in
Camelo at al. (2015). As in that reference, the goal of the present proposal is
to achieve a faster solution of the SVM problem without a significance loss in
the prediction error. The performance of the methodology is evaluated in
benchmark examples and theoretical aspects of subsample methods are discussed.
</summary>
    <author>
      <name>R. Bárcenas</name>
    </author>
    <author>
      <name>M. D. Gónzalez--Lima</name>
    </author>
    <author>
      <name>A. J. Quiroz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05904v1</id>
    <updated>2018-08-17T15:48:52Z</updated>
    <published>2018-08-17T15:48:52Z</published>
    <title>Correlated Multi-armed Bandits with a Latent Random Source</title>
    <summary>  We consider a novel multi-armed bandit framework where the rewards obtained
by pulling the arms are functions of a common latent random variable. The
correlation between arms due to the common random source can be used to design
a generalized upper-confidence-bound (UCB) algorithm that identifies certain
arms as $non-competitive$, and avoids exploring them. As a result, we reduce a
$K$-armed bandit problem to a $C+1$-armed problem, where $C+1$ includes the
best arm and $C$ $competitive$ arms. Our regret analysis shows that the
competitive arms need to be pulled $\mathcal{O}(\log T)$ times, while the
non-competitive arms are pulled only $\mathcal{O}(1)$ times. As a result, there
are regimes where our algorithm achieves a $\mathcal{O}(1)$ regret as opposed
to the typical logarithmic regret scaling of multi-armed bandit algorithms. We
also evaluate lower bounds on the expected regret and prove that our
correlated-UCB algorithm is order-wise optimal.
</summary>
    <author>
      <name>Samarth Gupta</name>
    </author>
    <author>
      <name>Gauri Joshi</name>
    </author>
    <author>
      <name>Osman Yağan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05902v1</id>
    <updated>2018-08-17T15:32:24Z</updated>
    <published>2018-08-17T15:32:24Z</published>
    <title>Learning Supervised Topic Models for Classification and Regression from
  Crowds</title>
    <summary>  The growing need to analyze large collections of documents has led to great
developments in topic modeling. Since documents are frequently associated with
other related variables, such as labels or ratings, much interest has been
placed on supervised topic models. However, the nature of most annotation
tasks, prone to ambiguity and noise, often with high volumes of documents, deem
learning under a single-annotator assumption unrealistic or unpractical for
most real-world applications. In this article, we propose two supervised topic
models, one for classification and another for regression problems, which
account for the heterogeneity and biases among different annotators that are
encountered in practice when learning from crowds. We develop an efficient
stochastic variational inference algorithm that is able to scale to very large
datasets, and we empirically demonstrate the advantages of the proposed model
over state-of-the-art approaches.
</summary>
    <author>
      <name>Filipe Rodrigues</name>
    </author>
    <author>
      <name>Mariana Lourenço</name>
    </author>
    <author>
      <name>Bernardete Ribeiro</name>
    </author>
    <author>
      <name>Francisco Pereira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2017.2648786</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2017.2648786" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Rodrigues, F., Lourenco, M., Ribeiro, B. and Pereira, F.C., 2017.
  Learning supervised topic models for classification and regression from
  crowds. IEEE transactions on pattern analysis and machine intelligence,
  39(12), pp.2409-2422</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.05902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02287v2</id>
    <updated>2018-08-17T15:27:29Z</updated>
    <published>2018-07-06T07:12:56Z</published>
    <title>Outperforming Good-Turing: Preliminary Report</title>
    <summary>  Estimating a large alphabet probability distribution from a limited number of
samples is a fundamental problem in machine learning and statistics. A variety
of estimation schemes have been proposed over the years, mostly inspired by the
early work of Laplace and the seminal contribution of Good and Turing. One of
the basic assumptions shared by most commonly-used estimators is the unique
correspondence between the symbol's sample frequency and its estimated
probability. In this work we tackle this paradigmatic assumption; we claim that
symbols with "similar" frequencies shall be assigned the same estimated
probability value. This way we regulate the number of parameters and improve
generalization. In this preliminary report we show that by applying an ensemble
of such regulated estimators, we introduce a dramatic enhancement in the
estimation accuracy (typically up to 50%), compared to currently known methods.
An implementation of our suggested method is publicly available at the first
author's web-page.
</summary>
    <author>
      <name>Amichai Painsky</name>
    </author>
    <author>
      <name>Meir Feder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has several inaccuracies in the experimental setup which
  require additional work</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11343v4</id>
    <updated>2018-08-17T14:37:09Z</updated>
    <published>2017-11-30T12:36:35Z</published>
    <title>Multivariate Time Series Classification with WEASEL+MUSE</title>
    <summary>  Multivariate time series (MTS) arise when multiple interconnected sensors
record data over time. Dealing with this high-dimensional data is challenging
for every classifier for at least two aspects: First, an MTS is not only
characterized by individual feature values, but also by the interplay of
features in different dimensions. Second, this typically adds large amounts of
irrelevant data and noise. We present our novel MTS classifier WEASEL+MUSE
which addresses both challenges. WEASEL+MUSE builds a multivariate feature
vector, first using a sliding-window approach applied to each dimension of the
MTS, then extracts discrete features per window and dimension. The feature
vector is subsequently fed through feature selection, removing
non-discriminative features, and analysed by a machine learning classifier. The
novelty of WEASEL+MUSE lies in its specific way of extracting and filtering
multivariate features from MTS by encoding context information into each
feature. Still the resulting feature set is small, yet very discriminative and
useful for MTS classification. Based on a popular benchmark of 20 MTS datasets,
we found that WEASEL+MUSE is among the most accurate classifiers, when compared
to the state of the art. The outstanding robustness of WEASEL+MUSE is further
confirmed based on motion gesture recognition data, where it out-of-the-box
achieved similar accuracies as domain-specific methods.
</summary>
    <author>
      <name>Patrick Schäfer</name>
    </author>
    <author>
      <name>Ulf Leser</name>
    </author>
    <link href="http://arxiv.org/abs/1711.11343v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11343v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06503v1</id>
    <updated>2018-08-17T14:26:47Z</updated>
    <published>2018-08-17T14:26:47Z</published>
    <title>Collaborative Pressure Ulcer Prevention: An Automated Skin Damage and
  Pressure Ulcer Assessment Tool for Nursing Professionals, Patients, Family
  Members and Carers</title>
    <summary>  This paper describes the Pressure Ulcers Online Website, which is a first
step solution towards a new and innovative platform for helping people to
detect, understand and manage pressure ulcers. It outlines the reasons why the
project has been developed and provides a central point of contact for pressure
ulcer analysis and ongoing research. Using state-of-the-art technologies in
convolutional neural networks and transfer learning along with end-to-end web
technologies, this platform allows pressure ulcers to be analysed and findings
to be reported. As the system evolves through collaborative partnerships,
future versions will provide decision support functions to describe the complex
characteristics of pressure ulcers along with information on wound care across
multiple user boundaries. This project is therefore intended to raise awareness
and support for people suffering with or providing care for pressure ulcers.
</summary>
    <author>
      <name>Paul Fergus</name>
    </author>
    <author>
      <name>Carl Chalmers</name>
    </author>
    <author>
      <name>David Tully</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 7 figures, Position Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05854v1</id>
    <updated>2018-08-17T13:31:30Z</updated>
    <published>2018-08-17T13:31:30Z</published>
    <title>Robust Compressive Phase Retrieval via Deep Generative Priors</title>
    <summary>  This paper proposes a new framework to regularize the highly ill-posed and
non-linear phase retrieval problem through deep generative priors using simple
gradient descent algorithm. We experimentally show effectiveness of proposed
algorithm for random Gaussian measurements (practically relevant in imaging
through scattering media) and Fourier friendly measurements (relevant in
optical set ups). We demonstrate that proposed approach achieves impressive
results when compared with traditional hand engineered priors including
sparsity and denoising frameworks for number of measurements and robustness
against noise. Finally, we show the effectiveness of the proposed approach on a
real transmission matrix dataset in an actual application of multiple
scattering media imaging.
</summary>
    <author>
      <name>Fahad Shamshad</name>
    </author>
    <author>
      <name>Ali Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07107v2</id>
    <updated>2018-08-17T13:08:31Z</updated>
    <published>2018-05-18T09:23:12Z</published>
    <title>Extending Dynamic Bayesian Networks for Anomaly Detection in Complex
  Logs</title>
    <summary>  Checking various log files from different processes can be a tedious task as
these logs contain lots of events, each with a (possibly large) number of
attributes. We developed a way to automatically model log files and detect
outlier traces in the data. For that we extend Dynamic Bayesian Networks to
model the normal behavior found in log files. We introduce a new algorithm that
is able to learn a model of a log file starting from the data itself. The model
is capable of scoring traces even when new values or new combinations of values
appear in the log file.
</summary>
    <author>
      <name>Stephen Pauwels</name>
    </author>
    <author>
      <name>Toon Calders</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07107v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07107v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05054v4</id>
    <updated>2018-08-17T11:51:01Z</updated>
    <published>2018-02-14T11:59:21Z</published>
    <title>GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement
  Learning Algorithms</title>
    <summary>  In continuous action domains, standard deep reinforcement learning algorithms
like DDPG suffer from inefficient exploration when facing sparse or deceptive
reward problems. Conversely, evolutionary and developmental methods focusing on
exploration like Novelty Search, Quality-Diversity or Goal Exploration
Processes explore more robustly but are less efficient at fine-tuning policies
using gradient descent. In this paper, we present the GEP-PG approach, taking
the best of both worlds by sequentially combining a Goal Exploration Process
and two variants of DDPG. We study the learning performance of these components
and their combination on a low dimensional deceptive reward problem and on the
larger Half-Cheetah benchmark. We show that DDPG fails on the former and that
GEP-PG improves over the best DDPG variant in both environments. Supplementary
videos and discussion can be found at http://frama.link/gep_pg, the code at
http://github.com/flowersteam/geppg.
</summary>
    <author>
      <name>Cédric Colas</name>
    </author>
    <author>
      <name>Olivier Sigaud</name>
    </author>
    <author>
      <name>Pierre-Yves Oudeyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at ICML 2018, 14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05054v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05054v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07562v2</id>
    <updated>2018-08-17T11:40:54Z</updated>
    <published>2018-06-20T05:53:32Z</published>
    <title>Efficient inference in stochastic block models with vertex labels</title>
    <summary>  We study the stochastic block model with two communities where vertices
contain side information in the form of a vertex label. These vertex labels may
have arbitrary label distributions, depending on the community memberships. We
analyze a linearized version of the popular belief propagation algorithm. We
show that this algorithm achieves the highest accuracy possible whenever a
certain function of the network parameters has a unique fixed point. Whenever
this function has multiple fixed points, the belief propagation algorithm may
not perform optimally. We show that increasing the information in the vertex
labels may reduce the number of fixed points and hence lead to optimality of
belief propagation.
</summary>
    <author>
      <name>Clara Stegehuis</name>
    </author>
    <author>
      <name>Laurent Massoulié</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07562v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07562v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05832v1</id>
    <updated>2018-08-17T11:25:19Z</updated>
    <published>2018-08-17T11:25:19Z</published>
    <title>Importance mixing: Improving sample reuse in evolutionary policy search
  methods</title>
    <summary>  Deep neuroevolution, that is evolutionary policy search methods based on deep
neural networks, have recently emerged as a competitor to deep reinforcement
learning algorithms due to their better parallelization capabilities. However,
these methods still suffer from a far worse sample efficiency. In this paper we
investigate whether a mechanism known as "importance mixing" can significantly
improve their sample efficiency. We provide a didactic presentation of
importance mixing and we explain how it can be extended to reuse more samples.
Then, from an empirical comparison based on a simple benchmark, we show that,
though it actually provides better sample efficiency, it is still far from the
sample efficiency of deep reinforcement learning, though it is more stable.
</summary>
    <author>
      <name>Aloïs Pourchot</name>
    </author>
    <author>
      <name>Nicolas Perrin</name>
    </author>
    <author>
      <name>Olivier Sigaud</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05819v1</id>
    <updated>2018-08-17T10:37:51Z</updated>
    <published>2018-08-17T10:37:51Z</published>
    <title>Motion Prediction of Traffic Actors for Autonomous Driving using Deep
  Convolutional Networks</title>
    <summary>  Recent algorithmic improvements and hardware breakthroughs resulted in a
number of success stories in the field of AI impacting our daily lives.
However, despite its ubiquity AI is only just starting to make advances in what
may arguably have the largest impact thus far, the nascent field of autonomous
driving. In this work we discuss this important topic and address one of
crucial aspects of the emerging area, the problem of predicting future state of
autonomous vehicle's surrounding necessary for safe and efficient operations.
We introduce a deep learning-based approach that takes into account current
state of traffic actors and produces rasterized representations of each actor's
vicinity. The raster images are then used by deep convolutional models to infer
future movement of actors while accounting for inherent uncertainty of the
prediction task. Extensive experiments on real-world data strongly suggest
benefits of the proposed approach. Moreover, following successful tests the
system was deployed to a fleet of autonomous vehicles.
</summary>
    <author>
      <name>Nemanja Djuric</name>
    </author>
    <author>
      <name>Vladan Radosavljevic</name>
    </author>
    <author>
      <name>Henggang Cui</name>
    </author>
    <author>
      <name>Thi Nguyen</name>
    </author>
    <author>
      <name>Fang-Chieh Chou</name>
    </author>
    <author>
      <name>Tsung-Han Lin</name>
    </author>
    <author>
      <name>Jeff Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0208v4</id>
    <updated>2018-08-17T09:09:37Z</updated>
    <published>2014-07-01T12:08:10Z</published>
    <title>A Bayes consistent 1-NN classifier</title>
    <summary>  We show that a simple modification of the 1-nearest neighbor classifier
yields a strongly Bayes consistent learner. Prior to this work, the only
strongly Bayes consistent proximity-based method was the k-nearest neighbor
classifier, for k growing appropriately with sample size. We will argue that a
margin-regularized 1-NN enjoys considerable statistical and algorithmic
advantages over the k-NN classifier. These include user-friendly finite-sample
error bounds, as well as time- and memory-efficient learning and test-point
evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging
empirical results are reported.
</summary>
    <author>
      <name>Aryeh Kontorovich</name>
    </author>
    <author>
      <name>Roi Weiss</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0208v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0208v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05777v1</id>
    <updated>2018-08-17T07:25:31Z</updated>
    <published>2018-08-17T07:25:31Z</published>
    <title>Unsupervised adversarial domain adaptation for acoustic scene
  classification</title>
    <summary>  A general problem in acoustic scene classification task is the mismatched
conditions between training and testing data, which significantly reduces the
performance of the developed methods on classification accuracy. As a
countermeasure, we present the first method of unsupervised adversarial domain
adaptation for acoustic scene classification. We employ a model pre-trained on
data from one set of conditions and by using data from other set of conditions,
we adapt the model in order that its output cannot be used for classifying the
set of conditions that input data belong to. We use a freely available dataset
from the DCASE 2018 challenge Task 1, subtask B, that contains data from
mismatched recording devices. We consider the scenario where the annotations
are available for the data recorded from one device, but not for the rest. Our
results show that with our model agnostic method we can achieve $\sim 10\%$
increase at the accuracy on an unseen and unlabeled dataset, while keeping
almost the same performance on the labeled dataset.
</summary>
    <author>
      <name>Shayan Gharib</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Emre Çakir</name>
    </author>
    <author>
      <name>Dmitriy Serdyuk</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05770v1</id>
    <updated>2018-08-17T06:34:53Z</updated>
    <published>2018-08-17T06:34:53Z</published>
    <title>Reinforcement Learning for Autonomous Defence in Software-Defined
  Networking</title>
    <summary>  Despite the successful application of machine learning (ML) in a wide range
of domains, adaptability---the very property that makes machine learning
desirable---can be exploited by adversaries to contaminate training and evade
classification. In this paper, we investigate the feasibility of applying a
specific class of machine learning algorithms, namely, reinforcement learning
(RL) algorithms, for autonomous cyber defence in software-defined networking
(SDN). In particular, we focus on how an RL agent reacts towards different
forms of causative attacks that poison its training process, including
indiscriminate and targeted, white-box and black-box attacks. In addition, we
also study the impact of the attack timing, and explore potential
countermeasures such as adversarial training.
</summary>
    <author>
      <name>Yi Han</name>
    </author>
    <author>
      <name>Benjamin I. P. Rubinstein</name>
    </author>
    <author>
      <name>Tamas Abraham</name>
    </author>
    <author>
      <name>Tansu Alpcan</name>
    </author>
    <author>
      <name>Olivier De Vel</name>
    </author>
    <author>
      <name>Sarah Erfani</name>
    </author>
    <author>
      <name>David Hubczenko</name>
    </author>
    <author>
      <name>Christopher Leckie</name>
    </author>
    <author>
      <name>Paul Montague</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02016v2</id>
    <updated>2018-08-17T06:00:41Z</updated>
    <published>2018-08-04T15:48:39Z</published>
    <title>MCRM: Mother Compact Recurrent Memory</title>
    <summary>  LSTMs and GRUs are the most common recurrent neural network architectures
used to solve temporal sequence problems. The two architectures have differing
data flows dealing with a common component called the cell state (also referred
to as the memory). We attempt to enhance the memory by presenting a
modification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are
a type of a nested LSTM-GRU architecture where the cell state is the GRU hidden
state. The concatenation of the forget gate and input gate interactions from
the LSTM are considered an input to the GRU cell. Because MCRMs has this type
of nesting, MCRMs have a compact memory pattern consisting of neurons that acts
explicitly in both long-term and short-term fashions. For some specific tasks,
empirical results show that MCRMs outperform previously used architectures.
</summary>
    <author>
      <name>Abduallah A. Mohamed</name>
    </author>
    <author>
      <name>Christian Claudel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to AAAI-19</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02390v2</id>
    <updated>2018-08-17T05:24:31Z</updated>
    <published>2017-06-07T21:50:15Z</published>
    <title>Creating Virtual Universes Using Generative Adversarial Networks</title>
    <summary>  Inferring model parameters from experimental data is a grand challenge in
many sciences, including cosmology. This often relies critically on high
fidelity numerical simulations, which are prohibitively computationally
expensive. The application of deep learning techniques to generative modeling
is renewing interest in using high dimensional density estimators as
computationally inexpensive emulators of fully-fledged simulations. These
generative models have the potential to make a dramatic shift in the field of
scientific simulations, but for that shift to happen we need to study the
performance of such generators in the precision regime needed for science
applications. To this end, in this letter we apply Generative Adversarial
Networks to the problem of generating cosmological weak lensing convergence
maps. We show that our generator network produces maps that are described by,
with high statistical confidence, the same summary statistics as the fully
simulated maps.
</summary>
    <author>
      <name>Mustafa Mustafa</name>
    </author>
    <author>
      <name>Deborah Bard</name>
    </author>
    <author>
      <name>Wahid Bhimji</name>
    </author>
    <author>
      <name>Zarija Lukić</name>
    </author>
    <author>
      <name>Rami Al-Rfou</name>
    </author>
    <author>
      <name>Jan Kratochvil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02390v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02390v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08271v1</id>
    <updated>2018-08-17T02:33:55Z</updated>
    <published>2018-08-17T02:33:55Z</published>
    <title>An elementary introduction to information geometry</title>
    <summary>  We describe the fundamental differential-geometric structures of information
manifolds, state the fundamental theorem of information geometry, and
illustrate some uses of these information manifolds in information sciences.
The exposition is self-contained by concisely introducing the necessary
concepts of differential geometry with proofs omitted for brevity.
</summary>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05731v1</id>
    <updated>2018-08-17T02:24:23Z</updated>
    <published>2018-08-17T02:24:23Z</published>
    <title>Efficiently Learning Mixtures of Mallows Models</title>
    <summary>  Mixtures of Mallows models are a popular generative model for ranking data
coming from a heterogeneous population. They have a variety of applications
including social choice, recommendation systems and natural language
processing. Here we give the first polynomial time algorithm for provably
learning the parameters of a mixture of Mallows models with any constant number
of components. Prior to our work, only the two component case had been settled.
Our analysis revolves around a determinantal identity of Zagier which was
proven in the context of mathematical physics, which we use to show polynomial
identifiability and ultimately to construct test functions to peel off one
component at a time.
  To complement our upper bounds, we show information-theoretic lower bounds on
the sample complexity as well as lower bounds against restricted families of
algorithms that make only local queries. Together, these results demonstrate
various impediments to improving the dependence on the number of components.
They also motivate the study of learning mixtures of Mallows models from the
perspective of beyond worst-case analysis. In this direction, we show that when
the scaling parameters of the Mallows models have separation, there are much
faster learning algorithms.
</summary>
    <author>
      <name>Allen Liu</name>
    </author>
    <author>
      <name>Ankur Moitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">FOCS 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.05731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06492v1</id>
    <updated>2018-08-17T02:15:39Z</updated>
    <published>2018-08-17T02:15:39Z</published>
    <title>Benchmarking Automatic Machine Learning Frameworks</title>
    <summary>  AutoML serves as the bridge between varying levels of expertise when
designing machine learning systems and expedites the data science process. A
wide range of techniques is taken to address this, however there does not exist
an objective comparison of these techniques. We present a benchmark of current
open source AutoML solutions using open source datasets. We test auto-sklearn,
TPOT, auto_ml, and H2O's AutoML solution against a compiled set of regression
and classification datasets sourced from OpenML and find that auto-sklearn
performs the best across classification datasets and TPOT performs the best
across regression datasets.
</summary>
    <author>
      <name>Adithya Balaji</name>
    </author>
    <author>
      <name>Alexander Allen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05726v1</id>
    <updated>2018-08-17T01:59:57Z</updated>
    <published>2018-08-17T01:59:57Z</published>
    <title>An N Time-Slice Dynamic Chain Event Graph</title>
    <summary>  The Dynamic Chain Event Graph (DCEG) is able to depict many classes of
discrete random processes exhibiting asymmetries in their developments and
context-specific conditional probabilities structures. However, paradoxically,
this very generality has so far frustrated its wide application. So in this
paper we develop an object-oriented method to fully analyse a particularly
useful and feasibly implementable new subclass of these graphical models called
the N Time-Slice DCEG (NT-DCEG). After demonstrating a close relationship
between an NT-DCEG and a specific class of Markov processes, we discuss how
graphical modellers can exploit this connection to gain a deep understanding of
their processes. We also show how to read from the topology of this graph
context-specific independence statements that can then be checked by domain
experts. Our methods are illustrated throughout using examples of dynamic
multivariate processes describing inmate radicalisation in a prison.
</summary>
    <author>
      <name>Rodrigo A. Collazo</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04659v3</id>
    <updated>2018-08-17T01:57:44Z</updated>
    <published>2018-04-12T14:06:05Z</published>
    <title>Asynch-SGBDT: Asynchronous Parallel Stochastic Gradient Boosting
  Decision Tree based on Parameters Server</title>
    <summary>  In AI research and industry, machine learning is the most widely used tool.
One of the most important machine learning algorithms is Gradient Boosting
Decision Tree, i.e. GBDT whose training process needs considerable
computational resources and time. To shorten GBDT training time, many works
tried to apply GBDT on Parameter Server. However, those GBDT algorithms are
synchronous parallel algorithms which fail to make full use of Parameter
Server. In this paper, we examine the possibility of using asynchronous
parallel methods to train GBDT model and name this algorithm as asynch-SGBDT
(asynchronous parallel stochastic gradient boosting decision tree). Our
theoretical and experimental results indicate that the scalability of
asynch-SGBDT is influenced by the sample diversity of datasets, sampling rate,
step length and the setting of GBDT tree. Experimental results also show
asynch-SGBDT training process reaches a linear speedup in asynchronous parallel
manner when datasets and GBDT trees meet high scalability requirements.
</summary>
    <author>
      <name>Cheng Daning</name>
    </author>
    <author>
      <name>Xia Fen</name>
    </author>
    <author>
      <name>Li Shigang</name>
    </author>
    <author>
      <name>Zhang Yunquan</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04659v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04659v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07755v2</id>
    <updated>2018-08-17T00:20:11Z</updated>
    <published>2018-06-19T14:01:27Z</published>
    <title>An empirical study on evaluation metrics of generative adversarial
  networks</title>
    <summary>  Evaluating generative adversarial networks (GANs) is inherently challenging.
In this paper, we revisit several representative sample-based evaluation
metrics for GANs, and address the problem of how to evaluate the evaluation
metrics. We start with a few necessary conditions for metrics to produce
meaningful scores, such as distinguishing real from generated samples,
identifying mode dropping and mode collapsing, and detecting overfitting. With
a series of carefully designed experiments, we comprehensively investigate
existing sample-based metrics and identify their strengths and limitations in
practical settings. Based on these results, we observe that kernel Maximum Mean
Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to
satisfy most of the desirable properties, provided that the distances between
samples are computed in a suitable feature space. Our experiments also unveil
interesting properties about the behavior of several popular GAN models, such
as whether they are memorizing training samples, and how far they are from
learning the target distribution.
</summary>
    <author>
      <name>Qiantong Xu</name>
    </author>
    <author>
      <name>Gao Huang</name>
    </author>
    <author>
      <name>Yang Yuan</name>
    </author>
    <author>
      <name>Chuan Guo</name>
    </author>
    <author>
      <name>Yu Sun</name>
    </author>
    <author>
      <name>Felix Wu</name>
    </author>
    <author>
      <name>Kilian Weinberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1802.03446 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07755v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07755v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07025v3</id>
    <updated>2018-08-16T22:56:41Z</updated>
    <published>2017-05-19T14:42:48Z</published>
    <title>Effective Representations of Clinical Notes</title>
    <summary>  Clinical notes are a rich source of information about patient state. However,
using them to predict clinical events with machine learning models is
challenging. They are very high dimensional, sparse and have complex structure.
Furthermore, training data is often scarce because it is expensive to obtain
reliable labels for many clinical events. These difficulties have traditionally
been addressed by manual feature engineering encoding task specific domain
knowledge. We explored the use of neural networks and transfer learning to
learn representations of clinical notes that are useful for predicting future
clinical events of interest, such as all causes mortality, inpatient
admissions, and emergency room visits. Our data comprised 2.7 million notes and
115 thousand patients at Stanford Hospital. We used the learned
representations, along with commonly used bag of words and topic model
representations, as features for predictive models of clinical events. We
evaluated the effectiveness of these representations with respect to the
performance of the models trained on small datasets. Models using the neural
network derived representations performed significantly better than models
using the baseline representations with small ($N &lt; 1000$) training datasets.
The learned representations offer significant performance gains over commonly
used baseline representations for a range of predictive modeling tasks and
cohort sizes, offering an effective alternative to task specific feature
engineering when plentiful labeled training data is not available.
</summary>
    <author>
      <name>Sebastien Dubois</name>
    </author>
    <author>
      <name>Nathanael Romano</name>
    </author>
    <author>
      <name>David C. Kale</name>
    </author>
    <author>
      <name>Nigam Shah</name>
    </author>
    <author>
      <name>Kenneth Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07025v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07025v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05935v2</id>
    <updated>2018-08-16T22:48:20Z</updated>
    <published>2018-07-16T15:56:55Z</published>
    <title>Siamese Survival Analysis with Competing Risks</title>
    <summary>  Survival analysis in the presence of multiple possible adverse events, i.e.,
competing risks, is a pervasive problem in many industries (healthcare,
finance, etc.). Since only one event is typically observed, the incidence of an
event of interest is often obscured by other related competing events. This
nonidentifiability, or inability to estimate true cause-specific survival
curves from empirical data, further complicates competing risk survival
analysis. We introduce Siamese Survival Prognosis Network (SSPN), a novel deep
learning architecture for estimating personalized risk scores in the presence
of competing risks. SSPN circumvents the nonidentifiability problem by avoiding
the estimation of cause-specific survival curves and instead determines
pairwise concordant time-dependent risks, where longer event times are assigned
lower risks. Furthermore, SSPN is able to directly optimize an approximation to
the C-discrimination index, rather than relying on well-known metrics which are
unable to capture the unique requirements of survival analysis with competing
risks.
</summary>
    <author>
      <name>Anton Nemchenko</name>
    </author>
    <author>
      <name>Trent Kyono</name>
    </author>
    <author>
      <name>Mihaela Van Der Schaar</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05935v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05935v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05671v1</id>
    <updated>2018-08-16T20:25:28Z</updated>
    <published>2018-08-16T20:25:28Z</published>
    <title>On the Convergence of Adaptive Gradient Methods for Nonconvex
  Optimization</title>
    <summary>  Adaptive gradient methods are workhorses in deep learning. However, the
convergence guarantees of adaptive gradient methods for nonconvex optimization
have not been sufficiently studied. In this paper, we provide a sharp analysis
of a recently proposed adaptive gradient method namely partially adaptive
momentum estimation method (Padam) (Chen and Gu, 2018), which admits many
existing adaptive gradient methods such as AdaGrad, RMSProp and AMSGrad as
special cases. Our analysis shows that, for smooth nonconvex functions, Padam
converges to a first-order stationary point at the rate of
$O\big((\sum_{i=1}^d\|\mathbf{g}_{1:T,i}\|_2)^{1/2}/T^{3/4} + d/T\big)$, where
$T$ is the number of iterations, $d$ is the dimension,
$\mathbf{g}_1,\ldots,\mathbf{g}_T$ are the stochastic gradients, and
$\mathbf{g}_{1:T,i} = [g_{1,i},g_{2,i},\ldots,g_{T,i}]^\top$. Our theoretical
result also suggests that in order to achieve faster convergence rate, it is
necessary to use Padam instead of AMSGrad. This is well-aligned with the
empirical results of deep learning reported in Chen and Gu (2018).
</summary>
    <author>
      <name>Dongruo Zhou</name>
    </author>
    <author>
      <name>Yiqi Tang</name>
    </author>
    <author>
      <name>Ziyan Yang</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Quanquan Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08272v1</id>
    <updated>2018-08-16T18:11:56Z</updated>
    <published>2018-08-16T18:11:56Z</published>
    <title>Probabilistic Model of Object Detection Based on Convolutional Neural
  Network</title>
    <summary>  The combination of a CNN detector and a search framework forms the basis for
local object/pattern detection. To handle the waste of regional information and
the defective compromise between efficiency and accuracy, this paper proposes a
probabilistic model with a powerful search framework. By mapping an image into
a probabilistic distribution of objects, this new model gives more informative
outputs with less computation. The setting and analytic traits are elaborated
in this paper, followed by a series of experiments carried out on FDDB, which
show that the proposed model is sound, efficient and analytic.
</summary>
    <author>
      <name>Fang-Qi Li</name>
    </author>
    <author>
      <name>Xu-Die Ren</name>
    </author>
    <author>
      <name>Hao-Nan Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, International Conference on Communication, Signal
  Processing and Systems (CSPS 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05587v1</id>
    <updated>2018-08-16T17:20:58Z</updated>
    <published>2018-08-16T17:20:58Z</published>
    <title>Deep Convolutional Networks as shallow Gaussian Processes</title>
    <summary>  We show that the output of a (residual) convolutional neural network (CNN)
with an appropriate prior over the weights and biases is a Gaussian process
(GP) in the limit of infinitely many convolutional filters, extending similar
results for dense networks. For a CNN, the equivalent kernel can be computed
exactly and, unlike "deep kernels", has very few parameters: only the
hyperparameters of the original CNN. Further, we show that this kernel has two
properties that allow it to be computed efficiently; the cost of evaluating the
kernel for a pair of images is similar to a single forward pass through the
original CNN with only one filter per layer. The kernel equivalent to a
32-layer ResNet obtains 0.84% classification error on MNIST, a new record for
GPs with a comparable number of parameters.
</summary>
    <author>
      <name>Adrià Garriga-Alonso</name>
    </author>
    <author>
      <name>Laurence Aitchison</name>
    </author>
    <author>
      <name>Carl Edward Rasmussen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03300v2</id>
    <updated>2018-08-16T17:15:44Z</updated>
    <published>2018-08-09T18:43:18Z</published>
    <title>$α$-Approximation Density-based Clustering of Multi-valued Objects</title>
    <summary>  Multi-valued data are commonly found in many real applications. During the
process of clustering multi-valued data, most existing methods use sampling or
aggregation mechanisms that cannot reflect the real distribution of objects and
their instances and thus fail to obtain high-quality clusters. In this paper, a
concept of $\alpha$-approximation distance is introduced to measure the
connectivity between multi-valued objects by taking account of the distribution
of the instances. An $\alpha$-approximation density-based clustering algorithm
(DBCMO) is proposed to efficiently cluster the multi-valued objects by using
global and local R* tree structures. To speed up the algorithm, four pruning
rules on the tree structures are implemented. Empirical studies on synthetic
and real datasets demonstrate that DBCMO can efficiently and effectively
discover the multi-valued object clusters. A comparison with two existing
methods further shows that DBCMO can better handle a continuous decrease in the
cluster density and detect clusters of varying density.
</summary>
    <author>
      <name>Zhilin Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03300v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03300v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05584v1</id>
    <updated>2018-08-16T17:02:24Z</updated>
    <published>2018-08-16T17:02:24Z</published>
    <title>BlockQNN: Efficient Block-wise Neural Network Architecture Generation</title>
    <summary>  Convolutional neural networks have gained a remarkable success in computer
vision. However, most usable network architectures are hand-crafted and usually
require expertise and elaborate design. In this paper, we provide a block-wise
network generation pipeline called BlockQNN which automatically builds
high-performance networks using the Q-Learning paradigm with epsilon-greedy
exploration strategy. The optimal network block is constructed by the learning
agent which is trained to choose component layers sequentially. We stack the
block to construct the whole auto-generated network. To accelerate the
generation process, we also propose a distributed asynchronous framework and an
early stop strategy. The block-wise generation brings unique advantages: (1) it
yields state-of-the-art results in comparison to the hand-crafted networks on
image classification, particularly, the best network generated by BlockQNN
achieves 2.35% top-1 error rate on CIFAR-10. (2) it offers tremendous reduction
of the search space in designing networks, spending only 3 days with 32 GPUs. A
faster version can yield a comparable result with only 1 GPU in 20 hours. (3)
it has strong generalizability in that the network built on CIFAR also performs
well on the larger-scale dataset. The best network achieves very competitive
accuracy of 82.0% top-1 and 96.0% top-5 on ImageNet.
</summary>
    <author>
      <name>Zhao Zhong</name>
    </author>
    <author>
      <name>Zichen Yang</name>
    </author>
    <author>
      <name>Boyang Deng</name>
    </author>
    <author>
      <name>Junjie Yan</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Jing Shao</name>
    </author>
    <author>
      <name>Cheng-Lin Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05578v1</id>
    <updated>2018-08-16T16:48:56Z</updated>
    <published>2018-08-16T16:48:56Z</published>
    <title>LARNN: Linear Attention Recurrent Neural Network</title>
    <summary>  The Linear Attention Recurrent Neural Network (LARNN) is a recurrent
attention module derived from the Long Short-Term Memory (LSTM) cell and ideas
from the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The
LARNN uses attention on its past cell state values for a limited window size
$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)
cell and the Transformer Network for its Multi-Head Attention Mechanism. The
Multi-Head Attention Mechanism is used inside the cell such that it can query
its own $k$ past values with the attention window. This has the effect of
augmenting the rank of the tensor with the attention mechanism, such that the
cell can perform complex queries to question its previous inner memories, which
should augment the long short-term effect of the memory. With a clever trick,
the LARNN cell with attention can be easily used inside a loop on the cell
state, just like how any other Recurrent Neural Network (RNN) cell can be
looped linearly through time series. This is due to the fact that its state,
which is looped upon throughout time steps within time series, stores the inner
states in a "first in, first out" queue which contains the $k$ most recent
states and on which it is easily possible to add static positional encoding
when the queue is represented as a tensor. This neural architecture yields
better results than the vanilla LSTM cells. It can obtain results of 91.92% for
the test accuracy, compared to the previously attained 91.65% using vanilla
LSTM cells. Note that this is not to compare to other research, where up to
93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3
cells as analyzed here. Finally, an interesting discovery is made, such that
adding activation within the multi-head attention mechanism's linear layers can
yield better results in the context researched hereto.
</summary>
    <author>
      <name>Guillaume Chevalier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06175v2</id>
    <updated>2018-08-16T16:48:42Z</updated>
    <published>2018-02-17T02:44:16Z</published>
    <title>An Alternative View: When Does SGD Escape Local Minima?</title>
    <summary>  Stochastic gradient descent (SGD) is widely used in machine learning.
Although being commonly viewed as a fast but not accurate version of gradient
descent (GD), it always finds better solutions than GD for modern neural
networks.
  In order to understand this phenomenon, we take an alternative view that SGD
is working on the convolved (thus smoothed) version of the loss function. We
show that, even if the function $f$ has many bad local minima or saddle points,
as long as for every point $x$, the weighted average of the gradients of its
neighborhoods is one point convex with respect to the desired solution $x^*$,
SGD will get close to, and then stay around $x^*$ with constant probability.
More specifically, SGD will not get stuck at "sharp" local minima with small
diameters, as long as the neighborhoods of these regions contain enough
gradient information. The neighborhood size is controlled by step size and
gradient noise.
  Our result identifies a set of functions that SGD provably works, which is
much larger than the set of convex functions. Empirically, we observe that the
loss surface of neural networks enjoys nice one point convexity properties
locally, therefore our theorem helps explain why SGD works so well for neural
networks.
</summary>
    <author>
      <name>Robert Kleinberg</name>
    </author>
    <author>
      <name>Yuanzhi Li</name>
    </author>
    <author>
      <name>Yang Yuan</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06175v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06175v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05577v1</id>
    <updated>2018-08-16T16:42:10Z</updated>
    <published>2018-08-16T16:42:10Z</published>
    <title>Deeper Image Quality Transfer: Training Low-Memory Neural Networks for
  3D Images</title>
    <summary>  In this paper we address the memory demands that come with the processing of
3-dimensional, high-resolution, multi-channeled medical images in deep
learning. We exploit memory-efficient backpropagation techniques, to reduce the
memory complexity of network training from being linear in the network's depth,
to being roughly constant $ - $ permitting us to elongate deep architectures
with negligible memory increase. We evaluate our methodology in the paradigm of
Image Quality Transfer, whilst noting its potential application to various
tasks that use deep learning. We study the impact of depth on accuracy and show
that deeper models have more predictive power, which may exploit larger
training sets. We obtain substantially better results than the previous
state-of-the-art model with a slight memory increase, reducing the
root-mean-squared-error by $ 13\% $. Our code is publicly available.
</summary>
    <author>
      <name>Stefano B. Blumberg</name>
    </author>
    <author>
      <name>Ryutaro Tanno</name>
    </author>
    <author>
      <name>Iasonas Kokkinos</name>
    </author>
    <author>
      <name>Daniel C. Alexander</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in: MICCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05563v1</id>
    <updated>2018-08-16T16:12:39Z</updated>
    <published>2018-08-16T16:12:39Z</published>
    <title>Learning Invariances using the Marginal Likelihood</title>
    <summary>  Generalising well in supervised learning tasks relies on correctly
extrapolating the training data to a large region of the input space. One way
to achieve this is to constrain the predictions to be invariant to
transformations on the input that are known to be irrelevant (e.g.
translation). Commonly, this is done through data augmentation, where the
training set is enlarged by applying hand-crafted transformations to the
inputs. We argue that invariances should instead be incorporated in the model
structure, and learned using the marginal likelihood, which correctly rewards
the reduced complexity of invariant models. We demonstrate this for Gaussian
process models, due to the ease with which their marginal likelihood can be
estimated. Our main contribution is a variational inference scheme for Gaussian
processes containing invariances described by a sampling procedure. We learn
the sampling procedure by back-propagating through it to maximise the marginal
likelihood.
</summary>
    <author>
      <name>Mark van der Wilk</name>
    </author>
    <author>
      <name>Matthias Bauer</name>
    </author>
    <author>
      <name>ST John</name>
    </author>
    <author>
      <name>James Hensman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05537v1</id>
    <updated>2018-08-16T15:20:55Z</updated>
    <published>2018-08-16T15:20:55Z</published>
    <title>Distributionally Adversarial Attack</title>
    <summary>  Recent work on adversarial attack has shown that Projected Gradient Descent
(PGD) Adversary is a universal first-order adversary, and the classifier
adversarially trained by PGD is robust against a wide range of first-order
attacks. However, it is worth noting that the objective of an attacking/defense
model relies on a data distribution, typically in the form of risk
maximization/minimization: $\max\!/\!\min \mathbb{E}_{p(\mathbf{x})}
\mathcal{L}(\mathbf{x})$, with $p(\mathbf{x})$ the data distribution and
$\mathcal{L}(\cdot)$ a loss function. While PGD generates attack samples
independently for each data point, the procedure does not necessary lead to
good generalization in terms of risk maximization. In the paper, we achieve the
goal by proposing distributionally adversarial attack (DAA), a framework to
solve an optimal {\em adversarial data distribution}, a perturbed distribution
that is close to the original data distribution but increases the
generalization risk maximally. Algorithmically, DAA performs optimization on
the space of probability measures, which introduces direct dependency between
all data points when generating adversarial samples. DAA is evaluated by
attacking state-of-the-art defense models, including the adversarially trained
models provided by MadryLab. Notably, DAA outperforms all the attack algorithms
listed in MadryLab's white-box leaderboard, reducing the accuracy of their
secret MNIST model to $88.79\%$ (with $l_\infty$ perturbations of $\epsilon =
0.3$) and the accuracy of their secret CIFAR model to $44.73\%$ (with
$l_\infty$ perturbations of $\epsilon = 8.0$). Code for the experiments is
released on https://github.com/tianzheng4/Distributionally-Adversarial-Attack
</summary>
    <author>
      <name>Tianhang Zheng</name>
    </author>
    <author>
      <name>Changyou Chen</name>
    </author>
    <author>
      <name>Kui Ren</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05535v1</id>
    <updated>2018-08-16T15:19:34Z</updated>
    <published>2018-08-16T15:19:34Z</published>
    <title>Combining time-series and textual data for taxi demand prediction in
  event areas: a deep learning approach</title>
    <summary>  Accurate time-series forecasting is vital for numerous areas of application
such as transportation, energy, finance, economics, etc. However, while modern
techniques are able to explore large sets of temporal data to build forecasting
models, they typically neglect valuable information that is often available
under the form of unstructured text. Although this data is in a radically
different format, it often contains contextual explanations for many of the
patterns that are observed in the temporal data. In this paper, we propose two
deep learning architectures that leverage word embeddings, convolutional layers
and attention mechanisms for combining text information with time-series data.
We apply these approaches for the problem of taxi demand forecasting in event
areas. Using publicly available taxi data from New York, we empirically show
that by fusing these two complementary cross-modal sources of information, the
proposed models are able to significantly reduce the error in the forecasts.
</summary>
    <author>
      <name>Filipe Rodrigues</name>
    </author>
    <author>
      <name>Ioulia Markou</name>
    </author>
    <author>
      <name>Francisco Pereira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.inffus.2018.07.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.inffus.2018.07.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Rodrigues, F., Markou, I., Pereira, F. Combining time-series and
  textual data for taxi demand prediction in event areas: a deep learning
  approach. In Information Fusion, Elsevier, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.05535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11271v2</id>
    <updated>2018-08-16T14:51:32Z</updated>
    <published>2018-04-30T15:21:23Z</published>
    <title>Gaussian Process Behaviour in Wide Deep Neural Networks</title>
    <summary>  Whilst deep neural networks have shown great empirical success, there is
still much work to be done to understand their theoretical properties. In this
paper, we study the relationship between random, wide, fully connected,
feedforward networks with more than one hidden layer and Gaussian processes
with a recursive kernel definition. We show that, under broad conditions, as we
make the architecture increasingly wide, the implied random function converges
in distribution to a Gaussian process, formalising and extending existing
results by Neal (1996) to deep networks. To evaluate convergence rates
empirically, we use maximum mean discrepancy. We then compare finite Bayesian
deep networks from the literature to Gaussian processes in terms of the key
predictive quantities of interest, finding that in some cases the agreement can
be very close. We discuss the desirability of Gaussian process behaviour and
review non-Gaussian alternative models from the literature.
</summary>
    <author>
      <name>Alexander G. de G. Matthews</name>
    </author>
    <author>
      <name>Mark Rowland</name>
    </author>
    <author>
      <name>Jiri Hron</name>
    </author>
    <author>
      <name>Richard E. Turner</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work substantially extends the work of Matthews et al. (2018)
  published at the International Conference on Learning Representations (ICLR)
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.11271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05517v1</id>
    <updated>2018-08-16T14:39:10Z</updated>
    <published>2018-08-16T14:39:10Z</published>
    <title>Network Decoupling: From Regular to Depthwise Separable Convolutions</title>
    <summary>  Depthwise separable convolution has shown great efficiency in network design,
but requires time-consuming training procedure with full training-set
available. This paper first analyzes the mathematical relationship between
regular convolutions and depthwise separable convolutions, and proves that the
former one could be approximated with the latter one in closed form. We show
depthwise separable convolutions are principal components of regular
convolutions. And then we propose network decoupling (ND), a training-free
method to accelerate convolutional neural networks (CNNs) by transferring
pre-trained CNN models into the MobileNet-like depthwise separable convolution
structure, with a promising speedup yet negligible accuracy loss. We further
verify through experiments that the proposed method is orthogonal to other
training-free methods like channel decomposition, spatial decomposition, etc.
Combining the proposed method with them will bring even larger CNN speedup. For
instance, ND itself achieves about 2X speedup for the widely used VGG16, and
combined with other methods, it reaches 3.7X speedup with graceful accuracy
degradation. We demonstrate that ND is widely applicable to classification
networks like ResNet, and object detection network like SSD300.
</summary>
    <author>
      <name>Jianbo Guo</name>
    </author>
    <author>
      <name>Yuxi Li</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Yurong Chen</name>
    </author>
    <author>
      <name>Jianguo Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07828v2</id>
    <updated>2018-08-16T14:36:25Z</updated>
    <published>2018-05-20T21:46:29Z</published>
    <title>A VEST of the Pseudoinverse Learning Algorithm</title>
    <summary>  In this paper, we briefly review the basic scheme of the pseudoinverse
learning (PIL) algorithm and present some discussions on the PIL, as well as
its variants. The PIL algorithm, first presented in 1995, is a non-gradient
descent and non-iterative learning algorithm for multi-layer neural networks
and has several advantages compared with gradient descent based algorithms.
Some new viewpoints to PIL algorithm are presented, and several common pitfalls
in practical implementation of the neural network learning task are also
addressed. In addition, we show that so called extreme learning machine is a
Variant crEated by Simple name alTernation (VEST) of the PIL algorithm for
single hidden layer feedforward neural networks.
</summary>
    <author>
      <name>Ping Guo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Systems Science, Beijing Normal University, Beijing, China</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ELM is another name of the PIL</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05500v1</id>
    <updated>2018-08-16T14:09:22Z</updated>
    <published>2018-08-16T14:09:22Z</published>
    <title>Robust training of recurrent neural networks to handle missing data for
  disease progression modeling</title>
    <summary>  Disease progression modeling (DPM) using longitudinal data is a challenging
task in machine learning for healthcare that can provide clinicians with better
tools for diagnosis and monitoring of disease. Existing DPM algorithms neglect
temporal dependencies among measurements and make parametric assumptions about
biomarker trajectories. In addition, they do not model multiple biomarkers
jointly and need to align subjects' trajectories. In this paper, recurrent
neural networks (RNNs) are utilized to address these issues. However, in many
cases, longitudinal cohorts contain incomplete data, which hinders the
application of standard RNNs and requires a pre-processing step such as
imputation of the missing values. We, therefore, propose a generalized training
rule for the most widely used RNN architecture, long short-term memory (LSTM)
networks, that can handle missing values in both target and predictor
variables. This algorithm is applied for modeling the progression of
Alzheimer's disease (AD) using magnetic resonance imaging (MRI) biomarkers. The
results show that the proposed LSTM algorithm achieves a lower mean absolute
error for prediction of measurements across all considered MRI biomarkers
compared to using standard LSTM networks with data imputation or using a
regression-based DPM method. Moreover, applying linear discriminant analysis to
the biomarkers' values predicted by the proposed algorithm results in a larger
area under the receiver operating characteristic curve (AUC) for clinical
diagnosis of AD compared to the same alternatives, and the AUC is comparable to
state-of-the-art AUCs from a recent cross-sectional medical image
classification challenge. This paper shows that built-in handling of missing
values in LSTM network training paves the way for application of RNNs in
disease progression modeling.
</summary>
    <author>
      <name>Mostafa Mehdipour Ghazi</name>
    </author>
    <author>
      <name>Mads Nielsen</name>
    </author>
    <author>
      <name>Akshay Pai</name>
    </author>
    <author>
      <name>M. Jorge Cardoso</name>
    </author>
    <author>
      <name>Marc Modat</name>
    </author>
    <author>
      <name>Sebastien Ourselin</name>
    </author>
    <author>
      <name>Lauge Sørensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure, MIDL conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05443v1</id>
    <updated>2018-08-16T12:28:11Z</updated>
    <published>2018-08-16T12:28:11Z</published>
    <title>Transfer Learning and Organic Computing for Autonomous Vehicles</title>
    <summary>  Autonomous Vehicles(AV) are one of the brightest promises of the future which
would help cut down fatalities and improve travel time while working in
harmony. Autonomous vehicles will face with challenging situations and
experiences not seen before. These experiences should be converted to knowledge
and help the vehicle prepare better in the future. Online Transfer Learning
will help transferring prior knowledge to a new task and also keep the
knowledge updated as the task evolves. This paper presents the different
methods of transfer learning, online transfer learning and organic computing
that could be adapted to the domain of autonomous vehicles.
</summary>
    <author>
      <name>Christofer Fellicious</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, survey of papers and methods in transfer
  learning, organic computing and online transfer learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01020v2</id>
    <updated>2018-08-16T10:50:05Z</updated>
    <published>2018-07-03T08:33:01Z</published>
    <title>Coopetitive Soft Gating Ensemble</title>
    <summary>  In this article, we propose the Coopetititve Soft Gating Ensemble or CSGE for
general machine learning tasks and interwoven systems. The goal of machine
learning is to create models that generalize well for unknown datasets. Often,
however, the problems are too complex to be solved with a single model, so
several models are combined. Similar, Autonomic Computing requires the
integration of different systems. Here, especially, the local, temporal online
evaluation and the resulting (re-)weighting scheme of the CSGE makes the
approach highly applicable for self-improving system integrations. To achieve
the best potential performance the CSGE can be optimized according to arbitrary
loss functions making it accessible for a broader range of problems. We
introduce a novel training procedure including a hyper-parameter initialisation
at its heart. We show that the CSGE approach reaches state-of-the-art
performance for both classification and regression tasks. Further on, the CSGE
provides a human-readable quantification on the influence of all base
estimators employing the three weighting aspects. Moreover, we provide a
scikit-learn compatible implementation.
</summary>
    <author>
      <name>Stephan Deist</name>
    </author>
    <author>
      <name>Maarten Bieshaar</name>
    </author>
    <author>
      <name>Jens Schreiber</name>
    </author>
    <author>
      <name>Andre Gensler</name>
    </author>
    <author>
      <name>Bernhard Sick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures, 4 tables, submitted (accepted for publication) -
  SISSY 2018 - Workshop on Self-Improving System Integration at IEEE ICAC/ SASO
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05403v1</id>
    <updated>2018-08-16T10:25:48Z</updated>
    <published>2018-08-16T10:25:48Z</published>
    <title>Nonconvex Regularization Based Sparse and Low-Rank Recovery in Signal
  Processing, Statistics, and Machine Learning</title>
    <summary>  In the past decade, sparse and low-rank recovery have drawn much attention in
many areas such as signal/image processing, statistics, bioinformatics and
machine learning. To achieve sparsity and/or low-rankness inducing, the
$\ell_1$ norm and nuclear norm are of the most popular regularization penalties
due to their convexity. While the $\ell_1$ and nuclear norm are convenient as
the related convex optimization problems are usually tractable, it has been
shown in many applications that a nonconvex penalty can yield significantly
better performance. In recent, nonconvex regularization based sparse and
low-rank recovery is of considerable interest and it in fact is a main driver
of the recent progress in nonconvex and nonsmooth optimization. This paper
gives an overview of this topic in various fields in signal processing,
statistics and machine learning, including compressive sensing (CS), sparse
regression and variable selection, sparse signals separation, sparse principal
component analysis (PCA), large covariance and inverse covariance matrices
estimation, matrix completion, and robust PCA. We present recent developments
of nonconvex regularization based sparse and low-rank recovery in these fields,
addressing the issues of penalty selection, applications and the convergence of
nonconvex algorithms.
</summary>
    <author>
      <name>Fei Wen</name>
    </author>
    <author>
      <name>Lei Chu</name>
    </author>
    <author>
      <name>Peilin Liu</name>
    </author>
    <author>
      <name>Robert C. Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05906v1</id>
    <updated>2018-08-16T09:02:37Z</updated>
    <published>2018-08-16T09:02:37Z</published>
    <title>Story Disambiguation: Tracking Evolving News Stories across News and
  Social Streams</title>
    <summary>  Following a particular news story online is an important but difficult task,
as the relevant information is often scattered across different domains/sources
(e.g., news articles, blogs, comments, tweets), presented in various formats
and language styles, and may overlap with thousands of other stories. In this
work we join the areas of topic tracking and entity disambiguation, and propose
a framework named Story Disambiguation - a cross-domain story tracking approach
that builds on real-time entity disambiguation and a learning-to-rank framework
to represent and update the rich semantic structure of news stories. Given a
target news story, specified by a seed set of documents, the goal is to
effectively select new story-relevant documents from an incoming document
stream. We represent stories as entity graphs and we model the story tracking
problem as a learning-to-rank task. This enables us to track content with high
accuracy, from multiple domains, in real-time. We study a range of text, entity
and graph based features to understand which type of features are most
effective for representing stories. We further propose new semi-supervised
learning techniques to automatically update the story representation over time.
Our empirical study shows that we outperform the accuracy of state-of-the-art
methods for tracking mixed-domain document streams, while requiring fewer
labeled data to seed the tracked stories. This is particularly the case for
local news stories that are easily over shadowed by other trending stories, and
for complex news stories with ambiguous content in noisy stream environments.
</summary>
    <author>
      <name>Bichen Shi</name>
    </author>
    <author>
      <name>Thanh-Binh Le</name>
    </author>
    <author>
      <name>Neil Hurley</name>
    </author>
    <author>
      <name>Georgiana Ifrim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00121v2</id>
    <updated>2018-08-16T08:16:50Z</updated>
    <published>2018-04-30T22:38:05Z</published>
    <title>A Missing Information Loss function for implicit feedback datasets</title>
    <summary>  Latent factor models for Recommender Systems with implicit feedback typically
treat unobserved user-item interactions (i.e. missing information) as negative
feedback. This is frequently done either through negative sampling (point--wise
loss) or with a ranking loss function (pair-- or list--wise estimation). Since
a zero preference recommendation is a valid solution for most common objective
functions, regarding unknown values as actual zeros results in users having a
zero preference recommendation for most of the available items. In this paper
we propose a novel objective function, the \emph{Missing Information Loss}
(MIL), that explicitly forbids treating unobserved user-item interactions as
positive or negative feedback. We apply this loss to both traditional Matrix
Factorization and user--based Denoising Autoencoder, and compare it with other
established objective functions such as cross-entropy (both point- and
pair-wise) or the recently proposed multinomial log-likelihood. MIL achieves
competitive performance in ranking-aware metrics when applied to three
datasets. Furthermore, we show that such a relevance in the recommendation is
obtained while displaying popular items less frequently (up to a $20 \%$
decrease with respect to the best competing method). This debiasing from the
recommendation of popular items favours the appearance of infrequent items (up
to a $50 \%$ increase of long-tail recommendations), a valuable feature for
Recommender Systems with a large catalogue of products.
</summary>
    <author>
      <name>Juan Arévalo</name>
    </author>
    <author>
      <name>Juan Ramón Duque</name>
    </author>
    <author>
      <name>Marco Creatura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00121v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00121v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00635v2</id>
    <updated>2018-08-16T06:41:37Z</updated>
    <published>2017-12-02T16:46:27Z</published>
    <title>Network Coding Based Evolutionary Network Formation for Dynamic Wireless
  Networks</title>
    <summary>  In this paper, we aim to find a robust network formation strategy that can
adaptively evolve the network topology against network dynamics in a
distributed manner. We consider a network coding deployed wireless ad hoc
network where source nodes are connected to terminal nodes with the help of
intermediate nodes. We show that mixing operations in network coding can induce
packet anonymity that allows the inter-connections in a network to be
decoupled. This enables each intermediate node to consider complex network
inter-connections as a node-environment interaction such that the Markov
decision process (MDP) can be employed at each intermediate node. The optimal
policy that can be obtained by solving the MDP provides each node with optimal
amount of changes in transmission range given network dynamics (e.g., the
number of nodes in the range and channel condition). Hence, the network can be
adaptively and optimally evolved by responding to the network dynamics. The
proposed strategy is used to maximize long-term utility, which is achieved by
considering both current network conditions and future network dynamics. We
define the utility of an action to include network throughput gain and the cost
of transmission power. We show that the resulting network of the proposed
strategy eventually converges to stationary networks, which maintain the states
of the nodes. Moreover, we propose to determine initial transmission ranges and
initial network topology that can expedite the convergence of the proposed
algorithm. Our simulation results confirm that the proposed strategy builds a
network which adaptively changes its topology in the presence of network
dynamics. Moreover, the proposed strategy outperforms existing strategies in
terms of system goodput and successful connectivity ratio.
</summary>
    <author>
      <name>Minhae Kwon</name>
    </author>
    <author>
      <name>Hyunggon Park</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMC.2018.2861001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMC.2018.2861001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Mobile Computing (Early Access)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05355v1</id>
    <updated>2018-08-16T06:25:24Z</updated>
    <published>2018-08-16T06:25:24Z</published>
    <title>Conceptual Domain Adaptation Using Deep Learning</title>
    <summary>  Deep learning has recently been shown to be instrumental in the problem of
domain adaptation, where the goal is to learn a model on a target domain using
a similar --but not identical-- source domain. The rationale for coupling both
techniques is the possibility of extracting common concepts across domains.
Considering (strictly) local representations, traditional deep learning assumes
common concepts must be captured in the same hidden units. We contend that
jointly training a model with source and target data using a single deep
network is prone to failure when there is inherently lower-level
representational discrepancy between the two domains; such discrepancy leads to
a misalignment of corresponding concepts in separate hidden units. We introduce
a search framework to correctly align high-level representations when training
deep networks; such framework leads to the notion of conceptual --as opposed to
representational-- domain adaptation.
</summary>
    <author>
      <name>Behrang Mehrparvar</name>
    </author>
    <author>
      <name>Ricardo Vilalta</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01367v2</id>
    <updated>2018-08-16T06:21:56Z</updated>
    <published>2018-06-26T03:51:53Z</published>
    <title>EmbNum: Semantic labeling for numerical values with deep metric learning</title>
    <summary>  Semantic labeling for numerical values is a task of assigning semantic labels
to unknown numerical attributes. The semantic labels could be numerical
properties in ontologies, instances in knowledge bases, or labeled data that
are manually annotated by domain experts. In this paper, we refer to semantic
labeling as a retrieval setting where the label of an unknown attribute is
assigned by the label of the most relevant attribute in labeled data. One of
the greatest challenges is that an unknown attribute rarely has the same set of
values with the similar one in the labeled data. To overcome the issue,
statistical interpretation of value distribution is taken into account.
However, the existing studies assume a specific form of distribution. It is not
appropriate in particular to apply open data where there is no knowledge of
data in advance. To address these problems, we propose a neural numerical
embedding model (EmbNum) to learn useful representation vectors for numerical
attributes without prior assumptions on the distribution of data. Then, the
"semantic similarities" between the attributes are measured on these
representation vectors by the Euclidean distance. Our empirical experiments on
City Data and Open Data show that EmbNum significantly outperforms
state-of-the-art methods for the task of numerical attribute semantic labeling
regarding effectiveness and efficiency.
</summary>
    <author>
      <name>Phuc Nguyen</name>
    </author>
    <author>
      <name>Khai Nguyen</name>
    </author>
    <author>
      <name>Ryutaro Ichise</name>
    </author>
    <author>
      <name>Hideaki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05353v1</id>
    <updated>2018-08-16T06:13:26Z</updated>
    <published>2018-08-16T06:13:26Z</published>
    <title>Identifying Implementation Bugs in Machine Learning based Image
  Classifiers using Metamorphic Testing</title>
    <summary>  We have recently witnessed tremendous success of Machine Learning (ML) in
practical applications. Computer vision, speech recognition and language
translation have all seen a near human level performance. We expect, in the
near future, most business applications will have some form of ML. However,
testing such applications is extremely challenging and would be very expensive
if we follow today's methodologies. In this work, we present an articulation of
the challenges in testing ML based applications. We then present our solution
approach, based on the concept of Metamorphic Testing, which aims to identify
implementation bugs in ML based image classifiers. We have developed
metamorphic relations for an application based on Support Vector Machine and a
Deep Learning based application. Empirical validation showed that our approach
was able to catch 71% of the implementation bugs in the ML applications.
</summary>
    <author>
      <name>Anurag Dwarakanath</name>
    </author>
    <author>
      <name>Manish Ahuja</name>
    </author>
    <author>
      <name>Samarth Sikand</name>
    </author>
    <author>
      <name>Raghotham M. Rao</name>
    </author>
    <author>
      <name>R. P. Jagadeesh Chandra Bose</name>
    </author>
    <author>
      <name>Neville Dubash</name>
    </author>
    <author>
      <name>Sanjay Podder</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3213846.3213858</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3213846.3213858" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at 27th ACM SIGSOFT International Symposium on Software
  Testing and Analysis (ISSTA 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05347v1</id>
    <updated>2018-08-16T04:45:15Z</updated>
    <published>2018-08-16T04:45:15Z</published>
    <title>Tool Breakage Detection using Deep Learning</title>
    <summary>  In manufacture, steel and other metals are mainly cut and shaped during the
fabrication process by computer numerical control (CNC) machines. To keep high
productivity and efficiency of the fabrication process, engineers need to
monitor the real-time process of CNC machines, and the lifetime management of
machine tools. In a real manufacturing process, breakage of machine tools
usually happens without any indication, this problem seriously affects the
fabrication process for many years. Previous studies suggested many different
approaches for monitoring and detecting the breakage of machine tools. However,
there still exists a big gap between academic experiments and the complex real
fabrication processes such as the high demands of real-time detections, the
difficulty in data acquisition and transmission. In this work, we use the
spindle current approach to detect the breakage of machine tools, which has the
high performance of real-time monitoring, low cost, and easy to install. We
analyze the features of the current of a milling machine spindle through tools
wearing processes, and then we predict the status of tool breakage by a
convolutional neural network(CNN). In addition, we use a BP neural network to
understand the reliability of the CNN. The results show that our CNN approach
can detect tool breakage with an accuracy of 93%, while the best performance of
BP is 80%.
</summary>
    <author>
      <name>Guang Li</name>
    </author>
    <author>
      <name>Xin Yang</name>
    </author>
    <author>
      <name>Duanbing Chen</name>
    </author>
    <author>
      <name>Anxing Song</name>
    </author>
    <author>
      <name>Yuke Fang</name>
    </author>
    <author>
      <name>Junlin Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,BCD2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05341v1</id>
    <updated>2018-08-16T04:10:02Z</updated>
    <published>2018-08-16T04:10:02Z</published>
    <title>Automatic Chord Recognition with Higher-Order Harmonic Language
  Modelling</title>
    <summary>  Common temporal models for automatic chord recognition model chord changes on
a frame-wise basis. Due to this fact, they are unable to capture musical
knowledge about chord progressions. In this paper, we propose a temporal model
that enables explicit modelling of chord changes and durations. We then apply
N-gram models and a neural-network-based acoustic model within this framework,
and evaluate the effect of model overconfidence. Our results show that model
overconfidence plays only a minor role (but target smoothing still improves the
acoustic model), and that stronger chord language models do improve recognition
results, however their effects are small compared to other domains.
</summary>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First published in the Proceedings of the 26th European Signal
  Processing Conference (EUSIPCO-2018) in 2018, published by EURASIP</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05340v1</id>
    <updated>2018-08-16T03:57:03Z</updated>
    <published>2018-08-16T03:57:03Z</published>
    <title>Genre-Agnostic Key Classification With Convolutional Neural Networks</title>
    <summary>  We propose modifications to the model structure and training procedure to a
recently introduced Convolutional Neural Network for musical key
classification. These modifications enable the network to learn a
genre-independent model that performs better than models trained for specific
music styles, which has not been the case in existing work. We analyse this
generalisation capability on three datasets comprising distinct genres. We then
evaluate the model on a number of unseen data sets, and show its superior
performance compared to the state of the art. Finally, we investigate the
model's performance on short excerpts of audio. From these experiments, we
conclude that models need to consider the harmonic coherence of the whole piece
when classifying the local key of short segments of audio.
</summary>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the 19th International Society for Music Information
  Retrieval Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04936v2</id>
    <updated>2018-08-16T03:51:53Z</updated>
    <published>2018-07-13T06:47:06Z</published>
    <title>Non-Gaussian Component Analysis using Entropy Methods</title>
    <summary>  Non-Gaussian component analysis (NGCA) is a problem in multidimensional data
analysis. Since its formulation in 2006, NGCA has attracted considerable
attention in statistics and machine learning. In this problem, we have a random
variable $X$ in $n$-dimensional Euclidean space. There is an unknown subspace
$U$ of the $n$-dimensional Euclidean space such that the orthogonal projection
of $X$ onto $U$ is standard multidimensional Gaussian and the orthogonal
projection of $X$ onto $V$, the orthogonal complement of $U$, is non-Gaussian,
in the sense that all its one-dimensional marginals are different from the
Gaussian in a certain metric defined in terms of moments. The NGCA problem is
to approximate the non-Gaussian subspace $V$ given samples of $X$.
  Vectors in $V$ corresponds to "interesting" directions, whereas vectors in
$U$ correspond to the directions where data is very noisy. The most interesting
applications of the NGCA model is for the case when the magnitude of the noise
is comparable to that of the true signal, a setting in which traditional noise
reduction techniques such as PCA don't apply directly. NGCA is also related to
dimensionality reduction and to other data analysis problems such as ICA.
NGCA-like problems have been studied in statistics for a long time using
techniques such as projection pursuit.
  We give an algorithm that takes polynomial time in the dimension $n$ and has
an inverse polynomial dependence on the error parameter measuring the angle
distance between the non-Gaussian subspace and the subspace output by the
algorithm. Our algorithm is based on relative entropy as the contrast function
and fits under the projection pursuit framework. The techniques we develop for
analyzing our algorithm maybe of use for other related problems.
</summary>
    <author>
      <name>Navin Goyal</name>
    </author>
    <author>
      <name>Abhishek Shetty</name>
    </author>
    <link href="http://arxiv.org/abs/1807.04936v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04936v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05335v1</id>
    <updated>2018-08-16T03:34:27Z</updated>
    <published>2018-08-16T03:34:27Z</published>
    <title>Improved Chord Recognition by Combining Duration and Harmonic Language
  Models</title>
    <summary>  Chord recognition systems typically comprise an acoustic model that predicts
chords for each audio frame, and a temporal model that casts these predictions
into labelled chord segments. However, temporal models have been shown to only
smooth predictions, without being able to incorporate musical information about
chord progressions. Recent research discovered that it might be the low
hierarchical level such models have been applied to (directly on audio frames)
which prevents learning musical relationships, even for expressive models such
as recurrent neural networks (RNNs). However, if applied on the level of chord
sequences, RNNs indeed can become powerful chord predictors. In this paper, we
disentangle temporal models into a harmonic language model---to be applied on
chord sequences---and a chord duration model that connects the chord-level
predictions of the language model to the frame-level predictions of the
acoustic model. In our experiments, we explore the impact of each model on the
chord recognition score, and show that using harmonic language and duration
models improves the results.
</summary>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at 19th International Society for Music Information
  Retrieval Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03333v3</id>
    <updated>2018-08-16T03:29:23Z</updated>
    <published>2018-08-09T20:11:09Z</published>
    <title>Linked Causal Variational Autoencoder for Inferring Paired Spillover
  Effects</title>
    <summary>  Modeling spillover effects from observational data is an important problem in
economics, business, and other fields of research. % It helps us infer the
causality between two seemingly unrelated set of events. For example, if
consumer spending in the United States declines, it has spillover effects on
economies that depend on the U.S. as their largest export market. In this
paper, we aim to infer the causation that results in spillover effects between
pairs of entities (or units), we call this effect as \textit{paired spillover}.
To achieve this, we leverage the recent developments in variational inference
and deep learning techniques to propose a generative model called Linked Causal
Variational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA
incorporates an encoder neural network to learn the latent attributes and a
decoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the
\textit{latent attributes as confounders that are assumed to affect both the
treatment and the outcome of units}. Specifically, given a pair of units $u$
and $\bar{u}$, their individual treatment and outcomes, the encoder network of
LCVA samples the confounders by conditioning on the observed covariates of $u$,
the treatments of both $u$ and $\bar{u}$ and the outcome of $u$. Once inferred,
the latent attributes (or confounders) of $u$ captures the spillover effect of
$\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde
(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that
LCVA is significantly more robust than existing methods in capturing spillover
effects.
</summary>
    <author>
      <name>Vineeth Rakesh</name>
    </author>
    <author>
      <name>Ruocheng Guo</name>
    </author>
    <author>
      <name>Raha Moraffah</name>
    </author>
    <author>
      <name>Nitin Agarwal</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First two authors contributed equally, 4 pages, CIKM'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03333v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03333v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05334v1</id>
    <updated>2018-08-16T03:25:09Z</updated>
    <published>2018-08-16T03:25:09Z</published>
    <title>Active Distribution Learning from Indirect Samples</title>
    <summary>  This paper studies the problem of {\em learning} the probability distribution
$P_X$ of a discrete random variable $X$ using indirect and sequential samples.
At each time step, we choose one of the possible $K$ functions, $g_1, \ldots,
g_K$ and observe the corresponding sample $g_i(X)$. The goal is to estimate the
probability distribution of $X$ by using a minimum number of such sequential
samples. This problem has several real-world applications including inference
under non-precise information and privacy-preserving statistical estimation. We
establish necessary and sufficient conditions on the functions $g_1, \ldots,
g_K$ under which asymptotically consistent estimation is possible. We also
derive lower bounds on the estimation error as a function of total samples and
show that it is order-wise achievable. Leveraging these results, we propose an
iterative algorithm that i) chooses the function to observe at each step based
on past observations; and ii) combines the obtained samples to estimate $p_X$.
The performance of this algorithm is investigated numerically under various
scenarios, and shown to outperform baseline approaches.
</summary>
    <author>
      <name>Samarth Gupta</name>
    </author>
    <author>
      <name>Gauri Joshi</name>
    </author>
    <author>
      <name>Osman Yağan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Allerton Conference on Communication, Control and Computing, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05329v1</id>
    <updated>2018-08-16T02:58:54Z</updated>
    <published>2018-08-16T02:58:54Z</published>
    <title>Sequential Behavioral Data Processing Using Deep Learning and the Markov
  Transition Field in Online Fraud Detection</title>
    <summary>  Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.
</summary>
    <author>
      <name>Ruinan Zhang</name>
    </author>
    <author>
      <name>Fanglan Zheng</name>
    </author>
    <author>
      <name>Wei Min</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD2018 Data Science in Fintech Workshop Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02633v2</id>
    <updated>2018-08-16T02:47:58Z</updated>
    <published>2018-08-08T05:45:24Z</published>
    <title>Courteous Autonomous Cars</title>
    <summary>  Typically, autonomous cars optimize for a combination of safety, efficiency,
and driving quality. But as we get better at this optimization, we start seeing
behavior go from too conservative to too aggressive. The car's behavior exposes
the incentives we provide in its cost function. In this work, we argue for cars
that are not optimizing a purely selfish cost, but also try to be courteous to
other interactive drivers. We formalize courtesy as a term in the objective
that measures the increase in another driver's cost induced by the autonomous
car's behavior. Such a courtesy term enables the robot car to be aware of
possible irrationality of the human behavior, and plan accordingly. We analyze
the effect of courtesy in a variety of scenarios. We find, for example, that
courteous robot cars leave more space when merging in front of a human driver.
Moreover, we find that such a courtesy term can help explain real human driver
behavior on the NGSIM dataset.
</summary>
    <author>
      <name>Liting Sun</name>
    </author>
    <author>
      <name>Wei Zhan</name>
    </author>
    <author>
      <name>Masayoshi Tomizuka</name>
    </author>
    <author>
      <name>Anca D. Dragan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Intelligent Robots (IROS) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06442v4</id>
    <updated>2018-08-16T02:38:06Z</updated>
    <published>2018-03-17T01:47:57Z</published>
    <title>Replica Symmetry Breaking in Bipartite Spin Glasses and Neural Networks</title>
    <summary>  Some interesting recent advances in the theoretical understanding of neural
networks have been informed by results from the physics of disordered many-body
systems. Motivated by these findings, this work uses the replica technique to
study the mathematically tractable bipartite Sherrington-Kirkpatrick (SK) spin
glass model, which is formally similar to a Restricted Boltzmann Machine (RBM)
neural network. The bipartite SK model has been previously studied assuming
replica symmetry; here this assumption is relaxed and a replica symmetry
breaking analysis is performed. The bipartite SK model is found to have many
features in common with Parisi's solution of the original, unipartite SK model,
including the existence of a multitude of pure states which are related in a
hierarchical, ultrametric fashion. As an application of this analysis, the
optimal cost for a graph partitioning problem is shown to be simply related to
the ground state energy of the bipartite SK model. As a second application,
empirical investigations reveal that the Gibbs sampled outputs of an RBM
trained on the MNIST data set are more ultrametrically distributed than the
input data itself.
</summary>
    <author>
      <name>Gavin Hartnett</name>
    </author>
    <author>
      <name>Edward Parker</name>
    </author>
    <author>
      <name>Edward Geist</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.98.022116</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.98.022116" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 14 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 98, 022116 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.06442v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06442v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06536v1</id>
    <updated>2018-08-15T23:49:51Z</updated>
    <published>2018-08-15T23:49:51Z</published>
    <title>Study of Set-Membership Adaptive Kernel Algorithms</title>
    <summary>  In the last decade, a considerable research effort has been devoted to
developing adaptive algorithms based on kernel functions. One of the main
features of these algorithms is that they form a family of universal
approximation techniques, solving problems with nonlinearities elegantly. In
this paper, we present data-selective adaptive kernel normalized least-mean
square (KNLMS) algorithms that can increase their learning rate and reduce
their computational complexity. In fact, these methods deal with kernel
expansions, creating a growing structure also known as the dictionary, whose
size depends on the number of observations and their innovation. The algorithms
described herein use an adaptive step-size to accelerate the learning and can
offer an excellent tradeoff between convergence speed and steady state, which
allows them to solve nonlinear filtering and estimation problems with a large
number of parameters without requiring a large computational cost. The
data-selective update scheme also limits the number of operations performed and
the size of the dictionary created by the kernel expansion, saving
computational resources and dealing with one of the major problems of kernel
adaptive algorithms. A statistical analysis is carried out along with a
computational complexity analysis of the proposed algorithms. Simulations show
that the proposed KNLMS algorithms outperform existing algorithms in examples
of nonlinear system identification and prediction of a time series originating
from a nonlinear difference equation.
</summary>
    <author>
      <name>A. Flores</name>
    </author>
    <author>
      <name>R. C. de Lamare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10251v2</id>
    <updated>2018-08-15T20:55:00Z</updated>
    <published>2018-07-26T17:22:29Z</published>
    <title>Aggregated Learning: A Vector Quantization Approach to Learning with
  Neural Networks</title>
    <summary>  We establish an equivalence between information bottleneck (IB) learning and
an unconventional quantization problem, `IB quantization'. Under this
equivalence, standard neural network models correspond to scalar IB quantizers.
We prove a coding theorem for IB quantization, which implies that scalar IB
quantizers are in general inferior to vector IB quantizers. This inspires us to
develop a learning framework for neural networks, AgrLearn, that corresponds to
vector IB quantizers. We experimentally verify that AgrLearn applied to some
deep network models of current art improves upon them, while requiring less
training data. With a heuristic smoothing, AgrLearn further improves its
performance, resulting in new state of the art in image classification on
Cifar10.
</summary>
    <author>
      <name>Hongyu Guo</name>
    </author>
    <author>
      <name>Yongyi Mao</name>
    </author>
    <author>
      <name>Richong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10588v2</id>
    <updated>2018-08-15T19:41:38Z</updated>
    <published>2018-07-18T20:16:00Z</published>
    <title>A Modality-Adaptive Method for Segmenting Brain Tumors and
  Organs-at-Risk in Radiation Therapy Planning</title>
    <summary>  In this paper we present a method for simultaneously segmenting brain tumors
and an extensive set of organs-at-risk for radiation therapy planning of
glioblastomas. The method combines a contrast-adaptive generative model for
whole-brain segmentation with a new spatial regularization model of tumor shape
using convolutional restricted Boltzmann machines. We demonstrate
experimentally that the method is able to adapt to image acquisitions that
differ substantially from any available training data, ensuring its
applicability across treatment sites; that its tumor segmentation accuracy is
comparable to that of the current state of the art; and that it captures most
organs-at-risk sufficiently well for radiation therapy planning purposes. The
proposed method may be a valuable step towards automating the delineation of
brain tumors and organs-at-risk in glioblastoma patients undergoing radiation
therapy.
</summary>
    <author>
      <name>Mikael Agn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Per Munck af Rosenschöld</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radiation Physics, Department of Hematology, Oncology and Radiation Physics, Skåne University Hospital, Lund, Sweden</arxiv:affiliation>
    </author>
    <author>
      <name>Oula Puonti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Michael J. Lundemann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Oncology, Copenhagen University Hospital Rigshospitalet, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Laura Mancini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroradiological Academic Unit, Department of Brain Repair and Rehabilitation, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Anastasia Papadaki</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroradiological Academic Unit, Department of Brain Repair and Rehabilitation, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Steffi Thust</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroradiological Academic Unit, Department of Brain Repair and Rehabilitation, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, UK</arxiv:affiliation>
    </author>
    <author>
      <name>John Ashburner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wellcome Centre for Human Neuroimaging, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Law</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Clinical Physiology, Nuclear Medicine and PET, Copenhagen University Hospital Rigshospitalet, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Koen Van Leemput</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Harvard Medical School, USA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">corrected one reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05264v1</id>
    <updated>2018-08-15T19:22:15Z</updated>
    <published>2018-08-15T19:22:15Z</published>
    <title>DeepDownscale: a Deep Learning Strategy for High-Resolution Weather
  Forecast</title>
    <summary>  Running high-resolution physical models is computationally expensive and
essential for many disciplines. Agriculture, transportation, and energy are
sectors that depend on high-resolution weather models, which typically consume
many hours of large High Performance Computing (HPC) systems to deliver timely
results. Many users cannot afford to run the desired resolution and are forced
to use low resolution output. One simple solution is to interpolate results for
visualization. It is also possible to combine an ensemble of low resolution
models to obtain a better prediction. However, these approaches fail to capture
the redundant information and patterns in the low-resolution input that could
help improve the quality of prediction. In this paper, we propose and evaluate
a strategy based on a deep neural network to learn a high-resolution
representation from low-resolution predictions using weather forecast as a
practical use case. We take a supervised learning approach, since obtaining
labeled data can be done automatically. Our results show significant
improvement when compared with standard practices and the strategy is still
lightweight enough to run on modest computer systems.
</summary>
    <author>
      <name>Eduardo R. Rodrigues</name>
    </author>
    <author>
      <name>Igor Oliveira</name>
    </author>
    <author>
      <name>Renato L. F. Cunha</name>
    </author>
    <author>
      <name>Marco A. S. Netto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, accepted for publication at 14th IEEE eScience</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05238v1</id>
    <updated>2018-08-15T18:03:12Z</updated>
    <published>2018-08-15T18:03:12Z</published>
    <title>AnatomyNet: Deep 3D Squeeze-and-excitation U-Nets for fast and fully
  automated whole-volume anatomical segmentation</title>
    <summary>  Radiation therapy (RT) is a common treatment for head and neck (HaN) cancer
where therapists are often required to manually delineate boundaries of the
organs-at-risks (OARs). Automated head and neck anatomical segmentation
provides a way to speed up and improve the reproducibility of radiation therapy
planning. In this work, we propose the AnatomyNet, an end-to-end and atlas-free
three dimensional squeeze-and-excitation U-Net (3D SE U-Net), for fast and
fully automated whole-volume HaN anatomical segmentation.
  There are two main challenges for fully automated HaN OARs segmentation: 1)
challenge in segmenting small anatomies (i.e., optic chiasm and optic nerves)
occupying only a few slices, and 2) training model with inconsistent data
annotations with missing ground truth for some anatomical structures because of
different RT planning. We propose the AnatomyNet that has one down-sampling
layer with the trade-off between GPU memory and feature representation
capacity, and 3D SE residual blocks for effective feature learning to alleviate
these challenges. Moreover, we design a hybrid loss function with the Dice loss
and the focal loss. The Dice loss is a class level distribution loss that
depends less on the number of voxels in the anatomy, and the focal loss is
designed to deal with highly unbalanced segmentation. For missing annotations,
we propose masked loss and weighted loss for accurate and balanced weights
updating in the learning of the AnatomyNet.
  We collect 261 HaN CT images to train the AnatomyNet for segmenting nine
anatomies. Compared to previous state-of-the-art methods for each anatomy from
the MICCAI 2015 competition, the AnatomyNet increases Dice similarity
coefficient (DSC) by 3.3% on average. The proposed AnatomyNet takes only 0.12
seconds on average to segment a whole-volume HaN CT image of an average
dimension of 178x302x225.
</summary>
    <author>
      <name>Wentao Zhu</name>
    </author>
    <author>
      <name>Yufang Huang</name>
    </author>
    <author>
      <name>Hui Tang</name>
    </author>
    <author>
      <name>Zhen Qian</name>
    </author>
    <author>
      <name>Nan Du</name>
    </author>
    <author>
      <name>Wei Fan</name>
    </author>
    <author>
      <name>Xiaohui Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 figures, 4 videos in GitHub and YouTube. Submitted to Medical
  Physics. Code and videos are available on GitHub. Video:
  https://www.youtube.com/watch?v=_PpIUIm4XLU</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05394v2</id>
    <updated>2018-08-15T17:19:40Z</updated>
    <published>2018-06-14T07:04:31Z</published>
    <title>Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables
  Signal Propagation in Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks have gained widespread use in modeling sequence
data across various domains. While many successful recurrent architectures
employ a notion of gating, the exact mechanism that enables such remarkable
performance is not well understood. We develop a theory for signal propagation
in recurrent networks after random initialization using a combination of mean
field theory and random matrix theory. To simplify our discussion, we introduce
a new RNN cell with a simple gating mechanism that we call the minimalRNN and
compare it with vanilla RNNs. Our theory allows us to define a maximum
timescale over which RNNs can remember an input. We show that this theory
predicts trainability for both recurrent architectures. We show that gated
recurrent networks feature a much broader, more robust, trainable region than
vanilla RNNs, which corroborates recent experimental findings. Finally, we
develop a closed-form critical initialization scheme that achieves dynamical
isometry in both vanilla RNNs and minimalRNNs. We show that this results in
significantly improvement in training dynamics. Finally, we demonstrate that
the minimalRNN achieves comparable performance to its more complex
counterparts, such as LSTMs or GRUs, on a language modeling task.
</summary>
    <author>
      <name>Minmin Chen</name>
    </author>
    <author>
      <name>Jeffrey Pennington</name>
    </author>
    <author>
      <name>Samuel S. Schoenholz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018 Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05394v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05394v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05174v1</id>
    <updated>2018-08-15T16:34:08Z</updated>
    <published>2018-08-15T16:34:08Z</published>
    <title>Recycle-GAN: Unsupervised Video Retargeting</title>
    <summary>  We introduce a data-driven approach for unsupervised video retargeting that
translates content from one domain to another while preserving the style native
to a domain, i.e., if contents of John Oliver's speech were to be transferred
to Stephen Colbert, then the generated content/speech should be in Stephen
Colbert's style. Our approach combines both spatial and temporal information
along with adversarial losses for content translation and style preservation.
In this work, we first study the advantages of using spatiotemporal constraints
over spatial constraints for effective retargeting. We then demonstrate the
proposed approach for the problems where information in both space and time
matters such as face-to-face translation, flower-to-flower, wind and cloud
synthesis, sunrise and sunset.
</summary>
    <author>
      <name>Aayush Bansal</name>
    </author>
    <author>
      <name>Shugao Ma</name>
    </author>
    <author>
      <name>Deva Ramanan</name>
    </author>
    <author>
      <name>Yaser Sheikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018; Please refer to project webpage for videos -
  http://www.cs.cmu.edu/~aayushb/Recycle-GAN</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04272v2</id>
    <updated>2018-08-15T16:24:39Z</updated>
    <published>2018-05-11T08:28:55Z</published>
    <title>An $O(N)$ Sorting Algorithm: Machine Learning Sort</title>
    <summary>  We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method,
which shows a huge potential sorting big data. This sorting algorithm can be
applied to parallel sorting and is suitable for GPU or TPU acceleration.
Furthermore, we discuss the application of this algorithm to sparse hash table.
</summary>
    <author>
      <name>Hanqing Zhao</name>
    </author>
    <author>
      <name>Yuehan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10511v1</id>
    <updated>2018-08-15T16:07:33Z</updated>
    <published>2018-08-15T16:07:33Z</published>
    <title>Development and Evaluation of Recurrent Neural Network based Models for
  Hourly Traffic Volume and AADT Prediction</title>
    <summary>  The prediction of high-resolution hourly traffic volumes of a given roadway
is essential for future planning in a transportation system. Traditionally,
Automatic Traffic Recorders (ATR) are used to collect this hourly volume data,
which is often used to predict future traffic volumes accurately. However,
these large data sets generated from these various ATRs are time series data
characterized by long-term temporal dependencies and missing data sets.
Regarding the temporal dependencies, all roadways are characterized by seasonal
variations that can be weekly, monthly or yearly, depending on the cause of the
variation. Regarding the missing data in a time-series sequence, traditional
time series forecasting models perform poorly under their influence. Therefore,
a robust, Recurrent Neural Network (RNN) based, multi-step ahead time-series
forecasting model is developed in this study. The simple RNN, the Gated
Recurrent Unit (GRU) and the Long Short-Term Memory (LSTM) units are used to
develop the model and evaluate its performance. Two novel approaches are used
to address the missing value issue: masking and imputation, in conjunction with
the RNN unit. Six different imputation algorithms are then used to identify the
best model. Our analyses indicate that the LSTM model performs better than
simple RNN and GRU models, and imputation performs better than masking. The
performance of imputation methods is also dependent on the percentage of
missing data in the input dataset. Overall, the LSTM-Median model is deemed the
best model in all scenarios for AADT prediction, with an average accuracy of
98.5%.
</summary>
    <author>
      <name>MD Zadid Khan</name>
    </author>
    <author>
      <name>Sakib Mahmud Chowdhury</name>
    </author>
    <author>
      <name>Mashrur Chowdhury</name>
    </author>
    <author>
      <name>Kakan Dey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 18 figures, 6393 words, Submitted to Transportation
  Research Record (TRR) Journal, Currently under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02234v2</id>
    <updated>2018-08-15T15:44:25Z</updated>
    <published>2018-08-07T07:15:54Z</published>
    <title>Deep Stacked Stochastic Configuration Networks for Non-Stationary Data
  Streams</title>
    <summary>  The concept of stochastic configuration networks (SCNs) others a solid
framework for fast implementation of feedforward neural networks through
randomized learning. Unlike conventional randomized approaches, SCNs provide an
avenue to select appropriate scope of random parameters to ensure the universal
approximation property. In this paper, a deep version of stochastic
configuration networks, namely deep stacked stochastic configuration network
(DSSCN), is proposed for modeling non-stationary data streams. As an extension
of evolving stochastic connfiguration networks (eSCNs), this work contributes a
way to grow and shrink the structure of deep stochastic configuration networks
autonomously from data streams. The performance of DSSCN is evaluated by six
benchmark datasets. Simulation results, compared with prominent data stream
algorithms, show that the proposed method is capable of achieving comparable
accuracy and evolving compact and parsimonious deep stacked network
architecture.
</summary>
    <author>
      <name>Mahardhika Pratama</name>
    </author>
    <author>
      <name>Dianhui Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02234v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02234v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05128v1</id>
    <updated>2018-08-15T15:20:49Z</updated>
    <published>2018-08-15T15:20:49Z</published>
    <title>Using Regular Languages to Explore the Representational Capacity of
  Recurrent Neural Architectures</title>
    <summary>  The presence of Long Distance Dependencies (LDDs) in sequential data poses
significant challenges for computational models. Various recurrent neural
architectures have been designed to mitigate this issue. In order to test these
state-of-the-art architectures, there is growing need for rich benchmarking
datasets. However, one of the drawbacks of existing datasets is the lack of
experimental control with regards to the presence and/or degree of LDDs. This
lack of control limits the analysis of model performance in relation to the
specific challenge posed by LDDs. One way to address this is to use synthetic
data having the properties of subregular languages. The degree of LDDs within
the generated data can be controlled through the k parameter, length of the
generated strings, and by choosing appropriate forbidden strings. In this
paper, we explore the capacity of different RNN extensions to model LDDs, by
evaluating these models on a sequence of SPk synthesized datasets, where each
subsequent dataset exhibits a longer degree of LDD. Even though SPk are simple
languages, the presence of LDDs does have significant impact on the performance
of recurrent neural architectures, thus making them prime candidate in
benchmarking tasks.
</summary>
    <author>
      <name>Abhijit Mahalunkar</name>
    </author>
    <author>
      <name>John D. Kelleher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference of Artificial Neural Networks (ICANN) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05110v1</id>
    <updated>2018-08-15T14:54:44Z</updated>
    <published>2018-08-15T14:54:44Z</published>
    <title>Joint &amp; Progressive Learning from High-Dimensional Data for Multi-Label
  Classification</title>
    <summary>  Despite the fact that nonlinear subspace learning techniques (e.g. manifold
learning) have successfully applied to data representation, there is still room
for improvement in explainability (explicit mapping), generalization
(out-of-samples), and cost-effectiveness (linearization). To this end, a novel
linearized subspace learning technique is developed in a joint and progressive
way, called \textbf{j}oint and \textbf{p}rogressive \textbf{l}earning
str\textbf{a}teg\textbf{y} (J-Play), with its application to multi-label
classification. The J-Play learns high-level and semantically meaningful
feature representation from high-dimensional data by 1) jointly performing
multiple subspace learning and classification to find a latent subspace where
samples are expected to be better classified; 2) progressively learning
multi-coupled projections to linearly approach the optimal mapping bridging the
original space with the most discriminative subspace; 3) locally embedding
manifold structure in each learnable latent subspace. Extensive experiments are
performed to demonstrate the superiority and effectiveness of the proposed
method in comparison with previous state-of-the-art methods.
</summary>
    <author>
      <name>Danfeng Hong</name>
    </author>
    <author>
      <name>Naoto Yokoya</name>
    </author>
    <author>
      <name>Jian Xu</name>
    </author>
    <author>
      <name>Xiaoxiang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted in ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05480v1</id>
    <updated>2018-08-15T12:59:14Z</updated>
    <published>2018-08-15T12:59:14Z</published>
    <title>A novel Empirical Bayes with Reversible Jump Markov Chain in User-Movie
  Recommendation system</title>
    <summary>  In this article we select the unknown dimension of the feature by re-
versible jump MCMC inside a simulated annealing in bayesian set up of
collaborative filter. We implement the same in MovieLens small dataset. We also
tune the hyper parameter by using a modified empirical bayes. It can also be
used to guess an initial choice for hyper-parameters in grid search procedure
even for the datasets where MCMC oscillates around the true value or takes long
time to converge.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Himanshu Jhamb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1707.02294</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05054v1</id>
    <updated>2018-08-15T12:25:02Z</updated>
    <published>2018-08-15T12:25:02Z</published>
    <title>Shedding Light on Black Box Machine Learning Algorithms: Development of
  an Axiomatic Framework to Assess the Quality of Methods that Explain
  Individual Predictions</title>
    <summary>  From self-driving vehicles and back-flipping robots to virtual assistants who
book our next appointment at the hair salon or at that restaurant for dinner -
machine learning systems are becoming increasingly ubiquitous. The main reason
for this is that these methods boast remarkable predictive capabilities.
However, most of these models remain black boxes, meaning that it is very
challenging for humans to follow and understand their intricate inner workings.
Consequently, interpretability has suffered under this ever-increasing
complexity of machine learning models. Especially with regards to new
regulations, such as the General Data Protection Regulation (GDPR), the
necessity for plausibility and verifiability of predictions made by these black
boxes is indispensable. Driven by the needs of industry and practice, the
research community has recognised this interpretability problem and focussed on
developing a growing number of so-called explanation methods over the past few
years. These methods explain individual predictions made by black box machine
learning models and help to recover some of the lost interpretability. With the
proliferation of these explanation methods, it is, however, often unclear,
which explanation method offers a higher explanation quality, or is generally
better-suited for the situation at hand. In this thesis, we thus propose an
axiomatic framework, which allows comparing the quality of different
explanation methods amongst each other. Through experimental validation, we
find that the developed framework is useful to assess the explanation quality
of different explanation methods and reach conclusions that are consistent with
independent research.
</summary>
    <author>
      <name>Milo Honegger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: black box, machine learning, interpretability, explanation
  methods, explanation quality, axiomatic explanation consistency</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05032v1</id>
    <updated>2018-08-15T10:30:41Z</updated>
    <published>2018-08-15T10:30:41Z</published>
    <title>Deep RTS: A Game Environment for Deep Reinforcement Learning in
  Real-Time Strategy Games</title>
    <summary>  Reinforcement learning (RL) is an area of research that has blossomed
tremendously in recent years and has shown remarkable potential for artificial
intelligence based opponents in computer games. This success is primarily due
to the vast capabilities of convolutional neural networks, that can extract
useful features from noisy and complex data. Games are excellent tools to test
and push the boundaries of novel RL algorithms because they give valuable
insight into how well an algorithm can perform in isolated environments without
the real-life consequences. Real-time strategy games (RTS) is a genre that has
tremendous complexity and challenges the player in short and long-term
planning. There is much research that focuses on applied RL in RTS games, and
novel advances are therefore anticipated in the not too distant future.
However, there are to date few environments for testing RTS AIs. Environments
in the literature are often either overly simplistic, such as microRTS, or
complex and without the possibility for accelerated learning on consumer
hardware like StarCraft II. This paper introduces the Deep RTS game environment
for testing cutting-edge artificial intelligence algorithms for RTS games. Deep
RTS is a high-performance RTS game made specifically for artificial
intelligence research. It supports accelerated learning, meaning that it can
learn at a magnitude of 50 000 times faster compared to existing RTS games.
Deep RTS has a flexible configuration, enabling research in several different
RTS scenarios, including partially observable state-spaces and map complexity.
We show that Deep RTS lives up to our promises by comparing its performance
with microRTS, ELF, and StarCraft II on high-end consumer hardware. Using Deep
RTS, we show that a Deep Q-Network agent beats random-play agents over 70% of
the time. Deep RTS is publicly available at https://github.com/cair/DeepRTS.
</summary>
    <author>
      <name>Per-Arne Andersen</name>
    </author>
    <author>
      <name>Morten Goodwin</name>
    </author>
    <author>
      <name>Ole-Christoffer Granmo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE International Conference on Computational
  Intelligence and Games (CIG 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00245v2</id>
    <updated>2018-08-15T10:06:20Z</updated>
    <published>2018-08-01T09:43:50Z</published>
    <title>Robbins-Mobro conditions for persistent exploration learning strategies</title>
    <summary>  We formulate simple assumptions, implying the Robbins-Monro conditions for
the $Q$-learning algorithm with the local learning rate, depending on the
number of visits of a particular state-action pair (local clock) and the number
of iteration (global clock). It is assumed that the Markov decision process is
communicating and the learning policy ensures the persistent exploration. The
restrictions are imposed on the functional dependence of the learning rate on
the local and global clocks. The result partially confirms the conjecture of
Bradkte (1994).
</summary>
    <author>
      <name>Dmitry B. Rokhlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Theorem 2 was improved</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00245v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00245v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="93E35, 62L20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09802v1</id>
    <updated>2018-08-15T07:36:01Z</updated>
    <published>2018-08-15T07:36:01Z</published>
    <title>Modelling Irregular Spatial Patterns using Graph Convolutional Neural
  Networks</title>
    <summary>  The understanding of geographical reality is a process of data representation
and pattern discovery. Former studies mainly adopted continuous-field models to
represent spatial variables and to investigate the underlying spatial
continuity/heterogeneity in the regular spatial domain. In this article, we
introduce a more generalized model based on graph convolutional neural networks
(GCNs) that can capture the complex parameters of spatial patterns underlying
graph-structured spatial data, which generally contain both Euclidean spatial
information and non-Euclidean feature information. A trainable semi-supervised
prediction framework is proposed to model the spatial distribution patterns of
intra-urban points of interest(POI) check-ins. This work demonstrates the
feasibility of GCNs in complex geographic decision problems and provides a
promising tool to analyze irregular spatial data.
</summary>
    <author>
      <name>Di Zhu</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, preprint for arxiv</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06537v1</id>
    <updated>2018-08-15T04:23:31Z</updated>
    <published>2018-08-15T04:23:31Z</published>
    <title>Ricean K-factor Estimation based on Channel Quality Indicator in OFDM
  Systems using Neural Network</title>
    <summary>  Ricean channel model is widely used in wireless communications to
characterize the channels with a line-of-sight path. The Ricean K factor,
defined as the ratio of direct path and scattered paths, provides a good
indication of the link quality. Most existing works estimate K factor based on
either maximum-likelihood criterion or higher-order moments, and the existing
works are targeted at K-factor estimation at receiver side. In this work, a
novel approach is proposed. Cast as a classification problem, the estimation of
K factor by neural network provides high accuracy. Moreover, the proposed
K-factor estimation is done at transmitter side for transmit processing, thus
saving the limited feedback bandwidth.
</summary>
    <author>
      <name>Kun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12111v3</id>
    <updated>2018-08-15T03:00:45Z</updated>
    <published>2018-05-24T04:03:39Z</published>
    <title>Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend
  Prediction of Critical Metal Companies</title>
    <summary>  The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more "critical," and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different "advisors." creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
</summary>
    <author>
      <name>Zhengyang Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Major revision. Methodology was optimized, more case study companies
  were added, and more experiments were conducted for more detailed method
  analyses. Changed the writing to focus more on stock prediction and less on
  critical metals</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.12111v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12111v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04952v1</id>
    <updated>2018-08-15T02:39:35Z</updated>
    <published>2018-08-15T02:39:35Z</published>
    <title>Convolutional Neural Networks on 3D Surfaces Using Parallel Frames</title>
    <summary>  We extend Convolutional Neural Networks (CNNs) on flat and regular domains
(e.g. 2D images) to curved surfaces embedded in 3D Euclidean space that are
discretized as irregular meshes and widely used to represent geometric data in
Computer Vision and Graphics. We define surface convolution on tangent spaces
of a surface domain, where the convolution has two desirable properties: 1) the
distortion of surface domain signals is locally minimal when being projected to
the tangent space, and 2) the translation equi-variance property holds locally,
by aligning tangent spaces with the canonical parallel transport that preserves
metric. For computation, we rely on a parallel N-direction frame field on the
surface that minimizes field variation and therefore is as compatible as
possible to and approximates the parallel transport. On the tangent spaces
equipped with parallel frames, the computation of surface convolution becomes
standard routine. The frames have rotational symmetry which we disambiguate by
constructing the covering space of surface induced by the parallel frames and
grouping the feature maps into N sets accordingly; convolution is computed on
the N branches of the cover space with respective feature maps while the kernel
weights are shared. To handle irregular points of a discrete mesh while sharing
kernel weights, we make the convolution semi-discrete, i.e. the convolution
kernels are polynomial functions, and their convolution with discrete surface
points becomes sampling and weighted summation. Pooling and unpooling
operations are computed along a mesh hierarchy built through simplification.
The presented surface CNNs allow effective deep learning on meshes. We show
that for tasks of classification, segmentation and non-rigid registration,
surface CNNs using only raw input signals achieve superior performances than
previous models using sophisticated input features.
</summary>
    <author>
      <name>Hao Pan</name>
    </author>
    <author>
      <name>Shilin Liu</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Xin Tong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04381v3</id>
    <updated>2018-08-15T02:36:36Z</updated>
    <published>2018-02-12T22:35:38Z</published>
    <title>Classification from Pairwise Similarity and Unlabeled Data</title>
    <summary>  Supervised learning needs a huge amount of labeled data, which can be a big
bottleneck under the situation where there is a privacy concern or labeling
cost is high. To overcome this problem, we propose a new weakly-supervised
learning setting where only similar (S) data pairs (two examples belong to the
same class) and unlabeled (U) data points are needed instead of fully labeled
data, which is called SU classification. We show that an unbiased estimator of
the classification risk can be obtained only from SU data, and the estimation
error of its empirical risk minimizer achieves the optimal parametric
convergence rate. Finally, we demonstrate the effectiveness of the proposed
method through experiments.
</summary>
    <author>
      <name>Han Bao</name>
    </author>
    <author>
      <name>Gang Niu</name>
    </author>
    <author>
      <name>Masashi Sugiyama</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04381v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04381v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04947v1</id>
    <updated>2018-08-15T02:15:45Z</updated>
    <published>2018-08-15T02:15:45Z</published>
    <title>Collapse of Deep and Narrow Neural Nets</title>
    <summary>  Recent theoretical work has demonstrated that deep neural networks have
superior performance over shallow networks, but their training is more
difficult, e.g., they suffer from the vanishing gradient problem. This problem
can be typically resolved by the rectified linear unit (ReLU) activation.
However, here we show that even for such activation, deep and narrow neural
networks will converge to erroneous mean or median states of the target
function depending on the loss with high probability. We demonstrate this
collapse of deep and narrow neural networks both numerically and theoretically,
and provide estimates of the probability of collapse. We also construct a
diagram of a safe region of designing neural networks that avoid the collapse
to erroneous states. Finally, we examine different ways of initialization and
normalization that may avoid the collapse problem.
</summary>
    <author>
      <name>Lu Lu</name>
    </author>
    <author>
      <name>Yanhui Su</name>
    </author>
    <author>
      <name>George Em Karniadakis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04855v2</id>
    <updated>2018-08-15T01:15:27Z</updated>
    <published>2018-07-12T22:57:19Z</published>
    <title>A feature agnostic approach for glaucoma detection in OCT volumes</title>
    <summary>  Optical coherence tomography (OCT) based measurements of retinal layer
thickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell
with inner plexiform layer (GCIPL) are commonly used for the diagnosis and
monitoring of glaucoma. Previously, machine learning techniques have utilized
segmentation-based imaging features such as the peripapillary RNFL thickness
and the cup-to-disc ratio. Here, we propose a deep learning technique that
classifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT
volumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network
(CNN). We compared the accuracy of this technique with various feature-based
machine learning algorithms and demonstrated the superiority of the proposed
deep learning based method.
  Logistic regression was found to be the best performing classical machine
learning technique with an AUC of 0.89. In direct comparison, the deep learning
approach achieved a substantially higher AUC of 0.94 with the additional
advantage of providing insight into which regions of an OCT volume are
important for glaucoma detection.
  Computing Class Activation Maps (CAM), we found that the CNN identified
neuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and
its surrounding areas as the regions significantly associated with the glaucoma
classification. These regions anatomically correspond to the well established
and commonly used clinical markers for glaucoma diagnosis such as increased cup
volume, cup diameter, and neuroretinal rim thinning at the superior and
inferior segments.
</summary>
    <author>
      <name>Stefan Maetschke</name>
    </author>
    <author>
      <name>Bhavna Antony</name>
    </author>
    <author>
      <name>Hiroshi Ishikawa</name>
    </author>
    <author>
      <name>Gadi Wollstein</name>
    </author>
    <author>
      <name>Joel S. Schuman</name>
    </author>
    <author>
      <name>Rahil Garvani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04855v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04855v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04929v1</id>
    <updated>2018-08-15T00:20:35Z</updated>
    <published>2018-08-15T00:20:35Z</published>
    <title>Holographic Visualisation of Radiology Data and Automated Machine
  Learning-based Medical Image Segmentation</title>
    <summary>  Within this thesis we propose a platform for combining Augmented Reality (AR)
hardware with machine learning in a user-oriented pipeline, offering to the
medical staff an intuitive 3D visualization of volumetric Computed Tomography
(CT) and Magnetic Resonance Imaging (MRI) medical image segmentations inside
the AR headset, that does not need human intervention for loading, processing
and segmentation of medical images. The AR visualization, based on Microsoft
HoloLens, employs a modular and thus scalable frontend-backend architecture for
real-time visualizations on multiple AR headsets. As Convolutional Neural
Networks (CNNs) have lastly demonstrated superior performance for the machine
learning task of image semantic segmentation, the pipeline also includes a
fully automated CNN algorithm for the segmentation of the liver from CT scans.
The model is based on the Deep Retinal Image Understanding (DRIU) model which
is a Fully Convolutional Network with side outputs from feature maps with
different resolution, extracted at different stages of the network. The
algorithm is 2.5D which means that the input is a set of consecutive scan
slices. The experiments have been performed on the Liver Tumor Segmentation
Challenge (LiTS) dataset for liver segmentation and demonstrated good results
and flexibility. While multiple approaches exist in the domain, only few of
them have focused on overcoming the practical aspects which still largely hold
this technology away from the operating rooms. In line with this, we also are
next planning an evaluation from medical doctors and radiologists in a
real-world environment.
</summary>
    <author>
      <name>Lucian Trestioreanu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04928v1</id>
    <updated>2018-08-15T00:10:55Z</updated>
    <published>2018-08-15T00:10:55Z</published>
    <title>Deep EHR: Chronic Disease Prediction Using Medical Notes</title>
    <summary>  Early detection of preventable diseases is important for better disease
management, improved inter-ventions, and more efficient health-care resource
allocation. Various machine learning approacheshave been developed to utilize
information in Electronic Health Record (EHR) for this task. Majorityof
previous attempts, however, focus on structured fields and lose the vast amount
of information inthe unstructured notes. In this work we propose a general
multi-task framework for disease onsetprediction that combines both free-text
medical notes and structured information. We compareperformance of different
deep learning architectures including CNN, LSTM and hierarchical models.In
contrast to traditional text-based prediction models, our approach does not
require disease specificfeature engineering, and can handle negations and
numerical values that exist in the text. Ourresults on a cohort of about 1
million patients show that models using text outperform modelsusing just
structured data, and that models capable of using numerical values and
negations in thetext, in addition to the raw text, further improve performance.
Additionally, we compare differentvisualization methods for medical
professionals to interpret model predictions.
</summary>
    <author>
      <name>Jingshu Liu</name>
    </author>
    <author>
      <name>Zachariah Zhang</name>
    </author>
    <author>
      <name>Narges Razavian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Health Care conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05637v2</id>
    <updated>2018-08-15T00:02:46Z</updated>
    <published>2018-02-15T16:19:21Z</published>
    <title>cGANs with Projection Discriminator</title>
    <summary>  We propose a novel, projection based way to incorporate the conditional
information into the discriminator of GANs that respects the role of the
conditional information in the underlining probabilistic model. This approach
is in contrast with most frameworks of conditional GANs used in application
today, which use the conditional information by concatenating the (embedded)
conditional vector to the feature vectors. With this modification, we were able
to significantly improve the quality of the class conditional image generation
on ILSVRC2012 (ImageNet) 1000-class image dataset from the current
state-of-the-art result, and we achieved this with a single pair of a
discriminator and a generator. We were also able to extend the application to
super-resolution and succeeded in producing highly discriminative
super-resolution images. This new structure also enabled high quality category
transformation based on parametric functional transformation of conditional
batch normalization layers in the generator.
</summary>
    <author>
      <name>Takeru Miyato</name>
    </author>
    <author>
      <name>Masanori Koyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05637v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05637v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07519v4</id>
    <updated>2018-08-14T23:07:39Z</updated>
    <published>2018-03-20T16:52:12Z</published>
    <title>DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems</title>
    <summary>  Deep learning (DL) defines a new data-driven programming paradigm that
constructs the internal system logic of a crafted neuron network through a set
of training data. We have seen wide adoption of DL in many safety-critical
scenarios. However, a plethora of studies have shown that the state-of-the-art
DL systems suffer from various vulnerabilities which can lead to severe
consequences when applied to real-world applications. Currently, the testing
adequacy of a DL system is usually measured by the accuracy of test data.
Considering the limitation of accessible high quality test data, good accuracy
performance on test data can hardly provide confidence to the testing adequacy
and generality of DL systems. Unlike traditional software systems that have
clear and controllable logic and functionality, the lack of interpretability in
a DL system makes system analysis and defect detection difficult, which could
potentially hinder its real-world deployment. In this paper, we propose
DeepGauge, a set of multi-granularity testing criteria for DL systems, which
aims at rendering a multi-faceted portrayal of the testbed. The in-depth
evaluation of our proposed testing criteria is demonstrated on two well-known
datasets, five DL systems, and with four state-of-the-art adversarial attack
techniques against DL. The potential usefulness of DeepGauge sheds light on the
construction of more generic and robust DL systems.
</summary>
    <author>
      <name>Lei Ma</name>
    </author>
    <author>
      <name>Felix Juefei-Xu</name>
    </author>
    <author>
      <name>Fuyuan Zhang</name>
    </author>
    <author>
      <name>Jiyuan Sun</name>
    </author>
    <author>
      <name>Minhui Xue</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Chunyang Chen</name>
    </author>
    <author>
      <name>Ting Su</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Jianjun Zhao</name>
    </author>
    <author>
      <name>Yadong Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3238147.3238202</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3238147.3238202" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 33rd IEEE/ACM International Conference on Automated Software
  Engineering (ASE 2018)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DeepGauge: Multi-Granularity Testing Criteria for Deep Learning
  Systems. In Proceedings of the 33rd ACM/IEEE International Conference on
  Automated Software Engineering (ASE 18), September 3-7, 2018, Montpellier,
  France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.07519v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07519v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04888v1</id>
    <updated>2018-08-14T20:39:17Z</updated>
    <published>2018-08-14T20:39:17Z</published>
    <title>Skill Rating for Generative Models</title>
    <summary>  We explore a new way to evaluate generative models using insights from
evaluation of competitive games between human players. We show experimentally
that tournaments between generators and discriminators provide an effective way
to evaluate generative models. We introduce two methods for summarizing
tournament outcomes: tournament win rate and skill rating. Evaluations are
useful in different contexts, including monitoring the progress of a single
model as it learns during the training process, and comparing the capabilities
of two different fully trained models. We show that a tournament consisting of
a single model playing against past and future versions of itself produces a
useful measure of training progress. A tournament containing multiple separate
models (using different seeds, hyperparameters, and architectures) provides a
useful relative comparison between different trained GANs. Tournament-based
rating methods are conceptually distinct from numerous previous categories of
approaches to evaluation of generative models, and have complementary
advantages and disadvantages.
</summary>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Surya Bhupatiraju</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Augustus Odena</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04880v1</id>
    <updated>2018-08-14T20:08:33Z</updated>
    <published>2018-08-14T20:08:33Z</published>
    <title>A Precision Environment-Wide Association Study of Hypertension via
  Supervised Cadre Models</title>
    <summary>  We consider the problem in precision health of grouping people into
subpopulations based on their degree of vulnerability to a risk factor. These
subpopulations cannot be discovered with traditional clustering techniques
because their quality is evaluated with a supervised metric: the ease of
modeling a response variable over observations within them. Instead, we apply
the supervised cadre model (SCM), which does use this metric. We extend the SCM
formalism so that it may be applied to multivariate regression and binary
classification problems. We also develop a way to use conditional entropy to
assess the confidence in the process by which a subject is assigned their
cadre. Using the SCM, we generalize the environment-wide association study
(EWAS) workflow to be able to model heterogeneity in population risk. In our
EWAS, we consider more than two hundred environmental exposure factors and find
their association with diastolic blood pressure, systolic blood pressure, and
hypertension. This requires adapting the SCM to be applicable to data generated
by a complex survey design. After correcting for false positives, we found 25
exposure variables that had a significant association with at least one of our
response variables. Eight of these were significant for a discovered
subpopulation but not for the overall population. Some of these associations
have been identified by previous researchers, while others appear to be novel
associations. We examine several learned subpopulations in detail, and we find
that they are interpretable and that they suggest further research questions.
</summary>
    <author>
      <name>Alexander New</name>
    </author>
    <author>
      <name>Kristin P. Bennett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04875v1</id>
    <updated>2018-08-14T19:55:38Z</updated>
    <published>2018-08-14T19:55:38Z</published>
    <title>Multi-user Communication Networks: A Coordinated Multi-armed Bandit
  Approach</title>
    <summary>  Communication networks shared by many users are a widespread challenge
nowadays. In this paper we address several aspects of this challenge
simultaneously: learning unknown stochastic network characteristics, sharing
resources with other users while keeping coordination overhead to a minimum.
The proposed solution combines Multi-Armed Bandit learning with a lightweight
signalling-based coordination scheme, and ensures convergence to a stable
allocation of resources. Our work considers single-user level algorithms for
two scenarios: an unknown fixed number of users, and a dynamic number of users.
Analytic performance guarantees, proving convergence to stable marriage
configurations, are presented for both setups. The algorithms are designed
based on a system-wide perspective, rather than focusing on single user
welfare. Thus, maximal resource utilization is ensured. An extensive
experimental analysis covers convergence to a stable configuration as well as
reward maximization. Experiments are carried out over a wide range of setups,
demonstrating the advantages of our approach over existing state-of-the-art
methods.
</summary>
    <author>
      <name>Orly Avner</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04873v1</id>
    <updated>2018-08-14T19:41:12Z</updated>
    <published>2018-08-14T19:41:12Z</published>
    <title>Generalization of Equilibrium Propagation to Vector Field Dynamics</title>
    <summary>  The biological plausibility of the backpropagation algorithm has long been
doubted by neuroscientists. Two major reasons are that neurons would need to
send two different types of signal in the forward and backward phases, and that
pairs of neurons would need to communicate through symmetric bidirectional
connections. We present a simple two-phase learning procedure for fixed point
recurrent networks that addresses both these issues. In our model, neurons
perform leaky integration and synaptic weights are updated through a local
mechanism. Our learning method generalizes Equilibrium Propagation to vector
field dynamics, relaxing the requirement of an energy function. As a
consequence of this generalization, the algorithm does not compute the true
gradient of the objective function, but rather approximates it at a precision
which is proven to be directly related to the degree of symmetry of the
feedforward and feedback weights. We show experimentally that our algorithm
optimizes the objective function.
</summary>
    <author>
      <name>Benjamin Scellier</name>
    </author>
    <author>
      <name>Anirudh Goyal</name>
    </author>
    <author>
      <name>Jonathan Binas</name>
    </author>
    <author>
      <name>Thomas Mesnard</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04866v1</id>
    <updated>2018-08-14T19:20:35Z</updated>
    <published>2018-08-14T19:20:35Z</published>
    <title>Mitigating Sybils in Federated Learning Poisoning</title>
    <summary>  Machine learning (ML) over distributed data is relevant to a variety of
domains. Existing approaches, such as federated learning, compose the outputs
computed by a group of devices at a central aggregator and run multi-round
algorithms to generate a globally shared model. Unfortunately, such approaches
are susceptible to a variety of attacks, including model poisoning, which is
made substantially worse in the presence of sybils.
  In this paper we first evaluate the vulnerability of federated learning to
sybil-based poisoning attacks. We then describe FoolsGold, a novel defense to
this problem that identifies poisoning sybils based on the diversity of client
contributions in the distributed learning process. Unlike prior work, our
system does not assume that the attackers are in the minority, requires no
auxiliary information outside of the learning process, and makes fewer
assumptions about clients and their data.
  In our evaluation we show that FoolsGold exceeds the capabilities of existing
state of the art approaches to countering ML poisoning attacks. Our results
hold for a variety of conditions, including different distributions of data,
varying poisoning targets, and various attack strategies.
</summary>
    <author>
      <name>Clement Fung</name>
    </author>
    <author>
      <name>Chris J. M. Yoon</name>
    </author>
    <author>
      <name>Ivan Beschastnikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10837v2</id>
    <updated>2018-08-14T19:03:32Z</updated>
    <published>2018-03-28T20:14:08Z</published>
    <title>Learning Deep Representations with Probabilistic Knowledge Transfer</title>
    <summary>  Knowledge Transfer (KT) techniques tackle the problem of transferring the
knowledge from a large and complex neural network into a smaller and faster
one. However, existing KT methods are tailored towards classification tasks and
they cannot be used efficiently for other representation learning tasks. In
this paper a novel knowledge transfer technique, that is capable of training a
student model that maintains the same amount of mutual information between the
learned representation and a set of (possible unknown) labels as the teacher
model, is proposed. Apart from outperforming existing KT techniques, the
proposed method allows for overcoming several limitations of existing methods
providing new insight into KT as well as novel KT applications, ranging from
knowledge transfer from handcrafted feature extractors to {cross-modal} KT from
the textual modality into the representation extracted from the visual modality
of the data.
</summary>
    <author>
      <name>Nikolaos Passalis</name>
    </author>
    <author>
      <name>Anastasios Tefas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.10837v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10837v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04732v2</id>
    <updated>2018-08-14T18:44:12Z</updated>
    <published>2018-04-12T21:17:54Z</published>
    <title>Multimodal Unsupervised Image-to-Image Translation</title>
    <summary>  Unsupervised image-to-image translation is an important and challenging
problem in computer vision. Given an image in the source domain, the goal is to
learn the conditional distribution of corresponding images in the target
domain, without seeing any pairs of corresponding images. While this
conditional distribution is inherently multimodal, existing approaches make an
overly simplified assumption, modeling it as a deterministic one-to-one
mapping. As a result, they fail to generate diverse outputs from a given source
domain image. To address this limitation, we propose a Multimodal Unsupervised
Image-to-image Translation (MUNIT) framework. We assume that the image
representation can be decomposed into a content code that is domain-invariant,
and a style code that captures domain-specific properties. To translate an
image to another domain, we recombine its content code with a random style code
sampled from the style space of the target domain. We analyze the proposed
framework and establish several theoretical results. Extensive experiments with
comparisons to the state-of-the-art approaches further demonstrates the
advantage of the proposed framework. Moreover, our framework allows users to
control the style of translation outputs by providing an example style image.
Code and pretrained models are available at https://github.com/nvlabs/MUNIT
</summary>
    <author>
      <name>Xun Huang</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04732v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04732v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02513v2</id>
    <updated>2018-08-14T18:34:04Z</updated>
    <published>2016-09-08T17:52:26Z</published>
    <title>Functorial Hierarchical Clustering with Overlaps</title>
    <summary>  This work draws inspiration from three important sources of research on
dissimilarity-based clustering and intertwines those three threads into a
consistent principled functorial theory of clustering. Those three are the
overlapping clustering of Jardine and Sibson, the functorial approach of
Carlsson and M\'{e}moli to partition-based clustering, and the Isbell/Dress
school's study of injective envelopes. Carlsson and M\'{e}moli introduce the
idea of viewing clustering methods as functors from a category of metric spaces
to a category of clusters, with functoriality subsuming many desirable
properties. Our first series of results extends their theory of functorial
clustering schemes to methods that allow overlapping clusters in the spirit of
Jardine and Sibson. This obviates some of the unpleasant effects of chaining
that occur, for example with single-linkage clustering. We prove an equivalence
between these general overlapping clustering functors and projections of weight
spaces to what we term clustering domains, by focusing on the order structure
determined by the morphisms. As a specific application of this machinery, we
are able to prove that there are no functorial projections to cut metrics, or
even to tree metrics. Finally, although we focus less on the construction of
clustering methods (clustering domains) derived from injective envelopes, we
lay out some preliminary results, that hopefully will give a feel for how the
third leg of the stool comes into play.
</summary>
    <author>
      <name>Jared Culbertson</name>
    </author>
    <author>
      <name>Dan P. Guralnik</name>
    </author>
    <author>
      <name>Peter F. Stiller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dam.2017.10.015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dam.2017.10.015" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor revisions. 24 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Applied Mathematics, Volume 236, 19 February 2018,
  pp.108--123</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.02513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="51K05, 68P01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04839v1</id>
    <updated>2018-08-14T18:06:58Z</updated>
    <published>2018-08-14T18:06:58Z</published>
    <title>Discrete gradient descent differs qualitatively from gradient flow</title>
    <summary>  We consider gradient descent on functions of the form $L_1 = |f|$ and $L_2 =
f^2$, where $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is any smooth function
with 0 as a regular value. We show that gradient descent implemented with a
discrete step size $\tau$ behaves qualitatively differently from continuous
gradient descent. We show that over long time scales, continuous and discrete
gradient descent on $L_1$ find different minima of $L_1$, and we can
characterize the difference - the minima that tend to be found by discrete
gradient descent lie in a secondary critical submanifold $M' \subset M$, the
locus within $M$ where the function $K=|\nabla f|^2 \big|_M$ is minimized. In
this paper, we explain this behavior. We also study the more subtle behavior of
discrete gradient descent on $L_2$.
</summary>
    <author>
      <name>Y. Cooper</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04819v1</id>
    <updated>2018-08-14T18:00:01Z</updated>
    <published>2018-08-14T18:00:01Z</published>
    <title>VizML: A Machine Learning Approach to Visualization Recommendation</title>
    <summary>  Data visualization should be accessible for all analysts with data, not just
the few with technical expertise. Visualization recommender systems aim to
lower the barrier to exploring basic visualizations by automatically generating
results for analysts to search and select, rather than manually specify. Here,
we demonstrate a novel machine learning-based approach to visualization
recommendation that learns visualization design choices from a large corpus of
datasets and associated visualizations. First, we identify five key design
choices made by analysts while creating visualizations, such as selecting a
visualization type and choosing to encode a column along the X- or Y-axis. We
train models to predict these design choices using one million
dataset-visualization pairs collected from a popular online visualization
platform. Neural networks predict these design choices with high accuracy
compared to baseline models. We report and interpret feature importances from
one of these baseline models. To evaluate the generalizability and uncertainty
of our approach, we benchmark with a crowdsourced test set, and show that the
performance of our model is comparable to human performance when predicting
consensus visualization type, and exceeds that of other ML-based systems.
</summary>
    <author>
      <name>Kevin Z. Hu</name>
    </author>
    <author>
      <name>Michiel A. Bakker</name>
    </author>
    <author>
      <name>Stephen Li</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <author>
      <name>César A. Hidalgo</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02483v4</id>
    <updated>2018-08-14T17:46:39Z</updated>
    <published>2018-05-07T12:50:36Z</published>
    <title>The Logistic Network Lasso</title>
    <summary>  We apply the network Lasso to solve binary classification and clustering
problems for network-structured data. To this end, we generalize ordinary
logistic regression to non-Euclidean data with an intrinsic network structure.
The resulting "logistic network Lasso" amounts to solving a non-smooth convex
regularized empirical risk minimization. The risk is measured using the
logistic loss incurred over a small set of labeled nodes. For the
regularization, we propose to use the total variation of the classifier
requiring it to conform to the underlying network structure. A scalable
implementation of the learning method is obtained using an inexact variant of
the alternating direction methods of multipliers which results in a scalable
learning algorithm
</summary>
    <author>
      <name>Henrik Ambos</name>
    </author>
    <author>
      <name>Nguyen Tran</name>
    </author>
    <author>
      <name>Alexander Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02483v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02483v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04803v1</id>
    <updated>2018-08-14T17:32:29Z</updated>
    <published>2018-08-14T17:32:29Z</published>
    <title>Hierarchical binary CNNs for landmark localization with limited
  resources</title>
    <summary>  Our goal is to design architectures that retain the groundbreaking
performance of Convolutional Neural Networks (CNNs) for landmark localization
and at the same time are lightweight, compact and suitable for applications
with limited computational resources. To this end, we make the following
contributions: (a) we are the first to study the effect of neural network
binarization on localization tasks, namely human pose estimation and face
alignment. We exhaustively evaluate various design choices, identify
performance bottlenecks, and more importantly propose multiple orthogonal ways
to boost performance. (b) Based on our analysis, we propose a novel
hierarchical, parallel and multi-scale residual architecture that yields large
performance improvement over the standard bottleneck block while having the
same number of parameters, thus bridging the gap between the original network
and its binarized counterpart. (c) We perform a large number of ablation
studies that shed light on the properties and the performance of the proposed
block. (d) We present results for experiments on the most challenging datasets
for human pose estimation and face alignment, reporting in many cases
state-of-the-art performance. (e) We further provide additional results for the
problem of facial part segmentation. Code can be downloaded from
https://www.adrianbulat.com/binary-cnn-landmark
</summary>
    <author>
      <name>Adrian Bulat</name>
    </author>
    <author>
      <name>Georgios Tzimiropoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2018.2866051</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2018.2866051" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE TPAMI18: Best of ICCV 2017 SI. Previously portions
  of this work appeared as arXiv:1703.00862, which was the conference version</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00868v4</id>
    <updated>2018-08-14T17:25:59Z</updated>
    <published>2018-05-02T15:35:52Z</published>
    <title>A Dynamic Model for Traffic Flow Prediction Using Improved DRN</title>
    <summary>  Real-time traffic flow prediction can not only provide travelers with
reliable traffic information so that it can save people's time, but also assist
the traffic management agency to manage traffic system. It can greatly improve
the efficiency of the transportation system. Traditional traffic flow
prediction approaches usually need a large amount of data but still give poor
performances. With the development of deep learning, researchers begin to pay
attention to artificial neural networks (ANNs) such as RNN and LSTM. However,
these ANNs are very time-consuming. In our research, we improve the Deep
Residual Network and build a dynamic model which previous researchers hardly
use. We firstly integrate the input and output of the $i^{th}$ layer to the
input of the $i+1^{th}$ layer and prove that each layer will fit a simpler
function so that the error rate will be much smaller. Then, we use the concept
of online learning in our model to update pre-trained model during prediction.
Our result shows that our model has higher accuracy than some state-of-the-art
models. In addition, our dynamic model can perform better in practical
applications.
</summary>
    <author>
      <name>Zeren Tan</name>
    </author>
    <author>
      <name>Ruimin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00868v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00868v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04794v1</id>
    <updated>2018-08-14T16:58:11Z</updated>
    <published>2018-08-14T16:58:11Z</published>
    <title>Improving Hearthstone AI by Combining MCTS and Supervised Learning
  Algorithms</title>
    <summary>  We investigate the impact of supervised prediction models on the strength and
efficiency of artificial agents that use the Monte-Carlo Tree Search (MCTS)
algorithm to play a popular video game Hearthstone: Heroes of Warcraft. We
overview our custom implementation of the MCTS that is well-suited for games
with partially hidden information and random effects. We also describe
experiments which we designed to quantify the performance of our Hearthstone
agent's decision making. We show that even simple neural networks can be
trained and successfully used for the evaluation of game states. Moreover, we
demonstrate that by providing a guidance to the game state search heuristic, it
is possible to substantially improve the win rate, and at the same time reduce
the required computations.
</summary>
    <author>
      <name>Maciej Świechowski</name>
    </author>
    <author>
      <name>Tomasz Tajmajer</name>
    </author>
    <author>
      <name>Andrzej Janusz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2018 IEEE Conference on Computational Intelligence
  and Games (CIG'18); pages 445-452; ISBN: 978-1-5386-4358-7</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07273v4</id>
    <updated>2018-08-14T16:54:22Z</updated>
    <published>2016-10-24T03:39:35Z</published>
    <title>Encoding Temporal Markov Dynamics in Graph for Visualizing and Mining
  Time Series</title>
    <summary>  Time series and signals are attracting more attention across statistics,
machine learning and pattern recognition as it appears widely in the industry
especially in sensor and IoT related research and applications, but few
advances has been achieved in effective time series visual analytics and
interaction due to its temporal dimensionality and complex dynamics. Inspired
by recent effort on using network metrics to characterize time series for
classification, we present an approach to visualize time series as complex
networks based on the first order Markov process in its temporal ordering. In
contrast to the classical bar charts, line plots and other statistics based
graph, our approach delivers more intuitive visualization that better preserves
both the temporal dependency and frequency structures. It provides a natural
inverse operation to map the graph back to raw signals, making it possible to
use graph statistics to characterize time series for better visual exploration
and statistical analysis. Our experimental results suggest the effectiveness on
various tasks such as pattern discovery and classification on both synthetic
and the real time series and sensor data.
</summary>
    <author>
      <name>Lu Liu</name>
    </author>
    <author>
      <name>Zhiguang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2018 workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.07273v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07273v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04768v1</id>
    <updated>2018-08-14T16:07:41Z</updated>
    <published>2018-08-14T16:07:41Z</published>
    <title>Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical
  Models</title>
    <summary>  We introduce a method which enables a recurrent dynamics model to be
temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI),
is based on the observation that in many sequential prediction tasks, the exact
time at which events occur is irrelevant to the underlying objective. Moreover,
in many situations, there exist prediction intervals which result in
particularly easy-to-predict transitions. We show that there are prediction
tasks for which we gain both computational efficiency and prediction accuracy
by allowing the model to make predictions at a sampling rate which it can
choose itself.
</summary>
    <author>
      <name>Alexander Neitz</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Stefan Bauer</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04761v1</id>
    <updated>2018-08-14T15:50:15Z</updated>
    <published>2018-08-14T15:50:15Z</published>
    <title>Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN
  Architectures</title>
    <summary>  Deep Neural Networks (DNNs) are fast becoming ubiquitous for their ability to
attain good accuracy in various machine learning tasks. A DNN's architecture
(i.e., its hyper-parameters) broadly determines the DNN's accuracy and
performance, and is often confidential. Attacking a DNN in the cloud to obtain
its architecture can potentially provide major commercial value. Further,
attaining a DNN's architecture facilitates other, existing DNN attacks.
  This paper presents Cache Telepathy: a fast and accurate mechanism to steal a
DNN's architecture using the cache side channel. Our attack is based on the
insight that DNN inference relies heavily on tiled GEMM (Generalized Matrix
Multiply), and that DNN architecture parameters determine the number of GEMM
calls and the dimensions of the matrices used in the GEMM functions. Such
information can be leaked through the cache side channel.
  This paper uses Prime+Probe and Flush+Reload to attack VGG and ResNet DNNs
running OpenBLAS and Intel MKL libraries. Our attack is effective in helping
obtain the architectures by very substantially reducing the search space of
target DNN architectures. For example, for VGG using OpenBLAS, it reduces the
search space from more than $10^{35}$ architectures to just 16.
</summary>
    <author>
      <name>Mengjia Yan</name>
    </author>
    <author>
      <name>Christopher Fletcher</name>
    </author>
    <author>
      <name>Josep Torrellas</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00931v2</id>
    <updated>2018-08-14T15:47:44Z</updated>
    <published>2018-08-02T17:31:54Z</published>
    <title>Machine Learning of Space-Fractional Differential Equations</title>
    <summary>  Data-driven discovery of "hidden physics" -- i.e., machine learning of
differential equation models underlying observed data -- has recently been
approached by embedding the discovery problem into a Gaussian Process
regression of spatial data, treating and discovering unknown equation
parameters as hyperparameters of a modified "physics informed" Gaussian Process
kernel. This kernel includes the parametrized differential operators applied to
a prior covariance kernel. We extend this framework to linear space-fractional
differential equations. The methodology is compatible with a wide variety of
fractional operators in $\mathbb{R}^d$ and stationary covariance kernels,
including the Matern class, and can optimize the Matern parameter during
training. We provide a user-friendly and feasible way to perform fractional
derivatives of kernels, via a unified set of d-dimensional Fourier integral
formulas amenable to generalized Gauss-Laguerre quadrature.
  The implementation of fractional derivatives has several benefits. First, it
allows for discovering fractional-order PDEs for systems characterized by heavy
tails or anomalous diffusion, bypassing the analytical difficulty of fractional
calculus. Data sets exhibiting such features are of increasing prevalence in
physical and financial domains. Second, a single fractional-order archetype
allows for a derivative of arbitrary order to be learned, with the order itself
being a parameter in the regression. This is advantageous even when used for
discovering integer-order equations; the user is not required to assume a
"dictionary" of derivatives of various orders, and directly controls the
parsimony of the models being discovered. We illustrate on several examples,
including fractional-order interpolation of advection-diffusion and modeling
relative stock performance in the S&amp;P 500 with alpha-stable motion via a
fractional diffusion equation.
</summary>
    <author>
      <name>Mamikon Gulian</name>
    </author>
    <author>
      <name>Maziar Raissi</name>
    </author>
    <author>
      <name>Paris Perdikaris</name>
    </author>
    <author>
      <name>George Karniadakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 10 figures. In v2, a minor change to the formatting of a
  handful of references was made in the bibliography; the main text was
  unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35R11, 65N21, 62M10, 62F15, 60G15, 60G52" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04760v1</id>
    <updated>2018-08-14T15:47:32Z</updated>
    <published>2018-08-14T15:47:32Z</published>
    <title>Parallel Statistical and Machine Learning Methods for Estimation of
  Physical Load</title>
    <summary>  Several statistical and machine learning methods are proposed to estimate the
type and intensity of physical load and accumulated fatigue . They are based on
the statistical analysis of accumulated and moving window data subsets with
construction of a kurtosis-skewness diagram. This approach was applied to the
data gathered by the wearable heart monitor for various types and levels of
physical activities, and for people with various physical conditions. The
different levels of physical activities, loads, and fitness can be
distinguished from the kurtosis-skewness diagram, and their evolution can be
monitored. Several metrics for estimation of the instant effect and accumulated
effect (physical fatigue) of physical loads were proposed. The data and results
presented allow to extend application of these methods for modeling and
characterization of complex human activity patterns, for example, to estimate
the actual and accumulated physical load and fatigue, model the potential
dangerous development, and give cautions and advice in real time.
</summary>
    <author>
      <name>Sergii Stirenko</name>
    </author>
    <author>
      <name>Gang Peng</name>
    </author>
    <author>
      <name>Wei Zeng</name>
    </author>
    <author>
      <name>Yuri Gordienko</name>
    </author>
    <author>
      <name>Oleg Alienin</name>
    </author>
    <author>
      <name>Oleksandr Rokovyi</name>
    </author>
    <author>
      <name>Nikita Gordienko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures, accepted for 18th International Conference on
  Algorithms and Architectures for Parallel Processing (ICA3PP) 15-17 November,
  2018 (Guangzhou, China)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04759v1</id>
    <updated>2018-08-14T15:45:48Z</updated>
    <published>2018-08-14T15:45:48Z</published>
    <title>An Overview and a Benchmark of Active Learning for One-Class
  Classification</title>
    <summary>  Active learning stands for methods which increase classification quality by
means of user feedback. An important subcategory is active learning for
one-class classifiers, i.e., for imbalanced class distributions. While various
methods in this category exist, selecting one for a given application scenario
is difficult. This is because existing methods rely on different assumptions,
have different objectives, and often are tailored to a specific use case. All
this calls for a comprehensive comparison, the topic of this article. This
article starts with a categorization of the various methods. We then propose
ways to evaluate active learning results. Next, we run extensive experiments to
compare existing methods, for a broad variety of scenarios. One result is that
the practicality and the performance of an active learning method strongly
depend on its category and on the assumptions behind it. Another observation is
that there only is a small subset of our experiments where existing approaches
outperform random baselines. Finally, we show that a well-laid-out
categorization and a rigorous specification of assumptions can facilitate the
selection of a good method for one-class classification.
</summary>
    <author>
      <name>Holger Trittenbach</name>
    </author>
    <author>
      <name>Adrian Englhardt</name>
    </author>
    <author>
      <name>Klemens Böhm</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04750v1</id>
    <updated>2018-08-14T15:27:22Z</updated>
    <published>2018-08-14T15:27:22Z</published>
    <title>Quantifying the Influences on Probabilistic Wind Power Forecasts</title>
    <summary>  In recent years, probabilistic forecasts techniques were proposed in research
as well as in applications to integrate volatile renewable energy resources
into the electrical grid. These techniques allow decision makers to take the
uncertainty of the prediction into account and, therefore, to devise optimal
decisions, e.g., related to costs and risks in the electrical grid. However, it
was yet not studied how the input, such as numerical weather predictions,
affects the model output of forecasting models in detail. Therefore, we examine
the potential influences with techniques from the field of sensitivity analysis
on three different black-box models to obtain insights into differences and
similarities of these probabilistic models. The analysis shows a considerable
number of potential influences in those models depending on, e.g., the
predicted probability and the type of model. These effects motivate the need to
take various influences into account when models are tested, analyzed, or
compared. Nevertheless, results of the sensitivity analysis will allow us to
select a model with advantages in the practical application.
</summary>
    <author>
      <name>Jens Schreiber</name>
    </author>
    <author>
      <name>Bernhard Sick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; 1 table; 3 figures; This work has been submitted to the IEEE
  for possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04730v1</id>
    <updated>2018-08-14T14:58:59Z</updated>
    <published>2018-08-14T14:58:59Z</published>
    <title>Analyzing Inverse Problems with Invertible Neural Networks</title>
    <summary>  In many tasks, in particular in natural science, the goal is to determine
hidden system parameters from a set of measurements. Often, the forward process
from parameter- to measurement-space is a well-defined function, whereas the
inverse problem is ambiguous: one measurement may map to multiple different
sets of parameters. In this setting, the posterior parameter distribution,
conditioned on an input measurement, has to be determined. We argue that a
particular class of neural networks is well suited for this task -- so-called
Invertible Neural Networks (INNs). Although INNs are not new, they have, so
far, received little attention in literature. While classical neural networks
attempt to solve the ambiguous inverse problem directly, INNs are able to learn
it jointly with the well-defined forward process, using additional latent
output variables to capture the information otherwise lost. Given a specific
measurement and sampled latent variables, the inverse pass of the INN provides
a full distribution over parameter space. We verify experimentally, on
artificial data and real-world problems from astrophysics and medicine, that
INNs are a powerful analysis tool to find multi-modalities in parameter space,
to uncover parameter correlations, and to identify unrecoverable parameters.
</summary>
    <author>
      <name>Lynton Ardizzone</name>
    </author>
    <author>
      <name>Jakob Kruse</name>
    </author>
    <author>
      <name>Sebastian Wirkert</name>
    </author>
    <author>
      <name>Daniel Rahner</name>
    </author>
    <author>
      <name>Eric W. Pellegrini</name>
    </author>
    <author>
      <name>Ralf S. Klessen</name>
    </author>
    <author>
      <name>Lena Maier-Hein</name>
    </author>
    <author>
      <name>Carsten Rother</name>
    </author>
    <author>
      <name>Ullrich Köthe</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04728v1</id>
    <updated>2018-08-14T14:54:37Z</updated>
    <published>2018-08-14T14:54:37Z</published>
    <title>CosmoFlow: Using Deep Learning to Learn the Universe at Scale</title>
    <summary>  Deep learning is a promising tool to determine the physical model that
describes our universe. To handle the considerable computational cost of this
problem, we present CosmoFlow: a highly scalable deep learning application
built on top of the TensorFlow framework. CosmoFlow uses efficient
implementations of 3D convolution and pooling primitives, together with
improvements in threading for many element-wise operations, to improve training
performance on Intel(C) Xeon Phi(TM) processors. We also utilize the Cray PE
Machine Learning Plugin for efficient scaling to multiple nodes. We demonstrate
fully synchronous data-parallel training on 8192 nodes of Cori with 77%
parallel efficiency, achieving 3.5 Pflop/s sustained performance. To our
knowledge, this is the first large-scale science application of the TensorFlow
framework at supercomputer scale with fully-synchronous training. These
enhancements enable us to process large 3D dark matter distribution and predict
the cosmological parameters $\Omega_M$, $\sigma_8$ and n$_s$ with unprecedented
accuracy.
</summary>
    <author>
      <name>Amrita Mathuriya</name>
    </author>
    <author>
      <name>Deborah Bard</name>
    </author>
    <author>
      <name>Peter Mendygral</name>
    </author>
    <author>
      <name>Lawrence Meadows</name>
    </author>
    <author>
      <name>James Arnemann</name>
    </author>
    <author>
      <name>Lei Shao</name>
    </author>
    <author>
      <name>Siyu He</name>
    </author>
    <author>
      <name>Tuomas Karna</name>
    </author>
    <author>
      <name>Daina Moise</name>
    </author>
    <author>
      <name>Simon J. Pennycook</name>
    </author>
    <author>
      <name>Kristyn Maschoff</name>
    </author>
    <author>
      <name>Jason Sewall</name>
    </author>
    <author>
      <name>Nalini Kumar</name>
    </author>
    <author>
      <name>Shirley Ho</name>
    </author>
    <author>
      <name>Mike Ringenburg</name>
    </author>
    <author>
      <name> Prabhat</name>
    </author>
    <author>
      <name>Victor Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 pages, accepted to SuperComputing 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12528v4</id>
    <updated>2018-08-14T14:38:50Z</updated>
    <published>2018-05-31T15:57:33Z</published>
    <title>Fusion Graph Convolutional Networks</title>
    <summary>  Semi-supervised node classification involves learning to classify unlabelled
nodes given a partially labeled graph. In transductive learning, all unlabelled
nodes to be classified are observed during training and in inductive learning,
predictions are to be made for nodes not seen at training. In this paper, we
focus on both these settings for node classification in attributed graphs,
i.e., graphs in which nodes have additional features. State-of-the-art models
for node classification on such attributed graphs use differentiable recursive
functions. These differentiable recursive functions enable aggregation and
filtering of neighborhood information from multiple hops (depths). Despite
being powerful, these variants are limited in their ability to combine
information from different hops efficiently. In this work, we analyze this
limitation of recursive graph functions in terms of their representation
capacity to effectively capture multi-hop neighborhood information. Further, we
provide a simple fusion component which is mathematically motivated to address
this limitation and improve the existing models to explicitly learn the
importance of information from different hops. This proposed mechanism is shown
to improve over existing methods across 8 popular datasets from different
domains. Specifically, our model improves the Graph Convolutional Network (GCN)
and a variant of Graph SAGE by a significant margin providing highly
competitive state-of-the-art results.
</summary>
    <author>
      <name>Priyesh Vijayan</name>
    </author>
    <author>
      <name>Yash Chandak</name>
    </author>
    <author>
      <name>Mitesh M. Khapra</name>
    </author>
    <author>
      <name>Balaraman Ravindran</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12528v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12528v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04699v1</id>
    <updated>2018-08-14T14:03:47Z</updated>
    <published>2018-08-14T14:03:47Z</published>
    <title>Improving Generalization via Scalable Neighborhood Component Analysis</title>
    <summary>  Current major approaches to visual recognition follow an end-to-end
formulation that classifies an input image into one of the pre-determined set
of semantic categories. Parametric softmax classifiers are a common choice for
such a closed world with fixed categories, especially when big labeled data is
available during training. However, this becomes problematic for open-set
scenarios where new categories are encountered with very few examples for
learning a generalizable parametric classifier. We adopt a non-parametric
approach for visual recognition by optimizing feature embeddings instead of
parametric classifiers. We use a deep neural network to learn the visual
feature that preserves the neighborhood structure in the semantic space, based
on the Neighborhood Component Analysis (NCA) criterion. Limited by its
computational bottlenecks, we devise a mechanism to use augmented memory to
scale NCA for large datasets and very deep networks. Our experiments deliver
not only remarkable performance on ImageNet classification for such a simple
non-parametric method, but most importantly a more generalizable feature
representation for sub-category discovery and few-shot recognition.
</summary>
    <author>
      <name>Zhirong Wu</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <author>
      <name>Stella X. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09859v2</id>
    <updated>2018-08-14T14:02:55Z</updated>
    <published>2017-10-26T18:38:28Z</published>
    <title>Kernel k-Groups via Hartigan's Method</title>
    <summary>  Energy statistics was proposed by Sz\'{e}kely in the 80's inspired by
Newton's gravitational potential in classical mechanics, and it provides a
model-free hypothesis test for equality of distributions. In its original form,
energy statistics was formulated in Euclidean spaces. More recently, it was
generalized to metric spaces of negative type. In this paper, we consider a
formulation for the clustering problem using a weighted version of energy
statistics in spaces of negative type. We show that this approach leads to a
quadratically constrained quadratic program in the associated kernel space,
establishing connections with graph partitioning problems and kernel methods in
unsupervised machine learning. To find local solutions of such an optimization
problem, we propose an extension of Hartigan's method to kernel spaces. Our
method has the same computational cost as kernel k-means algorithm, which is
based on Lloyd's heuristic, but our numerical results show an improved
performance, especially in high dimensions.
</summary>
    <author>
      <name>Guilherme França</name>
    </author>
    <author>
      <name>Maria L. Rizzo</name>
    </author>
    <author>
      <name>Joshua T. Vogelstein</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09859v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09859v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07119v3</id>
    <updated>2018-08-14T13:46:31Z</updated>
    <published>2017-06-21T21:00:44Z</published>
    <title>"Parallel Training Considered Harmful?": Comparing series-parallel and
  parallel feedforward network training</title>
    <summary>  Neural network models for dynamic systems can be trained either in parallel
or in series-parallel configurations. Influenced by early arguments, several
papers justify the choice of series-parallel rather than parallel configuration
claiming it has a lower computational cost, better stability properties during
training and provides more accurate results. Other published results, on the
other hand, defend parallel training as being more robust and capable of
yielding more accu- rate long-term predictions. The main contribution of this
paper is to present a study comparing both methods under the same unified
framework. We focus on three aspects: i) robustness of the estimation in the
presence of noise; ii) computational cost; and, iii) convergence. A unifying
mathematical framework and simulation studies show situations where each
training method provides better validation results, being parallel training
better in what is believed to be more realistic scenarios. An example using
measured data seems to reinforce such claim. We also show, with a novel
complexity analysis and numerical examples, that both methods have similar
computational cost, being series series-parallel training, however, more
amenable to parallelization. Some informal discussion about stability and
convergence properties is presented and explored in the examples.
</summary>
    <author>
      <name>Antônio H. Ribeiro</name>
    </author>
    <author>
      <name>Luis A. Aguirre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2018.07.071</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2018.07.071" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.07119v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07119v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04685v1</id>
    <updated>2018-08-14T13:45:34Z</updated>
    <published>2018-08-14T13:45:34Z</published>
    <title>Learning ReLU Networks on Linearly Separable Data: Algorithm,
  Optimality, and Generalization</title>
    <summary>  Neural networks with ReLU activations have achieved great empirical success
in various domains. However, existing results for learning ReLU networks either
pose assumptions on the underlying data distribution being e.g. Gaussian, or
require the network size and/or training size to be sufficiently large. In this
context, the problem of learning a two-layer ReLU network is approached in a
binary classification setting, where the data are linearly separable and a
hinge loss criterion is adopted. Leveraging the power of random noise, this
contribution presents a novel stochastic gradient descent (SGD) algorithm,
which can provably train any single-hidden-layer ReLU network to attain global
optimality, despite the presence of infinitely many bad local minima and saddle
points in general. This result is the first of its kind, requiring no
assumptions on the data distribution, training/network size, or initialization.
Convergence of the resultant iterative algorithm to a global minimum is
analyzed by establishing both an upper bound and a lower bound on the number of
effective (non-zero) updates to be performed. Furthermore, generalization
guarantees are developed for ReLU networks trained with the novel SGD. These
guarantees highlight a fundamental difference (at least in the worst case)
between learning a ReLU network as well as a leaky ReLU network in terms of
sample complexity. Numerical tests using synthetic data and real images
validate the effectiveness of the algorithm and the practical merits of the
theory.
</summary>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Georgios B. Giannakis</name>
    </author>
    <author>
      <name>Jie Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures, work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04670v1</id>
    <updated>2018-08-14T13:15:43Z</updated>
    <published>2018-08-14T13:15:43Z</published>
    <title>R-grams: Unsupervised Learning of Semantic Units in Natural Language</title>
    <summary>  This paper introduces a novel type of data-driven segmented unit that we call
r-grams. We illustrate one algorithm for calculating r-grams, and discuss its
properties and impact on the frequency distribution of text representations.
The proposed approach is evaluated by demonstrating its viability in embedding
techniques, both in monolingual and multilingual test settings. We also provide
a number of qualitative examples of the proposed methodology, demonstrating its
viability as a language-invariant segmentation procedure.
</summary>
    <author>
      <name>Ariel Ekgren</name>
    </author>
    <author>
      <name>Amaru Cuba Gyllensten</name>
    </author>
    <author>
      <name>Magnus Sahlgren</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07572v2</id>
    <updated>2018-08-14T13:06:04Z</updated>
    <published>2018-02-21T14:01:20Z</published>
    <title>Information Theoretic Co-Training</title>
    <summary>  This paper introduces an information theoretic co-training objective for
unsupervised learning. We consider the problem of predicting the future. Rather
than predict future sensations (image pixels or sound waves) we predict
"hypotheses" to be confirmed by future sensations. More formally, we assume a
population distribution on pairs $(x,y)$ where we can think of $x$ as a past
sensation and $y$ as a future sensation. We train both a predictor model
$P_\Phi(z|x)$ and a confirmation model $P_\Psi(z|y)$ where we view $z$ as
hypotheses (when predicted) or facts (when confirmed). For a population
distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual
information between $x$ and $y$. By the data processing inequality this mutual
information is at least as large as the mutual information between $x$ and $z$
under the distribution on triples $(x,z,y)$ defined by the confirmation model
$P_\Psi(z|y)$. The information theoretic training objective for $P_\Phi(z|x)$
and $P_\Psi(z|y)$ can be viewed as a form of co-training where we want the
prediction from $x$ to match the confirmation from $y$.
</summary>
    <author>
      <name>David McAllester</name>
    </author>
    <link href="http://arxiv.org/abs/1802.07572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06470v1</id>
    <updated>2018-08-14T09:34:16Z</updated>
    <published>2018-08-14T09:34:16Z</published>
    <title>predictSLUMS: A new model for identifying and predicting informal
  settlements and slums in cities from street intersections using machine
  learning</title>
    <summary>  Identifying current and future informal regions within cities remains a
crucial issue for policymakers and governments in developing countries. The
delineation process of identifying such regions in cities requires a lot of
resources. While there are various studies that identify informal settlements
based on satellite image classification, relying on both supervised or
unsupervised machine learning approaches, these models either require multiple
input data to function or need further development with regards to precision.
In this paper, we introduce a novel method for identifying and predicting
informal settlements using only street intersections data, regardless of the
variation of urban form, number of floors, materials used for construction or
street width. With such minimal input data, we attempt to provide planners and
policy-makers with a pragmatic tool that can aid in identifying informal zones
in cities. The algorithm of the model is based on spatial statistics and a
machine learning approach, using Multinomial Logistic Regression (MNL) and
Artificial Neural Networks (ANN). The proposed model relies on defining
informal settlements based on two ubiquitous characteristics that these regions
tend to be filled in with smaller subdivided lots of housing relative to the
formal areas within the local context, and the paucity of services and
infrastructure within the boundary of these settlements that require relatively
bigger lots. We applied the model in five major cities in Egypt and India that
have spatial structures in which informality is present. These cities are
Greater Cairo, Alexandria, Hurghada and Minya in Egypt, and Mumbai in India.
The predictSLUMS model shows high validity and accuracy for identifying and
predicting informality within the same city the model was trained on or in
different ones of a similar context.
</summary>
    <author>
      <name>Mohamed R. Ibrahim</name>
    </author>
    <author>
      <name>Helena Titheridge</name>
    </author>
    <author>
      <name>Tao Cheng</name>
    </author>
    <author>
      <name>James Haworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04580v1</id>
    <updated>2018-08-14T08:24:01Z</updated>
    <published>2018-08-14T08:24:01Z</published>
    <title>NFFT meets Krylov methods: Fast matrix-vector products for the graph
  Laplacian of fully connected networks</title>
    <summary>  The graph Laplacian is a standard tool in data science, machine learning, and
image processing. The corresponding matrix inherits the complex structure of
the underlying network and is in certain applications densely populated. This
makes computations, in particular matrix-vector products, with the graph
Laplacian a hard task. A typical application is the computation of a number of
its eigenvalues and eigenvectors. Standard methods become infeasible as the
number of nodes in the graph is too large. We propose the use of the fast
summation based on the nonequispaced fast Fourier transform (NFFT) to perform
the dense matrix-vector product with the graph Laplacian fast without ever
forming the whole matrix. The enormous flexibility of the NFFT algorithm allows
us to embed the accelerated multiplication into Lanczos-based eigenvalues
routines or iterative linear system solvers. We illustrate the feasibility of
our approach on a number of test problems from image segmentation to
semi-supervised learning based on graph-based PDEs. In particular, we compare
our approach with the Nystr\"om method. Moreover, we present and test an
enhanced, hybrid version of the Nystr\"om method, which internally uses the
NFFT.
</summary>
    <author>
      <name>Dominik Alfke</name>
    </author>
    <author>
      <name>Daniel Potts</name>
    </author>
    <author>
      <name>Martin Stoll</name>
    </author>
    <author>
      <name>Toni Volkmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R10, 05C50, 65F15, 65T50, 68T05, 62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09133v2</id>
    <updated>2018-08-14T07:30:11Z</updated>
    <published>2018-04-24T16:50:54Z</published>
    <title>Improving Native Ads CTR Prediction by Large Scale Event Embedding and
  Recurrent Networks</title>
    <summary>  Click through rate (CTR) prediction is very important for Native
advertisement but also hard as there is no direct query intent. In this paper
we propose a large-scale event embedding scheme to encode the each user
browsing event by training a Siamese network with weak supervision on the
users' consecutive events. The CTR prediction problem is modeled as a
supervised recurrent neural network, which naturally model the user history as
a sequence of events. Our proposed recurrent models utilizing pretrained event
embedding vectors and an attention layer to model the user history. Our
experiments demonstrate that our model significantly outperforms the baseline
and some variants.
</summary>
    <author>
      <name>Mehul Parsana</name>
    </author>
    <author>
      <name>Krishna Poola</name>
    </author>
    <author>
      <name>Yajun Wang</name>
    </author>
    <author>
      <name>Zhiguang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version has some language error, and the authors all agree to
  withdraw it at the moment to further edit and update with some new results</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.09133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03298v2</id>
    <updated>2018-08-14T06:49:53Z</updated>
    <published>2018-06-26T04:48:57Z</published>
    <title>Probabilistic Ensemble of Collaborative Filters</title>
    <summary>  Collaborative filtering is an important technique for recommendation. Whereas
it has been repeatedly shown to be effective in previous work, its performance
remains unsatisfactory in many real-world applications, especially those where
the items or users are highly diverse. In this paper, we explore an
ensemble-based framework to enhance the capability of a recommender in handling
diverse data. Specifically, we formulate a probabilistic model which integrates
the items, the users, as well as the associations between them into a
generative process. On top of this formulation, we further derive a progressive
algorithm to construct an ensemble of collaborative filters. In each iteration,
a new filter is derived from re-weighted entries and incorporated into the
ensemble. It is noteworthy that while the algorithmic procedure of our
algorithm is apparently similar to boosting, it is derived from an essentially
different formulation and thus differs in several key technical aspects. We
tested the proposed method on three large datasets, and observed substantial
improvement over the state of the art, including L2Boost, an effective method
based on boosting.
</summary>
    <author>
      <name>Zhiyu Min</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. In Proceedings of AAAI-2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03298v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03298v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04550v1</id>
    <updated>2018-08-14T06:40:01Z</updated>
    <published>2018-08-14T06:40:01Z</published>
    <title>SciSports: Learning football kinematics through two-dimensional tracking
  data</title>
    <summary>  SciSports is a Dutch startup company specializing in football analytics. This
paper describes a joint research effort with SciSports, during the Study Group
Mathematics with Industry 2018 at Eindhoven, the Netherlands. The main
challenge that we addressed was to automatically process empirical football
players' trajectories, in order to extract useful information from them. The
data provided to us was two-dimensional positional data during entire matches.
We developed methods based on Newtonian mechanics and the Kalman filter,
Generative Adversarial Nets and Variational Autoencoders. In addition, we
trained a discriminator network to recognize and discern different movement
patterns of players. The Kalman-filter approach yields an interpretable model,
in which a small number of player-dependent parameters can be fit; in theory
this could be used to distinguish among players. The
Generative-Adversarial-Nets approach appears promising in theory, and some
initial tests showed an improvement with respect to the baseline, but the
limits in time and computational power meant that we could not fully explore
it. We also trained a Discriminator network to distinguish between two players
based on their trajectories; after training, the network managed to distinguish
between some pairs of players, but not between others. After training, the
Variational Autoencoders generated trajectories that are difficult to
distinguish, visually, from the data. These experiments provide an indication
that deep generative models can learn the underlying structure and statistics
of football players' trajectories. This can serve as a starting point for
determining player qualities based on such trajectory data.
</summary>
    <author>
      <name>Anatoliy Babic</name>
    </author>
    <author>
      <name>Harshit Bansal</name>
    </author>
    <author>
      <name>Gianluca Finocchio</name>
    </author>
    <author>
      <name>Julian Golak</name>
    </author>
    <author>
      <name>Mark Peletier</name>
    </author>
    <author>
      <name>Jim Portegies</name>
    </author>
    <author>
      <name>Clara Stegehuis</name>
    </author>
    <author>
      <name>Anuj Tyagi</name>
    </author>
    <author>
      <name>Roland Vincze</name>
    </author>
    <author>
      <name>William Weimin Yoo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This report was made for the Study Group Mathematics with Industry
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03733v2</id>
    <updated>2018-08-14T06:26:12Z</updated>
    <published>2018-08-11T01:14:50Z</published>
    <title>Familia: A Configurable Topic Modeling Framework for Industrial Text
  Engineering</title>
    <summary>  In the last decade, a variety of topic models have been proposed for text
engineering. However, except Probabilistic Latent Semantic Analysis (PLSA) and
Latent Dirichlet Allocation (LDA), most of existing topic models are seldom
applied or considered in industrial scenarios. This phenomenon is caused by the
fact that there are very few convenient tools to support these topic models so
far. Intimidated by the demanding expertise and labor of designing and
implementing parameter inference algorithms, software engineers are prone to
simply resort to PLSA/LDA, without considering whether it is proper for their
problem at hand or not. In this paper, we propose a configurable topic modeling
framework named Familia, in order to bridge the huge gap between academic
research fruits and current industrial practice. Familia supports an important
line of topic models that are widely applicable in text engineering scenarios.
In order to relieve burdens of software engineers without knowledge of Bayesian
networks, Familia is able to conduct automatic parameter inference for a
variety of topic models. Simply through changing the data organization of
Familia, software engineers are able to easily explore a broad spectrum of
existing topic models or even design their own topic models, and find the one
that best suits the problem at hand. With its superior extendability, Familia
has a novel sampling mechanism that strikes balance between effectiveness and
efficiency of parameter inference. Furthermore, Familia is essentially a big
topic modeling framework that supports parallel parameter inference and
distributed parameter storage. The utilities and necessity of Familia are
demonstrated in real-life industrial applications. Familia would significantly
enlarge software engineers' arsenal of topic models and pave the way for
utilizing highly customized topic models in real-life problems.
</summary>
    <author>
      <name>Di Jiang</name>
    </author>
    <author>
      <name>Yuanfeng Song</name>
    </author>
    <author>
      <name>Rongzhong Lian</name>
    </author>
    <author>
      <name>Siqi Bao</name>
    </author>
    <author>
      <name>Jinhua Peng</name>
    </author>
    <author>
      <name>Huang He</name>
    </author>
    <author>
      <name>Hua Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03733v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03733v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04545v1</id>
    <updated>2018-08-14T06:21:03Z</updated>
    <published>2018-08-14T06:21:03Z</published>
    <title>MT-VAE: Learning Motion Transformations to Generate Multimodal Human
  Dynamics</title>
    <summary>  Long-term human motion can be represented as a series of motion
modes---motion sequences that capture short-term temporal dynamics---with
transitions between them. We leverage this structure and present a novel Motion
Transformation Variational Auto-Encoders (MT-VAE) for learning motion sequence
generation. Our model jointly learns a feature embedding for motion modes (that
the motion sequence can be reconstructed from) and a feature transformation
that represents the transition of one motion mode to the next motion mode. Our
model is able to generate multiple diverse and plausible motion sequences in
the future from the same input. We apply our approach to both facial and full
body motion, and demonstrate applications like analogy-based motion transfer
and video synthesis.
</summary>
    <author>
      <name>Xinchen Yan</name>
    </author>
    <author>
      <name>Akash Rastogi</name>
    </author>
    <author>
      <name>Ruben Villegas</name>
    </author>
    <author>
      <name>Kalyan Sunkavalli</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Sunil Hadap</name>
    </author>
    <author>
      <name>Ersin Yumer</name>
    </author>
    <author>
      <name>Honglak Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04538v1</id>
    <updated>2018-08-14T05:45:25Z</updated>
    <published>2018-08-14T05:45:25Z</published>
    <title>Text-to-Image-to-Text Translation using Cycle Consistent Adversarial
  Networks</title>
    <summary>  Text-to-Image translation has been an active area of research in the recent
past. The ability for a network to learn the meaning of a sentence and generate
an accurate image that depicts the sentence shows ability of the model to think
more like humans. Popular methods on text to image translation make use of
Generative Adversarial Networks (GANs) to generate high quality images based on
text input, but the generated images don't always reflect the meaning of the
sentence given to the model as input. We address this issue by using a
captioning network to caption on generated images and exploit the distance
between ground truth captions and generated captions to improve the network
further. We show extensive comparisons between our method and existing methods.
</summary>
    <author>
      <name>Satya Krishna Gorti</name>
    </author>
    <author>
      <name>Jeremy Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04521v1</id>
    <updated>2018-08-14T04:38:58Z</updated>
    <published>2018-08-14T04:38:58Z</published>
    <title>A Comprehensive Survey for Low Rank Regularization</title>
    <summary>  Low rank regularization, in essence, involves introducing a low rank or
approximately low rank assumption for matrix we aim to learn, which has
achieved great success in many fields including machine learning, data mining
and computer version. Over the last decade, much progress has been made in
theories and practical applications. Nevertheless, the intersection between
them is very slight. In order to construct a bridge between practical
applications and theoretical research, in this paper we provide a comprehensive
survey for low rank regularization. We first review several traditional machine
learning models using low rank regularization, and then show their (or their
variants) applications in solving practical issues, such as non-rigid structure
from motion and image denoising. Subsequently, we summarize the regularizers
and optimization methods that achieve great success in traditional machine
learning tasks but are rarely seen in solving practical issues. Finally, we
provide a discussion and comparison for some representative regularizers
including convex and non-convex relaxations. Extensive experimental results
demonstrate that non-convex regularizers can provide a large advantage over the
nuclear norm, the regularizer widely used in solving practical issues.
</summary>
    <author>
      <name>Zhanxuan Hu</name>
    </author>
    <author>
      <name>Feiping Nie</name>
    </author>
    <author>
      <name>Lai Tian</name>
    </author>
    <author>
      <name>Xuelong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages,4 figures,4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03133v2</id>
    <updated>2018-08-14T04:24:35Z</updated>
    <published>2018-06-29T18:00:03Z</published>
    <title>Outfit Generation and Style Extraction via Bidirectional LSTM and
  Autoencoder</title>
    <summary>  When creating an outfit, style is a criterion in selecting each fashion item.
This means that style can be regarded as a feature of the overall outfit.
However, in various previous studies on outfit generation, there have been few
methods focusing on global information obtained from an outfit. To address this
deficiency, we have incorporated an unsupervised style extraction module into a
model to learn outfits. Using the style information of an outfit as a whole,
the proposed model succeeded in generating outfits more flexibly without
requiring additional information. Moreover, the style information extracted by
the proposed model is easy to interpret. The proposed model was evaluated on
two human-generated outfit datasets. In a fashion item prediction task (missing
prediction task), the proposed model outperformed a baseline method. In a style
extraction task, the proposed model extracted some easily distinguishable
styles. In an outfit generation task, the proposed model generated an outfit
while controlling its styles. This capability allows us to generate fashionable
outfits according to various preferences.
</summary>
    <author>
      <name>Takuma Nakamura</name>
    </author>
    <author>
      <name>Ryosuke Goto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, KDD Workshop AI for fashion</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03875v4</id>
    <updated>2018-08-14T03:32:12Z</updated>
    <published>2017-10-11T01:31:14Z</published>
    <title>Learning Task Specifications from Demonstrations</title>
    <summary>  Real world applications often naturally decompose into several sub-tasks. In
many settings (e.g., robotics) demonstrations provide a natural way to specify
the sub-tasks. However, most methods for learning from demonstrations either do
not provide guarantees that the artifacts learned for the subtasks can be
safely recombined or limit the types of composition available. Motivated by
this deficit, we consider the problem of inferring binary non-Markovian
rewards, also known as logical trace properties or \emph{specifications}, from
demonstrations provided by an agent operating in an uncertain, stochastic
environment. Crucially, specifications admit well-defined composition rules
that are typically easy to interpret. In this paper, we formulate the
specification inference task as a maximum a posteriori (MAP) probability
inference problem, apply the principle of maximum entropy to derive an analytic
demonstration likelihood model and give an efficient approach to search for the
most likely specification in a large candidate pool of specifications. In our
experiments, we demonstrate how learning specifications can help avoid common
bugs that often occur due to ad-hoc reward composition.
</summary>
    <author>
      <name>Marcell Vazquez-Chanlatte</name>
    </author>
    <author>
      <name>Susmit Jha</name>
    </author>
    <author>
      <name>Ashish Tiwari</name>
    </author>
    <author>
      <name>Mark K. Ho</name>
    </author>
    <author>
      <name>Sanjit A. Seshia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submission to NIPS2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03875v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03875v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11622v2</id>
    <updated>2018-08-14T02:56:53Z</updated>
    <published>2018-07-31T01:25:44Z</published>
    <title>Count-Based Exploration with the Successor Representation</title>
    <summary>  The problem of exploration in reinforcement learning is well-understood in
the tabular case and many sample-efficient algorithms are known. Nevertheless,
it is often unclear how the algorithms in the tabular setting can be extended
to tasks with large state-spaces where generalization is required. Recent
promising developments generally depend on problem-specific density models or
handcrafted features. In this paper we introduce a simple approach for
exploration that allows us to develop theoretically justified algorithms in the
tabular case but that also give us intuitions for new algorithms applicable to
settings where function approximation is required. Our approach and its
underlying theory is based on the substochastic successor representation, a
concept we develop here. While the traditional successor representation is a
representation that defines state generalization by the similarity of successor
states, the substochastic successor representation is also able to implicitly
count the number of times each state (or feature) has been observed. This
extension connects two until now disjoint areas of research. We show in
traditional tabular domains (RiverSwim and SixArms) that our algorithm
empirically performs as well as other sample-efficient algorithms. We then
describe a deep reinforcement learning algorithm inspired by these ideas and
show that it matches the performance of recent pseudo-count-based methods in
hard exploration Atari 2600 games.
</summary>
    <author>
      <name>Marlos C. Machado</name>
    </author>
    <author>
      <name>Marc G. Bellemare</name>
    </author>
    <author>
      <name>Michael Bowling</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11622v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11622v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04900v2</id>
    <updated>2018-08-14T01:02:12Z</updated>
    <published>2018-06-13T09:00:36Z</published>
    <title>A Machine-Learning Item Recommendation System for Video Games</title>
    <summary>  Video-game players generate huge amounts of data, as everything they do
within a game is recorded. In particular, among all the stored actions and
behaviors, there is information on the in-game purchases of virtual products.
Such information is of critical importance in modern free-to-play titles, where
gamers can select or buy a profusion of items during the game in order to
progress and fully enjoy their experience.
  To try to maximize these kind of purchases, one can use a recommendation
system so as to present players with items that might be interesting for them.
Such systems can better achieve their goal by employing machine learning
algorithms that are able to predict the rating of an item or product by a
particular user. In this paper we evaluate and compare two of these algorithms,
an ensemble-based model (extremely randomized trees) and a deep neural network,
both of which are promising candidates for operational video-game recommender
engines.
  Item recommenders can help developers improve the game. But, more
importantly, it should be possible to integrate them into the game, so that
users automatically get personalized recommendations while playing. The
presented models are not only able to meet this challenge, providing accurate
predictions of the items that a particular player will find attractive, but
also sufficiently fast and robust to be used in operational settings.
</summary>
    <author>
      <name>Paul Bertens</name>
    </author>
    <author>
      <name>Anna Guitart</name>
    </author>
    <author>
      <name>Pei Pei Chen</name>
    </author>
    <author>
      <name>África Periáñez</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01433v4</id>
    <updated>2018-08-13T23:29:31Z</updated>
    <published>2018-01-31T01:35:46Z</published>
    <title>Interactive Grounded Language Acquisition and Generalization in a 2D
  World</title>
    <summary>  We build a virtual agent for learning language in a 2D maze-like world. The
agent sees images of the surrounding environment, listens to a virtual teacher,
and takes actions to receive rewards. It interactively learns the teacher's
language from scratch based on two language use cases: sentence-directed
navigation and question answering. It learns simultaneously the visual
representations of the world, the language, and the action control. By
disentangling language grounding from other computational routines and sharing
a concept detection function between language grounding and prediction, the
agent reliably interpolates and extrapolates to interpret sentences that
contain new word combinations or new words missing from training sentences. The
new words are transferred from the answers of language prediction. Such a
language ability is trained and evaluated on a population of over 1.6 million
distinct sentences consisting of 119 object words, 8 color words, 9
spatial-relation words, and 50 grammatical words. The proposed model
significantly outperforms five comparison methods for interpreting zero-shot
sentences. In addition, we demonstrate human-interpretable intermediate outputs
of the model in the appendix.
</summary>
    <author>
      <name>Haonan Yu</name>
    </author>
    <author>
      <name>Haichao Zhang</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2018 (Figure 6 caption improved)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.01433v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01433v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05140v1</id>
    <updated>2018-08-13T22:46:41Z</updated>
    <published>2018-08-13T22:46:41Z</published>
    <title>A Framework for Automated Cellular Network Tuning with Reinforcement
  Learning</title>
    <summary>  Tuning cellular network performance against always occurring wireless
impairments can dramatically improve reliability to end users. In this paper,
we formulate cellular network performance tuning as a reinforcement learning
(RL) problem and provide a solution to improve the signal to
interference-plus-noise ratio (SINR) for indoor and outdoor environments. By
leveraging the ability of Q-learning to estimate future SINR improvement
rewards, we propose two algorithms: (1) voice over LTE (VoLTE) downlink closed
loop power control (PC) and (2) self-organizing network (SON) fault management.
The VoLTE PC algorithm uses RL to adjust the indoor base station transmit power
so that the effective SINR meets the target SINR. The SON fault management
algorithm uses RL to improve the performance of an outdoor cluster by resolving
faults in the network through configuration management. Both algorithms exploit
measurements from the connected users, wireless impairments, and relevant
configuration parameters to solve a non-convex SINR optimization problem using
RL. Simulation results show that our proposed RL based algorithms outperform
the industry standards today in realistic cellular communication environments.
</summary>
    <author>
      <name>Faris B. Mismar</name>
    </author>
    <author>
      <name>Jinseok Choi</name>
    </author>
    <author>
      <name>Brian L. Evans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 9 figures, submitted to IEEE Transactions on Communications
  on August 13, 2018. arXiv admin note: text overlap with arXiv:1707.03269</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04475v1</id>
    <updated>2018-08-13T21:43:20Z</updated>
    <published>2018-08-13T21:43:20Z</published>
    <title>Kernel Flows: from learning kernels from data into the abyss</title>
    <summary>  Learning can be seen as approximating an unknown function by interpolating
the training data. Kriging offers a solution to this problem based on the prior
specification of a kernel. We explore a numerical approximation approach to
kernel selection/construction based on the simple premise that a kernel must be
good if the number of interpolation points can be halved without significant
loss in accuracy (measured using the intrinsic RKHS norm $\|\cdot\|$ associated
with the kernel). We first test and motivate this idea on a simple problem of
recovering the Green's function of an elliptic PDE (with inhomogeneous
coefficients) from the sparse observation of one of its solutions. Next we
consider the problem of learning non-parametric families of deep kernels of the
form $K_1(F_n(x),F_n(x'))$ with $F_{n+1}=(I_d+\epsilon G_{n+1})\circ F_n$ and
$G_{n+1} \in \operatorname{Span}\{K_1(F_n(x_i),\cdot)\}$. With the proposed
approach constructing the kernel becomes equivalent to integrating a stochastic
data driven dynamical system, which allows for the training of very deep
(bottomless) networks and the exploration of their properties. These networks
learn by constructing flow maps in the kernel and input spaces via incremental
data-dependent deformations/perturbations (appearing as the cooperative
counterpart of adversarial examples) and, at profound depths, they (1) can
achieve accurate classification from only one data point per class (2) appear
to learn archetypes of each class (3) expand distances between points that are
in different classes and contract distances between points in the same class.
For kernels parameterized by the weights of Convolutional Neural Network,
minimizing approximation errors incurred by halving random subsets of
interpolation points, appears to outperform training (the same CNN
architecture) with relative entropy and dropout.
</summary>
    <author>
      <name>Houman Owhadi</name>
    </author>
    <author>
      <name>Gene Ryan Yoo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 28 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J02, 68T01, 91C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04468v1</id>
    <updated>2018-08-13T21:08:46Z</updated>
    <published>2018-08-13T21:08:46Z</published>
    <title>Risk-Sensitive Generative Adversarial Imitation Learning</title>
    <summary>  We study risk-sensitive imitation learning where the agent's goal is to
perform at least as well as the expert in terms of a risk profile. We first
formulate our risk-sensitive imitation learning setting. We consider the
generative adversarial approach to imitation learning (GAIL) and derive an
optimization problem for our formulation, which we call risk-sensitive GAIL
(RS-GAIL). We then derive two different versions of our RS-GAIL optimization
problem that aim at matching the risk profiles of the agent and the expert
w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop
risk-sensitive generative adversarial imitation learning algorithms based on
these optimization problems. We evaluate the performance of our JS-based
algorithm and compare it with GAIL and the risk-averse imitation learning
(RAIL) algorithm in two MuJoCo tasks.
</summary>
    <author>
      <name>Jonathan Lacotte</name>
    </author>
    <author>
      <name>Yinlam Chow</name>
    </author>
    <author>
      <name>Mohammad Ghavamzadeh</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04456v1</id>
    <updated>2018-08-13T20:36:08Z</updated>
    <published>2018-08-13T20:36:08Z</published>
    <title>Multimodal Deep Neural Networks using Both Engineered and Learned
  Representations for Biodegradability Prediction</title>
    <summary>  Deep learning algorithms excel at extracting patterns from raw data. Through
representation learning and automated feature engineering on large datasets,
such models have been highly successful in computer vision and natural language
applications. However, in many other technical domains, large datasets on which
to learn representations from may not be feasible. In this work, we develop a
novel multimodal CNN-MLP neural network architecture that utilizes both
domain-specific feature engineering as well as learned representations from raw
data. We illustrate the effectiveness of such an approach in the chemical
sciences, for predicting chemical properties, where labeled data is scarce
owing to the high costs associated with acquiring labels through experimental
measurements. By training on both raw chemical data and using engineered
chemical features, while leveraging weak supervised learning and transfer
learning methods, we show that the multimodal CNN-MLP network is more accurate
than either a standalone CNN or MLP network that uses only raw data or
engineered features respectively. Using this multimodal network, we then
develop the DeepBioD model for predicting chemical biodegradability, which
achieves an error classification rate of 0.125 that is 27% lower than the
current state-of-the-art. Thus, our work indicates that combining traditional
feature engineering with representation learning on raw data can be an
effective approach, particularly in situations where labeled training data is
limited. Such a framework can also be potentially applied to other technical
fields, where substantial research efforts into feature engineering has been
established.
</summary>
    <author>
      <name>Garrett B. Goh</name>
    </author>
    <author>
      <name>Khusheemn Sakloth</name>
    </author>
    <author>
      <name>Charles Siegel</name>
    </author>
    <author>
      <name>Abhinav Vishnu</name>
    </author>
    <author>
      <name>Jim Pfaendtner</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04411v1</id>
    <updated>2018-08-13T19:21:41Z</updated>
    <published>2018-08-13T19:21:41Z</published>
    <title>Murmur Detection Using Parallel Recurrent &amp; Convolutional Neural
  Networks</title>
    <summary>  In this article, we propose a novel technique for classification of the
Murmurs in heart sound. We introduce a novel deep neural network architecture
using parallel combination of the Recurrent Neural Network (RNN) based
Bidirectional Long Short-Term Memory (BiLSTM) &amp; Convolutional Neural Network
(CNN) to learn visual and time-dependent characteristics of Murmur in PCG
waveform. Set of acoustic features are presented to our proposed deep neural
network to discriminate between Normal and Murmur class. The proposed method
was evaluated on a large dataset using 5-fold cross-validation, resulting in a
sensitivity and specificity of 96 +- 0.6 % , 100 +- 0 % respectively and F1
Score of 98 +- 0.3 %.
</summary>
    <author>
      <name>Shahnawaz Alam</name>
    </author>
    <author>
      <name>Rohan Banerjee</name>
    </author>
    <author>
      <name>Soma Bandyopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Machine Learning for Medicine and Healthcare Workshop, KDD
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04357v1</id>
    <updated>2018-08-13T19:02:47Z</updated>
    <published>2018-08-13T19:02:47Z</published>
    <title>RedSync : Reducing Synchronization Traffic for Distributed Deep Learning</title>
    <summary>  Data parallelism has already become a dominant method to scale Deep Neural
Network (DNN) training to multiple computation nodes. Considering that the
synchronization of local model or gradient between iterations can be a
bottleneck for large-scale distributed training, compressing communication
traffic has gained widespread attention recently. Among several recent proposed
compression algorithms, Residual Gradient Compression (RGC) is one of the most
successful approaches---it can significantly compress the message size (0.1% of
the original size) and still preserve accuracy. However, the literature on
compressing deep networks focuses almost exclusively on finding good
compression rate, while the efficiency of RGC in real implementation has been
less investigated. In this paper, we explore the potential of application RGC
method in the real distributed system. Targeting the widely adopted multi-GPU
system, we proposed an RGC system design call RedSync, which includes a set of
optimizations to reduce communication bandwidth while introducing limited
overhead. We examine the performance of RedSync on two different multiple GPU
platforms, including a supercomputer and a multi-card server. Our test cases
include image classification and language modeling tasks on Cifar10, ImageNet,
Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to
computation ratio, which have long been considered with poor scalability,
RedSync shows significant performance improvement.
</summary>
    <author>
      <name>Jiarui Fang</name>
    </author>
    <author>
      <name>Haohuan Fu</name>
    </author>
    <author>
      <name>Guangwen Yang</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06475v1</id>
    <updated>2018-08-13T18:43:11Z</updated>
    <published>2018-08-13T18:43:11Z</published>
    <title>The perceived quality of process discovery tools</title>
    <summary>  Process discovery has seen a rise in popularity in the last decade for both
researchers and businesses. Recent developments mainly focused on the power and
the functionalities of the discovery algorithm. While continuous improvement of
these functional aspects is very important, non-functional aspects such as
visualization and usability are often overlooked. However, these aspects are
considered valuable for end-users and play an important part in the experience
of these end-users when working with a process discovery tool. A questionnaire
has been sent out to give end-users the opportunity to voice their opinion on
available process discovery tools and about the state of process discovery as a
domain in general. The results of 66 respondents are presented and compared
with the answers of 63 respondents that were contacted through one particular
software vendor's employee and customer base (i.e., Celonis).
</summary>
    <author>
      <name>Francis Bru</name>
    </author>
    <author>
      <name>Jan Claes</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11248v2</id>
    <updated>2018-08-13T18:33:24Z</updated>
    <published>2017-10-30T21:22:28Z</published>
    <title>Learning Robust Rewards with Adversarial Inverse Reinforcement Learning</title>
    <summary>  Reinforcement learning provides a powerful and general framework for decision
making and control, but its application in practice is often hindered by the
need for extensive feature and reward engineering. Deep reinforcement learning
methods can remove the need for explicit engineering of policy or value
features, but still require a manually specified reward function. Inverse
reinforcement learning holds the promise of automatic reward acquisition, but
has proven exceptionally difficult to apply to large, high-dimensional problems
with unknown dynamics. In this work, we propose adverserial inverse
reinforcement learning (AIRL), a practical and scalable inverse reinforcement
learning algorithm based on an adversarial reward learning formulation. We
demonstrate that AIRL is able to recover reward functions that are robust to
changes in dynamics, enabling us to learn policies even under significant
variation in the environment seen during training. Our experiments show that
AIRL greatly outperforms prior methods in these transfer settings.
</summary>
    <author>
      <name>Justin Fu</name>
    </author>
    <author>
      <name>Katie Luo</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <link href="http://arxiv.org/abs/1710.11248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04355v1</id>
    <updated>2018-08-13T17:58:01Z</updated>
    <published>2018-08-13T17:58:01Z</published>
    <title>Large-Scale Study of Curiosity-Driven Learning</title>
    <summary>  Reinforcement learning algorithms rely on carefully engineering environment
rewards that are extrinsic to the agent. However, annotating each environment
with hand-designed, dense rewards is not scalable, motivating the need for
developing reward functions that are intrinsic to the agent. Curiosity is a
type of intrinsic reward function which uses prediction error as reward signal.
In this paper: (a) We perform the first large-scale study of purely
curiosity-driven learning, i.e. without any extrinsic rewards, across 54
standard benchmark environments, including the Atari game suite. Our results
show surprisingly good performance, and a high degree of alignment between the
intrinsic curiosity objective and the hand-designed extrinsic rewards of many
game environments. (b) We investigate the effect of using different feature
spaces for computing prediction error and show that random features are
sufficient for many popular RL game benchmarks, but learned features appear to
generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We
demonstrate limitations of the prediction-based rewards in stochastic setups.
Game-play videos and code are at
https://pathak22.github.io/large-scale-curiosity/
</summary>
    <author>
      <name>Yuri Burda</name>
    </author>
    <author>
      <name>Harri Edwards</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <author>
      <name>Amos Storkey</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First three authors contributed equally and ordered alphabetically.
  Website at https://pathak22.github.io/large-scale-curiosity/</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10377v2</id>
    <updated>2018-08-13T17:27:52Z</updated>
    <published>2018-05-25T21:55:12Z</published>
    <title>Ergodic Measure Preserving Flows</title>
    <summary>  Probabilistic modelling is a general and elegant framework to capture the
uncertainty, ambiguity and diversity of data. Probabilistic inference is the
core technique for developing training and simulation algorithms on
probabilistic models. However, the classic inference methods, like Markov chain
Monte Carlo (MCMC) methods and mean-field variational inference (VI), are not
computationally scalable for the recent developed probabilistic models with
neural networks (NNs). This motivates many recent works on improving classic
inference methods using NNs, especially, NN empowered VI. However, even with
powerful NNs, VI still suffers its fundamental limitations. In this work, we
propose a novel computational scalable general inference framework. With the
theoretical foundation in ergodic theory, the proposed methods are not only
computationally scalable like NN-based VI methods but also asymptotically
accurate like MCMC. We test our method on popular benchmark problems and the
results suggest that our methods can outperform NN-based VI and MCMC on deep
generative models and Bayesian neural networks.
</summary>
    <author>
      <name>Yichuan Zhang</name>
    </author>
    <author>
      <name>Jose Miguel Hernandez-Lobato</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04334v1</id>
    <updated>2018-08-13T17:20:20Z</updated>
    <published>2018-08-13T17:20:20Z</published>
    <title>Angular-Based Word Meta-Embedding Learning</title>
    <summary>  Ensembling word embeddings to improve distributed word representations has
shown good success for natural language processing tasks in recent years. These
approaches either carry out straightforward mathematical operations over a set
of vectors or use unsupervised learning to find a lower-dimensional
representation. This work compares meta-embeddings trained for different
losses, namely loss functions that account for angular distance between the
reconstructed embedding and the target and those that account normalized
distances based on the vector length. We argue that meta-embeddings are better
to treat the ensemble set equally in unsupervised learning as the respective
quality of each embedding is unknown for upstream tasks prior to
meta-embedding. We show that normalization methods that account for this such
as cosine and KL-divergence objectives outperform meta-embedding trained on
standard $\ell_1$ and $\ell_2$ loss on \textit{defacto} word similarity and
relatedness datasets and find it outperforms existing meta-learning strategies.
</summary>
    <author>
      <name>James O' Neill</name>
    </author>
    <author>
      <name>Danushka Bollegala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04327v1</id>
    <updated>2018-08-13T16:37:56Z</updated>
    <published>2018-08-13T16:37:56Z</published>
    <title>Hidden Fluid Mechanics: A Navier-Stokes Informed Deep Learning Framework
  for Assimilating Flow Visualization Data</title>
    <summary>  We present hidden fluid mechanics (HFM), a physics informed deep learning
framework capable of encoding an important class of physical laws governing
fluid motions, namely the Navier-Stokes equations. In particular, we seek to
leverage the underlying conservation laws (i.e., for mass, momentum, and
energy) to infer hidden quantities of interest such as velocity and pressure
fields merely from spatio-temporal visualizations of a passive scaler (e.g.,
dye or smoke), transported in arbitrarily complex domains (e.g., in human
arteries or brain aneurysms). Our approach towards solving the aforementioned
data assimilation problem is unique as we design an algorithm that is agnostic
to the geometry or the initial and boundary conditions. This makes HFM highly
flexible in choosing the spatio-temporal domain of interest for data
acquisition as well as subsequent training and predictions. Consequently, the
predictions made by HFM are among those cases where a pure machine learning
strategy or a mere scientific computing approach simply cannot reproduce. The
proposed algorithm achieves accurate predictions of the pressure and velocity
fields in both two and three dimensional flows for several benchmark problems
motivated by real-world applications. Our results demonstrate that this
relatively simple methodology can be used in physical and biomedical problems
to extract valuable quantitative information (e.g., lift and drag forces or
wall shear stresses in arteries) for which direct measurements may not be
possible.
</summary>
    <author>
      <name>Maziar Raissi</name>
    </author>
    <author>
      <name>Alireza Yazdani</name>
    </author>
    <author>
      <name>George Em Karniadakis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04308v1</id>
    <updated>2018-08-13T16:04:34Z</updated>
    <published>2018-08-13T16:04:34Z</published>
    <title>What is Unique in Individual Gait Patterns? Understanding and
  Interpreting Deep Learning in Gait Analysis</title>
    <summary>  Machine learning (ML) techniques such as (deep) artificial neural networks
(DNN) are solving very successfully a plethora of tasks and provide new
predictive models for complex physical, chemical, biological and social
systems. However, in most cases this comes with the disadvantage of acting as a
black box, rarely providing information about what made them arrive at a
particular prediction. This black box aspect of ML techniques can be
problematic especially in medical diagnoses, so far hampering a clinical
acceptance. The present paper studies the uniqueness of individual gait
patterns in clinical biomechanics using DNNs. By attributing portions of the
model predictions back to the input variables (ground reaction forces and
full-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique
reliably demonstrates which variables at what time windows of the gait cycle
are most relevant for the characterisation of gait patterns from a certain
individual. By measuring the timeresolved contribution of each input variable
to the prediction of ML techniques such as DNNs, our method describes the first
general framework that enables to understand and interpret non-linear ML
methods in (biomechanical) gait analysis and thereby supplies a powerful tool
for analysis, diagnosis and treatment of human gait.
</summary>
    <author>
      <name>Fabian Horst</name>
    </author>
    <author>
      <name>Sebastian Lapuschkin</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <author>
      <name>Klaus-Robert Müller</name>
    </author>
    <author>
      <name>Wolfgang I. Schöllhorn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages (23 pages including references, 24 pages including
  references and auxiliary statements, 33 pages including references, auxiliary
  statements and and supplementary material). 5 figures, 3 tables, 4
  supplementary figures, 9 supplementary tables. Under review at Scientific
  Reports</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04302v1</id>
    <updated>2018-08-13T15:54:50Z</updated>
    <published>2018-08-13T15:54:50Z</published>
    <title>Simple Root Cause Analysis by Separable Likelihoods</title>
    <summary>  Root Cause Analysis for Anomalies is challenging because of the trade-off
between the accuracy and its explanatory friendliness, required for industrial
applications. In this paper we propose a framework for simple and friendly RCA
within the Bayesian regime under certain restrictions (that Hessian at the mode
is diagonal, here referred to as \emph{separability}) imposed on the predictive
posterior. We show that this assumption is satisfied for important base models,
including Multinomal, Dirichlet-Multinomial and Naive Bayes. To demonstrate the
usefulness of the framework, we embed it into the Bayesian Net and validate on
web server error logs (real world data set).
</summary>
    <author>
      <name>Maciej Skorski</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05327v3</id>
    <updated>2018-08-13T15:46:23Z</updated>
    <published>2017-02-17T13:05:04Z</published>
    <title>Solving Equations of Random Convex Functions via Anchored Regression</title>
    <summary>  We consider the question of estimating a solution to a system of equations
that involve convex nonlinearities, a problem that is common in machine
learning and signal processing. Because of these nonlinearities, conventional
estimators based on empirical risk minimization generally involve solving a
non-convex optimization program. We propose anchored regression, a new approach
based on convex programming that amounts to maximizing a linear functional
(perhaps augmented by a regularizer) over a convex set. The proposed convex
program is formulated in the natural space of the problem, and avoids the
introduction of auxiliary variables, making it computationally favorable.
Working in the native space also provides great flexibility as structural
priors (e.g., sparsity) can be seamlessly incorporated.
  For our analysis, we model the equations as being drawn from a fixed set
according to a probability law. Our main results provide guarantees on the
accuracy of the estimator in terms of the number of equations we are solving,
the amount of noise present, a measure of statistical complexity of the random
equations, and the geometry of the regularizer at the true solution. We also
provide recipes for constructing the anchor vector (that determines the linear
functional to maximize) directly from the observed data.
</summary>
    <author>
      <name>Sohail Bahmani</name>
    </author>
    <author>
      <name>Justin Romberg</name>
    </author>
    <link href="http://arxiv.org/abs/1702.05327v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05327v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04293v1</id>
    <updated>2018-08-13T15:37:29Z</updated>
    <published>2018-08-13T15:37:29Z</published>
    <title>Fast, Better Training Trick -- Random Gradient</title>
    <summary>  In this paper, we will show an unprecedented method to accelerate training
and improve performance, which called random gradient (RG). This method can be
easier to the training of any model without extra calculation cost, we use
Image classification, Semantic segmentation, and GANs to confirm this method
can improve speed which is training model in computer vision. The central idea
is using the loss multiplied by a random number to random reduce the
back-propagation gradient. We can use this method to produce a better result in
Pascal VOC, Cifar, Cityscapes datasets.
</summary>
    <author>
      <name>Jiakai Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1708.07120 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04287v1</id>
    <updated>2018-08-13T15:24:01Z</updated>
    <published>2018-08-13T15:24:01Z</published>
    <title>Visual Sensor Network Reconfiguration with Deep Reinforcement Learning</title>
    <summary>  We present an approach for reconfiguration of dynamic visual sensor networks
with deep reinforcement learning (RL). Our RL agent uses a modified
asynchronous advantage actor-critic framework and the recently proposed
Relational Network module at the foundation of its network architecture. To
address the issue of sample inefficiency in current approaches to model-free
reinforcement learning, we train our system in an abstract simulation
environment that represents inputs from a dynamic scene. Our system is
validated using inputs from a real-world scenario and preexisting object
detection and tracking algorithms.
</summary>
    <author>
      <name>Paul Jasek</name>
    </author>
    <author>
      <name>Bernard Abayowa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06126v3</id>
    <updated>2018-08-13T15:13:05Z</updated>
    <published>2018-01-18T16:59:19Z</published>
    <title>Non-Adversarial Unsupervised Word Translation</title>
    <summary>  Unsupervised word translation from non-parallel inter-lingual corpora has
attracted much research interest. Very recently, neural network methods trained
with adversarial loss functions achieved high accuracy on this task. Despite
the impressive success of the recent techniques, they suffer from the typical
drawbacks of generative adversarial models: sensitivity to hyper-parameters,
long training time and lack of interpretability. In this paper, we make the
observation that two sufficiently similar distributions can be aligned
correctly with iterative matching methods. We present a novel method that first
aligns the second moment of the word distributions of the two languages and
then iteratively refines the alignment. Extensive experiments on word
translation of European and Non-European languages show that our method
achieves better performance than recent state-of-the-art deep adversarial
approaches and is competitive with the supervised baseline. It is also
efficient, easy to parallelize on CPU and interpretable.
</summary>
    <author>
      <name>Yedid Hoshen</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06126v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06126v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04281v1</id>
    <updated>2018-08-13T15:07:55Z</updated>
    <published>2018-08-13T15:07:55Z</published>
    <title>Estimating Heterogeneous Causal Effects in the Presence of Irregular
  Assignment Mechanisms</title>
    <summary>  This paper provides a link between causal inference and machine learning
techniques - specifically, Classification and Regression Trees (CART) - in
observational studies where the receipt of the treatment is not randomized, but
the assignment to the treatment can be assumed to be randomized (irregular
assignment mechanism). The paper contributes to the growing applied machine
learning literature on causal inference, by proposing a modified version of the
Causal Tree (CT) algorithm to draw causal inference from an irregular
assignment mechanism. The proposed method is developed by merging the CT
approach with the instrumental variable framework to causal inference, hence
the name Causal Tree with Instrumental Variable (CT-IV). As compared to CT, the
main strength of CT-IV is that it can deal more efficiently with the
heterogeneity of causal effects, as demonstrated by a series of numerical
results obtained on synthetic data. Then, the proposed algorithm is used to
evaluate a public policy implemented by the Tuscan Regional Administration
(Italy), which aimed at easing the access to credit for small firms. In this
context, CT-IV breaks fresh ground for target-based policies, identifying
interesting heterogeneous causal effects.
</summary>
    <author>
      <name>Falco J. Bargagli Stoffi</name>
    </author>
    <author>
      <name>Giorgio Gnecco</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04260v1</id>
    <updated>2018-08-13T14:23:15Z</updated>
    <published>2018-08-13T14:23:15Z</published>
    <title>iNNvestigate neural networks!</title>
    <summary>  In recent years, deep neural networks have revolutionized many application
domains of machine learning and are key components of many critical decision or
predictive processes. Therefore, it is crucial that domain specialists can
understand and analyze actions and pre- dictions, even of the most complex
neural network architectures. Despite these arguments neural networks are often
treated as black boxes. In the attempt to alleviate this short- coming many
analysis methods were proposed, yet the lack of reference implementations often
makes a systematic comparison between the methods a major effort. The presented
library iNNvestigate addresses this by providing a common interface and
out-of-the- box implementation for many analysis methods, including the
reference implementation for PatternNet and PatternAttribution as well as for
LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an
analysis of image classifications for variety of state-of-the-art neural
network architectures.
</summary>
    <author>
      <name>Maximilian Alber</name>
    </author>
    <author>
      <name>Sebastian Lapuschkin</name>
    </author>
    <author>
      <name>Philipp Seegerer</name>
    </author>
    <author>
      <name>Miriam Hägele</name>
    </author>
    <author>
      <name>Kristof T. Schütt</name>
    </author>
    <author>
      <name>Grégoire Montavon</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <author>
      <name>Klaus-Robert Müller</name>
    </author>
    <author>
      <name>Sven Dähne</name>
    </author>
    <author>
      <name>Pieter-Jan Kindermans</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04752v1</id>
    <updated>2018-08-13T14:11:43Z</updated>
    <published>2018-08-13T14:11:43Z</published>
    <title>A Survey on Methods and Theories of Quantized Neural Networks</title>
    <summary>  Deep neural networks are the state-of-the-art methods for many real-world
tasks, such as computer vision, natural language processing and speech
recognition. For all its popularity, deep neural networks are also criticized
for consuming a lot of memory and draining battery life of devices during
training and inference. This makes it hard to deploy these models on mobile or
embedded devices which have tight resource constraints. Quantization is
recognized as one of the most effective approaches to satisfy the extreme
memory requirements that deep neural network models demand. Instead of adopting
32-bit floating point format to represent weights, quantized representations
store weights using more compact formats such as integers or even binary
numbers. Despite a possible degradation in predictive performance, quantization
provides a potential solution to greatly reduce the model size and the energy
consumption. In this survey, we give a thorough review of different aspects of
quantized neural networks. Current challenges and trends of quantized neural
networks are also discussed.
</summary>
    <author>
      <name>Yunhui Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 0.0</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09865v2</id>
    <updated>2018-08-13T13:17:15Z</updated>
    <published>2018-07-25T21:19:48Z</published>
    <title>Predicting Acute Kidney Injury at Hospital Re-entry Using
  High-dimensional Electronic Health Record Data</title>
    <summary>  Acute Kidney Injury (AKI), a sudden decline in kidney function, is associated
with increased mortality, morbidity, length of stay, and hospital cost. Since
AKI is sometimes preventable, there is great interest in prediction. Most
existing studies consider all patients and therefore restrict to features
available in the first hours of hospitalization. Here, the focus is instead on
rehospitalized patients, a cohort in which rich longitudinal features from
prior hospitalizations can be analyzed. Our objective is to provide a risk
score directly at hospital re-entry. Gradient boosting, penalized logistic
regression (with and without stability selection), and a recurrent neural
network are trained on two years of adult inpatient EHR data (3,387 attributes
for 34,505 patients who generated 90,013 training samples with 5,618 cases and
84,395 controls). Predictions are internally evaluated with 50 iterations of
5-fold grouped cross-validation with special emphasis on calibration, an
analysis of which is performed at the patient as well as hospitalization level.
Error is assessed with respect to diagnosis, race, age, gender, AKI
identification method, and hospital utilization. In an additional experiment,
the regularization penalty is severely increased to induce parsimony and
interpretability. Predictors identified for rehospitalized patients are also
reported with a special analysis of medications that might be modifiable risk
factors. Insights from this study might be used to construct a predictive tool
for AKI in rehospitalized patients. An accurate estimate of AKI risk at
hospital entry might serve as a prior for an admitting provider or another
predictive algorithm.
</summary>
    <author>
      <name>Samuel J. Weisenthal</name>
    </author>
    <author>
      <name>Caroline Quill</name>
    </author>
    <author>
      <name>Samir Farooq</name>
    </author>
    <author>
      <name>Henry Kautz</name>
    </author>
    <author>
      <name>Martin S. Zand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In revision</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04883v1</id>
    <updated>2018-08-13T11:45:16Z</updated>
    <published>2018-08-13T11:45:16Z</published>
    <title>COLA: Communication-Efficient Decentralized Linear Learning</title>
    <summary>  Decentralized machine learning is a promising emerging paradigm in view of
global challenges of data ownership and privacy. We consider learning of linear
classification and regression models, in the setting where the training data is
decentralized over many user devices, and the learning algorithm must run
on-device, on an arbitrary communication network, without a central
coordinator. We propose COLA, a new decentralized training algorithm with
strong theoretical guarantees and superior practical performance. Our framework
overcomes many limitations of existing methods, and achieves communication
efficiency, scalability, elasticity as well as resilience to changes in data
and participating devices.
</summary>
    <author>
      <name>Lie He</name>
    </author>
    <author>
      <name>An Bian</name>
    </author>
    <author>
      <name>Martin Jaggi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04152v1</id>
    <updated>2018-08-13T11:18:49Z</updated>
    <published>2018-08-13T11:18:49Z</published>
    <title>Learning Discriminative Hashing Codes for Cross-Modal Retrieval based on
  Multiorder Statistical Features</title>
    <summary>  Hashing techniques have been applied broadly in large-scale retrieval tasks
due to their low storage requirements and high speed of processing. Many
hashing methods have shown promising performance but as they fail to exploit
all structural information in learning the hashing function, they leave a scope
for improvement. The paper proposes a novel discrete hashing learning framework
which jointly performs classifier learning and subspace learning for
cross-modal retrieval. Concretely, the framework proposed in the paper includes
two stages, namely a kernelization process and a quantization process. The aim
of kernelization is to learn a common subspace where heterogeneous data can be
fused. The quantization process is designed to learn discriminative unified
hashing codes. Extensive experiments on three publicly available datasets
clearly indicate the superiority of our method compared with the
state-of-the-art methods.
</summary>
    <author>
      <name>Jun Yu</name>
    </author>
    <author>
      <name>Xiao-Jun Wu</name>
    </author>
    <author>
      <name>Josef Kittler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10pages,6figures,4tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04127v1</id>
    <updated>2018-08-13T09:51:46Z</updated>
    <published>2018-08-13T09:51:46Z</published>
    <title>Learning Explanations from Language Data</title>
    <summary>  PatternAttribution is a recent method, introduced in the vision domain, that
explains classifications of deep neural networks. We demonstrate that it also
generates meaningful interpretations in the language domain.
</summary>
    <author>
      <name>David Harbecke</name>
    </author>
    <author>
      <name>Robert Schwarzenberg</name>
    </author>
    <author>
      <name>Christoph Alt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in 2018 EMNLP Workshop on Analyzing and Interpreting Neural
  Networks for NLP (BlackboxNLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04096v1</id>
    <updated>2018-08-13T08:12:22Z</updated>
    <published>2018-08-13T08:12:22Z</published>
    <title>Directed Policy Gradient for Safe Reinforcement Learning with Human
  Advice</title>
    <summary>  Many currently deployed Reinforcement Learning agents work in an environment
shared with humans, be them co-workers, users or clients. It is desirable that
these agents adjust to people's preferences, learn faster thanks to their help,
and act safely around them. We argue that most current approaches that learn
from human feedback are unsafe: rewarding or punishing the agent a-posteriori
cannot immediately prevent it from wrong-doing. In this paper, we extend Policy
Gradient to make it robust to external directives, that would otherwise break
the fundamentally on-policy nature of Policy Gradient. Our technique, Directed
Policy Gradient (DPG), allows a teacher or backup policy to override the agent
before it acts undesirably, while allowing the agent to leverage human advice
or directives to learn faster. Our experiments demonstrate that DPG makes the
agent learn much faster than reward-based approaches, while requiring an order
of magnitude less advice.
</summary>
    <author>
      <name>Hélène Plisnier</name>
    </author>
    <author>
      <name>Denis Steckelmacher</name>
    </author>
    <author>
      <name>Tim Brys</name>
    </author>
    <author>
      <name>Diederik M. Roijers</name>
    </author>
    <author>
      <name>Ann Nowé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the European Workshop on Reinforcement Learning 2018
  (EWRL14)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06396v3</id>
    <updated>2018-08-13T03:15:02Z</updated>
    <published>2018-03-16T20:57:36Z</published>
    <title>Reviving and Improving Recurrent Back-Propagation</title>
    <summary>  In this paper, we revisit the recurrent back-propagation (RBP) algorithm,
discuss the conditions under which it applies as well as how to satisfy them in
deep neural networks. We show that RBP can be unstable and propose two variants
based on conjugate gradient on the normal equations (CG-RBP) and Neumann series
(Neumann-RBP). We further investigate the relationship between Neumann-RBP and
back propagation through time (BPTT) and its truncated version (TBPTT). Our
Neumann-RBP has the same time complexity as TBPTT but only requires constant
memory, whereas TBPTT's memory cost scales linearly with the number of
truncation steps. We examine all RBP variants along with BPTT and TBPTT in
three different application domains: associative memory with continuous
Hopfield networks, document classification in citation networks using graph
neural networks and hyperparameter optimization for fully connected networks.
All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are
efficient and effective for optimizing convergent recurrent neural networks.
</summary>
    <author>
      <name>Renjie Liao</name>
    </author>
    <author>
      <name>Yuwen Xiong</name>
    </author>
    <author>
      <name>Ethan Fetaya</name>
    </author>
    <author>
      <name>Lisa Zhang</name>
    </author>
    <author>
      <name>KiJung Yoon</name>
    </author>
    <author>
      <name>Xaq Pitkow</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <author>
      <name>Richard Zemel</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06396v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06396v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02229v2</id>
    <updated>2018-08-13T02:19:08Z</updated>
    <published>2018-08-07T06:54:06Z</published>
    <title>Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep
  Learning</title>
    <summary>  Modern machine learning algorithms have been adopted in a range of
signal-processing applications spanning computer vision, natural language
processing, and artificial intelligence. Many relevant problems involve
subspace-structured features, orthogonality constrained or low-rank constrained
objective functions, or subspace distances. These mathematical characteristics
are expressed naturally using the Grassmann manifold. Unfortunately, this fact
is not yet explored in many traditional learning algorithms. In the last few
years, there have been growing interests in studying Grassmann manifold to
tackle new learning problems. Such attempts have been reassured by substantial
performance improvements in both classic learning and learning using deep
neural networks. We term the former as shallow and the latter deep Grassmannian
learning. The aim of this paper is to introduce the emerging area of
Grassmannian learning by surveying common mathematical problems and primary
solution approaches, and overviewing various applications. We hope to inspire
practitioners in different fields to adopt the powerful tool of Grassmannian
learning in their research.
</summary>
    <author>
      <name>Jiayao Zhang</name>
    </author>
    <author>
      <name>Guangxu Zhu</name>
    </author>
    <author>
      <name>Robert W. Heath Jr.</name>
    </author>
    <author>
      <name>Kaibin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Signal Processing Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02229v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02229v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04022v1</id>
    <updated>2018-08-12T23:47:10Z</updated>
    <published>2018-08-12T23:47:10Z</published>
    <title>Interpretable Time Series Classification using All-Subsequence Learning
  and Symbolic Representations in Time and Frequency Domains</title>
    <summary>  The time series classification literature has expanded rapidly over the last
decade, with many new classification approaches published each year. The
research focus has mostly been on improving the accuracy and efficiency of
classifiers, while their interpretability has been somewhat neglected.
Classifier interpretability has become a critical constraint for many
application domains and the introduction of the 'right to explanation' GDPR EU
legislation in May 2018 is likely to further emphasize the importance of
explainable learning algorithms. In this work we analyse the state-of-the-art
for time series classification, and propose new algorithms that aim to maintain
the classifier accuracy and efficiency, but keep interpretability as a key
design constraint. We present new time series classification algorithms that
advance the state-of-the-art by implementing the following three key ideas: (1)
Multiple resolutions of symbolic approximations: we combine symbolic
representations obtained using different parameters; (2) Multiple domain
representations: we combine symbolic approximations in time (e.g., SAX) and
frequency (e.g., SFA) domains; (3) Efficient navigation of a huge
symbolic-words space: we adapt a symbolic sequence classifier named SEQL, to
make it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL),
and use its greedy feature selection strategy to effectively filter the best
features for each representation. We show that a multi-resolution multi-domain
linear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the
state-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but
uses a fraction of the time required by either COTE or FCN. We discuss the
accuracy, efficiency and interpretability of our proposed algorithms. To
further analyse the interpretability aspect of our classifiers, we present a
case study on an ecology benchmark.
</summary>
    <author>
      <name>Thach Le Nguyen</name>
    </author>
    <author>
      <name>Severin Gsponer</name>
    </author>
    <author>
      <name>Iulia Ilie</name>
    </author>
    <author>
      <name>Georgiana Ifrim</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04008v1</id>
    <updated>2018-08-12T22:23:43Z</updated>
    <published>2018-08-12T22:23:43Z</published>
    <title>PAC-Battling Bandits with Plackett-Luce: Tradeoff between Sample
  Complexity and Subset Size</title>
    <summary>  We introduce the probably approximately correct (PAC) version of the problem
of {Battling-bandits} with the Plackett-Luce (PL) model -- an online learning
framework where in each trial, the learner chooses a subset of $k \le n$ arms
from a pool of fixed set of $n$ arms, and subsequently observes a stochastic
feedback indicating preference information over the items in the chosen subset;
e.g., the most preferred item or ranking of the top $m$ most preferred items
etc. The objective is to recover an `approximate-best' item of the underlying
PL model with high probability. This framework is motivated by practical
settings such as recommendation systems and information retrieval, where it is
easier and more efficient to collect relative feedback for multiple arms at
once. Our framework can be seen as a generalization of the well-studied
PAC-{Dueling-Bandit} problem over set of $n$ arms. We propose two different
feedback models: just the winner information (WI), and ranking of top-$m$ items
(TR), for any $2\le m \le k$. We show that with just the winner information
(WI), one cannot recover the `approximate-best' item with sample complexity
lesser than $\Omega\bigg( \frac{n}{\epsilon^2} \ln \frac{1}{\delta}\bigg)$,
which is independent of $k$, and same as the one required for standard dueling
bandit setting ($k=2$). However with top-$m$ ranking (TR) feedback, our lower
analysis proves an improved sample complexity guarantee of $\Omega\bigg(
\frac{n}{m\epsilon^2} \ln \frac{1}{\delta}\bigg)$, which shows a relative
improvement of $\frac{1}{m}$ factor compared to WI feedback, rightfully
justifying the additional information gain due to the knowledge of ranking of
topmost $m$ items. We also provide algorithms for each of the above feedback
models, our theoretical analyses proves the {optimality} of their sample
complexities which matches the derived lower bounds (upto logarithmic factors).
</summary>
    <author>
      <name>Aditya Gopalan</name>
    </author>
    <author>
      <name>Aadirupa Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03987v1</id>
    <updated>2018-08-12T19:04:29Z</updated>
    <published>2018-08-12T19:04:29Z</published>
    <title>A simulation study to distinguish prompt photon from $π^0$ and beam
  halo in a granular calorimeter using deep networks</title>
    <summary>  In a hadron collider environment identification of prompt photons originating
in a hard partonic scattering process and rejection of non-prompt photons
coming from hadronic jets or from beam related sources, is the first step for
study of processes with photons in final state. Photons coming from decay of
$\pi_0$'s produced inside a hadronic jet and photons produced in catastrophic
bremsstrahlung by beam halo muons are two major sources of non-prompt photons.
In this paper the potential of deep learning methods for separating the prompt
photons from beam halo and $\pi^0$'s in the electromagnetic calorimeter of a
collider detector is investigated, using an approximate description of the CMS
detector. It is shown that, using only calorimetric information as images with
a Convolutional Neural Network, beam halo (and $\pi^{0}$) can be separated from
photon with 99.96% (97.7%) background rejection for 99.00% (90.0%) signal
efficiency which is much better than traditionally employed variables.
</summary>
    <author>
      <name>Shamik Ghosh</name>
    </author>
    <author>
      <name>Abhirami Harilal</name>
    </author>
    <author>
      <name>A. R. Sahasransu</name>
    </author>
    <author>
      <name>Ritesh Kumar Singh</name>
    </author>
    <author>
      <name>Satyaki Bhattacharya</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03965v1</id>
    <updated>2018-08-12T16:22:12Z</updated>
    <published>2018-08-12T16:22:12Z</published>
    <title>Large-Scale Learnable Graph Convolutional Networks</title>
    <summary>  Convolutional neural networks (CNNs) have achieved great success on grid-like
data such as images, but face tremendous challenges in learning from more
generic data such as graphs. In CNNs, the trainable local filters enable the
automatic extraction of high-level features. The computation with filters
requires a fixed number of ordered units in the receptive fields. However, the
number of neighboring units is neither fixed nor are they ordered in generic
graphs, thereby hindering the applications of convolutional operations. Here,
we address these challenges by proposing the learnable graph convolutional
layer (LGCL). LGCL automatically selects a fixed number of neighboring nodes
for each feature based on value ranking in order to transform graph data into
grid-like structures in 1-D format, thereby enabling the use of regular
convolutional operations on generic graphs. To enable model training on
large-scale graphs, we propose a sub-graph training method to reduce the
excessive memory and computational resource requirements suffered by prior
methods on graph convolutions. Our experimental results on node classification
tasks in both transductive and inductive learning settings demonstrate that our
methods can achieve consistently better performance on the Cora, Citeseer,
Pubmed citation network, and protein-protein interaction network datasets. Our
results also indicate that the proposed methods using sub-graph training
strategy are more efficient as compared to prior approaches.
</summary>
    <author>
      <name>Hongyang Gao</name>
    </author>
    <author>
      <name>Zhengyang Wang</name>
    </author>
    <author>
      <name>Shuiwang Ji</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219947</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219947" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 24th ACM SIGKDD International Conference on
  Knowledge Discovery &amp; Data Mining (pp. 1416-1424). ACM (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.03965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00967v2</id>
    <updated>2018-08-12T16:08:00Z</updated>
    <published>2018-03-02T17:40:18Z</published>
    <title>Active model learning and diverse action sampling for task and motion
  planning</title>
    <summary>  The objective of this work is to augment the basic abilities of a robot by
learning to use new sensorimotor primitives to enable the solution of complex
long-horizon problems. Solving long-horizon problems in complex domains
requires flexible generative planning that can combine primitive abilities in
novel combinations to solve problems as they arise in the world. In order to
plan to combine primitive actions, we must have models of the preconditions and
effects of those actions: under what circumstances will executing this
primitive achieve some particular effect in the world?
  We use, and develop novel improvements on, state-of-the-art methods for
active learning and sampling. We use Gaussian process methods for learning the
conditions of operator effectiveness from small numbers of expensive training
examples collected by experimentation on a robot. We develop adaptive sampling
methods for generating diverse elements of continuous sets (such as robot
configurations and object poses) during planning for solving a new task, so
that planning is as efficient as possible. We demonstrate these methods in an
integrated system, combining newly learned models with an efficient
continuous-space robot task and motion planner to learn to solve long horizon
problems more efficiently than was previously possible.
</summary>
    <author>
      <name>Zi Wang</name>
    </author>
    <author>
      <name>Caelan Reed Garrett</name>
    </author>
    <author>
      <name>Leslie Pack Kaelbling</name>
    </author>
    <author>
      <name>Tomás Lozano-Pérez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2018 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS), Madrid, Spain.
  https://www.youtube.com/playlist?list=PLoWhBFPMfSzDbc8CYelsbHZa1d3uz-W_c</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00967v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00967v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.06423v4</id>
    <updated>2018-08-12T16:03:00Z</updated>
    <published>2015-10-21T20:35:13Z</published>
    <title>Optimization as Estimation with Gaussian Processes in Bandit Settings</title>
    <summary>  Recently, there has been rising interest in Bayesian optimization -- the
optimization of an unknown function with assumptions usually expressed by a
Gaussian Process (GP) prior. We study an optimization strategy that directly
uses an estimate of the argmax of the function. This strategy offers both
practical and theoretical advantages: no tradeoff parameter needs to be
selected, and, moreover, we establish close connections to the popular GP-UCB
and GP-PI strategies. Our approach can be understood as automatically and
adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We
illustrate the effects of this adaptive tuning via bounds on the regret as well
as an extensive empirical evaluation on robotics and vision tasks,
demonstrating the robustness of this strategy for a range of performance
criteria.
</summary>
    <author>
      <name>Zi Wang</name>
    </author>
    <author>
      <name>Bolei Zhou</name>
    </author>
    <author>
      <name>Stefanie Jegelka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 19th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.06423v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06423v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03958v1</id>
    <updated>2018-08-12T15:37:40Z</updated>
    <published>2018-08-12T15:37:40Z</published>
    <title>Characterizing Neuronal Circuits with Spike-triggered Non-negative
  Matrix Factorization</title>
    <summary>  Neuronal circuits formed in the brain are complex with intricate connection
patterns. Such a complexity is also observed in the retina as a relatively
simple neuronal circuit. A retinal ganglion cell receives excitatory inputs
from neurons in previous layers as driving forces to fire spikes. Analytical
methods are required that can decipher these components in a systematic manner.
Recently a method termed spike-triggered non-negative matrix factorization
(STNMF) has been proposed for this purpose. In this study, we extend the scope
of the STNMF method. By using the retinal ganglion cell as a model system, we
show that STNMF can detect various biophysical properties of upstream bipolar
cells, including spatial receptive fields, temporal filters, and transfer
nonlinearity. In addition, we recover synaptic connection strengths from the
weight matrix of STNMF. Furthermore, we show that STNMF can separate spikes of
a ganglion cell into a few subsets of spikes where each subset is contributed
by one presynaptic bipolar cell. Taken together, these results corroborate that
STNMF is a useful method for deciphering the structure of neuronal circuits.
</summary>
    <author>
      <name>Shanshan Jia</name>
    </author>
    <author>
      <name>Zhaofei Yu</name>
    </author>
    <author>
      <name>Arno Onken</name>
    </author>
    <author>
      <name>Yonghong Tian</name>
    </author>
    <author>
      <name>Tiejun Huang</name>
    </author>
    <author>
      <name>Jian K. Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages,11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03953v1</id>
    <updated>2018-08-12T15:14:04Z</updated>
    <published>2018-08-12T15:14:04Z</published>
    <title>A Fourier View of REINFORCE</title>
    <summary>  We show a connection between the Fourier spectrum of Boolean functions and
the REINFORCE gradient estimator for binary latent variable models. We show
that REINFORCE estimates (up to a factor) the degree-1 Fourier coefficients of
a Boolean function. Using this connection we offer a new perspective on
variance reduction in gradient estimation for latent variable models: namely,
that variance reduction involves eliminating or reducing Fourier coefficients
that do not have degree 1. We then use this connection to develop low-variance
unbiased gradient estimators for binary latent variable models such as sigmoid
belief networks. The estimator is based upon properties of the noise operator
from Boolean Fourier theory and involves a sample-dependent baseline added to
the REINFORCE estimator in a way that keeps the estimator unbiased. The
baseline can be plugged into existing gradient estimators for further variance
reduction.
</summary>
    <author>
      <name>Adeel Pervez</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03944v1</id>
    <updated>2018-08-12T13:49:19Z</updated>
    <published>2018-08-12T13:49:19Z</published>
    <title>Unsupervised learning for cross-domain medical image synthesis using
  deformation invariant cycle consistency networks</title>
    <summary>  Recently, the cycle-consistent generative adversarial networks (CycleGAN) has
been widely used for synthesis of multi-domain medical images. The
domain-specific nonlinear deformations captured by CycleGAN make the
synthesized images difficult to be used for some applications, for example,
generating pseudo-CT for PET-MR attenuation correction. This paper presents a
deformation-invariant CycleGAN (DicycleGAN) method using deformable
convolutional layers and new cycle-consistency losses. Its robustness dealing
with data that suffer from domain-specific nonlinear deformations has been
evaluated through comparison experiments performed on a multi-sequence brain MR
dataset and a multi-modality abdominal dataset. Our method has displayed its
ability to generate synthesized data that is aligned with the source while
maintaining a proper quality of signal compared to CycleGAN-generated data. The
proposed model also obtained comparable performance with CycleGAN when data
from the source and target domains are alignable through simple affine
transformations.
</summary>
    <author>
      <name>Chengjia Wang</name>
    </author>
    <author>
      <name>Gillian Macnaught</name>
    </author>
    <author>
      <name>Giorgos Papanastasiou</name>
    </author>
    <author>
      <name>Tom MacGillivray</name>
    </author>
    <author>
      <name>David Newby</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03926v1</id>
    <updated>2018-08-12T11:53:04Z</updated>
    <published>2018-08-12T11:53:04Z</published>
    <title>Sequence Labeling: A Practical Approach</title>
    <summary>  We take a practical approach to solving sequence labeling problem assuming
unavailability of domain expertise and scarcity of informational and
computational resources. To this end, we utilize a universal end-to-end
Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP
tasks and languages. The model combines morphological, semantic, and structural
cues extracted from data to arrive at informed predictions. The model's
performance is evaluated on eight benchmark datasets (covering three tasks:
POS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and
Spanish). We observe state-of-the-art results on four of them: CoNLL-2012
(English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus
(German POS-tagging), and competitive performance on the rest.
</summary>
    <author>
      <name>Adnan Akhundov</name>
    </author>
    <author>
      <name>Dietrich Trautmann</name>
    </author>
    <author>
      <name>Georg Groh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For the source code and detailed experimental results, see
  http://github.com/aakhundov/sequence-labeling</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03920v1</id>
    <updated>2018-08-12T10:04:45Z</updated>
    <published>2018-08-12T10:04:45Z</published>
    <title>Multimodal Language Analysis with Recurrent Multistage Fusion</title>
    <summary>  Computational modeling of human multimodal language is an emerging research
area in natural language processing spanning the language, visual and acoustic
modalities. Comprehending multimodal language requires modeling not only the
interactions within each modality (intra-modal interactions) but more
importantly the interactions between modalities (cross-modal interactions). In
this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which
decomposes the fusion problem into multiple stages, each of them focused on a
subset of multimodal signals for specialized, effective fusion. Cross-modal
interactions are modeled using this multistage fusion approach which builds
upon intermediate representations of previous stages. Temporal and intra-modal
interactions are modeled by integrating our proposed fusion approach with a
system of recurrent neural networks. The RMFN displays state-of-the-art
performance in modeling human multimodal language across three public datasets
relating to multimodal sentiment analysis, emotion recognition, and speaker
traits recognition. We provide visualizations to show that each stage of fusion
focuses on a different subset of multimodal signals, learning increasingly
discriminative multimodal representations.
</summary>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Ziyin Liu</name>
    </author>
    <author>
      <name>Amir Zadeh</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03912v1</id>
    <updated>2018-08-12T08:56:06Z</updated>
    <published>2018-08-12T08:56:06Z</published>
    <title>Outer Product-based Neural Collaborative Filtering</title>
    <summary>  In this work, we contribute a new multi-layer neural network architecture
named ONCF to perform collaborative filtering. The idea is to use an outer
product to explicitly model the pairwise correlations between the dimensions of
the embedding space. In contrast to existing neural recommender models that
combine user embedding and item embedding via a simple concatenation or
element-wise product, our proposal of using outer product above the embedding
layer results in a two-dimensional interaction map that is more expressive and
semantically plausible. Above the interaction map obtained by outer product, we
propose to employ a convolutional neural network to learn high-order
correlations among embedding dimensions. Extensive experiments on two public
implicit feedback data demonstrate the effectiveness of our proposed ONCF
framework, in particular, the positive effect of using outer product to model
the correlations between embedding dimensions in the low level of multi-layer
neural recommender model. The experiment codes are available at:
https://github.com/duxy-me/ConvNCF
</summary>
    <author>
      <name>Xiangnan He</name>
    </author>
    <author>
      <name>Xiaoyu Du</name>
    </author>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Feng Tian</name>
    </author>
    <author>
      <name>Jinhui Tang</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03908v1</id>
    <updated>2018-08-12T08:26:25Z</updated>
    <published>2018-08-12T08:26:25Z</published>
    <title>Adversarial Personalized Ranking for Recommendation</title>
    <summary>  Item recommendation is a personalized ranking task. To this end, many
recommender systems optimize models with pairwise ranking objectives, such as
the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) ---
the most widely used model in recommendation --- as a demonstration, we show
that optimizing it with BPR leads to a recommender model that is not robust. In
particular, we find that the resultant model is highly vulnerable to
adversarial perturbations on its model parameters, which implies the possibly
large error in generalization.
  To enhance the robustness of a recommender model and thus improve its
generalization performance, we propose a new optimization framework, namely
Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise
ranking method BPR by performing adversarial training. It can be interpreted as
playing a minimax game, where the minimization of the BPR objective function
meanwhile defends an adversary, which adds adversarial perturbations on model
parameters to maximize the BPR objective function. To illustrate how it works,
we implement APR on MF by adding adversarial perturbations on the embedding
vectors of users and items. Extensive experiments on three public real-world
datasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it
outperforms BPR with a relative improvement of 11.2% on average and achieves
state-of-the-art performance for item recommendation. Our implementation is
available at: https://github.com/hexiangnan/adversarial_personalized_ranking.
</summary>
    <author>
      <name>Xiangnan He</name>
    </author>
    <author>
      <name>Zhankui He</name>
    </author>
    <author>
      <name>Xiaoyu Du</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3209978.3209981</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3209978.3209981" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03894v1</id>
    <updated>2018-08-12T05:42:26Z</updated>
    <published>2018-08-12T05:42:26Z</published>
    <title>Interpreting Recurrent and Attention-Based Neural Models: a Case Study
  on Natural Language Inference</title>
    <summary>  Deep learning models have achieved remarkable success in natural language
inference (NLI) tasks. While these models are widely explored, they are hard to
interpret and it is often unclear how and why they actually work. In this
paper, we take a step toward explaining such deep learning based models through
a case study on a popular neural model for NLI. In particular, we propose to
interpret the intermediate layers of NLI models by visualizing the saliency of
attention and LSTM gating signals. We present several examples for which our
methods are able to reveal interesting insights and identify the critical
information contributing to the model decisions.
</summary>
    <author>
      <name>Reza Ghaeini</name>
    </author>
    <author>
      <name>Xiaoli Z. Fern</name>
    </author>
    <author>
      <name>Prasad Tadepalli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures, accepted as a short paper at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03880v1</id>
    <updated>2018-08-12T01:56:17Z</updated>
    <published>2018-08-12T01:56:17Z</published>
    <title>Parallelization does not Accelerate Convex Optimization: Adaptivity
  Lower Bounds for Non-smooth Convex Minimization</title>
    <summary>  In this paper we study the limitations of parallelization in convex
optimization. A convenient approach to study parallelization is through the
prism of \emph{adaptivity} which is an information theoretic measure of the
parallel runtime of an algorithm. Informally, adaptivity is the number of
sequential rounds an algorithm needs to make when it can execute
polynomially-many queries in parallel at every round. For combinatorial
optimization with black-box oracle access, the study of adaptivity has recently
led to exponential accelerations in parallel runtime and the natural question
is whether dramatic accelerations are achievable for convex optimization.
  Our main result is a spoiler. We show that, in general, parallelization does
not accelerate convex optimization. In particular, for the problem of
minimizing a non-smooth Lipschitz and strongly convex function with black-box
oracle access we give information theoretic lower bounds that indicate that the
number of adaptive rounds of any randomized algorithm exactly match the upper
bounds of single-query-per-round (i.e. non-parallel) algorithms.
</summary>
    <author>
      <name>Eric Balkanski</name>
    </author>
    <author>
      <name>Yaron Singer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03873v1</id>
    <updated>2018-08-11T22:28:11Z</updated>
    <published>2018-08-11T22:28:11Z</published>
    <title>A Consistent Method for Learning OOMs from Asymptotically Stationary
  Time Series Data Containing Missing Values</title>
    <summary>  In the traditional framework of spectral learning of stochastic time series
models, model parameters are estimated based on trajectories of fully recorded
observations. However, real-world time series data often contain missing
values, and worse, the distributions of missingness events over time are often
not independent of the visible process. Recently, a spectral OOM learning
algorithm for time series with missing data was introduced and proved to be
consistent, albeit under quite strong conditions. Here we refine the algorithm
and prove that the original strong conditions can be very much relaxed. We
validate our theoretical findings by numerical experiments, showing that the
algorithm can consistently handle missingness patterns whose dynamic interacts
with the visible process.
</summary>
    <author>
      <name>Tianlin Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03857v1</id>
    <updated>2018-08-11T20:15:28Z</updated>
    <published>2018-08-11T20:15:28Z</published>
    <title>Ranking with Features: Algorithm and A Graph Theoretic Analysis</title>
    <summary>  We consider the problem of ranking a set of items from pairwise comparisons
in the presence of features associated with the items. Recent works have
established that $O(n\log(n))$ samples are needed to rank well when there is no
feature information present. However, this might be sub-optimal in the presence
of associated features. We introduce a new probabilistic preference model
called feature-Bradley-Terry-Luce (f-BTL) model that generalizes the standard
BTL model to incorporate feature information. We present a new least squares
based algorithm called fBTL-LS which we show requires much lesser than
$O(n\log(n))$ pairs to obtain a good ranking -- precisely our new sample
complexity bound is of $O(\alpha\log \alpha)$, where $\alpha$ denotes the
number of `independent items' of the set, in general $\alpha &lt;&lt; n$. Our
analysis is novel and makes use of tools from classical graph matching theory
to provide tighter bounds that sheds light on the true complexity of the
ranking problem, capturing the item dependencies in terms of their feature
representations. This was not possible with earlier matrix completion based
tools used for this problem. We also prove an information theoretic lower bound
on the required sample complexity for recovering the underlying ranking, which
essentially shows the tightness of our proposed algorithms. The efficacy of our
proposed algorithms are validated through extensive experimental evaluations on
a variety of synthetic and real world datasets.
</summary>
    <author>
      <name>Aadirupa Saha</name>
    </author>
    <author>
      <name>Arun Rajkumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03856v1</id>
    <updated>2018-08-11T20:12:49Z</updated>
    <published>2018-08-11T20:12:49Z</published>
    <title>Neural Importance Sampling</title>
    <summary>  We propose to use deep neural networks for generating samples in Monte Carlo
integration. Our work is based on non-linear independent component analysis,
which we extend in numerous ways to improve performance and enable its
application to integration problems. First, we introduce piecewise-polynomial
coupling transforms that greatly increase the modeling power of individual
coupling layers. Second, we propose to preprocess the inputs of neural networks
using one-blob encoding, which stimulates localization of computation and
improves inference. Third, we derive a gradient-descent-based optimization for
the KL and the $\chi^2$ divergence for the specific application of Monte Carlo
integration with stochastic estimates of the target distribution. Our approach
enables fast and accurate inference and efficient sample generation independent
of the dimensionality of the integration domain. We demonstrate the benefits of
our approach for generating natural images and in two applications to
light-transport simulation. First, we show how to learn joint path-sampling
densities in primary sample space and how to importance sample
multi-dimensional path prefixes thereof. Second, we use our technique to
extract conditional directional densities driven by the triple product of the
rendering equation, and leverage them for path guiding. In all applications,
our approach yields on-par or higher performance at equal sample count than
competing techniques.
</summary>
    <author>
      <name>Thomas Müller</name>
    </author>
    <author>
      <name>Brian McWilliams</name>
    </author>
    <author>
      <name>Fabrice Rousselle</name>
    </author>
    <author>
      <name>Markus Gross</name>
    </author>
    <author>
      <name>Jan Novák</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05990v2</id>
    <updated>2018-08-11T19:52:59Z</updated>
    <published>2016-06-20T07:05:14Z</published>
    <title>A New Training Method for Feedforward Neural Networks Based on Geometric
  Contraction Property of Activation Functions</title>
    <summary>  We propose a new training method for a feedforward neural network having the
activation functions with the geometric contraction property. The method
consists of constructing a new functional that is less nonlinear in comparison
with the classical functional by removing the nonlinearity of the activation
function from the output layer. We validate this new method by a series of
experiments that show an improved learning speed and better classification
error.
</summary>
    <author>
      <name>Petre Birtea</name>
    </author>
    <author>
      <name>Cosmin Cernazanu-Glavan</name>
    </author>
    <author>
      <name>Alexandru Sisu</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05990v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05990v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04362v1</id>
    <updated>2018-08-11T19:43:22Z</updated>
    <published>2018-08-11T19:43:22Z</published>
    <title>A Domain Guided CNN Architecture for Predicting Age from Structural
  Brain Images</title>
    <summary>  Given the wide success of convolutional neural networks (CNNs) applied to
natural images, researchers have begun to apply them to neuroimaging data. To
date, however, exploration of novel CNN architectures tailored to neuroimaging
data has been limited. Several recent works fail to leverage the 3D structure
of the brain, instead treating the brain as a set of independent 2D slices.
Approaches that do utilize 3D convolutions rely on architectures developed for
object recognition tasks in natural 2D images. Such architectures make
assumptions about the input that may not hold for neuroimaging. For example,
existing architectures assume that patterns in the brain exhibit translation
invariance. However, a pattern in the brain may have different meaning
depending on where in the brain it is located. There is a need to explore novel
architectures that are tailored to brain images. We present two simple
modifications to existing CNN architectures based on brain image structure.
Applied to the task of brain age prediction, our network achieves a mean
absolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline
that achieves a MAE of 1.6 years. Our results suggest that lessons learned from
developing models on natural images may not directly transfer to neuroimaging
tasks. Instead, there remains a large space of unexplored questions regarding
model development in this area, whose answers may differ from conventional
wisdom.
</summary>
    <author>
      <name>Pascal Sturmfels</name>
    </author>
    <author>
      <name>Saige Rutherford</name>
    </author>
    <author>
      <name>Mike Angstadt</name>
    </author>
    <author>
      <name>Mark Peterson</name>
    </author>
    <author>
      <name>Chandra Sripada</name>
    </author>
    <author>
      <name>Jenna Wiens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Healthcare 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03843v1</id>
    <updated>2018-08-11T17:36:10Z</updated>
    <published>2018-08-11T17:36:10Z</published>
    <title>Matrix Factorization on GPUs with Memory Optimization and Approximate
  Computing</title>
    <summary>  Matrix factorization (MF) discovers latent features from observations, which
has shown great promises in the fields of collaborative filtering, data
compression, feature extraction, word embedding, etc. While many
problem-specific optimization techniques have been proposed, alternating least
square (ALS) remains popular due to its general applicability e.g. easy to
handle positive-unlabeled inputs, fast convergence and parallelization
capability. Current MF implementations are either optimized for a single
machine or with a need of a large computer cluster but still are insufficient.
This is because a single machine provides limited compute power for large-scale
data while multiple machines suffer from the network communication bottleneck.
  To address the aforementioned challenge, accelerating ALS on graphics
processing units (GPUs) is a promising direction. We propose the novel approach
in enhancing the MF efficiency via both memory optimization and approximate
computing. The former exploits GPU memory hierarchy to increase data reuse,
while the later reduces unnecessary computing without hurting the convergence
of learning algorithms. Extensive experiments on large-scale datasets show that
our solution not only outperforms the competing CPU solutions by a large margin
but also has a 2x-4x performance gain compared to the state-of-the-art GPU
solutions. Our implementations are open-sourced and publicly available.
</summary>
    <author>
      <name>Wei Tan</name>
    </author>
    <author>
      <name>Shiyu Chang</name>
    </author>
    <author>
      <name>Liana Fong</name>
    </author>
    <author>
      <name>Cheng Li</name>
    </author>
    <author>
      <name>Zijun Wang</name>
    </author>
    <author>
      <name>Liangliang Cao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11048v2</id>
    <updated>2018-08-11T17:26:17Z</updated>
    <published>2018-05-25T04:46:48Z</published>
    <title>Scalable Spectral Clustering Using Random Binning Features</title>
    <summary>  Spectral clustering is one of the most effective clustering approaches that
capture hidden cluster structures in the data. However, it does not scale well
to large-scale problems due to its quadratic complexity in constructing
similarity graphs and computing subsequent eigendecomposition. Although a
number of methods have been proposed to accelerate spectral clustering, most of
them compromise considerable information loss in the original data for reducing
computational bottlenecks. In this paper, we present a novel scalable spectral
clustering method using Random Binning features (RB) to simultaneously
accelerate both similarity graph construction and the eigendecomposition.
Specifically, we implicitly approximate the graph similarity (kernel) matrix by
the inner product of a large sparse feature matrix generated by RB. Then we
introduce a state-of-the-art SVD solver to effectively compute eigenvectors of
this large matrix for spectral clustering. Using these two building blocks, we
reduce the computational cost from quadratic to linear in the number of data
points while achieving similar accuracy. Our theoretical analysis shows that
spectral clustering via RB converges faster to the exact spectral clustering
than the standard Random Feature approximation. Extensive experiments on 8
benchmarks show that the proposed method either outperforms or matches the
state-of-the-art methods in both accuracy and runtime. Moreover, our method
exhibits linear scalability in both the number of data samples and the number
of RB features.
</summary>
    <author>
      <name>Lingfei Wu</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Ian En-Hsu Yen</name>
    </author>
    <author>
      <name>Fangli Xu</name>
    </author>
    <author>
      <name>Yinglong Xia</name>
    </author>
    <author>
      <name>Charu Aggarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD'18, Oral Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11048v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11048v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03835v1</id>
    <updated>2018-08-11T16:47:58Z</updated>
    <published>2018-08-11T16:47:58Z</published>
    <title>jLDADMM: A Java package for the LDA and DMM topic models</title>
    <summary>  In this technical report, we present jLDADMM---an easy-to-use Java toolkit
for conventional topic models. jLDADMM is released to provide alternatives for
topic modeling on normal or short texts. It provides implementations of the
Latent Dirichlet Allocation topic model and the one-topic-per-document
Dirichlet Multinomial Mixture model (i.e. mixture of unigrams), using collapsed
Gibbs sampling. In addition, jLDADMM supplies a document clustering evaluation
to compare topic models. jLDADMM is open-source and available to download at:
https://github.com/datquocnguyen/jLDADMM
</summary>
    <author>
      <name>Dat Quoc Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04530v2</id>
    <updated>2018-08-11T16:30:35Z</updated>
    <published>2016-01-18T14:31:12Z</published>
    <title>Domain based classification</title>
    <summary>  The majority of traditional classification ru les minimizing the expected
probability of error (0-1 loss) are inappropriate if the class probability
distributions are ill-defined or impossible to estimate. We argue that in such
cases class domains should be used instead of class distributions or densities
to construct a reliable decision function. Proposals are presented for some
evaluation criteria and classifier learning schemes, illustrated by an example.
</summary>
    <author>
      <name>Robert P. W. Duin</name>
    </author>
    <author>
      <name>Elzbieta Pekalska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, unpublished paper written in 2005, discussing a significant,
  still almost not studied problem. missing reference links corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09704v3</id>
    <updated>2018-08-11T14:24:07Z</updated>
    <published>2018-03-26T16:36:37Z</published>
    <title>MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time
  Series Forecasting</title>
    <summary>  Time series forecasting is ubiquitous in the modern world. Applications range
from health care to astronomy, and include climate modelling, financial trading
and monitoring of critical engineering equipment. To offer value over this
range of activities, models must not only provide accurate forecasts, but also
quantify and adjust their uncertainty over time. In this work, we directly
tackle this task with a novel, fully end-to-end deep learning method for time
series forecasting. By recasting time series forecasting as an ordinal
regression task, we develop a principled methodology to assess long-term
predictive uncertainty and describe rich multimodal, non-Gaussian behaviour,
which arises regularly in applied settings.
  Notably, our framework is a wholly general-purpose approach that requires
little to no user intervention to be used. We showcase this key feature in a
large-scale benchmark test with 45 datasets drawn from both, a wide range of
real-world application domains, as well as a comprehensive list of synthetic
maps. This wide comparison encompasses state-of-the-art methods in both the
Machine Learning and Statistics modelling literature, such as the Gaussian
Process. We find that our approach does not only provide excellent predictive
forecasts, shadowing true future values, but also allows us to infer valuable
information, such as the predictive distribution of the occurrence of critical
events of interest, accurately and reliably even over long time horizons.
</summary>
    <author>
      <name>Bernardo Pérez Orozco</name>
    </author>
    <author>
      <name>Gabriele Abbati</name>
    </author>
    <author>
      <name>Stephen Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 4 figures * This version includes a new large-scale
  experiment where we evaluate our model over 45 datasets and 4 different state
  of the art baselines</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09704v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09704v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03793v1</id>
    <updated>2018-08-11T12:16:09Z</updated>
    <published>2018-08-11T12:16:09Z</published>
    <title>Document Informed Neural Autoregressive Topic Models</title>
    <summary>  Context information around words helps in determining their actual meaning,
for example "networks" used in contexts of artificial neural networks or
biological neuron networks. Generative topic models infer topic-word
distributions, taking no or only little context into account. Here, we extend a
neural autoregressive topic model to exploit the full context information
around words in a document in a language modeling fashion. This results in an
improved performance in terms of generalization, interpretability and
applicability. We apply our modeling approach to seven data sets from various
domains and demonstrate that our approach consistently outperforms
stateof-the-art generative topic models. With the learned representations, we
show on an average a gain of 9.6% (0.57 Vs 0.52) in precision at retrieval
fraction 0.02 and 7.2% (0.582 Vs 0.543) in F1 for text categorization.
</summary>
    <author>
      <name>Pankaj Gupta</name>
    </author>
    <author>
      <name>Florian Buettner</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08958v2</id>
    <updated>2018-08-11T10:59:42Z</updated>
    <published>2018-05-23T04:30:48Z</published>
    <title>A Brand-level Ranking System with the Customized Attention-GRU Model</title>
    <summary>  In e-commerce websites like Taobao, brand is playing a more important role in
influencing users' decision of click/purchase, partly because users are now
attaching more importance to the quality of products and brand is an indicator
of quality. However, existing ranking systems are not specifically designed to
satisfy this kind of demand. Some design tricks may partially alleviate this
problem, but still cannot provide satisfactory results or may create additional
interaction cost. In this paper, we design the first brand-level ranking system
to address this problem. The key challenge of this system is how to
sufficiently exploit users' rich behavior in e-commerce websites to rank the
brands. In our solution, we firstly conduct the feature engineering
specifically tailored for the personalized brand ranking problem and then rank
the brands by an adapted Attention-GRU model containing three important
modifications. Note that our proposed modifications can also apply to many
other machine learning models on various tasks. We conduct a series of
experiments to evaluate the effectiveness of our proposed ranking model and
test the response to the brand-level ranking system from real users on a
large-scale e-commerce platform, i.e. Taobao.
</summary>
    <author>
      <name>Yu Zhu</name>
    </author>
    <author>
      <name>Junxiong Zhu</name>
    </author>
    <author>
      <name>Jie Hou</name>
    </author>
    <author>
      <name>Yongliang Li</name>
    </author>
    <author>
      <name>Beidou Wang</name>
    </author>
    <author>
      <name>Ziyu Guan</name>
    </author>
    <author>
      <name>Deng Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, 3 tables. Published in IJCAI 2018. Make some
  figures and tables more clear</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Joint Conferences on Artificial Intelligence, 2018:
  3947-3953</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.08958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04968v2</id>
    <updated>2018-08-11T06:56:39Z</updated>
    <published>2017-08-16T16:37:52Z</published>
    <title>Fault in your stars: An Analysis of Android App Reviews</title>
    <summary>  Mobile app distribution platforms such as Google Play Store allow users to
share their feedback about downloaded apps in the form of a review comment and
a corresponding star rating. Typically, the star rating ranges from one to five
stars, with one star denoting a high sense of dissatisfaction with the app and
five stars denoting a high sense of satisfaction.
  Unfortunately, due to a variety of reasons, often the star rating provided by
a user is inconsistent with the opinion expressed in the review. For example,
consider the following review for the Facebook App on Android; "Awesome App".
One would reasonably expect the rating for this review to be five stars, but
the actual rating is one star!
  Such inconsistent ratings can lead to a deflated (or inflated) overall
average rating of an app which can affect user downloads, as typically users
look at the average star ratings while making a decision on downloading an app.
Also, the app developers receive a biased feedback about the application that
does not represent ground reality. This is especially significant for small
apps with a few thousand downloads as even a small number of mismatched reviews
can bring down the average rating drastically.
  In this paper, we conducted a study on this review-rating mismatch problem.
We manually examined 8600 reviews from 10 popular Android apps and found that
20% of the ratings in our dataset were inconsistent with the review. Further,
we developed three systems; two of which were based on traditional machine
learning and one on deep learning to automatically identify reviews whose
rating did not match with the opinion expressed in the review. Our deep
learning system performed the best and had an accuracy of 92% in identifying
the correct star rating to be associated with a given review.
</summary>
    <author>
      <name>Rahul Aralikatte</name>
    </author>
    <author>
      <name>Giriprasad Sridhara</name>
    </author>
    <author>
      <name>Neelamadhav Gantayat</name>
    </author>
    <author>
      <name>Senthil Mani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3152494.3152500</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3152494.3152500" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in CoDS-COMAD 2018. Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04968v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04968v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03753v1</id>
    <updated>2018-08-11T05:05:26Z</updated>
    <published>2018-08-11T05:05:26Z</published>
    <title>MARVIN: An Open Machine Learning Corpus and Environment for Automated
  Machine Learning Primitive Annotation and Execution</title>
    <summary>  In this demo paper, we introduce the DARPA D3M program for automatic machine
learning (ML) and JPL's MARVIN tool that provides an environment to locate,
annotate, and execute machine learning primitives for use in ML pipelines.
MARVIN is a web-based application and associated back-end interface written in
Python that enables composition of ML pipelines from hundreds of primitives
from the world of Scikit-Learn, Keras, DL4J and other widely used libraries.
MARVIN allows for the creation of Docker containers that run on Kubernetes
clusters within DARPA to provide an execution environment for automated machine
learning. MARVIN currently contains over 400 datasets and challenge problems
from a wide array of ML domains including routine classification and regression
to advanced video/image classification and remote sensing.
</summary>
    <author>
      <name>Chris A. Mattmann</name>
    </author>
    <author>
      <name>Sujen Shah</name>
    </author>
    <author>
      <name>Brian Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03752v1</id>
    <updated>2018-08-11T05:05:06Z</updated>
    <published>2018-08-11T05:05:06Z</published>
    <title>Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network</title>
    <summary>  Knowledge Graph Embedding (KGE) aims to represent entities and relations of
knowledge graph in a low-dimensional continuous vector space. Recent works
focus on incorporating structural knowledge with additional information, such
as entity descriptions, relation paths and so on. However, common used
additional information usually contains plenty of noise, which makes it hard to
learn valuable representation. In this paper, we propose a new kind of
additional information, called entity neighbors, which contain both semantic
and topological features about given entity. We then develop a deep memory
network model to encode information from neighbors. Employing a gating
mechanism, representations of structure and neighbors are integrated into a
joint representation. The experimental results show that our model outperforms
existing KGE methods utilizing entity descriptions and achieves
state-of-the-art metrics on 4 datasets.
</summary>
    <author>
      <name>Kai Wang</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Xiujuan Xu</name>
    </author>
    <author>
      <name>Dan Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03749v1</id>
    <updated>2018-08-11T04:36:53Z</updated>
    <published>2018-08-11T04:36:53Z</published>
    <title>Neural Network Encapsulation</title>
    <summary>  A capsule is a collection of neurons which represents different variants of a
pattern in the network. The routing scheme ensures only certain capsules which
resemble lower counterparts in the higher layer should be activated. However,
the computational complexity becomes a bottleneck for scaling up to larger
networks, as lower capsules need to correspond to each and every higher
capsule. To resolve this limitation, we approximate the routing process with
two branches: a master branch which collects primary information from its
direct contact in the lower layer and an aide branch that replenishes master
based on pattern variants encoded in other lower capsules. Compared with
previous iterative and unsupervised routing scheme, these two branches are
communicated in a fast, supervised and one-time pass fashion. The complexity
and runtime of the model are therefore decreased by a large margin. Motivated
by the routing to make higher capsule have agreement with lower capsule, we
extend the mechanism as a compensation for the rapid loss of information in
nearby layers. We devise a feedback agreement unit to send back higher capsules
as feedback. It could be regarded as an additional regularization to the
network. The feedback agreement is achieved by comparing the optimal transport
divergence between two distributions (lower and higher capsules). Such an
add-on witnesses a unanimous gain in both capsule and vanilla networks. Our
proposed EncapNet performs favorably better against previous state-of-the-arts
on CIFAR10/100, SVHN and a subset of ImageNet.
</summary>
    <author>
      <name>Hongyang Li</name>
    </author>
    <author>
      <name>Xiaoyang Guo</name>
    </author>
    <author>
      <name>Bo Dai</name>
    </author>
    <author>
      <name>Wanli Ouyang</name>
    </author>
    <author>
      <name>Xiaogang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01371v2</id>
    <updated>2018-08-11T01:59:23Z</updated>
    <published>2018-08-03T21:44:29Z</published>
    <title>Large Scale Language Modeling: Converging on 40GB of Text in Four Hours</title>
    <summary>  Recent work has shown how to train Convolutional Neural Networks (CNNs)
rapidly on large image datasets, then transfer the knowledge gained from these
models to a variety of tasks. Following [Radford 2017], in this work, we
demonstrate similar scalability and transfer for Recurrent Neural Networks
(RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and
a 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to
train a character-level 4096-dimension multiplicative LSTM (mLSTM) for
unsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews
dataset in four hours. This runtime compares favorably with previous work
taking one month to train the same size and configuration for one epoch over
the same dataset. Converging large batch RNN models can be challenging. Recent
work has suggested scaling the learning rate as a function of batch size, but
we find that simply scaling the learning rate as a function of batch size leads
either to significantly worse convergence or immediate divergence for this
problem. We provide a learning rate schedule that allows our model to converge
with a 32k batch size. Since our model converges over the Amazon Reviews
dataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while
substantial, is commercially available, this work opens up large scale
unsupervised NLP training to most commercial applications and deep learning
researchers. A model can be trained over most public or private text datasets
overnight.
</summary>
    <author>
      <name>Raul Puri</name>
    </author>
    <author>
      <name>Robert Kirby</name>
    </author>
    <author>
      <name>Nikolai Yakovenko</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; To appear in High Performance Machine Learning Workshop
  (HPML) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06952v2</id>
    <updated>2018-08-10T23:17:06Z</updated>
    <published>2018-04-19T00:34:15Z</published>
    <title>Distributed Simulation and Distributed Inference</title>
    <summary>  Independent samples from an unknown probability distribution $\bf p$ on a
domain of size $k$ are distributed across $n$ players, with each player holding
one sample. Each player can communicate $\ell$ bits to a central referee in a
simultaneous message passing model of communication to help the referee infer a
property of the unknown $\bf p$. What is the least number of players for
inference required in the communication-starved setting of $\ell&lt;\log k$? We
begin by exploring a general "simulate-and-infer" strategy for such inference
problems where the center simulates the desired number of samples from the
unknown distribution and applies standard inference algorithms for the
collocated setting. Our first result shows that for $\ell&lt;\log k$ perfect
simulation of even a single sample is not possible. Nonetheless, we present a
Las Vegas algorithm that simulates a single sample from the unknown
distribution using $O(k/2^\ell)$ samples in expectation. As an immediate
corollary, we get that simulate-and-infer attains the optimal sample complexity
of $\Theta(k^2/2^\ell\epsilon^2)$ for learning the unknown distribution to
total variation distance $\epsilon$.
  For the prototypical testing problem of identity testing, simulate-and-infer
works with $O(k^{3/2}/2^\ell\epsilon^2)$ samples, a requirement that seems to
be inherent for all communication protocols not using any additional resources.
Interestingly, we can break this barrier using public coins. Specifically, we
exhibit a public-coin communication protocol that performs identity testing
using $O(k/\sqrt{2^\ell}\epsilon^2)$ samples. Furthermore, we show that this is
optimal up to constant factors. Our theoretically sample-optimal protocol is
easy to implement in practice. Our proof of lower bound entails showing a
contraction in $\chi^2$ distance of product distributions due to communication
constraints and may be of independent interest.
</summary>
    <author>
      <name>Jayadev Acharya</name>
    </author>
    <author>
      <name>Clément L. Canonne</name>
    </author>
    <author>
      <name>Himanshu Tyagi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improved presentation, and added a "smooth" protocol for uniformity
  testing</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03715v1</id>
    <updated>2018-08-10T21:53:51Z</updated>
    <published>2018-08-10T21:53:51Z</published>
    <title>This Time with Feeling: Learning Expressive Musical Performance</title>
    <summary>  Music generation has generally been focused on either creating scores or
interpreting them. We discuss differences between these two problems and
propose that, in fact, it may be valuable to work in the space of direct $\it
performance$ generation: jointly predicting the notes $\it and$ $\it also$
their expressive timing and dynamics. We consider the significance and
qualities of the data set needed for this. Having identified both a problem
domain and characteristics of an appropriate data set, we show an LSTM-based
recurrent network model that subjectively performs quite well on this task.
Critically, we provide generated examples. We also include feedback from
professional composers and musicians about some of these examples.
</summary>
    <author>
      <name>Sageev Oore</name>
    </author>
    <author>
      <name>Ian Simon</name>
    </author>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Includes links to urls for audio samples</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03620v1</id>
    <updated>2018-08-10T16:55:33Z</updated>
    <published>2018-08-10T16:55:33Z</published>
    <title>Ensemble Kalman Inversion: A Derivative-Free Technique For Machine
  Learning Tasks</title>
    <summary>  The standard probabilistic perspective on machine learning gives rise to
empirical risk-minimization tasks that are frequently solved by stochastic
gradient descent (SGD) and variants thereof. We present a formulation of these
tasks as classical inverse or filtering problems and, furthermore, we propose
an efficient, gradient-free algorithm for finding a solution to these problems
using ensemble Kalman inversion (EKI). Applications of our approach include
offline and online supervised learning with deep neural networks, as well as
graph-based semi-supervised learning. The essence of the EKI procedure is an
ensemble based approximate gradient descent in which derivatives are replaced
by differences from within the ensemble. We suggest several modifications to
the basic method, derived from empirically successful heuristics developed in
the context of SGD. Numerical results demonstrate wide applicability and
robustness of the proposed algorithm.
</summary>
    <author>
      <name>Nikola B. Kovachki</name>
    </author>
    <author>
      <name>Andrew M. Stuart</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20, 65L09, 65K10, 49M15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03666v3</id>
    <updated>2018-08-10T16:37:59Z</updated>
    <published>2018-03-09T19:26:11Z</published>
    <title>Standing Wave Decomposition Gaussian Process</title>
    <summary>  We propose a Standing Wave Decomposition (SWD) approximation to Gaussian
Process regression (GP). GP involves a costly matrix inversion operation, which
limits applicability to large data analysis. For an input space that can be
approximated by a grid and when correlations among data are short-ranged, the
kernel matrix inversion can be replaced by analytic diagonalization using the
SWD. We show that this approach applies to uni- and multi-dimensional input
data, extends to include longer-range correlations, and the grid can be in a
latent space and used as inducing points. Through simulations, we show that our
approximate method applied to the squared exponential kernel outperforms
existing methods in predictive accuracy per unit time in the regime where data
are plentiful. Our SWD-GP is recommended for regression analyses where there is
a relatively large amount of data and/or there are constraints on computation
time.
</summary>
    <author>
      <name>Chi-Ken Lu</name>
    </author>
    <author>
      <name>Scott Cheng-Hsin Yang</name>
    </author>
    <author>
      <name>Patrick Shafto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures; updated version includes a modified introduction
  and a new discussion on time complexity of our approximated GP method. New
  references are added. Simulation package will be announced later; updated
  with discussion of validity of perturbation treatment of Eq. (25) with added
  Fig. 6 as evidence</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03666v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03666v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10070v2</id>
    <updated>2018-08-10T16:35:17Z</updated>
    <published>2018-04-26T14:01:12Z</published>
    <title>Adaptive pooling operators for weakly labeled sound event detection</title>
    <summary>  Sound event detection (SED) methods are tasked with labeling segments of
audio recordings by the presence of active sound sources. SED is typically
posed as a supervised machine learning problem, requiring strong annotations
for the presence or absence of each sound source at every time instant within
the recording. However, strong annotations of this type are both labor- and
cost-intensive for human annotators to produce, which limits the practical
scalability of SED methods.
  In this work, we treat SED as a multiple instance learning (MIL) problem,
where training labels are static over a short excerpt, indicating the presence
or absence of sound sources but not their temporal locality. The models,
however, must still produce temporally dynamic predictions, which must be
aggregated (pooled) when comparing against static labels during training. To
facilitate this aggregation, we develop a family of adaptive pooling
operators---referred to as auto-pool---which smoothly interpolate between
common pooling operators, such as min-, max-, or average-pooling, and
automatically adapt to the characteristics of the sound sources in question. We
evaluate the proposed pooling operators on three datasets, and demonstrate that
in each case, the proposed methods outperform non-adaptive pooling operators
for static prediction, and nearly match the performance of models trained with
strong, dynamic annotations. The proposed method is evaluated in conjunction
with convolutional neural networks, but can be readily applied to any
differentiable model for time-series label prediction.
</summary>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Justin Salamon</name>
    </author>
    <author>
      <name>Juan Pablo Bello</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10070v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10070v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03604v1</id>
    <updated>2018-08-10T16:08:27Z</updated>
    <published>2018-08-10T16:08:27Z</published>
    <title>Disease Progression Timeline Estimation for Alzheimer's Disease using
  Discriminative Event Based Modeling</title>
    <summary>  Alzheimer's Disease (AD) is characterized by a cascade of biomarkers becoming
abnormal, the pathophysiology of which is very complex and largely unknown.
Event-based modeling (EBM) is a data-driven technique to estimate the sequence
in which biomarkers for a disease become abnormal based on cross-sectional
data. It can help in understanding the dynamics of disease progression and
facilitate early diagnosis and prognosis. In this work we propose a novel
discriminative approach to EBM, which is shown to be more accurate than
existing state-of-the-art EBM methods. The method first estimates for each
subject an approximate ordering of events. Subsequently, the central ordering
over all subjects is estimated by fitting a generalized Mallows model to these
approximate subject-specific orderings. We also introduce the concept of
relative distance between events which helps in creating a disease progression
timeline. Subsequently, we propose a method to stage subjects by placing them
on the estimated disease progression timeline. We evaluated the proposed method
on Alzheimer's Disease Neuroimaging Initiative (ADNI) data and compared the
results with existing state-of-the-art EBM methods. We also performed extensive
experiments on synthetic data simulating the progression of Alzheimer's
disease. The event orderings obtained on ADNI data seem plausible and are in
agreement with the current understanding of progression of AD. The proposed
patient staging algorithm performed consistently better than that of
state-of-the-art EBM methods. Event orderings obtained in simulation
experiments were more accurate than those of other EBM methods and the
estimated disease progression timeline was observed to correlate with the
timeline of actual disease progression. The results of these experiments are
encouraging and suggest that discriminative EBM is a promising approach to
disease progression modeling.
</summary>
    <author>
      <name>Vikram Venkatraghavan</name>
    </author>
    <author>
      <name>Esther E. Bron</name>
    </author>
    <author>
      <name>Wiro J. Niessen</name>
    </author>
    <author>
      <name>Stefan Klein</name>
    </author>
    <author>
      <name>for the Alzheimer's Disease Neuroimaging Initiative</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03601v1</id>
    <updated>2018-08-10T15:59:31Z</updated>
    <published>2018-08-10T15:59:31Z</published>
    <title>Using Randomness to Improve Robustness of Machine-Learning Models
  Against Evasion Attacks</title>
    <summary>  Machine learning models have been widely used in security applications such
as intrusion detection, spam filtering, and virus or malware detection.
However, it is well-known that adversaries are always trying to adapt their
attacks to evade detection. For example, an email spammer may guess what
features spam detection models use and modify or remove those features to avoid
detection. There has been some work on making machine learning models more
robust to such attacks. However, one simple but promising approach called {\em
randomization} is underexplored. This paper proposes a novel
randomization-based approach to improve robustness of machine learning models
against evasion attacks. The proposed approach incorporates randomization into
both model training time and model application time (meaning when the model is
used to detect attacks). We also apply this approach to random forest, an
existing ML method which already has some degree of randomness. Experiments on
intrusion detection and spam filtering data show that our approach further
improves robustness of random-forest method. We also discuss how this approach
can be applied to other ML models.
</summary>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Zhiyuan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03591v1</id>
    <updated>2018-08-10T15:38:14Z</updated>
    <published>2018-08-10T15:38:14Z</published>
    <title>How Complex is your classification problem? A survey on measuring
  classification complexity</title>
    <summary>  Extracting characteristics from the training datasets of classification
problems has proven effective in a number of meta-analyses. Among them,
measures of classification complexity can estimate the difficulty in separating
the data points into their expected classes. Descriptors of the spatial
distribution of the data and estimates of the shape and size of the decision
boundary are among the existent measures for this characterization. This
information can support the formulation of new data-driven pre-processing and
pattern recognition techniques, which can in turn be focused on challenging
characteristics of the problems. This paper surveys and analyzes measures which
can be extracted from the training datasets in order to characterize the
complexity of the respective classification problems. Their use in recent
literature is also reviewed and discussed, allowing to prospect opportunities
for future work in the area. Finally, descriptions are given on an R package
named Extended Complexity Library (ECoL) that implements a set of complexity
measures and is made publicly available.
</summary>
    <author>
      <name>Ana C. Lorena</name>
    </author>
    <author>
      <name>Luís P. F. Garcia</name>
    </author>
    <author>
      <name>Jens Lehmann</name>
    </author>
    <author>
      <name>Marcilio C. P. Souto</name>
    </author>
    <author>
      <name>Tin K. Ho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Survey paper, 27 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04439v1</id>
    <updated>2018-08-10T15:25:10Z</updated>
    <published>2018-08-10T15:25:10Z</published>
    <title>Image Registration and Predictive Modeling: Learning the Metric on the
  Space of Diffeomorphisms</title>
    <summary>  We present a method for metric optimization in the Large Deformation
Diffeomorphic Metric Mapping (LDDMM) framework, by treating the induced
Riemannian metric on the space of diffeomorphisms as a kernel in a machine
learning context. For simplicity, we choose the kernel Fischer Linear
Discriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters
in an Expectation-Maximization framework, we define model fidelity via the
hinge loss of the decision function. The resulting algorithm optimizes the
parameters of the LDDMM norm-inducing differential operator as a solution to a
group-wise registration and classification problem. In practice, this may lead
to a biology-aware registration, focusing its attention on the predictive task
at hand such as identifying the effects of disease. We first tested our
algorithm on a synthetic dataset, showing that our parameter selection improves
registration quality and classification accuracy. We then tested the algorithm
on 3D subcortical shapes from the Schizophrenia cohort Schizconnect. Our
Schizpohrenia-Control predictive model showed significant improvement in ROC
AUC compared to baseline parameters.
</summary>
    <author>
      <name>Ayagoz Mussabayeva</name>
    </author>
    <author>
      <name>Alexey Kroshnin</name>
    </author>
    <author>
      <name>Anvar Kurmukov</name>
    </author>
    <author>
      <name>Yulia Dodonova</name>
    </author>
    <author>
      <name>Li Shen</name>
    </author>
    <author>
      <name>Shan Cong</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Boris A. Gutman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ShapeMI workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03578v1</id>
    <updated>2018-08-10T15:06:05Z</updated>
    <published>2018-08-10T15:06:05Z</published>
    <title>Dropout is a special case of the stochastic delta rule: faster and more
  accurate deep learning</title>
    <summary>  Multi-layer neural networks have lead to remarkable performance on many kinds
of benchmark tasks in text, speech and image processing. Nonlinear parameter
estimation in hierarchical models is known to be subject to overfitting. One
approach to this overfitting and related problems (local minima, colinearity,
feature discovery etc.) is called dropout (Srivastava, et al 2014, Baldi et al
2016). This method removes hidden units with a Bernoulli random variable with
probability $p$ over updates. In this paper we will show that Dropout is a
special case of a more general model published originally in 1990 called the
stochastic delta rule ( SDR, Hanson, 1990). SDR parameterizes each weight in
the network as a random variable with mean $\mu_{w_{ij}}$ and standard
deviation $\sigma_{w_{ij}}$. These random variables are sampled on each forward
activation, consequently creating an exponential number of potential networks
with shared weights. Both parameters are updated according to prediction error,
thus implementing weight noise injections that reflect a local history of
prediction error and efficient model averaging. SDR therefore implements a
local gradient-dependent simulated annealing per weight converging to a bayes
optimal network. Tests on standard benchmarks (CIFAR) using a modified version
of DenseNet shows the SDR outperforms standard dropout in error by over 50% and
in loss by over 50%. Furthermore, the SDR implementation converges on a
solution much faster, reaching a training error of 5 in just 15 epochs with
DenseNet-40 compared to standard DenseNet-40's 94 epochs.
</summary>
    <author>
      <name>Noah Frazier-Logue</name>
    </author>
    <author>
      <name>Stephen José Hanson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures; submitted to NIPS</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03566v1</id>
    <updated>2018-08-10T14:35:38Z</updated>
    <published>2018-08-10T14:35:38Z</published>
    <title>Greedy Algorithms for Approximating the Diameter of Machine Learning
  Datasets in Multidimensional Euclidean Space</title>
    <summary>  Finding the diameter of a dataset in multidimensional Euclidean space is a
well-established problem, with well-known algorithms. However, most of the
algorithms found in the literature do not scale well with large values of data
dimension, so the time complexity grows exponentially in most cases, which
makes these algorithms impractical. Therefore, we implemented 4 simple greedy
algorithms to be used for approximating the diameter of a multidimensional
dataset; these are based on minimum/maximum l2 norms, hill climbing search,
Tabu search and Beam search approaches, respectively. The time complexity of
the implemented algorithms is near-linear, as they scale near-linearly with
data size and its dimensions. The results of the experiments (conducted on
different machine learning data sets) prove the efficiency of the implemented
algorithms and can therefore be recommended for finding the diameter to be used
by different machine learning applications when needed.
</summary>
    <author>
      <name>Ahmad B. Hassanat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03504v1</id>
    <updated>2018-08-10T12:21:47Z</updated>
    <published>2018-08-10T12:21:47Z</published>
    <title>Model Approximation Using Cascade of Tree Decompositions</title>
    <summary>  In this paper, we present a general, multistage framework for graphical model
approximation using a cascade of models such as trees. In particular, we look
at the problem of covariance matrix approximation for Gaussian distributions as
linear transformations of tree models. This is a new way to decompose the
covariance matrix. Here, we propose an algorithm which incorporates the
Cholesky factorization method to compute the decomposition matrix and thus can
approximate a simple graphical model using a cascade of the Cholesky
factorization of the tree approximation transformations. The Cholesky
decomposition enables us to achieve a tree structure factor graph at each
cascade stage of the algorithm which facilitates the use of the message passing
algorithm since the approximated graph has less loops compared to the original
graph. The overall graph is a cascade of factor graphs with each factor graph
being a tree. This is a different perspective on the approximation model, and
algorithms such as Gaussian belief propagation can be used on this overall
graph. Here, we present theoretical result that guarantees the convergence of
the proposed model approximation using the cascade of tree decompositions. In
the simulations, we look at synthetic and real data and measure the performance
of the proposed framework by comparing the KL divergences.
</summary>
    <author>
      <name>Navid Tafaghodi Khajavi</name>
    </author>
    <author>
      <name>Anthony Kuh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01280v3</id>
    <updated>2018-08-10T11:26:09Z</updated>
    <published>2018-08-03T02:27:40Z</published>
    <title>Geared Rotationally Identical and Invariant Convolutional Neural Network
  Systems</title>
    <summary>  Theorems and techniques to form different types of transformationally
invariant processing and to produce the same output quantitatively based on
either transformationally invariant operators or symmetric operations have
recently been introduced by the authors. In this study, we further propose to
compose a geared rotationally identical CNN system (GRI-CNN) with a small step
angle by connecting networks of participated processes at the first flatten
layer. Using an ordinary CNN structure as a base, requirements for constructing
a GRI-CNN include the use of either symmetric input vector or kernels with an
angle increment that can form a complete cycle as a "gearwheel". Four basic
GRI-CNN structures were studied. Each of them can produce quantitatively
identical output results when a rotation angle of the input vector is evenly
divisible by the step angle of the gear. Our study showed when an input vector
rotated with an angle does not match to a step angle, the GRI-CNN can also
produce a highly consistent result. With a design of using an ultra-fine
gear-tooth step angle (e.g., 1 degree or 0.1 degree), all four GRI-CNN systems
can be constructed virtually isotropically.
</summary>
    <author>
      <name>ShihChung B. Lo</name>
    </author>
    <author>
      <name>Ph. D.</name>
    </author>
    <author>
      <name>Matthew T. Freedman</name>
    </author>
    <author>
      <name>M. D.</name>
    </author>
    <author>
      <name>Seong K. Mun</name>
    </author>
    <author>
      <name>Ph. D.</name>
    </author>
    <author>
      <name>Heang-Ping Chan</name>
    </author>
    <author>
      <name>Ph. D</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01280v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01280v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04216v1</id>
    <updated>2018-08-10T10:09:54Z</updated>
    <published>2018-08-10T10:09:54Z</published>
    <title>Effective Unsupervised Author Disambiguation with Relative Frequencies</title>
    <summary>  This work addresses the problem of author name homonymy in the Web of
Science. Aiming for an efficient, simple and straightforward solution, we
introduce a novel probabilistic similarity measure for author name
disambiguation based on feature overlap. Using the researcher-ID available for
a subset of the Web of Science, we evaluate the application of this measure in
the context of agglomeratively clustering author mentions. We focus on a
concise evaluation that shows clearly for which problem setups and at which
time during the clustering process our approach works best. In contrast to most
other works in this field, we are sceptical towards the performance of author
name disambiguation methods in general and compare our approach to the trivial
single-cluster baseline. Our results are presented separately for each correct
clustering size as we can explain that, when treating all cases together, the
trivial baseline and more sophisticated approaches are hardly distinguishable
in terms of evaluation results. Our model shows state-of-the-art performance
for all correct clustering sizes without any discriminative training and with
tuning only one convergence parameter.
</summary>
    <author>
      <name>Tobias Backes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3197026.3197036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3197026.3197036" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of JCDL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05069v2</id>
    <updated>2018-08-10T09:40:07Z</updated>
    <published>2018-06-13T14:14:38Z</published>
    <title>Minimizing Regret in Bandit Online Optimization in Unconstrained and
  Constrained Action Spaces</title>
    <summary>  We consider online convex optimization with a zero-order oracle feedback. In
particular, the decision maker does not know the explicit representation of the
time-varying cost functions, or their gradients. At each time step, she
observes the value of the cost function evaluated at her chosen action. The
objective is to minimize the regret, that is, the difference between the sum of
the costs she accumulates and that of the static optimal action had she known
the sequence of cost functions a priori. We present a novel algorithm to
minimize the regret in both unconstrained and constrained action spaces. Our
algorithm hinges on a classical idea of one-point estimation of the gradients
of the cost functions based on their observed values. However, our choice of
the randomization introduced and consequently the proof techniques differ from
those of past work. Letting T denote the number of queries of the zero-order
oracle and n the problem dimension, the regret rate achieved is O(nT^{2/3}) for
both constrained and unconstrained action spaces. Moreover, we adapt the
presented algorithm to the setting with two-point feedback and demonstrate that
the adapted procedure achieves the theoretical lower bound on the regret.
</summary>
    <author>
      <name>Tatiana Tatarenko</name>
    </author>
    <author>
      <name>Maryam Kamgarpour</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04433v1</id>
    <updated>2018-08-10T09:30:52Z</updated>
    <published>2018-08-10T09:30:52Z</published>
    <title>Out of the Black Box: Properties of deep neural networks and their
  applications</title>
    <summary>  Deep neural networks are powerful machine learning approaches that have
exhibited excellent results on many classification tasks. However, they are
considered as black boxes and some of their properties remain to be formalized.
In the context of image recognition, it is still an arduous task to understand
why an image is recognized or not. In this study, we formalize some properties
shared by eight state-of-the-art deep neural networks in order to grasp the
principles allowing a given deep neural network to classify an image. Our
results, tested on these eight networks, show that an image can be sub-divided
into several regions (patches) responding at different degrees of probability
(local property). With the same patch, some locations in the image can answer
two (or three) orders of magnitude higher than other locations (spatial
property). Some locations are activators and others inhibitors
(activation-inhibition property). The repetition of the same patch can increase
(or decrease) the probability of recognition of an object (cumulative
property). Furthermore, we propose a new approach called Deepception that
exploits these properties to deceive a deep neural network. We obtain for the
VGG-VDD-19 neural network a fooling ratio of 88\%. Thanks to our
"Psychophysics" approach, no prior knowledge on the networks architectures is
required.
</summary>
    <author>
      <name>Nizar Ouarti</name>
    </author>
    <author>
      <name>David Carmona</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01840v2</id>
    <updated>2018-08-10T09:07:40Z</updated>
    <published>2018-03-02T19:26:16Z</published>
    <title>TACO: Learning Task Decomposition via Temporal Alignment for Control</title>
    <summary>  Many advanced Learning from Demonstration (LfD) methods consider the
decomposition of complex, real-world tasks into simpler sub-tasks. By reusing
the corresponding sub-policies within and between tasks, they provide training
data for each policy from different high-level tasks and compose them to
perform novel ones. Existing approaches to modular LfD focus either on learning
a single high-level task or depend on domain knowledge and temporal
segmentation. In contrast, we propose a weakly supervised, domain-agnostic
approach based on task sketches, which include only the sequence of sub-tasks
performed in each demonstration. Our approach simultaneously aligns the
sketches with the observed demonstrations and learns the required sub-policies.
This improves generalisation in comparison to separate optimisation procedures.
We evaluate the approach on multiple domains, including a simulated 3D robot
arm control task using purely image-based observations. The results show that
our approach performs commensurately with fully supervised approaches, while
requiring significantly less annotation effort.
</summary>
    <author>
      <name>Kyriacos Shiarlis</name>
    </author>
    <author>
      <name>Markus Wulfmeier</name>
    </author>
    <author>
      <name>Sasha Salter</name>
    </author>
    <author>
      <name>Shimon Whiteson</name>
    </author>
    <author>
      <name>Ingmar Posner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages. Published at ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01840v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01840v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04262v1</id>
    <updated>2018-08-10T08:54:31Z</updated>
    <published>2018-08-10T08:54:31Z</published>
    <title>Connectivity-Driven Brain Parcellation via Consensus Clustering</title>
    <summary>  We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
</summary>
    <author>
      <name>Anvar Kurmukov</name>
    </author>
    <author>
      <name>Ayagoz Mussabayeva</name>
    </author>
    <author>
      <name>Yulia Denisova</name>
    </author>
    <author>
      <name>Daniel Moyer</name>
    </author>
    <author>
      <name>Boris Gutman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04258v1</id>
    <updated>2018-08-10T07:16:49Z</updated>
    <published>2018-08-10T07:16:49Z</published>
    <title>Model Reduction with Memory and the Machine Learning of Dynamical
  Systems</title>
    <summary>  The well-known Mori-Zwanzig theory tells us that model reduction leads to
memory effect. For a long time, modeling the memory effect accurately and
efficiently has been an important but nearly impossible task in developing a
good reduced model. In this work, we explore a natural analogy between
recurrent neural networks and the Mori-Zwanzig formalism to establish a
systematic approach for developing reduced models with memory. Two training
models-a direct training model and a dynamically coupled training model-are
proposed and compared. We apply these methods to the Kuramoto-Sivashinsky
equation and the Navier-Stokes equation. Numerical experiments show that the
proposed method can produce reduced model with good performance on both
short-term prediction and long-term statistical properties.
</summary>
    <author>
      <name>Chao Ma</name>
    </author>
    <author>
      <name>Jianchun Wang</name>
    </author>
    <author>
      <name>Weinan E</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03425v1</id>
    <updated>2018-08-10T06:36:40Z</updated>
    <published>2018-08-10T06:36:40Z</published>
    <title>Learning and Inference on Generative Adversarial Quantum Circuits</title>
    <summary>  Quantum mechanics is inherently probabilistic in light of Born's rule. Using
quantum circuits as probabilistic generative models for classical data exploits
their superior expressibility and efficient direct sampling ability. However,
training of quantum circuits can be more challenging compared to classical
neural networks due to lack of efficient differentiable learning algorithm. We
devise an adversarial quantum-classical hybrid training scheme via coupling a
quantum circuit generator and a classical neural network discriminator
together. After training, the quantum circuit generative model can infer
missing data with quadratic speed up via amplitude amplification. We
numerically simulate the learning and inference of generative adversarial
quantum circuit using the prototypical Bars-and-Stripes dataset. Generative
adversarial quantum circuits is a fresh approach to machine learning which may
enjoy the practically useful quantum advantage on near-term quantum devices.
</summary>
    <author>
      <name>Jinfeng Zeng</name>
    </author>
    <author>
      <name>Yufeng Wu</name>
    </author>
    <author>
      <name>Jin-Guo Liu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Jiangping Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03064v2</id>
    <updated>2018-08-10T06:30:10Z</updated>
    <published>2018-08-09T09:19:34Z</published>
    <title>Gradient and Newton Boosting for Classification and Regression</title>
    <summary>  Boosting algorithms enjoy large popularity due to their high predictive
accuracy on a wide array of datasets. In this article, we argue that it is
important to distinguish between three types of statistical boosting
algorithms: gradient and Newton boosting as well as a hybrid variant of the
two. To date, both researchers and practitioners often do not discriminate
between these boosting variants. We compare the different boosting algorithms
on a wide range of real and simulated datasets for various choices of loss
functions using trees as base learners. In addition, we introduce a novel
tuning parameter for Newton boosting. We find that Newton boosting performs
substantially better than the other boosting variants for classification, and
that the novel tuning parameter is important for predictive accuracy
</summary>
    <author>
      <name>Fabio Sigrist</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03420v1</id>
    <updated>2018-08-10T05:53:12Z</updated>
    <published>2018-08-10T05:53:12Z</published>
    <title>Hierarchical Block Sparse Neural Networks</title>
    <summary>  Sparse deep neural networks(DNNs) are efficient in both memory and compute
when compared to dense DNNs. But due to irregularity in computation of sparse
DNNs, their efficiencies are much lower than that of dense DNNs on general
purpose hardwares. This leads to poor/no performance benefits for sparse DNNs.
Performance issue for sparse DNNs can be alleviated by bringing structure to
the sparsity and leveraging it for improving runtime efficiency. But such
structural constraints often lead to sparse models with suboptimal accuracies.
In this work, we jointly address both accuracy and performance of sparse DNNs
using our proposed class of neural networks called HBsNN ( Hierarchical Block
Sparse Neural Networks).
</summary>
    <author>
      <name>Dharma Teja Vooturi</name>
    </author>
    <author>
      <name>Dheevatsa Mudigree</name>
    </author>
    <author>
      <name>Sasikanth Avancha</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03408v1</id>
    <updated>2018-08-10T04:18:48Z</updated>
    <published>2018-08-10T04:18:48Z</published>
    <title>On the Convergence of AdaGrad with Momentum for Training Deep Neural
  Networks</title>
    <summary>  Adaptive stochastic gradient descent methods, such as AdaGrad, Adam,
AdaDelta, Nadam, AMSGrad, \textit{etc.}, have been demonstrated efficacious in
solving non-convex stochastic optimization, such as training deep neural
networks. However, their convergence rates have not been touched under the
non-convex stochastic circumstance except recent breakthrough results on
AdaGrad \cite{ward2018adagrad} and perturbed AdaGrad \cite{li2018convergence}.
In this paper, we propose two new adaptive stochastic gradient methods called
AdaHB and AdaNAG which integrate coordinate-wise AdaGrad with heavy ball
momentum and Nesterov accelerated gradient momentum, respectively. The
$\mathcal{O}(\frac{\log{T}}{\sqrt{T}})$ non-asymptotic convergence rates of
AdaHB and AdaNAG in non-convex stochastic setting are also jointly
characterized by leveraging a newly developed unified formulation of these two
momentum mechanisms. In particular, when momentum term vanishes we obtain
convergence rate of coordinate-wise AdaGrad in non-convex stochastic setting as
a byproduct.
</summary>
    <author>
      <name>Fangyu Zou</name>
    </author>
    <author>
      <name>Li Shen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00432v3</id>
    <updated>2018-08-10T02:35:34Z</updated>
    <published>2018-04-05T06:36:12Z</published>
    <title>Real-time Air Pollution prediction model based on Spatiotemporal Big
  data</title>
    <summary>  Air pollution is one of the most concerns for urban areas. Many countries
have constructed monitoring stations to hourly collect pollution values.
Recently, there is a research in Daegu city, Korea for real-time air quality
monitoring via sensors installed on taxis running across the whole city. The
collected data is huge (1-second interval) and in both Spatial and Temporal
format. In this paper, based on this spatiotemporal Big data, we propose a
real-time air pollution prediction model based on Convolutional Neural Network
(CNN) algorithm for image-like Spatial distribution of air pollution. Regarding
to Temporal information in the data, we introduce a combination of a Long
Short-Term Memory (LSTM) unit for time series data and a Neural Network model
for other air pollution impact factors such as weather conditions to build a
hybrid prediction model. This model is simple in architecture but still brings
good prediction ability.
</summary>
    <author>
      <name>V. Duc Le</name>
    </author>
    <author>
      <name>Sang Kyun Cha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00432v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00432v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06501v3</id>
    <updated>2018-08-10T02:33:08Z</updated>
    <published>2018-02-19T02:30:10Z</published>
    <title>Recommendations with Negative Feedback via Pairwise Deep Reinforcement
  Learning</title>
    <summary>  Recommender systems play a crucial role in mitigating the problem of
information overload by suggesting users' personalized items or services. The
vast majority of traditional recommender systems consider the recommendation
procedure as a static process and make recommendations following a fixed
strategy. In this paper, we propose a novel recommender system with the
capability of continuously improving its strategies during the interactions
with users. We model the sequential interactions between users and a
recommender system as a Markov Decision Process (MDP) and leverage
Reinforcement Learning (RL) to automatically learn the optimal strategies via
recommending trial-and-error items and receiving reinforcements of these items
from users' feedback. Users' feedback can be positive and negative and both
types of feedback have great potentials to boost recommendations. However, the
number of negative feedback is much larger than that of positive one; thus
incorporating them simultaneously is challenging since positive feedback could
be buried by negative one. In this paper, we develop a novel approach to
incorporate them into the proposed deep recommender system (DEERS) framework.
The experimental results based on real-world e-commerce data demonstrate the
effectiveness of the proposed framework. Further experiments have been
conducted to understand the importance of both positive and negative feedback
in recommendations.
</summary>
    <author>
      <name>Xiangyu Zhao</name>
    </author>
    <author>
      <name>Liang Zhang</name>
    </author>
    <author>
      <name>Zhuoye Ding</name>
    </author>
    <author>
      <name>Long Xia</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <author>
      <name>Dawei Yin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219886</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219886" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1801.00209</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06501v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06501v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03388v1</id>
    <updated>2018-08-10T01:58:27Z</updated>
    <published>2018-08-10T01:58:27Z</published>
    <title>Code-division multiplexed resistive pulse sensor networks for
  spatio-temporal detection of particles in microfluidic devices</title>
    <summary>  Spatial separation of suspended particles based on contrast in their physical
or chemical properties forms the basis of various biological assays performed
on lab-on-achip devices. To electronically acquire this information, we have
recently introduced a microfluidic sensing platform, called Microfluidic CODES,
which combines the resistive pulse sensing with the code division multiple
access in multiplexing a network of integrated electrical sensors. In this
paper, we enhance the multiplexing capacity of the Microfluidic CODES by
employing sensors that generate non-orthogonal code waveforms and a new
decoding algorithm that combines machine learning techniques with minimum
mean-squared error estimation. As a proof of principle, we fabricated a
microfluidic device with a network of 10 code-multiplexed sensors and
characterized it using cells suspended in phosphate buffer saline solution.
</summary>
    <author>
      <name>Ningquan Wang</name>
    </author>
    <author>
      <name>Ruxiu Liu</name>
    </author>
    <author>
      <name>Roozbeh Khodambashi</name>
    </author>
    <author>
      <name>Norh Asmare</name>
    </author>
    <author>
      <name>A. Fatih Sarioglu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MEMSYS.2017.7863416</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MEMSYS.2017.7863416" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE 30th International Conference on Micro Electro Mechanical
  Systems (MEMS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03364v1</id>
    <updated>2018-08-09T22:35:27Z</updated>
    <published>2018-08-09T22:35:27Z</published>
    <title>A Panel Quantile Approach to Attrition Bias in Big Data: Evidence from a
  Randomized Experiment</title>
    <summary>  This paper introduces a quantile regression estimator for panel data models
with individual heterogeneity and attrition. The method is motivated by the
fact that attrition bias is often encountered in Big Data applications. For
example, many users sign-up for the latest program but few remain active users
several months later, making the evaluation of such interventions inherently
very challenging. Building on earlier work by Hausman and Wise (1979), we
provide a simple identification strategy that leads to a two-step estimation
procedure. In the first step, the coefficients of interest in the selection
equation are consistently estimated using parametric or nonparametric methods.
In the second step, standard panel quantile methods are employed on a subset of
weighted observations. The estimator is computationally easy to implement in
Big Data applications with a large number of subjects. We investigate the
conditions under which the parameter estimator is asymptotically Gaussian and
we carry out a series of Monte Carlo simulations to investigate the finite
sample properties of the estimator. Lastly, using a simulation exercise, we
apply the method to the evaluation of a recent Time-of-Day electricity pricing
experiment inspired by the work of Aigner and Hausman (1980).
</summary>
    <author>
      <name>Matthew Harding</name>
    </author>
    <author>
      <name>Carlos Lamarche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">JEL: C21, C23, C25, C55. Keywords: Attrition; Big Data; Quantile
  regression; Individual Effects; Time-of-Day Pricing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03351v1</id>
    <updated>2018-08-09T21:36:02Z</updated>
    <published>2018-08-09T21:36:02Z</published>
    <title>Exploiting Structure for Fast Kernel Learning</title>
    <summary>  We propose two methods for exact Gaussian process (GP) inference and learning
on massive image, video, spatial-temporal, or multi-output datasets with
missing values (or "gaps") in the observed responses. The first method ignores
the gaps using sparse selection matrices and a highly effective low-rank
preconditioner is introduced to accelerate computations. The second method
introduces a novel approach to GP training whereby response values are inferred
on the gaps before explicitly training the model. We find this second approach
to be greatly advantageous for the class of problems considered. Both of these
novel approaches make extensive use of Kronecker matrix algebra to design
massively scalable algorithms which have low memory requirements. We
demonstrate exact GP inference for a spatial-temporal climate modelling problem
with 3.7 million training points as well as a video reconstruction problem with
1 billion points.
</summary>
    <author>
      <name>Trefor W. Evans</name>
    </author>
    <author>
      <name>Prasanth B. Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the proceedings of the SIAM International Conference on
  Data Mining (SDM), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06677v2</id>
    <updated>2018-08-09T20:25:43Z</updated>
    <published>2017-11-20T07:47:12Z</published>
    <title>Is prioritized sweeping the better episodic control?</title>
    <summary>  Episodic control has been proposed as a third approach to reinforcement
learning, besides model-free and model-based control, by analogy with the three
types of human memory. i.e. episodic, procedural and semantic memory. But the
theoretical properties of episodic control are not well investigated. Here I
show that in deterministic tree Markov decision processes, episodic control is
equivalent to a form of prioritized sweeping in terms of sample efficiency as
well as memory and computation demands. For general deterministic and
stochastic environments, prioritized sweeping performs better even when memory
and computation demands are restricted to be equal to those of episodic
control. These results suggest generalizations of prioritized sweeping to
partially observable environments, its combined use with function approximation
and the search for possible implementations of prioritized sweeping in brains.
</summary>
    <author>
      <name>Johanni Brea</name>
    </author>
    <link href="http://arxiv.org/abs/1711.06677v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06677v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03331v1</id>
    <updated>2018-08-09T20:08:13Z</updated>
    <published>2018-08-09T20:08:13Z</published>
    <title>The Effectiveness of Multitask Learning for Phenotyping with Electronic
  Health Records Data</title>
    <summary>  Electronic phenotyping, which is the task of ascertaining whether an
individual has a medical condition of interest by analyzing their medical
records, is a foundational task in clinical informatics. Increasingly,
electronic phenotyping is performed via supervised learning. We investigate the
effectiveness of multitask learning for phenotyping using electronic health
records (EHR) data. Multitask learning aims to improve model performance on a
target task by jointly learning additional auxiliary tasks, and has been used
to good effect in disparate areas of machine learning. However, its utility
when applied to EHR data has not been established, and prior work suggests that
its benefits are inconsistent. Here we present experiments that elucidate when
multitask learning with neural networks can improve performance for electronic
phenotyping using EHR data relative to well-tuned single task neural networks.
We find that multitask networks consistently outperform single task networks
for rare phenotypes but underperform for more common phenotypes. The effect
size increases as more auxiliary tasks are added.
</summary>
    <author>
      <name>Daisy Yi Ding</name>
    </author>
    <author>
      <name>Chloé Simpson</name>
    </author>
    <author>
      <name>Stephen Pfohl</name>
    </author>
    <author>
      <name>Dave C. Kale</name>
    </author>
    <author>
      <name>Kenneth Jung</name>
    </author>
    <author>
      <name>Nigam H. Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of an article submitted for consideration in Pacific
  Symposium on Biocomputing 2018, https://psb.stanford.edu/psb-online/, 12
  pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03327v1</id>
    <updated>2018-08-09T19:48:41Z</updated>
    <published>2018-08-09T19:48:41Z</published>
    <title>Fuzzy Clustering to Identify Clusters at Different Levels of Fuzziness:
  An Evolutionary Multi-Objective Optimization Approach</title>
    <summary>  Fuzzy clustering methods identify naturally occurring clusters in a dataset,
where the extent to which different clusters are overlapped can differ. Most
methods have a parameter to fix the level of fuzziness. However, the
appropriate level of fuzziness depends on the application at hand. This paper
presents Entropy $c$-Means (ECM), a method of fuzzy clustering that
simultaneously optimizes two contradictory objective functions, resulting in
the creation of fuzzy clusters with different levels of fuzziness. This allows
ECM to identify clusters with different degrees of overlap. ECM optimizes the
two objective functions using two multi-objective optimization methods,
Non-dominated Sorting Genetic Algorithm II (NSGA-II), and Multiobjective
Evolutionary Algorithm based on Decomposition (MOEA/D). We also propose a
method to select a suitable trade-off clustering from the Pareto front.
Experiments on challenging synthetic datasets as well as real-world datasets
show that ECM leads to better cluster detection compared to the conventional
fuzzy clustering methods as well as previously used multi-objective methods for
fuzzy clustering.
</summary>
    <author>
      <name>Avisek Gupta</name>
    </author>
    <author>
      <name>Shounak Datta</name>
    </author>
    <author>
      <name>Swagatam Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03305v1</id>
    <updated>2018-08-09T18:58:59Z</updated>
    <published>2018-08-09T18:58:59Z</published>
    <title>The Elephant in the Room</title>
    <summary>  We showcase a family of common failures of state-of-the art object detectors.
These are obtained by replacing image sub-regions by another sub-image that
contains a trained object. We call this "object transplanting". Modifying an
image in this manner is shown to have a non-local impact on object detection.
Slight changes in object position can affect its identity according to an
object detector as well as that of other objects in the image. We provide some
analysis and suggest possible reasons for the reported phenomena.
</summary>
    <author>
      <name>Amir Rosenfeld</name>
    </author>
    <author>
      <name>Richard Zemel</name>
    </author>
    <author>
      <name>John K. Tsotsos</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04444v1</id>
    <updated>2018-08-09T18:44:38Z</updated>
    <published>2018-08-09T18:44:38Z</published>
    <title>Character-Level Language Modeling with Deeper Self-Attention</title>
    <summary>  LSTMs and other RNN variants have shown strong performance on character-level
language modeling. These models are typically trained using truncated
backpropagation through time, and it is common to assume that their success
stems from their ability to remember long-term contexts. In this paper, we show
that a deep (64-layer) transformer model with fixed context outperforms RNN
variants by a large margin, achieving state of the art on two popular
benchmarks- 1.13 bits per character on text8 and 1.06 on enwik8. To get good
results at this depth, we show that it is important to add auxiliary losses,
both at intermediate network layers and intermediate sequence positions.
</summary>
    <author>
      <name>Rami Al-Rfou</name>
    </author>
    <author>
      <name>Dokook Choe</name>
    </author>
    <author>
      <name>Noah Constant</name>
    </author>
    <author>
      <name>Mandy Guo</name>
    </author>
    <author>
      <name>Llion Jones</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08898v5</id>
    <updated>2018-08-09T18:24:09Z</updated>
    <published>2018-02-24T19:23:21Z</published>
    <title>Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo</title>
    <summary>  Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from
high-dimensional distributions in Statistics and Machine learning. HMC is known
to run very efficiently in practice and its popular second-order "leapfrog"
implementation has long been conjectured to run in $d^{1/4}$ gradient
evaluations. Here we show that this conjecture is true when sampling from
strongly log-concave target distributions that satisfy a weak third-order
regularity property associated with the input data. Our regularity condition is
weaker than the Lipschitz Hessian property and allows us to show faster
convergence bounds for a much larger class of distributions than would be
possible with the usual Lipschitz Hessian constant alone. Important
distributions that satisfy our regularity condition include posterior
distributions used in Bayesian logistic regression for which the data satisfies
an "incoherence" property. Our result compares favorably with the best
available bounds for the class of strongly log-concave distributions, which
grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our
simulations on synthetic data suggest that, when our regularity condition is
satisfied, leapfrog HMC performs better than its competitors -- both in terms
of accuracy and in terms of the number of gradient evaluations it requires.
</summary>
    <author>
      <name>Oren Mangoubi</name>
    </author>
    <author>
      <name>Nisheeth K. Vishnoi</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08898v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08898v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04441v1</id>
    <updated>2018-08-09T17:55:31Z</updated>
    <published>2018-08-09T17:55:31Z</published>
    <title>Deep Morphing: Detecting bone structures in fluoroscopic X-ray images
  with prior knowledge</title>
    <summary>  We propose approaches based on deep learning to localize objects in images
when only a small training dataset is available and the images have low
quality. That applies to many problems in medical image processing, and in
particular to the analysis of fluoroscopic (low-dose) X-ray images, where the
images have low contrast. We solve the problem by incorporating high-level
information about the objects, which could be a simple geometrical model, like
a circular outline, or a more complex statistical model. A simple geometrical
representation can sufficiently describe some objects and only requires minimal
labeling. Statistical shape models can be used to represent more complex
objects. We propose computationally efficient two-stage approaches, which we
call deep morphing, for both representations by fitting the representation to
the output of a deep segmentation network.
</summary>
    <author>
      <name>Aaron Pries</name>
    </author>
    <author>
      <name>Peter J. Schreier</name>
    </author>
    <author>
      <name>Artur Lamm</name>
    </author>
    <author>
      <name>Stefan Pede</name>
    </author>
    <author>
      <name>Jürgen Schmidt</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03253v1</id>
    <updated>2018-08-09T17:35:52Z</updated>
    <published>2018-08-09T17:35:52Z</published>
    <title>Counterfactual Normalization: Proactively Addressing Dataset Shift and
  Improving Reliability Using Causal Mechanisms</title>
    <summary>  Predictive models can fail to generalize from training to deployment
environments because of dataset shift, posing a threat to model reliability and
the safety of downstream decisions made in practice. Instead of using samples
from the target distribution to reactively correct dataset shift, we use
graphical knowledge of the causal mechanisms relating variables in a prediction
problem to proactively remove relationships that do not generalize across
environments, even when these relationships may depend on unobserved variables
(violations of the "no unobserved confounders" assumption). To accomplish this,
we identify variables with unstable paths of statistical influence and remove
them from the model. We also augment the causal graph with latent
counterfactual variables that isolate unstable paths of statistical influence,
allowing us to retain stable paths that would otherwise be removed. Our
experiments demonstrate that models that remove vulnerable variables and use
estimates of the latent variables transfer better, often outperforming in the
target domain despite some accuracy loss in the training domain.
</summary>
    <author>
      <name>Adarsh Subbaswamy</name>
    </author>
    <author>
      <name>Suchi Saria</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 34th Conference on Uncertainty in Artificial
  Intelligence (UAI), 2018. Revised from print version</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03246v1</id>
    <updated>2018-08-09T17:30:48Z</updated>
    <published>2018-08-09T17:30:48Z</published>
    <title>Augmenting Physical Simulators with Stochastic Neural Networks: Case
  Study of Planar Pushing and Bouncing</title>
    <summary>  An efficient, generalizable physical simulator with universal uncertainty
estimates has wide applications in robot state estimation, planning, and
control. In this paper, we build such a simulator for two scenarios, planar
pushing and ball bouncing, by augmenting an analytical rigid-body simulator
with a neural network that learns to model uncertainty as residuals. Combining
symbolic, deterministic simulators with learnable, stochastic neural nets
provides us with expressiveness, efficiency, and generalizability
simultaneously. Our model outperforms both purely analytical and purely learned
simulators consistently on real, standard benchmarks. Compared with methods
that model uncertainty using Gaussian processes, our model runs much faster,
generalizes better to new object shapes, and is able to characterize the
complex distribution of object trajectories.
</summary>
    <author>
      <name>Anurag Ajay</name>
    </author>
    <author>
      <name>Jiajun Wu</name>
    </author>
    <author>
      <name>Nima Fazeli</name>
    </author>
    <author>
      <name>Maria Bauza</name>
    </author>
    <author>
      <name>Leslie P. Kaelbling</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <author>
      <name>Alberto Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IROS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03233v1</id>
    <updated>2018-08-09T16:56:04Z</updated>
    <published>2018-08-09T16:56:04Z</published>
    <title>OBOE: Collaborative Filtering for AutoML Initialization</title>
    <summary>  Algorithm selection and hyperparameter tuning remain two of the most
challenging tasks in machine learning. The number of machine learning
applications is growing much faster than the number of machine learning
experts, hence we see an increasing demand for efficient automation of learning
processes. Here, we introduce OBOE, an algorithm for time-constrained model
selection and hyperparameter tuning. Taking advantage of similarity between
datasets, OBOE finds promising algorithm and hyperparameter configurations
through collaborative filtering. Our system explores these models under time
constraints, so that rapid initializations can be provided to warm-start more
fine-grained optimization methods. One novel aspect of our approach is a new
heuristic for active learning in time-constrained matrix completion based on
optimal experiment design. Our experiments demonstrate that OBOE delivers
state-of-the-art performance faster than competing approaches on a test bed of
supervised learning problems.
</summary>
    <author>
      <name>Chengrun Yang</name>
    </author>
    <author>
      <name>Yuji Akimoto</name>
    </author>
    <author>
      <name>Dae Won Kim</name>
    </author>
    <author>
      <name>Madeleine Udell</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01521v2</id>
    <updated>2018-08-09T16:33:56Z</updated>
    <published>2018-07-04T11:23:57Z</published>
    <title>Curiosity Driven Exploration of Learned Disentangled Goal Spaces</title>
    <summary>  Intrinsically motivated goal exploration processes enable agents to
autonomously sample goals to explore efficiently complex environments with
high-dimensional continuous actions. They have been applied successfully to
real world robots to discover repertoires of policies producing a wide
diversity of effects. Often these algorithms relied on engineered goal spaces
but it was recently shown that one can use deep representation learning
algorithms to learn an adequate goal space in simple environments. However, in
the case of more complex environments containing multiple objects or
distractors, an efficient exploration requires that the structure of the goal
space reflects the one of the environment. In this paper we show that using a
disentangled goal space leads to better exploration performances than an
entangled goal space. We further show that when the representation is
disentangled, one can leverage it by sampling goals that maximize learning
progress in a modular manner. Finally, we show that the measure of learning
progress, used to drive curiosity-driven exploration, can be used
simultaneously to discover abstract independently controllable features of the
environment.
</summary>
    <author>
      <name>Adrien Laversanne-Finot</name>
    </author>
    <author>
      <name>Alexandre Péré</name>
    </author>
    <author>
      <name>Pierre-Yves Oudeyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The code used in the experiments is available at
  https://github.com/flowersteam/Curiosity_Driven_Goal_Exploration</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01521v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01521v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03216v1</id>
    <updated>2018-08-09T16:11:31Z</updated>
    <published>2018-08-09T16:11:31Z</published>
    <title>Data-driven polynomial chaos expansion for machine learning regression</title>
    <summary>  We present a regression technique for data driven problems based on
polynomial chaos expansion (PCE). PCE is a popular technique in the field of
uncertainty quantification (UQ), where it is typically used to replace a
runnable but expensive computational model subject to random inputs with an
inexpensive-to-evaluate polynomial function. The metamodel obtained enables a
reliable estimation of the statistics of the output, provided that a suitable
probabilistic model of the input is available.
  In classical machine learning (ML) regression settings, however, the system
is only known through observations of its inputs and output, and the interest
lies in obtaining accurate pointwise predictions of the latter. Here, we show
that a PCE metamodel purely trained on data can yield pointwise predictions
whose accuracy is comparable to that of other ML regression models, such as
neural networks and support vector machines. The comparisons are performed on
benchmark datasets available from the literature. The methodology also enables
the quantification of the output uncertainties and is robust to noise.
Furthermore, it enjoys additional desirable properties, such as good
performance for small training sets and simplicity of construction, with only
little parameter tuning required. In the presence of statistically dependent
inputs, we investigate two ways to build the PCE, and show through simulations
that one approach is superior to the other in the stated settings.
</summary>
    <author>
      <name>E. Torre</name>
    </author>
    <author>
      <name>S. Marelli</name>
    </author>
    <author>
      <name>P. Embrechts</name>
    </author>
    <author>
      <name>B. Sudret</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10668v3</id>
    <updated>2018-08-09T15:05:34Z</updated>
    <published>2018-07-27T15:06:26Z</published>
    <title>On the overfly algorithm in deep learning of neural networks</title>
    <summary>  In this paper we investigate the supervised backpropagation training of
multilayer neural networks from a dynamical systems point of view. We discuss
some links with the qualitative theory of differential equations and introduce
the overfly algorithm to tackle the local minima problem. Our approach is based
on the existence of first integrals of the generalised gradient system with
build-in dissipation.
</summary>
    <author>
      <name>Alexei Tsygvintsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10668v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10668v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03827v2</id>
    <updated>2018-08-09T14:24:47Z</updated>
    <published>2016-07-13T17:08:01Z</published>
    <title>The KIT Motion-Language Dataset</title>
    <summary>  Linking human motion and natural language is of great interest for the
generation of semantic representations of human activities as well as for the
generation of robot activities based on natural language input. However, while
there have been years of research in this area, no standardized and openly
available dataset exists to support the development and evaluation of such
systems. We therefore propose the KIT Motion-Language Dataset, which is large,
open, and extensible. We aggregate data from multiple motion capture databases
and include them in our dataset using a unified representation that is
independent of the capture system or marker set, making it easy to work with
the data regardless of its origin. To obtain motion annotations in natural
language, we apply a crowd-sourcing approach and a web-based tool that was
specifically build for this purpose, the Motion Annotation Tool. We thoroughly
document the annotation process itself and discuss gamification methods that we
used to keep annotators motivated. We further propose a novel method,
perplexity-based selection, which systematically selects motions for further
annotation that are either under-represented in our dataset or that have
erroneous annotations. We show that our method mitigates the two aforementioned
problems and ensures a systematic annotation process. We provide an in-depth
analysis of the structure and contents of our resulting dataset, which, as of
October 10, 2016, contains 3911 motions with a total duration of 11.23 hours
and 6278 annotations in natural language that contain 52,903 words. We believe
this makes our dataset an excellent choice that enables more transparent and
comparable research in this important area.
</summary>
    <author>
      <name>Matthias Plappert</name>
    </author>
    <author>
      <name>Christian Mandery</name>
    </author>
    <author>
      <name>Tamim Asfour</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1089/big.2016.0028</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1089/big.2016.0028" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 4 tables, submitted to Big Data journal, Special Issue on
  Robotics</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.03827v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03827v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08968v3</id>
    <updated>2018-08-09T14:17:45Z</updated>
    <published>2017-12-24T21:00:10Z</published>
    <title>Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</title>
    <summary>  We consider the optimization problem associated with training simple ReLU
neural networks of the form $\mathbf{x}\mapsto
\sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\}$ with respect to the
squared loss. We provide a computer-assisted proof that even if the input
distribution is standard Gaussian, even if the dimension is arbitrarily large,
and even if the target values are generated by such a network, with orthonormal
parameter vectors, the problem can still have spurious local minima once $6\le
k\le 20$. By a concentration of measure argument, this implies that in high
input dimensions, \emph{nearly all} target networks of the relevant sizes lead
to spurious local minima. Moreover, we conduct experiments which show that the
probability of hitting such local minima is quite high, and increasing with the
network size. On the positive side, mild over-parameterization appears to
drastically reduce such local minima, indicating that an over-parameterization
assumption is necessary to get a positive result in this setting.
</summary>
    <author>
      <name>Itay Safran</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08968v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08968v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04119v2</id>
    <updated>2018-08-09T13:27:52Z</updated>
    <published>2018-07-11T13:31:35Z</published>
    <title>Exploiting statistical dependencies of time series with hierarchical
  correlation reconstruction</title>
    <summary>  While we are usually focused on forecasting future values of time series, it
is often valuable to additionally predict their entire probability
distributions, e.g. to evaluate risk, Monte Carlo simulations. On example of
time series of $\approx$ 30000 Dow Jones Industrial Averages, there will be
presented application of hierarchical correlation reconstruction for this
purpose: mean-square estimating polynomial as joint density for (current value,
context), where context is for example a few previous values. Then substituting
the currently observed context and normalizing density to 1, we get predicted
probability distribution for the current value. In contrast to standard machine
learning approaches like neural networks, optimal polynomial coefficients here
can be inexpensively directly calculated, have controllable accuracy, are
unique and independent, each has a specific cumulant-like interpretation, and
such approximation using can approach complete description of any real joint
distribution - providing a perfect tool to quantitatively describe and exploit
statistical dependencies in time series. There is also discussed application
for non-stationary time series: adapting coefficients to local statistical
behavior.
</summary>
    <author>
      <name>Jarek Duda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04119v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04119v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11258v2</id>
    <updated>2018-08-09T13:12:09Z</updated>
    <published>2018-06-29T03:32:29Z</published>
    <title>Hierarchical Dirichlet Process-based Open Set Recognition</title>
    <summary>  In this paper, we propose a novel hierarchical dirichlet process-based
classification framework for open set recognition (HDP-OSR) where new
categories' samples unseen in training appear during testing. Unlike the
existing methods which deal with this problem from the perspective of
discriminative model, we reconsider this problem from the perspective of
generative model. We model each known class data in training set as a group in
hierarchical dirichlet process (HDP) while the testing set as a whole is
treated in the same way, then co-clustering all the groups under the HDP
framework. Based on the properties of HDP, our HDP-OSR does not overly depend
on training samples and can achieve adaptive change as the data changes. More
precisely, HDP-OSR can automatically reserve space for unknown categories while
it can also discover new categories, meaning it naturally adapts to the open
set recognition scenario. Furthermore, treating the testing set as a whole
makes our framework take the correlations among the testing samples into
account whereas the existing methods obviously ignore this information.
Experimental results on a set of benchmark data sets indicate the validity of
our learning framework.
</summary>
    <author>
      <name>Chuanxing Geng</name>
    </author>
    <author>
      <name>Songcan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11258v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11258v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03114v1</id>
    <updated>2018-08-09T12:34:33Z</updated>
    <published>2018-08-09T12:34:33Z</published>
    <title>Training De-Confusion: An Interactive, Network-Supported Visual Analysis
  System for Resolving Errors in Image Classification Training Data</title>
    <summary>  Convolutional neural networks gain more and more popularity in image
classification tasks since they are often even able to outperform human
classifiers. While much research has been targeted towards network architecture
optimization, the optimization of the labeled training data has not been
explicitly targeted yet. Since labeling of training data is time-consuming, it
is often performed by less experienced domain experts or even outsourced to
online services. Unfortunately, this results in labeling errors, which directly
impact the classification performance of the trained network. To overcome this
problem, we propose an interactive visual analysis system that helps to spot
and correct errors in the training dataset. For this purpose, we have
identified instance interpretation errors, class interpretation errors and
similarity errors as frequently occurring errors, which shall be resolved to
improve classification performance. After we detect these errors, users are
guided towards them through a two-step visual analysis process, in which they
can directly reassign labels to resolve the detected errors. Thus, with the
proposed visual analysis system, the user has to inspect far fewer items to
resolve labeling errors in the training dataset, and thus arrives at satisfying
training results more quickly.
</summary>
    <author>
      <name>Alex Bäuerle</name>
    </author>
    <author>
      <name>Heiko Neumann</name>
    </author>
    <author>
      <name>Timo Ropinski</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01975v2</id>
    <updated>2018-08-09T12:17:16Z</updated>
    <published>2018-08-06T16:12:04Z</published>
    <title>A Survey on Surrogate Approaches to Non-negative Matrix Factorization</title>
    <summary>  Motivated by applications in hyperspectral imaging we investigate methods for
approximating a high-dimensional non-negative matrix $\mathbf{\mathit{Y}}$ by a
product of two lower-dimensional, non-negative matrices $\mathbf{\mathit{K}}$
and $\mathbf{\mathit{X}}.$ This so-called non-negative matrix factorization is
based on defining suitable Tikhonov functionals, which combine a discrepancy
measure for $\mathbf{\mathit{Y}}\approx\mathbf{\mathit{KX}}$ with penalty terms
for enforcing additional properties of $\mathbf{\mathit{K}}$ and
$\mathbf{\mathit{X}}$. The minimization is based on alternating minimization
with respect to $\mathbf{\mathit{K}}$ or $\mathbf{\mathit{X}}$, where in each
iteration step one replaces the original Tikhonov functional by a locally
defined surrogate functional. The choice of surrogate functionals is crucial:
It should allow a comparatively simple minimization and simultaneously its
first order optimality condition should lead to multiplicative update rules,
which automatically preserve non-negativity of the iterates. We review the most
standard construction principles for surrogate functionals for Frobenius-norm
and Kullback-Leibler discrepancy measures. We extend the known surrogate
constructions by a general framework, which allows to add a large variety of
penalty terms. The paper finishes by deriving the corresponding alternating
minimization schemes explicitely and by applying these methods to MALDI imaging
data.
</summary>
    <author>
      <name>Pascal Fernsel</name>
    </author>
    <author>
      <name>Peter Maass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 6 figures. Submitted to the Vietnam Journal of Mathematics</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01975v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01975v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04189v2</id>
    <updated>2018-08-09T12:08:44Z</updated>
    <published>2018-03-12T11:07:58Z</published>
    <title>Noise2Noise: Learning Image Restoration without Clean Data</title>
    <summary>  We apply basic statistical reasoning to signal reconstruction by machine
learning -- learning to map corrupted observations to clean signals -- with a
simple and powerful conclusion: it is possible to learn to restore images by
only looking at corrupted examples, at performance at and sometimes exceeding
training using clean data, without explicit image priors or likelihood models
of the corruption. In practice, we show that a single model learns photographic
noise removal, denoising synthetic Monte Carlo images, and reconstruction of
undersampled MRI scans -- all corrupted by different processes -- based on
noisy data only.
</summary>
    <author>
      <name>Jaakko Lehtinen</name>
    </author>
    <author>
      <name>Jacob Munkberg</name>
    </author>
    <author>
      <name>Jon Hasselgren</name>
    </author>
    <author>
      <name>Samuli Laine</name>
    </author>
    <author>
      <name>Tero Karras</name>
    </author>
    <author>
      <name>Miika Aittala</name>
    </author>
    <author>
      <name>Timo Aila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final ICML 2018 version, supplemental pages included as appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03096v1</id>
    <updated>2018-08-09T11:27:11Z</updated>
    <published>2018-08-09T11:27:11Z</published>
    <title>On feature selection and evaluation of transportation mode prediction
  strategies</title>
    <summary>  Transportation modes prediction is a fundamental task for decision making in
smart cities and traffic management systems. Traffic policies designed based on
trajectory mining can save money and time for authorities and the public. It
may reduce the fuel consumption and commute time and moreover, may provide more
pleasant moments for residents and tourists. Since the number of features that
may be used to predict a user transportation mode can be substantial, finding a
subset of features that maximizes a performance measure is worth investigating.
In this work, we explore wrapper and information retrieval methods to find the
best subset of trajectory features. After finding the best classifier and the
best feature subset, our results were compared with two related papers that
applied deep learning methods and the results showed that our framework
achieved better performance. Furthermore, two types of cross-validation
approaches were investigated, and the performance results show that the random
cross-validation method provides optimistic results.
</summary>
    <author>
      <name>Mohammad Etemad</name>
    </author>
    <author>
      <name>Amilcar Soares Junior</name>
    </author>
    <author>
      <name>Stan Matwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1807.10876</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01422v3</id>
    <updated>2018-08-09T08:09:52Z</updated>
    <published>2017-08-04T08:38:20Z</published>
    <title>Exploring the Function Space of Deep-Learning Machines</title>
    <summary>  The function space of deep-learning machines is investigated by studying
growth in the entropy of functions of a given error with respect to a reference
function, realized by a deep-learning machine. Using physics-inspired methods
we study both sparsely and densely-connected architectures to discover a
layer-wise convergence of candidate functions, marked by a corresponding
reduction in entropy when approaching the reference function, gain insight into
the importance of having a large number of layers, and observe phase
transitions as the error increases.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>David Saad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.120.248301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.120.248301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">New examples of networks with ReLU activation and convolutional
  networks are included</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 120, 248301 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.01422v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01422v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06614v4</id>
    <updated>2018-08-09T07:36:18Z</updated>
    <published>2017-09-19T19:09:16Z</published>
    <title>An Analog Neural Network Computing Engine using CMOS-Compatible
  Charge-Trap-Transistor (CTT)</title>
    <summary>  An analog neural network computing engine based on CMOS-compatible
charge-trap transistor (CTT) is proposed in this paper. CTT devices are used as
analog multipliers. Compared to digital multipliers, CTT-based analog
multiplier shows significant area and power reduction. The proposed computing
engine is composed of a scalable CTT multiplier array and energy efficient
analog-digital interfaces. Through implementing the sequential analog fabric
(SAF), the engine mixed-signal interfaces are simplified and hardware overhead
remains constant regardless of the size of the array. A proof-of-concept 784 by
784 CTT computing engine is implemented using TSMC 28nm CMOS technology and
occupied 0.68mm2. The simulated performance achieves 76.8 TOPS (8-bit) with 500
MHz clock frequency and consumes 14.8 mW. As an example, we utilize this
computing engine to address a classic pattern recognition problem --
classifying handwritten digits on MNIST database and obtained a performance
comparable to state-of-the-art fully connected neural networks using 8-bit
fixed-point resolution.
</summary>
    <author>
      <name>Yuan Du</name>
    </author>
    <author>
      <name>Li Du</name>
    </author>
    <author>
      <name>Xuefeng Gu</name>
    </author>
    <author>
      <name>Jieqiong Du</name>
    </author>
    <author>
      <name>X. Shawn Wang</name>
    </author>
    <author>
      <name>Boyu Hu</name>
    </author>
    <author>
      <name>Mingzhe Jiang</name>
    </author>
    <author>
      <name>Xiaoliang Chen</name>
    </author>
    <author>
      <name>Junjie Su</name>
    </author>
    <author>
      <name>Subramanian S. Iyer</name>
    </author>
    <author>
      <name>Mau-Chung Frank Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06614v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06614v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03030v1</id>
    <updated>2018-08-09T06:19:39Z</updated>
    <published>2018-08-09T06:19:39Z</published>
    <title>Policy Optimization as Wasserstein Gradient Flows</title>
    <summary>  Policy optimization is a core component of reinforcement learning (RL), and
most existing RL methods directly optimize parameters of a policy based on
maximizing the expected total reward, or its surrogate. Though often achieving
encouraging empirical success, its underlying mathematical principle on {\em
policy-distribution} optimization is unclear. We place policy optimization into
the space of probability measures, and interpret it as Wasserstein gradient
flows. On the probability-measure space, under specified circumstances, policy
optimization becomes a convex problem in terms of distribution optimization. To
make optimization feasible, we develop efficient algorithms by numerically
solving the corresponding discrete gradient flows. Our technique is applicable
to several RL settings, and is related to many state-of-the-art
policy-optimization algorithms. Empirical results verify the effectiveness of
our framework, often obtaining better performance compared to related
algorithms.
</summary>
    <author>
      <name>Ruiyi Zhang</name>
    </author>
    <author>
      <name>Changyou Chen</name>
    </author>
    <author>
      <name>Chunyuan Li</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICML 2018; Initial version on Deep Reinforcement Learning
  Symposium at NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03265v1</id>
    <updated>2018-08-09T04:08:46Z</updated>
    <published>2018-08-09T04:08:46Z</published>
    <title>A Hybrid Recommender System for Patient-Doctor Matchmaking in Primary
  Care</title>
    <summary>  We partner with a leading European healthcare provider and design a mechanism
to match patients with family doctors in primary care. We define the
matchmaking process for several distinct use cases given different levels of
available information about patients. Then, we adopt a hybrid recommender
system to present each patient a list of family doctor recommendations. In
particular, we model patient trust of family doctors using a large-scale
dataset of consultation histories, while accounting for the temporal dynamics
of their relationships. Our proposed approach shows higher predictive accuracy
than both a heuristic baseline and a collaborative filtering approach, and the
proposed trust measure further improves model performance.
</summary>
    <author>
      <name>Qiwei Han</name>
    </author>
    <author>
      <name>Mengxin Ji</name>
    </author>
    <author>
      <name>Inigo Martinez de Rituerto de Troya</name>
    </author>
    <author>
      <name>Manas Gaur</name>
    </author>
    <author>
      <name>Leid Zejnilovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted at DSAA 2018 as a full paper, Proc. of the 5th
  IEEE International Conference on Data Science and Advanced Analytics (DSAA),
  Turin, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06896v2</id>
    <updated>2018-08-09T02:52:27Z</updated>
    <published>2018-04-17T07:00:42Z</published>
    <title>A Multi-task Selected Learning Approach for Solving New Type 3D Bin
  Packing Problem</title>
    <summary>  This paper studies a new type of 3D bin packing problem (BPP), in which a
number of cuboid-shaped items must be put into a bin one by one orthogonally.
The objective is to find a way to place these items that can minimize the
surface area of the bin. This problem is based on the fact that there is no
fixed-sized bin in many real business scenarios and the cost of a bin is
proportional to its surface area. Based on previous research on 3D BPP, the
surface area is determined by the sequence, spatial locations and orientations
of items. It is a new NP-hard combinatorial optimization problem on
unfixed-sized bin packing, for which we propose a multi-task framework based on
Selected Learning, generating the sequence and orientations of items packed
into the bin simultaneously. During training steps, Selected Learning chooses
one of loss functions derived from Deep Reinforcement Learning and Supervised
Learning corresponding to the training procedure. Numerical results show that
the method proposed significantly outperforms Lego baselines by a substantial
gain of 7.52%. Moreover, we produce large scale 3D Bin Packing order data set
for studying bin packing problems and will release it to the research
community.
</summary>
    <author>
      <name>Haoyuan Hu</name>
    </author>
    <author>
      <name>Lu Duan</name>
    </author>
    <author>
      <name>Yu Gong</name>
    </author>
    <author>
      <name>Kenny Zhu</name>
    </author>
    <author>
      <name>Xiaodong Zhang</name>
    </author>
    <author>
      <name>Yinghui Xu</name>
    </author>
    <author>
      <name>Jiangwen Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1708.05930</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06896v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06896v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03001v1</id>
    <updated>2018-08-09T02:50:24Z</updated>
    <published>2018-08-09T02:50:24Z</published>
    <title>Compressed Sensing Using Binary Matrices of Nearly Optimal Dimensions</title>
    <summary>  In this paper, we study the problem of compressed sensing using binary
measurement matrices, and $\ell_1$-norm minimization (basis pursuit) as the
recovery algorithm. We derive new upper and lower bounds on the number of
measurements to achieve robust sparse recovery with binary matrices. We
establish sufficient conditions for a column-regular binary matrix to satisfy
the robust null space property (RNSP), and show that the sparsity bounds for
robust sparse recovery obtained using the RNSP are better by a factor of $(3
\sqrt{3})/2 \approx 2.6$ compared to the restricted isometry property (RIP).
Next we derive universal lower bounds on the number of measurements that any
binary matrix needs to have in order to satisfy the weaker sufficient condition
based on the RNSP, and show that bipartite graphs of girth six are optimal.
Then we display two classes of binary matrices, namely parity check matrices of
array codes, and Euler squares, that have girth six and are nearly optimal in
the sense of almost satisfying the lower bound. In principle randomly generated
Gaussian measurement matrices are "order-optimal." So we compare the phase
transition behavior of the basis pursuit formulation using binary array code
and Gaussian matrices, and show that (i) there is essentially no difference
between the phase transition boundaries in the two cases, and (ii) the CPU time
of basis pursuit with binary matrices is hundreds of times faster than with
Gaussian matrices, and the storage requirements are less. Therefore it is
suggested that binary matrices are a viable alternative to Gaussian matrices
for compressed sensing using basis pursuit.
</summary>
    <author>
      <name>Mahsa Lotfi</name>
    </author>
    <author>
      <name>Mathukumalli Vidyasagar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05464v1</id>
    <updated>2018-08-08T23:06:43Z</updated>
    <published>2018-08-08T23:06:43Z</published>
    <title>Transfer Learning for Brain-Computer Interfaces: An Euclidean Space Data
  Alignment Approach</title>
    <summary>  Almost all EEG-based brain-computer interfaces (BCIs) need some labeled
subject-specific data to calibrate a new subject, as neural responses are
different across subjects to even the same stimulus. So, a major challenge in
developing high-performance and user-friendly BCIs is to cope with such
individual differences so that the calibration can be reduced or even
completely eliminated. This paper focuses on the latter. More specifically, we
consider an offline application scenario, in which we have unlabeled EEG trials
from a new subject, and would like to accurately label them by leveraging
auxiliary labeled EEG trials from other subjects in the same task. To
accommodate the individual differences, we propose a novel unsupervised
approach to align the EEG trials from different subjects in the Euclidean space
to make them more consistent. It has three desirable properties: 1) the aligned
trial lie in the Euclidean space, which can be used by any Euclidean space
signal processing and machine learning approach; 2) it can be computed very
efficiently; and, 3) it does not need any labeled trials from the new subject.
Experiments on motor imagery and event-related potentials demonstrated the
effectiveness and efficiency of our approach.
</summary>
    <author>
      <name>He He</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00929v1</id>
    <updated>2018-08-08T23:01:43Z</updated>
    <published>2018-08-08T23:01:43Z</published>
    <title>EEG-Based Driver Drowsiness Estimation Using Convolutional Neural
  Networks</title>
    <summary>  Deep learning, including convolutional neural networks (CNNs), has started
finding applications in brain-computer interfaces (BCIs). However, so far most
such approaches focused on BCI classification problems. This paper extends
EEGNet, a 3-layer CNN model for BCI classification, to BCI regression, and also
utilizes a novel spectral meta-learner for regression (SMLR) approach to
aggregate multiple EEGNets for improved performance. Our model uses the power
spectral density (PSD) of EEG signals as the input. Compared with raw EEG
inputs, the PSD inputs can reduce the computational cost significantly, yet
achieve much better regression performance. Experiments on driver drowsiness
estimation from EEG signals demonstrate the outstanding performance of our
approach.
</summary>
    <author>
      <name>Yuqi Cui</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05853v1</id>
    <updated>2018-08-08T22:52:12Z</updated>
    <published>2018-08-08T22:52:12Z</published>
    <title>Transfer Learning Enhanced Common Spatial Pattern Filtering for Brain
  Computer Interfaces (BCIs): Overview and a New Approach</title>
    <summary>  The electroencephalogram (EEG) is the most widely used input for brain
computer interfaces (BCIs), and common spatial pattern (CSP) is frequently used
to spatially filter it to increase its signal-to-noise ratio. However, CSP is a
supervised filter, which needs some subject-specific calibration data to
design. This is time-consuming and not user-friendly. A promising approach for
shortening or even completely eliminating this calibration session is transfer
learning, which leverages relevant data or knowledge from other subjects or
tasks. This paper reviews three existing approaches for incorporating transfer
learning into CSP, and also proposes a new transfer learning enhanced CSP
approach. Experiments on motor imagery classification demonstrate their
effectiveness. Particularly, our proposed approach achieves the best
performance when the number of target domain calibration samples is small.
</summary>
    <author>
      <name>He He</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04244v1</id>
    <updated>2018-08-08T22:39:46Z</updated>
    <published>2018-08-08T22:39:46Z</published>
    <title>Affect Estimation in 3D Space Using Multi-Task Active Learning for
  Regression</title>
    <summary>  Acquisition of labeled training samples for affective computing is usually
costly and time-consuming, as affects are intrinsically subjective, subtle and
uncertain, and hence multiple human assessors are needed to evaluate each
affective sample. Particularly, for affect estimation in the 3D space of
valence, arousal and dominance, each assessor has to perform the evaluations in
three dimensions, which makes the labeling problem even more challenging. Many
sophisticated machine learning approaches have been proposed to reduce the data
labeling requirement in various other domains, but so far few have considered
affective computing. This paper proposes two multi-task active learning for
regression approaches, which select the most beneficial samples to label, by
considering the three affect primitives simultaneously. Experimental results on
the VAM corpus demonstrated that our optimal sample selection approaches can
result in better estimation performance than random selection and several
traditional single-task active learning approaches. Thus, they can help
alleviate the data labeling problem in affective computing, i.e., better
estimation performance can be obtained from fewer labeling queries.
</summary>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06533v1</id>
    <updated>2018-08-08T22:39:13Z</updated>
    <published>2018-08-08T22:39:13Z</published>
    <title>Spatial Filtering for Brain Computer Interfaces: A Comparison between
  the Common Spatial Pattern and Its Variant</title>
    <summary>  The electroencephalogram (EEG) is the most popular form of input for brain
computer interfaces (BCIs). However, it can be easily contaminated by various
artifacts and noise, e.g., eye blink, muscle activities, powerline noise, etc.
Therefore, the EEG signals are often filtered both spatially and temporally to
increase the signal-to-noise ratio before they are fed into a machine learning
algorithm for recognition. This paper considers spatial filtering,
particularly, the common spatial pattern (CSP) filters for EEG classification.
In binary classification, CSP seeks a set of filters to maximize the variance
for one class while minimizing it for the other. We first introduce the
traditional solution, and then a new solution based on a slightly different
objective function. We performed comprehensive experiments on motor imagery to
compare the two approaches, and found that generally the traditional CSP
solution still gives better results. We also showed that adding regularization
to the covariance matrices can improve the final classification performance, no
matter which objective function is used.
</summary>
    <author>
      <name>He He</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02956v1</id>
    <updated>2018-08-08T22:29:45Z</updated>
    <published>2018-08-08T22:29:45Z</published>
    <title>Feature Dimensionality Reduction for Video Affect Classification: A
  Comparative Study</title>
    <summary>  Affective computing has become a very important research area in
human-machine interaction. However, affects are subjective, subtle, and
uncertain. So, it is very difficult to obtain a large number of labeled
training samples, compared with the number of possible features we could
extract. Thus, dimensionality reduction is critical in affective computing.
This paper presents our preliminary study on dimensionality reduction for
affect classification. Five popular dimensionality reduction approaches are
introduced and compared. Experiments on the DEAP dataset showed that no
approach can universally outperform others, and performing classification using
the raw features directly may not always be a bad choice.
</summary>
    <author>
      <name>Chenfeng Guo</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04245v1</id>
    <updated>2018-08-08T22:29:19Z</updated>
    <published>2018-08-08T22:29:19Z</published>
    <title>Active Learning for Regression Using Greedy Sampling</title>
    <summary>  Regression problems are pervasive in real-world applications. Generally a
substantial amount of labeled samples are needed to build a regression model
with good generalization ability. However, many times it is relatively easy to
collect a large number of unlabeled samples, but time-consuming or expensive to
label them. Active learning for regression (ALR) is a methodology to reduce the
number of labeled samples, by selecting the most beneficial ones to label,
instead of random selection. This paper proposes two new ALR approaches based
on greedy sampling (GS). The first approach (GSy) selects new samples to
increase the diversity in the output space, and the second (iGS) selects new
samples to increase the diversity in both input and output spaces. Extensive
experiments on 12 UCI and CMU StatLib datasets from various domains, and on 15
subjects on EEG-based driver drowsiness estimation, verified their
effectiveness and robustness.
</summary>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <author>
      <name>Chin-Teng Lin</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01069v2</id>
    <updated>2018-08-08T22:17:25Z</updated>
    <published>2018-07-03T10:25:26Z</published>
    <title>Adversarial Robustness Toolbox v0.3.0</title>
    <summary>  Adversarial examples have become an indisputable threat to the security of
modern AI systems based on deep neural networks (DNNs). The Adversarial
Robustness Toolbox (ART) is a Python library designed to support researchers
and developers in creating novel defence techniques, as well as in deploying
practical defences of real-world AI systems. Researchers can use ART to
benchmark novel defences against the state-of-the-art. For developers, the
library provides interfaces which support the composition of comprehensive
defence systems using individual methods as building blocks. The Adversarial
Robustness Toolbox supports machine learning models (and deep neural networks
(DNNs) specifically) implemented in any of the most popular deep learning
frameworks (TensorFlow, Keras, PyTorch and MXNet). Currently, the library is
primarily intended to improve the adversarial robustness of visual recognition
systems, however, future releases that will comprise adaptations to other data
modes (such as speech, text or time series) are envisioned. The ART source code
is released (https://github.com/IBM/adversarial-robustness-toolbox) under an
MIT license. The release includes code examples and extensive documentation
(http://adversarial-robustness-toolbox.readthedocs.io) to help researchers and
developers get quickly started.
</summary>
    <author>
      <name>Maria-Irina Nicolae</name>
    </author>
    <author>
      <name>Mathieu Sinn</name>
    </author>
    <author>
      <name>Minh Ngoc Tran</name>
    </author>
    <author>
      <name>Ambrish Rawat</name>
    </author>
    <author>
      <name>Martin Wistuba</name>
    </author>
    <author>
      <name>Valentina Zantedeschi</name>
    </author>
    <author>
      <name>Nathalie Baracaldo</name>
    </author>
    <author>
      <name>Bryant Chen</name>
    </author>
    <author>
      <name>Heiko Ludwig</name>
    </author>
    <author>
      <name>Ian M. Molloy</name>
    </author>
    <author>
      <name>Ben Edwards</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04339v2</id>
    <updated>2018-08-08T21:28:29Z</updated>
    <published>2018-01-12T22:13:48Z</published>
    <title>Estimating the Number of Connected Components in a Graph via Subgraph
  Sampling</title>
    <summary>  Learning properties of large graphs from samples has been an important
problem in statistical network analysis since the early work of Goodman
\cite{Goodman1949} and Frank \cite{Frank1978}. We revisit a problem formulated
by Frank \cite{Frank1978} of estimating the number of connected components in a
large graph based on the subgraph sampling model, in which we randomly sample a
subset of the vertices and observe the induced subgraph. The key question is
whether accurate estimation is achievable in the \emph{sublinear} regime where
only a vanishing fraction of the vertices are sampled. We show that it is
impossible if the parent graph is allowed to contain high-degree vertices or
long induced cycles. For the class of chordal graphs, where induced cycles of
length four or above are forbidden, we characterize the optimal sample
complexity within constant factors and construct linear-time estimators that
provably achieve these bounds. This significantly expands the scope of previous
results which have focused on unbiased estimators and special classes of graphs
such as forests or cliques.
  Both the construction and the analysis of the proposed methodology rely on
combinatorial properties of chordal graphs and identities of induced subgraph
counts. They, in turn, also play a key role in proving minimax lower bounds
based on construction of random instances of graphs with matching structures of
small subgraphs.
</summary>
    <author>
      <name>Jason M. Klusowski</name>
    </author>
    <author>
      <name>Yihong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04339v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04339v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62D05, 62C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01290v2</id>
    <updated>2018-08-08T21:27:08Z</updated>
    <published>2018-01-04T09:50:50Z</published>
    <title>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
  Learning with a Stochastic Actor</title>
    <summary>  Model-free deep reinforcement learning (RL) algorithms have been demonstrated
on a range of challenging decision making and control tasks. However, these
methods typically suffer from two major challenges: very high sample complexity
and brittle convergence properties, which necessitate meticulous hyperparameter
tuning. Both of these challenges severely limit the applicability of such
methods to complex, real-world domains. In this paper, we propose soft
actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum
entropy reinforcement learning framework. In this framework, the actor aims to
maximize expected reward while also maximizing entropy. That is, to succeed at
the task while acting as randomly as possible. Prior deep RL methods based on
this framework have been formulated as Q-learning methods. By combining
off-policy updates with a stable stochastic actor-critic formulation, our
method achieves state-of-the-art performance on a range of continuous control
benchmark tasks, outperforming prior on-policy and off-policy methods.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving very similar performance across
different random seeds.
</summary>
    <author>
      <name>Tuomas Haarnoja</name>
    </author>
    <author>
      <name>Aurick Zhou</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code:
  github.com/haarnoja/sac</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02240v2</id>
    <updated>2018-08-08T21:16:43Z</updated>
    <published>2018-08-07T07:49:25Z</published>
    <title>Speeding Up Distributed Gradient Descent by Utilizing Non-persistent
  Stragglers</title>
    <summary>  Distributed gradient descent (DGD) is an efficient way of implementing
gradient descent (GD), especially for large data sets, by dividing the
computation tasks into smaller subtasks and assigning to different computing
servers (CSs) to be executed in parallel. In standard parallel execution,
per-iteration waiting time is limited by the execution time of the straggling
servers. Coded DGD techniques have been introduced recently, which can tolerate
straggling servers via assigning redundant computation tasks to the CSs. In
most of the existing DGD schemes, either with coded computation or coded
communication, the non-straggling CSs transmit one message per iteration once
they complete all their assigned computation tasks. However, although the
straggling servers cannot complete all their assigned tasks, they are often
able to complete a certain portion of them. In this paper, we allow multiple
transmissions from each CS at each iteration in order to make sure a maximum
number of completed computations can be reported to the aggregating server
(AS), including the straggling servers. We numerically show that the average
completion time per iteration can be reduced significantly by slightly
increasing the communication load per server.
</summary>
    <author>
      <name>Emre Ozfatura</name>
    </author>
    <author>
      <name>Deniz Gunduz</name>
    </author>
    <author>
      <name>Sennur Ulukus</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02941v1</id>
    <updated>2018-08-08T21:14:07Z</updated>
    <published>2018-08-08T21:14:07Z</published>
    <title>On the Convergence of A Class of Adam-Type Algorithms for Non-Convex
  Optimization</title>
    <summary>  This paper studies a class of adaptive gradient based momentum algorithms
that update the search directions and learning rates simultaneously using past
gradients. This class, which we refer to as the "Adam-type", includes the
popular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their
popularity in training deep neural networks, the convergence of these
algorithms for solving nonconvex problems remains an open question. This paper
provides a set of mild sufficient conditions that guarantee the convergence for
the Adam-type methods. We prove that under our derived conditions, these
methods can achieve the convergence rate of order $O(\log{T}/\sqrt{T})$ for
nonconvex stochastic optimization. We show the conditions are essential in the
sense that violating them may make the algorithm diverge. Moreover, we propose
and analyze a class of (deterministic) incremental adaptive gradient
algorithms, which has the same $O(\log{T}/\sqrt{T})$ convergence rate. Our
study could also be extended to a broader class of adaptive gradient methods in
machine learning and optimization.
</summary>
    <author>
      <name>Xiangyi Chen</name>
    </author>
    <author>
      <name>Sijia Liu</name>
    </author>
    <author>
      <name>Ruoyu Sun</name>
    </author>
    <author>
      <name>Mingyi Hong</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02939v1</id>
    <updated>2018-08-08T20:59:26Z</updated>
    <published>2018-08-08T20:59:26Z</published>
    <title>Towards Learning Fine-Grained Disentangled Representations from Speech</title>
    <summary>  Learning disentangled representations of high-dimensional data is currently
an active research area. However, compared to the field of computer vision,
less work has been done for speech processing. In this paper, we provide a
review of two representative efforts on this topic and propose the novel
concept of fine-grained disentangled speech representation learning.
</summary>
    <author>
      <name>Yuan Gong</name>
    </author>
    <author>
      <name>Christian Poellabauer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02932v1</id>
    <updated>2018-08-08T20:40:15Z</updated>
    <published>2018-08-08T20:40:15Z</published>
    <title>Nonparametric Gaussian mixture models for the multi-armed contextual
  bandit</title>
    <summary>  The multi-armed bandit is a sequential allocation task where an agent must
learn a policy that maximizes long term payoff, where only the reward of the
played arm is observed at each iteration. In the stochastic setting, the reward
for each action is generated from an unknown distribution, which depends on a
given 'context', available at each interaction with the world. Thompson
sampling is a generative, interpretable multi-armed bandit algorithm that has
been shown both to perform well in practice, and to enjoy optimality properties
for certain reward functions. Nevertheless, Thompson sampling requires sampling
from parameter posteriors and calculation of expected rewards, which are
possible for a very limited choice of distributions. We here extend Thompson
sampling to more complex scenarios by adopting a very flexible set of reward
distributions: nonparametric Gaussian mixture models. The generative process of
Bayesian nonparametric mixtures naturally aligns with the Bayesian modeling of
multi-armed bandits. This allows for the implementation of an efficient and
flexible Thompson sampling algorithm: the nonparametric model autonomously
determines its complexity in an online fashion, as it observes new rewards for
the played arms. We show how the proposed method sequentially learns the
nonparametric mixture model that best approximates the true underlying reward
distribution. Our contribution is valuable for practical scenarios, as it
avoids stringent model specifications, and yet attains reduced regret.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03162v2</id>
    <updated>2018-08-08T20:20:27Z</updated>
    <published>2017-09-10T19:58:34Z</published>
    <title>Bayesian bandits: balancing the exploration-exploitation tradeoff via
  double sampling</title>
    <summary>  Reinforcement learning studies how to balance exploration and exploitation in
real-world systems, optimizing interactions with the world while simultaneously
learning how the world operates. One general class of algorithms for such
learning is the multi-armed bandit setting. Randomized probability matching,
based upon the Thompson sampling approach introduced in the 1930s, has recently
been shown to perform well and to enjoy provable optimality properties. It
permits generative, interpretable modeling in a Bayesian setting, where prior
knowledge is incorporated, and the computed posteriors naturally capture the
full state of knowledge. In this work, we harness the information contained in
the Bayesian posterior and estimate its sufficient statistics via sampling. In
several application domains, for example in health and medicine, each
interaction with the world can be expensive and invasive, whereas drawing
samples from the model is relatively inexpensive. Exploiting this viewpoint, we
develop a double sampling technique driven by the uncertainty in the learning
process: it favors exploitation when certain about the properties of each arm,
exploring otherwise. The proposed algorithm does not make any distributional
assumption and it is applicable to complex reward distributions, as long as
Bayesian posterior updates are computable. Utilizing the estimated posterior
sufficient statistics, double sampling autonomously balances the
exploration-exploitation tradeoff to make better informed decisions. We
empirically show its reduced cumulative regret when compared to
state-of-the-art alternatives in representative bandit settings.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03162v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03162v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03163v2</id>
    <updated>2018-08-08T20:16:40Z</updated>
    <published>2017-09-10T19:58:44Z</published>
    <title>Variational inference for the multi-armed contextual bandit</title>
    <summary>  In many biomedical, science, and engineering problems, one must sequentially
decide which action to take next so as to maximize rewards. One general class
of algorithms for optimizing interactions with the world, while simultaneously
learning how the world operates, is the multi-armed bandit setting and, in
particular, the contextual bandit case. In this setting, for each executed
action, one observes rewards that are dependent on a given 'context', available
at each interaction with the world. The Thompson sampling algorithm has
recently been shown to enjoy provable optimality properties for this set of
problems, and to perform well in real-world settings. It facilitates generative
and interpretable modeling of the problem at hand. Nevertheless, the design and
complexity of the model limit its application, since one must both sample from
the distributions modeled and calculate their expected rewards. We here show
how these limitations can be overcome using variational inference to
approximate complex models, applying to the reinforcement learning case
advances developed for the inference case in the machine learning community
over the past two decades. We consider contextual multi-armed bandit
applications where the true reward distribution is unknown and complex, which
we approximate with a mixture model whose parameters are inferred via
variational inference. We show how the proposed variational Thompson sampling
approach is accurate in approximating the true distribution, and attains
reduced regrets even with complex reward distributions. The proposed algorithm
is valuable for practical scenarios where restrictive modeling assumptions are
undesirable.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Twenty-First International Conference on
  Artificial Intelligence and Statistics, PMLR 84:698-706, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.03163v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03163v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05062v2</id>
    <updated>2018-08-08T19:36:06Z</updated>
    <published>2018-01-15T22:59:17Z</published>
    <title>Multi-Label Learning from Medical Plain Text with Convolutional Residual
  Models</title>
    <summary>  Predicting diagnoses from Electronic Health Records (EHRs) is an important
medical application of multi-label learning. We propose a convolutional
residual model for multi-label classification from doctor notes in EHR data. A
given patient may have multiple diagnoses, and therefore multi-label learning
is required. We employ a Convolutional Neural Network (CNN) to encode plain
text into a fixed-length sentence embedding vector. Since diagnoses are
typically correlated, a deep residual network is employed on top of the CNN
encoder, to capture label (diagnosis) dependencies and incorporate information
directly from the encoded sentence vector. A real EHR dataset is considered,
and we compare the proposed model with several well-known baselines, to predict
diagnoses based on doctor notes. Experimental results demonstrate the
superiority of the proposed convolutional residual model.
</summary>
    <author>
      <name>Xinyuan Zhang</name>
    </author>
    <author>
      <name>Ricardo Henao</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <author>
      <name>Yitong Li</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Healthcare 2018 spotlight paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05062v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05062v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09386v5</id>
    <updated>2018-08-08T18:58:11Z</updated>
    <published>2018-07-24T23:23:49Z</published>
    <title>On the Randomized Complexity of Minimizing a Convex Quadratic Function</title>
    <summary>  Minimizing a convex, quadratic objective is a fundamental problem in machine
learning and optimization. In this work, we study prove information-theoretic,
gradient query complexity lower bounds for minimizing convex quadratic
functions, which, unlike prior works, apply even for randomized algorithms.
Specifically, we construct a distribution over quadratic functions that
witnesses lower bounds which match those known for deterministic algorithms, up
to multiplicative constants. The distribution which witnesses our lower bound
is in fact quite benign: it is both closed form, and derived from classical
ensembles in random matrix theory. We believe that our construction constitutes
a plausible "average case" setting, and thus provides compelling evidence that
the worst case and average case complexity of convex-quadratic optimization are
essentially identical.
</summary>
    <author>
      <name>Max Simchowitz</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09386v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09386v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03920v3</id>
    <updated>2018-08-08T18:21:31Z</updated>
    <published>2018-06-11T11:36:21Z</published>
    <title>Convergence Rates for Projective Splitting</title>
    <summary>  Projective splitting is a family of methods for solving inclusions involving
sums of maximal monotone operators. First introduced by Eckstein and Svaiter in
2008, these methods have enjoyed significant innovation in recent years,
becoming one of the most flexible operator splitting frameworks available.
While weak convergence of the iterates to a solution has been established,
there have been few attempts to study convergence rates of projective
splitting. The purpose of this paper is to do so under various assumptions. To
this end, there are three main contributions. First, in the context of convex
optimization, we establish an $O(1/k)$ ergodic function convergence rate.
Second, for strongly monotone inclusions, strong convergence is established as
well as an ergodic $O(1/\sqrt{k})$ convergence rate for the distance of the
iterates to the solution. Finally, for inclusions featuring strong monotonicity
and cocoercivity, linear convergence is established.
</summary>
    <author>
      <name>Patrick R. Johnstone</name>
    </author>
    <author>
      <name>Jonathan Eckstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version adds references to the extragradient method</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03920v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03920v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07043v6</id>
    <updated>2018-08-08T18:14:49Z</updated>
    <published>2018-03-19T17:15:43Z</published>
    <title>Projective Splitting with Forward Steps: Asynchronous and
  Block-Iterative Operator Splitting</title>
    <summary>  This work is concerned with the classical problem of finding a zero of a sum
of maximal monotone operators. For the projective splitting framework recently
proposed by Combettes and Eckstein, we show how to replace the fundamental
subproblem calculation using a backward step with one based on two forward
steps. The resulting algorithms have the same kind of coordination procedure
and can be implemented in the same block-iterative and potentially distributed
and asynchronous manner, but may perform backward steps on some operators and
forward steps on others. Prior algorithms in the projective splitting family
have used only backward steps. Forward steps can be used for any
Lipschitz-continuous operators provided the stepsize is bounded by the inverse
of the Lipschitz constant. If the Lipschitz constant is unknown, a simple
backtracking linesearch procedure may be used. For affine operators, the
stepsize can be chosen adaptively without knowledge of the Lipschitz constant
and without any additional forward steps. We close the paper by empirically
studying the performance of several kinds of splitting algorithms on the lasso
problem.
</summary>
    <author>
      <name>Patrick R. Johnstone</name>
    </author>
    <author>
      <name>Jonathan Eckstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version adds references to the extragradient method</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07043v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07043v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02871v1</id>
    <updated>2018-08-08T17:26:37Z</updated>
    <published>2018-08-08T17:26:37Z</published>
    <title>Random directions stochastic approximation with deterministic
  perturbations</title>
    <summary>  We introduce deterministic perturbation schemes for the recently proposed
random directions stochastic approximation (RDSA) [17], and propose new
first-order and second-order algorithms. In the latter case, these are the
first second-order algorithms to incorporate deterministic perturbations. We
show that the gradient and/or Hessian estimates in the resulting algorithms
with deterministic perturbations are asymptotically unbiased, so that the
algorithms are provably convergent. Furthermore, we derive convergence rates to
establish the superiority of the first-order and second-order algorithms, for
the special case of a convex and quadratic optimization problem, respectively.
Numerical experiments are used to validate the theoretical results.
</summary>
    <author>
      <name>Prashanth L A</name>
    </author>
    <author>
      <name>Shalabh Bhatnagar</name>
    </author>
    <author>
      <name>Nirav Bhavsar</name>
    </author>
    <author>
      <name>Michael Fu</name>
    </author>
    <author>
      <name>Steven I. Marcus</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02838v1</id>
    <updated>2018-08-08T15:51:18Z</updated>
    <published>2018-08-08T15:51:18Z</published>
    <title>On the Effect of Task-to-Worker Assignment in Distributed Computing
  Systems with Stragglers</title>
    <summary>  We study the expected completion time of some recently proposed algorithms
for distributed computing which redundantly assign computing tasks to multiple
machines in order to tolerate a certain number of machine failures. We
analytically show that not only the amount of redundancy but also the
task-to-machine assignments affect the latency in a distributed system. We
study systems with a fixed number of computing tasks that are split in possibly
overlapping batches, and independent exponentially distributed machine service
times. We show that, for such systems, the uniform replication of non-
overlapping (disjoint) batches of computing tasks achieves the minimum expected
computing time.
</summary>
    <author>
      <name>Amir Behrouzi-Far</name>
    </author>
    <author>
      <name>Emina Soljanin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 56th Annual Allerton Conference on Communication,
  Control, and Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02822v1</id>
    <updated>2018-08-08T15:23:14Z</updated>
    <published>2018-08-08T15:23:14Z</published>
    <title>Backprop Evolution</title>
    <summary>  The back-propagation algorithm is the cornerstone of deep learning. Despite
its importance, few variations of the algorithm have been attempted. This work
presents an approach to discover new variations of the back-propagation
equation. We use a domain specific lan- guage to describe update equations as a
list of primitive functions. An evolution-based method is used to discover new
propagation rules that maximize the generalization per- formance after a few
epochs of training. We find several update equations that can train faster with
short training times than standard back-propagation, and perform similar as
standard back-propagation at convergence.
</summary>
    <author>
      <name>Maximilian Alber</name>
    </author>
    <author>
      <name>Irwan Bello</name>
    </author>
    <author>
      <name>Barret Zoph</name>
    </author>
    <author>
      <name>Pieter-Jan Kindermans</name>
    </author>
    <author>
      <name>Prajit Ramachandran</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02814v1</id>
    <updated>2018-08-08T15:15:29Z</updated>
    <published>2018-08-08T15:15:29Z</published>
    <title>Highly Accelerated Multishot EPI through Synergistic Combination of
  Machine Learning and Joint Reconstruction</title>
    <summary>  Purpose: To introduce a combined machine learning (ML) and physics-based
image reconstruction framework that enables navigator-free, highly accelerated
multishot echo planar imaging (msEPI), and demonstrate its application in
high-resolution structural imaging.
  Methods: Singleshot EPI is an efficient encoding technique, but does not lend
itself well to high-resolution imaging due to severe distortion artifacts and
blurring. While msEPI can mitigate these artifacts, high-quality msEPI has been
elusive because of phase mismatch arising from shot-to-shot physiological
variations which disrupt the combination of the multiple-shot data into a
single image. We employ Deep Learning to obtain an interim magnitude-valued
image with minimal artifacts, which permits estimation of image phase
variations due to shot-to-shot physiological changes. These variations are then
included in a Joint Virtual Coil Sensitivity Encoding (JVC-SENSE)
reconstruction to utilize data from all shots and improve upon the ML solution.
  Results: Our combined ML + physics approach enabled R=8-fold acceleration
from 2 EPI-shots while providing 1.8-fold error reduction compared to the
MUSSELS, a state-of-the-art reconstruction technique, which is also used as an
input to our ML network. Using 3 shots allowed us to push the acceleration to
R=10-fold, where we obtained a 1.7-fold error reduction over MUSSELS.
  Conclusion: Combination of ML and JVC-SENSE enabled navigator-free msEPI at
higher accelerations than previously possible while using fewer shots, with
reduced vulnerability to poor generalizability and poor acceptance of
end-to-end ML approaches.
</summary>
    <author>
      <name>Berkin Bilgic</name>
    </author>
    <author>
      <name>Itthi Chatnuntawech</name>
    </author>
    <author>
      <name>Mary Kate Manhard</name>
    </author>
    <author>
      <name>Qiyuan Tian</name>
    </author>
    <author>
      <name>Congyu Liao</name>
    </author>
    <author>
      <name>Stephen F. Cauley</name>
    </author>
    <author>
      <name>Susie Y. Huang</name>
    </author>
    <author>
      <name>Jonathan R. Polimeni</name>
    </author>
    <author>
      <name>Lawrence L. Wald</name>
    </author>
    <author>
      <name>Kawin Setsompop</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00243v4</id>
    <updated>2018-08-08T14:43:21Z</updated>
    <published>2017-04-29T22:02:14Z</published>
    <title>A General Theory of Sample Complexity for Multi-Item Profit Maximization</title>
    <summary>  The design of profit-maximizing multi-item mechanisms is a notoriously
challenging problem with tremendous real-world impact. The mechanism designer's
goal is to field a mechanism with high expected profit on the distribution over
buyers' values. Unfortunately, if the set of mechanisms he optimizes over is
complex, a mechanism may have high empirical profit over a small set of samples
but low expected profit. This raises the question, how many samples are
sufficient to ensure that the empirically optimal mechanism is nearly optimal
in expectation? We uncover structure shared by a myriad of pricing, auction,
and lottery mechanisms that allows us to prove strong sample complexity bounds:
for any set of buyers' values, profit is a piecewise linear function of the
mechanism's parameters. We prove new bounds for mechanism classes not yet
studied in the sample-based mechanism design literature and match or improve
over the best known guarantees for many classes. The profit functions we study
are significantly different from well-understood functions in machine learning,
so our analysis requires a sharp understanding of the interplay between
mechanism parameters and buyer values. We strengthen our main results with
data-dependent bounds when the distribution over buyers' values is
"well-behaved." Finally, we investigate a fundamental tradeoff in sample-based
mechanism design: complex mechanisms often have higher profit than simple
mechanisms, but more samples are required to ensure that empirical and expected
profit are close. We provide techniques for optimizing this tradeoff.
</summary>
    <author>
      <name>Maria-Florina Balcan</name>
    </author>
    <author>
      <name>Tuomas Sandholm</name>
    </author>
    <author>
      <name>Ellen Vitercik</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00243v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00243v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10023v2</id>
    <updated>2018-08-08T13:44:56Z</updated>
    <published>2018-04-26T12:47:52Z</published>
    <title>Candidate Labeling for Crowd Learning</title>
    <summary>  Crowdsourcing has become very popular among the machine learning community as
a way to obtain labels that allow a ground truth to be estimated for a given
dataset. In most of the approaches that use crowdsourced labels, annotators are
asked to provide, for each presented instance, a single class label. Such a
request could be inefficient, that is, considering that the labelers may not be
experts, that way to proceed could fail to take real advantage of the knowledge
of the labelers. In this paper, the use of candidate labeling for crowd
learning is proposed, where the annotators may provide more than a single label
per instance to try not to miss the real label. The main hypothesis is that, by
allowing candidate labeling, knowledge can be extracted from the labelers more
efficiently by than in the standard crowd learning scenario. Empirical evidence
which supports that hypothesis is presented.
</summary>
    <author>
      <name>Iker Beñaran-Muñoz</name>
    </author>
    <author>
      <name>Jerónimo Hernández-González</name>
    </author>
    <author>
      <name>Aritz Pérez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, to be published</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.10023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02776v1</id>
    <updated>2018-08-08T13:35:44Z</updated>
    <published>2018-08-08T13:35:44Z</published>
    <title>Can Network Analysis Techniques help to Predict Design Dependencies? An
  Initial Study</title>
    <summary>  The degree of dependencies among the modules of a software system is a key
attribute to characterize its design structure and its ability to evolve over
time. Several design problems are often correlated with undesired dependencies
among modules. Being able to anticipate those problems is important for
developers, so they can plan early for maintenance and refactoring efforts.
However, existing tools are limited to detecting undesired dependencies once
they appeared in the system. In this work, we investigate whether module
dependencies can be predicted (before they actually appear). Since the module
structure can be regarded as a network, i.e, a dependency graph, we leverage on
network features to analyze the dynamics of such a structure. In particular, we
apply link prediction techniques for this task. We conducted an evaluation on
two Java projects across several versions, using link prediction and machine
learning techniques, and assessed their performance for identifying new
dependencies from a project version to the next one. The results, although
preliminary, show that the link prediction approach is feasible for package
dependencies. Also, this work opens opportunities for further development of
software-specific strategies for dependency prediction.
</summary>
    <author>
      <name>J. Andrés Díaz-Pace</name>
    </author>
    <author>
      <name>Antonela Tommasel</name>
    </author>
    <author>
      <name>Daniela Godoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICSA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01743v2</id>
    <updated>2018-08-08T09:54:20Z</updated>
    <published>2018-06-05T15:23:10Z</published>
    <title>A Machine Learning Framework for Stock Selection</title>
    <summary>  This paper demonstrates how to apply machine learning algorithms to
distinguish good stocks from the bad stocks. To this end, we construct 244
technical and fundamental features to characterize each stock, and label stocks
according to their ranking with respect to the return-to-volatility ratio.
Algorithms ranging from traditional statistical learning methods to recently
popular deep learning method, e.g. Logistic Regression (LR), Random Forest
(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the
classification task. Genetic Algorithm (GA) is also used to implement feature
selection. The effectiveness of the stock selection strategy is validated in
Chinese stock market in both statistical and practical aspects, showing that:
1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic
Algorithm picks a subset of 114 features and the prediction performances of all
models remain almost unchanged after the selection procedure, which suggests
some features are indeed redundant; 3) LR and DNN are radical models; RF is
risk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios
constructed by our models outperform market average in back tests.
</summary>
    <author>
      <name>XingYu Fu</name>
    </author>
    <author>
      <name>JinHong Du</name>
    </author>
    <author>
      <name>YiFeng Guo</name>
    </author>
    <author>
      <name>MingWen Liu</name>
    </author>
    <author>
      <name>Tao Dong</name>
    </author>
    <author>
      <name>XiuWen Duan</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01743v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01743v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11805v2</id>
    <updated>2018-08-08T09:29:37Z</updated>
    <published>2018-07-31T13:24:31Z</published>
    <title>Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning</title>
    <summary>  Monitoring of disasters is crucial for mitigating their effects on the
environment and human population, and can be facilitated by the use of unmanned
aerial vehicles (UAV), equipped with camera sensors that produce aerial photos
of the areas of interest. A modern technique for recognition of events based on
aerial photos is deep learning. In this paper, we present the state of the art
work related to the use of deep learning techniques for disaster
identification. We demonstrate the potential of this technique in identifying
disasters with high accuracy, by means of a relatively simple deep learning
model. Based on a dataset of 544 images (containing disaster images such as
fires, earthquakes, collapsed buildings, tsunami and flooding, as well as
non-disaster scenes), our results show an accuracy of 91% achieved, indicating
that deep learning, combined with UAV equipped with camera sensors, have the
potential to predict disasters with high accuracy.
</summary>
    <author>
      <name>Andreas Kamilaris</name>
    </author>
    <author>
      <name>Francesc X. Prenafeta-Boldú</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Disaster Management for Resilience and Public Safety Workshop, Proc.
  of EnviroInfo 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05407v2</id>
    <updated>2018-08-08T08:49:15Z</updated>
    <published>2018-03-14T17:09:27Z</published>
    <title>Averaging Weights Leads to Wider Optima and Better Generalization</title>
    <summary>  Deep neural networks are typically trained by optimizing a loss function with
an SGD variant, in conjunction with a decaying learning rate, until
convergence. We show that simple averaging of multiple points along the
trajectory of SGD, with a cyclical or constant learning rate, leads to better
generalization than conventional training. We also show that this Stochastic
Weight Averaging (SWA) procedure finds much broader optima than SGD, and
approximates the recent Fast Geometric Ensembling (FGE) approach with a single
model. Using SWA we achieve notable improvement in test accuracy over
conventional SGD training on a range of state-of-the-art residual networks,
PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and
ImageNet. In short, SWA is extremely easy to implement, improves
generalization, and has almost no computational overhead.
</summary>
    <author>
      <name>Pavel Izmailov</name>
    </author>
    <author>
      <name>Dmitrii Podoprikhin</name>
    </author>
    <author>
      <name>Timur Garipov</name>
    </author>
    <author>
      <name>Dmitry Vetrov</name>
    </author>
    <author>
      <name>Andrew Gordon Wilson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears at the Conference on Uncertainty in Artificial Intelligence
  (UAI), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05407v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05407v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02651v1</id>
    <updated>2018-08-08T08:01:18Z</updated>
    <published>2018-08-08T08:01:18Z</published>
    <title>Adversarial Geometry and Lighting using a Differentiable Renderer</title>
    <summary>  Many machine learning classifiers are vulnerable to adversarial attacks,
inputs with perturbations designed to intentionally trigger misclassification.
Modern adversarial methods either directly alter pixel colors, or "paint"
colors onto a 3D shapes. We propose novel adversarial attacks that directly
alter the geometry of 3D objects and/or manipulate the lighting in a virtual
scene. We leverage a novel differentiable renderer that is efficient to
evaluate and analytically differentiate. Our renderer generates images
realistic enough for correct classification by common pre-trained models, and
we use it to design physical adversarial examples that consistently fool these
models. We conduct qualitative and quantitate experiments to validate our
adversarial geometry and adversarial lighting attack capabilities.
</summary>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Michael Tao</name>
    </author>
    <author>
      <name>Chun-Liang Li</name>
    </author>
    <author>
      <name>Derek Nowrouzezahrai</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09720v3</id>
    <updated>2018-08-08T07:54:14Z</updated>
    <published>2018-01-29T19:28:10Z</published>
    <title>A Generalized Circuit for the Hamiltonian Dynamics Through the Truncated
  Series</title>
    <summary>  In this paper, we present a method for the Hamiltonian simulation in the
context of eigenvalue estimation problems which improves earlier results
dealing with Hamiltonian simulation through the truncated Taylor series. In
particular, we present a fixed-quantum circuit design for the simulation of the
Hamiltonian dynamics, $H(t)$, through the truncated Taylor series method
described by Berry et al. \cite{berry2015simulating}. The circuit is general
and can be used to simulate any given matrix in the phase estimation algorithm
by only changing the angle values of the quantum gates implementing the time
variable $t$ in the series. The circuit complexity depends on the number of
summation terms composing the Hamiltonian and requires $O(Ln)$ number of
quantum gates for the simulation of a molecular Hamiltonian. Here, $n$ is the
number of states of a spin orbital, and $L$ is the number of terms in the
molecular Hamiltonian and generally bounded by $O(n^4)$. We also discuss how to
use the circuit in adaptive processes and eigenvalue related problems along
with a slight modified version of the iterative phase estimation algorithm. In
addition, a simple divide and conquer method is presented for mapping a matrix
which are not given as sums of unitary matrices into the circuit. The
complexity of the circuit is directly related to the structure of the matrix
and can be bounded by $O(poly(n))$ for a matrix with $poly(n)-$sparsity.
</summary>
    <author>
      <name>Ammar Daskin</name>
    </author>
    <author>
      <name>Sabre Kais</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MATLAB source code for the circuits can be downloaded from
  https://github.com/adaskin/circuitforTaylorseries</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09720v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09720v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00442v2</id>
    <updated>2018-08-08T06:47:30Z</updated>
    <published>2018-07-02T02:49:36Z</published>
    <title>Policy Optimization With Penalized Point Probability Distance: An
  Alternative To Proximal Policy Optimization</title>
    <summary>  This paper proposes a first order gradient reinforcement learning algorithm,
which can be seen as a variant for Trust Region Policy Optimization(TRPO). This
method, which we call policy optimization with penalized point probability
distance (POP3D), keeps almost all advantageous spheres of proximal policy
optimization (PPO) such as easy implementation, fast learning and high score
capability. In specific, a new surrogate objective without constraint is
proposed, where the point probability distance is applied to prevent update
step from growing too large while contributing to more exploration and
stability than Kullback-Leibler divergence. Conclusions can be drawn based on
Gym Atari and Mujoco experiments that POP3D is an alternative to PPO, because
it achieves state-of-the-art within 40 million frame steps on 49 Atari games
and competitive scores in continuous domain according to two common metrics:
final performance and fast learning ability. Moreover, we release the code on
github https://github.com/cxxgtxy/POP3D.git.
</summary>
    <author>
      <name>Xiangxiang Chu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review for AAAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02610v1</id>
    <updated>2018-08-08T03:10:24Z</updated>
    <published>2018-08-08T03:10:24Z</published>
    <title>L-Shapley and C-Shapley: Efficient Model Interpretation for Structured
  Data</title>
    <summary>  We study instancewise feature importance scoring as a method for model
interpretation. Any such method yields, for each predicted instance, a vector
of importance scores associated with the feature vector. Methods based on the
Shapley score have been proposed as a fair way of computing feature
attributions of this kind, but incur an exponential complexity in the number of
features. This combinatorial explosion arises from the definition of the
Shapley value and prevents these methods from being scalable to large data sets
and complex models. We focus on settings in which the data have a graph
structure, and the contribution of features to the target variable is
well-approximated by a graph-structured factorization. In such settings, we
develop two algorithms with linear complexity for instancewise feature
importance scoring. We establish the relationship of our methods to the Shapley
value and another closely related concept known as the Myerson value from
cooperative game theory. We demonstrate on both language and image data that
our algorithms compare favorably with other methods for model interpretation.
</summary>
    <author>
      <name>Jianbo Chen</name>
    </author>
    <author>
      <name>Le Song</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02602v1</id>
    <updated>2018-08-08T02:16:13Z</updated>
    <published>2018-08-08T02:16:13Z</published>
    <title>PIVETed-Granite: Computational Phenotypes through Constrained Tensor
  Factorization</title>
    <summary>  It has been recently shown that sparse, nonnegative tensor factorization of
multi-modal electronic health record data is a promising approach to
high-throughput computational phenotyping. However, such approaches typically
do not leverage available domain knowledge while extracting the phenotypes;
hence, some of the suggested phenotypes may not map well to clinical concepts
or may be very similar to other suggested phenotypes. To address these issues,
we present a novel, automatic approach called PIVETed-Granite that mines
existing biomedical literature (PubMed) to obtain cannot-link constraints that
are then used as side-information during a tensor-factorization based
computational phenotyping process. The resulting improvements are clearly
observed in experiments using a large dataset from VUMC to identify phenotypes
for hypertensive patients.
</summary>
    <author>
      <name>Jette Henderson</name>
    </author>
    <author>
      <name>Bradley A. Malin</name>
    </author>
    <author>
      <name>Joyce C. Ho</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05490v2</id>
    <updated>2018-08-08T02:08:15Z</updated>
    <published>2018-07-15T05:18:20Z</published>
    <title>Semi-Supervised Feature Learning for Off-Line Writer Identifications</title>
    <summary>  Conventional approaches used supervised learning to estimate off-line writer
identifications. In this study, we improved the off-line writer identifica-
tions by semi-supervised feature learning pipeline, which trained the extra
unla- beled data and the original labeled data simultaneously. In specific, we
proposed a weighted label smoothing regularization (WLSR) method, which
assigned the weighted uniform label distribution to the extra unlabeled data.
We regularized the convolutional neural network (CNN) baseline, which allows
learning more discriminative features to represent the properties of different
writing styles. Based on experiments on ICDAR2013, CVL and IAM benchmark
datasets, our results showed that semi-supervised feature learning improved the
baseline meas- urement and achieved better performance compared with existing
writer identifications approaches.
</summary>
    <author>
      <name>Shiming Chen</name>
    </author>
    <author>
      <name>Yisong Wang</name>
    </author>
    <author>
      <name>Chin-Teng Lin</name>
    </author>
    <author>
      <name>Zehong Cao</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05490v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05490v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09535v3</id>
    <updated>2018-08-08T01:40:46Z</updated>
    <published>2017-11-27T04:52:05Z</published>
    <title>Learning with Biased Complementary Labels</title>
    <summary>  In this paper, we study the classification problem in which we have access to
easily obtainable surrogate for true labels, namely complementary labels, which
specify classes that observations do \textbf{not} belong to. Let $Y$ and
$\bar{Y}$ be the true and complementary labels, respectively. We first model
the annotation of complementary labels via transition probabilities
$P(\bar{Y}=i|Y=j), i\neq j\in\{1,\cdots,c\}$, where $c$ is the number of
classes. Previous methods implicitly assume that $P(\bar{Y}=i|Y=j), \forall
i\neq j$, are identical, which is not true in practice because humans are
biased toward their own experience. For example, as shown in Figure 1, if an
annotator is more familiar with monkeys than prairie dogs when providing
complementary labels for meerkats, she is more likely to employ "monkey" as a
complementary label. We therefore reason that the transition probabilities will
be different. In this paper, we propose a framework that contributes three main
innovations to learning with \textbf{biased} complementary labels: (1) It
estimates transition probabilities with no bias. (2) It provides a general
method to modify traditional loss functions and extends standard deep neural
network classifiers to learn with biased complementary labels. (3) It
theoretically ensures that the classifier learned with complementary labels
converges to the optimal one learned with true labels. Comprehensive
experiments on several benchmark datasets validate the superiority of our
method to current state-of-the-art methods.
</summary>
    <author>
      <name>Xiyu Yu</name>
    </author>
    <author>
      <name>Tongliang Liu</name>
    </author>
    <author>
      <name>Mingming Gong</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 Oral</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.09535v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09535v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00374v3</id>
    <updated>2018-08-07T23:18:45Z</updated>
    <published>2018-07-01T19:16:11Z</published>
    <title>Augmented Cyclic Adversarial Learning for Domain Adaptation</title>
    <summary>  Training a model to perform a task typically requires a large amount of data
from the domains in which the task will be applied. However, it is often the
case that data are abundant in some domains but scarce in others. Domain
adaptation deals with the challenge of adapting a model trained from a
data-rich source domain to perform well in a data-poor target domain. In
general, this requires learning plausible mappings between domains. CycleGAN is
a powerful framework that efficiently learns to map inputs from one domain to
another using adversarial training and a cycle-consistency constraint. However,
the conventional approach of enforcing cycle-consistency via reconstruction may
be overly restrictive in cases where one or more domains have limited training
data. In this paper, we propose an augmented cyclic adversarial learning model
that enforces the cycle-consistency constraint through an external task
specific model, which encourages the preservation of task-relevant content as
opposed to exact reconstruction. We explore digit classification with MNIST and
SVHN in a low-resource setting in supervised, semi and unsupervised situation.
In low-resource supervised setting, the results show that our approach improves
absolute performance by $14\%$ and $4\%$ when adapting SVHN to MNIST and vice
versa, respectively, which outperforms unsupervised domain adaptation methods
that require high-resource unlabeled target domain. Moreover, using only few
unsupervised target data, our approach can still outperforms many high-resource
unsupervised models. In speech domains, we also adopt a speech recognition
model from each domain as the task specific model. Our approach improves
absolute performance of speech recognition by $2\%$ for female speakers in the
TIMIT dataset, where the majority of training samples are from male voices.
</summary>
    <author>
      <name>Ehsan Hosseini-Asl</name>
    </author>
    <author>
      <name>Yingbo Zhou</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00374v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00374v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07308v2</id>
    <updated>2018-08-07T21:28:34Z</updated>
    <published>2017-09-19T20:38:10Z</published>
    <title>Predicting Positive and Negative Links with Noisy Queries: Theory &amp;
  Practice</title>
    <summary>  Social networks involve both positive and negative relationships, which can
be captured in signed graphs. The {\em edge sign prediction problem} aims to
predict whether an interaction between a pair of nodes will be positive or
negative. We provide theoretical results for this problem that motivate natural
improvements to recent heuristics.
  The edge sign prediction problem is related to correlation clustering; a
positive relationship means being in the same cluster. We consider the
following model for two clusters: we are allowed to query any pair of nodes
whether they belong to the same cluster or not, but the answer to the query is
corrupted with some probability $0&lt;q&lt;\frac{1}{2}$. Let $\delta=1-2q$ be the
bias. We provide an algorithm that recovers all signs correctly with high
probability in the presence of noise with $O(\frac{n\log
n}{\delta^2}+\frac{\log^2 n}{\delta^6})$ queries. This is the best known result
for this problem for all but tiny $\delta$, improving on the recent work of
Mazumdar and Saha \cite{mazumdar2017clustering}. We also provide an algorithm
that performs $O(\frac{n\log n}{\delta^4})$ queries, and uses breadth first
search as its main algorithmic primitive. While both the running time and the
number of queries for this algorithm are sub-optimal, our result relies on
novel theoretical techniques, and naturally suggests the use of edge-disjoint
paths as a feature for predicting signs in online social networks.
Correspondingly, we experiment with using edge disjoint $s-t$ paths of short
length as a feature for predicting the sign of edge $(s,t)$ in real-world
signed networks. Empirical findings suggest that the use of such paths improves
the classification accuracy, especially for pairs of nodes with no common
neighbors.
</summary>
    <author>
      <name>Charalampos E. Tsourakakis</name>
    </author>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Kasper Green Larsen</name>
    </author>
    <author>
      <name>Jarosław Błasiok</name>
    </author>
    <author>
      <name>Ben Lawson</name>
    </author>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <author>
      <name>Vasileios Nakos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1609.00750</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02480v1</id>
    <updated>2018-08-07T21:23:21Z</updated>
    <published>2018-08-07T21:23:21Z</published>
    <title>Deep context: end-to-end contextual speech recognition</title>
    <summary>  In automatic speech recognition (ASR) what a user says depends on the
particular context she is in. Typically, this context is represented as a set
of word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E)
ASR sys- tem that utilizes such context. Our approach, which we re- fer to as
Contextual Listen, Attend and Spell (CLAS) jointly- optimizes the ASR
components along with embeddings of the context n-grams. During inference, the
CLAS system can be presented with context phrases which might contain out-of-
vocabulary (OOV) terms not seen during training. We com- pare our proposed
system to a more traditional contextualiza- tion approach, which performs
shallow-fusion between inde- pendently trained LAS and contextual n-gram models
during beam search. Across a number of tasks, we find that the pro- posed CLAS
system outperforms the baseline method by as much as 68% relative WER,
indicating the advantage of joint optimization over individually trained
components. Index Terms: speech recognition, sequence-to-sequence models,
listen attend and spell, LAS, attention, embedded speech recognition.
</summary>
    <author>
      <name>Golan Pundak</name>
    </author>
    <author>
      <name>Tara N. Sainath</name>
    </author>
    <author>
      <name>Rohit Prabhavalkar</name>
    </author>
    <author>
      <name>Anjuli Kannan</name>
    </author>
    <author>
      <name>Ding Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03972v3</id>
    <updated>2018-08-07T21:12:20Z</updated>
    <published>2018-06-06T19:21:09Z</published>
    <title>A Multi-task Deep Learning Architecture for Maritime Surveillance using
  AIS Data Streams</title>
    <summary>  In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular timesampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.
</summary>
    <author>
      <name>Duong Nguyen</name>
    </author>
    <author>
      <name>Rodolphe Vadaine</name>
    </author>
    <author>
      <name>Guillaume Hajduch</name>
    </author>
    <author>
      <name>René Garello</name>
    </author>
    <author>
      <name>Ronan Fablet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE DSAA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03972v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03972v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02546v1</id>
    <updated>2018-08-07T20:31:39Z</updated>
    <published>2018-08-07T20:31:39Z</published>
    <title>Parallel and Streaming Algorithms for K-Core Decomposition</title>
    <summary>  The $k$-core decomposition is a fundamental primitive in many machine
learning and data mining applications. We present the first distributed and the
first streaming algorithms to compute and maintain an approximate $k$-core
decomposition with provable guarantees. Our algorithms achieve rigorous bounds
on space complexity while bounding the number of passes or number of rounds of
computation. We do so by presenting a new powerful sketching technique for
$k$-core decomposition, and then by showing it can be computed efficiently in
both streaming and MapReduce models. Finally, we confirm the effectiveness of
our sketching technique empirically on a number of publicly available graphs.
</summary>
    <author>
      <name>Hossein Esfandiari</name>
    </author>
    <author>
      <name>Silvio Lattanzi</name>
    </author>
    <author>
      <name>Vahab Mirrokni</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03147v1</id>
    <updated>2018-08-07T19:04:40Z</updated>
    <published>2018-08-07T19:04:40Z</published>
    <title>A New Optimization Layer for Real-Time Bidding Advertising Campaigns</title>
    <summary>  While it is relatively easy to start an online advertising campaign,
obtaining a high Key Performance Indicator (KPI) can be challenging. A large
body of work on this subject has already been performed and platforms known as
DSPs are available on the market that deal with such an optimization. From the
advertiser's point of view, each DSP is a different black box, with its pros
and cons, that needs to be configured. In order to take advantage of the pros
of every DSP, advertisers are well-advised to use a combination of them when
setting up their campaigns. In this paper, we propose an algorithm for
advertisers to add an optimization layer on top of DSPs. The algorithm we
introduce, called SKOTT, maximizes the chosen KPI by optimally configuring the
DSPs and putting them in competition with each other. SKOTT is a highly
specialized iterative algorithm loosely based on gradient descent that is made
up of three independent sub-routines, each dealing with a different problem:
partitioning the budget, setting the desired average bid, and preventing
under-delivery. In particular, one of the novelties of our approach lies in our
taking the perspective of the advertisers rather than the DSPs. Synthetic
market data is used to evaluate the efficiency of SKOTT against other
state-of-the-art approaches adapted from similar problems. The results
illustrate the benefits of our proposals, which greatly outperforms the other
methods.
</summary>
    <author>
      <name>Gianluca Micchi</name>
    </author>
    <author>
      <name>Saeid Soheily-Khah</name>
    </author>
    <author>
      <name>Jacob Turner</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01894v2</id>
    <updated>2018-08-07T19:00:04Z</updated>
    <published>2018-02-06T11:56:25Z</published>
    <title>The steerable graph Laplacian and its application to filtering image
  data-sets</title>
    <summary>  In recent years, improvements in various image acquisition techniques gave
rise to the need for adaptive processing methods, aimed particularly for large
datasets corrupted by noise and deformations. In this work, we consider
datasets of images sampled from a low-dimensional manifold (i.e. an
image-valued manifold), where the images can assume arbitrary planar rotations.
To derive an adaptive and rotation-invariant framework for processing such
datasets, we introduce a graph Laplacian (GL)-like operator over the dataset,
termed ${\textit{steerable graph Laplacian}}$. Essentially, the steerable GL
extends the standard GL by accounting for all (infinitely-many) planar
rotations of all images. As it turns out, similarly to the standard GL, a
properly normalized steerable GL converges to the Laplace-Beltrami operator on
the low-dimensional manifold. However, the steerable GL admits an improved
convergence rate compared to the GL, where the improved convergence behaves as
if the intrinsic dimension of the underlying manifold is lower by one.
Moreover, it is shown that the steerable GL admits eigenfunctions of the form
of Fourier modes (along the orbits of the images' rotations) multiplied by
eigenvectors of certain matrices, which can be computed efficiently by the FFT.
For image datasets corrupted by noise, we employ a subset of these
eigenfunctions to "filter" the dataset via a Fourier-like filtering scheme,
essentially using all images and their rotations simultaneously. We demonstrate
our filtering framework by de-noising simulated single-particle cryo-EM image
datasets.
</summary>
    <author>
      <name>Boris Landa</name>
    </author>
    <author>
      <name>Yoel Shkolnisky</name>
    </author>
    <link href="http://arxiv.org/abs/1802.01894v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01894v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04434v3</id>
    <updated>2018-08-07T18:55:19Z</updated>
    <published>2018-02-13T02:14:35Z</published>
    <title>signSGD: Compressed Optimisation for Non-Convex Problems</title>
    <summary>  Training large neural networks requires distributing learning across multiple
workers, where the cost of communicating gradients can be a significant
bottleneck. signSGD alleviates this problem by transmitting just the sign of
each minibatch stochastic gradient. We prove that it can get the best of both
worlds: compressed gradients and SGD-level convergence rate. The relative
$\ell_1/\ell_2$ geometry of gradients, noise and curvature informs whether
signSGD or SGD is theoretically better suited to a particular problem. On the
practical side we find that the momentum counterpart of signSGD is able to
match the accuracy and convergence speed of Adam on deep Imagenet models. We
extend our theory to the distributed setting, where the parameter server uses
majority vote to aggregate gradient signs from each worker enabling 1-bit
compression of worker-server communication in both directions. Using a theorem
by Gauss we prove that majority vote can achieve the same reduction in variance
as full precision distributed SGD. Thus, there is great promise for sign-based
optimisation schemes to achieve fast communication and fast convergence. Code
to reproduce experiments is to be found at https://github.com/jxbz/signSGD .
</summary>
    <author>
      <name>Jeremy Bernstein</name>
    </author>
    <author>
      <name>Yu-Xiang Wang</name>
    </author>
    <author>
      <name>Kamyar Azizzadenesheli</name>
    </author>
    <author>
      <name>Anima Anandkumar</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04434v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04434v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02513v1</id>
    <updated>2018-08-07T18:42:09Z</updated>
    <published>2018-08-07T18:42:09Z</published>
    <title>Rethinking Numerical Representations for Deep Neural Networks</title>
    <summary>  With ever-increasing computational demand for deep learning, it is critical
to investigate the implications of the numeric representation and precision of
DNN model weights and activations on computational efficiency. In this work, we
explore unconventional narrow-precision floating-point representations as it
relates to inference accuracy and efficiency to steer the improved design of
future DNN platforms. We show that inference using these custom numeric
representations on production-grade DNNs, including GoogLeNet and VGG, achieves
an average speedup of 7.6x with less than 1% degradation in inference accuracy
relative to a state-of-the-art baseline platform representing the most
sophisticated hardware using single-precision floating point. To facilitate the
use of such customized precision, we also present a novel technique that
drastically reduces the time required to derive the optimal precision
configuration.
</summary>
    <author>
      <name>Parker Hill</name>
    </author>
    <author>
      <name>Babak Zamirai</name>
    </author>
    <author>
      <name>Shengshuo Lu</name>
    </author>
    <author>
      <name>Yu-Wei Chao</name>
    </author>
    <author>
      <name>Michael Laurenzano</name>
    </author>
    <author>
      <name>Mehrzad Samadi</name>
    </author>
    <author>
      <name>Marios Papaefthymiou</name>
    </author>
    <author>
      <name>Scott Mahlke</name>
    </author>
    <author>
      <name>Thomas Wenisch</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <author>
      <name>Lingjia Tang</name>
    </author>
    <author>
      <name>Jason Mars</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02510v1</id>
    <updated>2018-08-07T18:37:13Z</updated>
    <published>2018-08-07T18:37:13Z</published>
    <title>Message Passing Graph Kernels</title>
    <summary>  Graph kernels have recently emerged as a promising approach for tackling the
graph similarity and learning tasks at the same time. In this paper, we propose
a general framework for designing graph kernels. The proposed framework
capitalizes on the well-known message passing scheme on graphs. The kernels
derived from the framework consist of two components. The first component is a
kernel between vertices, while the second component is a kernel between graphs.
The main idea behind the proposed framework is that the representations of the
vertices are implicitly updated using an iterative procedure. Then, these
representations serve as the building blocks of a kernel that compares pairs of
graphs. We derive four instances of the proposed framework, and show through
extensive experiments that these instances are competitive with
state-of-the-art methods in various tasks.
</summary>
    <author>
      <name>Giannis Nikolentzos</name>
    </author>
    <author>
      <name>Michalis Vazirgiannis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05136v5</id>
    <updated>2018-08-07T18:12:10Z</updated>
    <published>2017-11-14T15:02:47Z</published>
    <title>Deep Rewiring: Training very sparse deep networks</title>
    <summary>  Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
</summary>
    <author>
      <name>Guillaume Bellec</name>
    </author>
    <author>
      <name>David Kappel</name>
    </author>
    <author>
      <name>Wolfgang Maass</name>
    </author>
    <author>
      <name>Robert Legenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ICLR 2018. 10 pages (12 with references,
  24 with appendix), 4 Figures in the main text. Reviews are available at:
  https://openreview.net/forum?id=BJ_wN01C- . This recent version contains
  minor corrections in the appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05136v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05136v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02474v1</id>
    <updated>2018-08-07T17:48:40Z</updated>
    <published>2018-08-07T17:48:40Z</published>
    <title>Multi-Label Zero-Shot Learning with Transfer-Aware Label Embedding
  Projection</title>
    <summary>  Zero-shot learning transfers knowledge from seen classes to novel unseen
classes to reduce human labor of labelling data for building new classifiers.
Much effort on zero-shot learning however has focused on the standard
multi-class setting, the more challenging multi-label zero-shot problem has
received limited attention. In this paper we propose a transfer-aware embedding
projection approach to tackle multi-label zero-shot learning. The approach
projects the label embedding vectors into a low-dimensional space to induce
better inter-label relationships and explicitly facilitate information transfer
from seen labels to unseen labels, while simultaneously learning a max-margin
multi-label classifier with the projected label embeddings. Auxiliary
information can be conveniently incorporated to guide the label embedding
projection to further improve label relation structures for zero-shot knowledge
transfer. We conduct experiments for zero-shot multi-label image
classification. The results demonstrate the efficacy of the proposed approach.
</summary>
    <author>
      <name>Meng Ye</name>
    </author>
    <author>
      <name>Yuhong Guo</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06063v2</id>
    <updated>2018-08-07T17:30:23Z</updated>
    <published>2018-05-29T19:37:37Z</published>
    <title>Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet
  Process Switching Linear Dynamical Systems</title>
    <summary>  Using movement primitive libraries is an effective means to enable robots to
solve more complex tasks. In order to build these movement libraries, current
algorithms require a prior segmentation of the demonstration trajectories. A
promising approach is to model the trajectory as being generated by a set of
Switching Linear Dynamical Systems and inferring a meaningful segmentation by
inspecting the transition points characterized by the switching dynamics. With
respect to the learning, a nonparametric Bayesian approach is employed
utilizing a Gibbs sampler.
</summary>
    <author>
      <name>Maximilian Sieb</name>
    </author>
    <author>
      <name>Matthias Schultheis</name>
    </author>
    <author>
      <name>Sebastian Szelag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Premature upload</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06063v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06063v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02458v1</id>
    <updated>2018-08-07T16:59:16Z</updated>
    <published>2018-08-07T16:59:16Z</published>
    <title>The Sample Complexity of Up-to-$\varepsilon$ Multi-Dimensional Revenue
  Maximization</title>
    <summary>  We consider the sample complexity of revenue maximization for multiple
bidders in unrestricted multi-dimensional settings. Specifically, we study the
standard model of $n$ additive bidders whose values for $m$ heterogeneous items
are drawn independently. For any such instance and any $\varepsilon&gt;0$, we show
that it is possible to learn an $\varepsilon$-Bayesian Incentive Compatible
auction whose expected revenue is within $\varepsilon$ of the optimal
$\varepsilon$-BIC auction from only polynomially many samples.
  Our approach is based on ideas that hold quite generally, and completely
sidestep the difficulty of characterizing optimal (or near-optimal) auctions
for these settings. Therefore, our results easily extend to general
multi-dimensional settings, including valuations that aren't necessarily even
subadditive, and arbitrary allocation constraints. For the cases of a single
bidder and many goods, or a single parameter (good) and many bidders, our
analysis yields exact incentive compatibility (and for the latter also
computational efficiency). Although the single-parameter case is already
well-understood, our corollary for this case extends slightly the
state-of-the-art.
</summary>
    <author>
      <name>Yannai A. Gonczarowski</name>
    </author>
    <author>
      <name>S. Matthew Weinberg</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02435v1</id>
    <updated>2018-08-07T15:59:17Z</updated>
    <published>2018-08-07T15:59:17Z</published>
    <title>Mixed Integer Linear Programming for Feature Selection in Support Vector
  Machine</title>
    <summary>  This work focuses on support vector machine (SVM) with feature selection. A
MILP formulation is proposed for the problem. The choice of suitable features
to construct the separating hyperplanes has been modelled in this formulation
by including a budget constraint that sets in advance a limit on the number of
features to be used in the classification process. We propose both an exact and
a heuristic procedure to solve this formulation in an efficient way. Finally,
the validation of the model is done by checking it with some well-known data
sets and comparing it with classical classification methods.
</summary>
    <author>
      <name>Martine Labbé</name>
    </author>
    <author>
      <name>Luisa I. Martínez-Merino</name>
    </author>
    <author>
      <name>Antonio M. Rodríguez-Chía</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02433v1</id>
    <updated>2018-08-07T15:59:08Z</updated>
    <published>2018-08-07T15:59:08Z</published>
    <title>Robust Implicit Backpropagation</title>
    <summary>  Arguably the biggest challenge in applying neural networks is tuning the
hyperparameters, in particular the learning rate. The sensitivity to the
learning rate is due to the reliance on backpropagation to train the network.
In this paper we present the first application of Implicit Stochastic Gradient
Descent (ISGD) to train neural networks, a method known in convex optimization
to be unconditionally stable and robust to the learning rate. Our key
contribution is a novel layer-wise approximation of ISGD which makes its
updates tractable for neural networks. Experiments show that our method is more
robust to high learning rates and generally outperforms standard
backpropagation on a variety of tasks.
</summary>
    <author>
      <name>Francois Fagan</name>
    </author>
    <author>
      <name>Garud Iyengar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08809v2</id>
    <updated>2018-08-07T15:45:48Z</updated>
    <published>2018-05-22T18:38:59Z</published>
    <title>Infinite-Task Learning with Vector-Valued RKHSs</title>
    <summary>  Machine learning has witnessed the tremendous success of solving tasks
depending on a hyperparameter. While multi-task learning is celebrated for its
capacity to solve jointly a finite number of tasks, learning a continuum of
tasks for various loss functions is still a challenge. A promising approach,
called Parametric Task Learning, has paved the way in the case of
piecewise-linear loss functions. We propose a generic approach, called
Infinite-Task Learning, to solve jointly a continuum of tasks via vector-valued
RKHSs. We provide generalization guarantees to the suggested scheme and
illustrate its efficiency in cost-sensitive classification, quantile regression
and density level set estimation.
</summary>
    <author>
      <name>Romain Brault</name>
    </author>
    <author>
      <name>Alex Lambert</name>
    </author>
    <author>
      <name>Zoltán Szabó</name>
    </author>
    <author>
      <name>Maxime Sangnier</name>
    </author>
    <author>
      <name>Florence d'Alché-Buc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 5 figures, 2 tables. Preprint NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46E22, 62G08, 65D15, 47B32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08273v2</id>
    <updated>2018-08-07T14:29:39Z</updated>
    <published>2018-05-21T20:01:52Z</published>
    <title>Multiple Causal Inference with Latent Confounding</title>
    <summary>  Causal inference from observational data requires assumptions. These
assumptions range from measuring confounders to identifying instruments.
Traditionally, these assumptions have focused on estimation in a single causal
problem. In this work, we develop techniques for causal estimation in causal
problems with multiple treatments. We develop two assumptions based on shared
confounding between treatments and independence of treatments given the
confounder. Together these assumptions lead to a confounder estimator
regularized by mutual information. For this estimator, we develop a tractable
lower bound. To fit the outcome model, we use the residual information in the
treatments given the confounder. We validate on simulations and an example from
clinical medicine.
</summary>
    <author>
      <name>Rajesh Ranganath</name>
    </author>
    <author>
      <name>Adler Perotte</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08273v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08273v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02394v1</id>
    <updated>2018-08-07T14:20:10Z</updated>
    <published>2018-08-07T14:20:10Z</published>
    <title>Application of End-to-End Deep Learning in Wireless Communications
  Systems</title>
    <summary>  Deep learning is a potential paradigm changer for the design of wireless
communications systems (WCS), from conventional handcrafted schemes based on
sophisticated mathematical models with assumptions to autonomous schemes based
on the end-to-end deep learning using a large number of data. In this article,
we present a basic concept of the deep learning and its application to WCS by
investigating the resource allocation (RA) scheme based on a deep neural
network (DNN) where multiple goals with various constraints can be satisfied
through the end-to-end deep learning. Especially, the optimality and
feasibility of the DNN based RA are verified through simulation. Then, we
discuss the technical challenges regarding the application of deep learning in
WCS.
</summary>
    <author>
      <name>Woongsup Lee</name>
    </author>
    <author>
      <name>Ohyun Jo</name>
    </author>
    <author>
      <name>Minhoe Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01010v2</id>
    <updated>2018-08-07T14:14:24Z</updated>
    <published>2018-06-04T08:42:09Z</published>
    <title>Meta Learner with Linear Nulling</title>
    <summary>  We propose a meta learning algorithm utilizing a linear transformer that
carries out null-space projection of neural network outputs. The main idea is
to construct a classification space such that the error signals during few-shot
training are zero-forced on that space. The final decision on a test sample is
obtained utilizing a null-space-projected distance measure between the network
output and label-dependent weights that have been trained in the initial meta
learning phase. Our meta learner achieves the best or near-best accuracies
among known methods in few-shot image classification tasks with Omniglot and
miniImageNet. In particular, our method shows stronger relative performance by
significant margins as the classification task becomes more complicated.
</summary>
    <author>
      <name>Sung Whan Yoon</name>
    </author>
    <author>
      <name>Jun Seo</name>
    </author>
    <author>
      <name>Jaekyun Moon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03733v2</id>
    <updated>2018-08-07T14:02:20Z</updated>
    <published>2018-07-10T16:09:29Z</published>
    <title>Network Classification in Temporal Networks Using Motifs</title>
    <summary>  Network classification has a variety of applications, such as detecting
communities within networks and finding similarities between those representing
different aspects of the real world. However, most existing work in this area
focus on examining static undirected networks without considering directed
edges or temporality. In this paper, we propose a new methodology that utilizes
feature representation for network classification based on the temporal motif
distribution of the network and a null model for comparing against random
graphs. Experimental results show that our method improves accuracy by up
$10\%$ compared to the state-of-the-art embedding method in network
classification, for tasks such as classifying network type, identifying
communities in email exchange network, and identifying users given their
app-switching behaviors.
</summary>
    <author>
      <name>Kun Tu</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Don Towsley</name>
    </author>
    <author>
      <name>Dave Braines</name>
    </author>
    <author>
      <name>Liam D. Turner</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03733v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03733v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06096v3</id>
    <updated>2018-08-07T09:19:13Z</updated>
    <published>2017-12-17T12:15:08Z</published>
    <title>Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF
  Data using Deep Learning</title>
    <summary>  In portable, three dimensional, and ultra-fast ultrasound imaging systems,
there is an increasing demand for the reconstruction of high quality images
from a limited number of radio-frequency (RF) measurements due to receiver (Rx)
or transmit (Xmit) event sub-sampling. However, due to the presence of side
lobe artifacts from RF sub-sampling, the standard beamformer often produces
blurry images with less contrast, which are unsuitable for diagnostic purposes.
Existing compressed sensing approaches often require either hardware changes or
computationally expensive algorithms, but their quality improvements are
limited. To address this problem, here we propose a novel deep learning
approach that directly interpolates the missing RF data by utilizing redundancy
in the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF
data from a multi-line acquisition B-mode system confirm that the proposed
method can effectively reduce the data rate without sacrificing image quality.
</summary>
    <author>
      <name>Yeo Hun Yoon</name>
    </author>
    <author>
      <name>Shujaat Khan</name>
    </author>
    <author>
      <name>Jaeyoung Huh</name>
    </author>
    <author>
      <name>Jong Chul Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The title has been changed. This version will appear in IEEE Trans.
  on Medical Imaging</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06096v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06096v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02266v1</id>
    <updated>2018-08-07T09:01:05Z</updated>
    <published>2018-08-07T09:01:05Z</published>
    <title>Multi-Output Convolution Spectral Mixture for Gaussian Processes</title>
    <summary>  Multi-output Gaussian processes (MOGPs) are recently extended by using
spectral mixture kernel, which enables expressively pattern extrapolation with
a strong interpretation. In particular, Multi-Output Spectral Mixture kernel
(MOSM) is a recent, powerful state of the art method. However, MOSM cannot
reduce to the ordinary spectral mixture kernel (SM) when using a single
channel. Moreover, when the spectral density of different channels is either
very close or very far from each other in the frequency domain, MOSM generates
unreasonable scale effects on cross weights which produces an incorrect
description of the channel correlation structure. In this paper, we tackle
these drawbacks and introduce a principled multi-output convolution spectral
mixture kernel (MOCSM) framework. In our framework, we model channel
dependencies through convolution of time and phase delayed spectral mixtures
between different channels. Results of extensive experiments on synthetic and
real datasets demontrate the advantages of MOCSM and its state of the art
performance.
</summary>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Perry Groot</name>
    </author>
    <author>
      <name>Jinsong Chen</name>
    </author>
    <author>
      <name>Elena Marchiori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 26 figures. arXiv admin note: text overlap with
  arXiv:1808.01132</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02237v1</id>
    <updated>2018-08-07T07:26:39Z</updated>
    <published>2018-08-07T07:26:39Z</published>
    <title>Inferring Molecular Pathology and micro-RNA Transcriptome from mRNA
  Profiles of Cancer Biopsies through Deep Multi-Task Learning</title>
    <summary>  Despite great advances, molecular cancer pathology is often limited to use a
small number of biomarkers rather than the whole transcriptome, partly due to
the computational challenges. Here, we introduce a novel architecture of DNNs
that is capable of simultaneous inference of various properties of biological
samples, through multi-task and transfer learning. We employed this
architecture on mRNA transcription profiles of 10787 clinical samples from 34
classes (one healthy and 33 different types of cancer) from 27 tissues. Our
system significantly outperforms prior works and classical machine learning
approaches in predicting tissue-of-origin, normal or disease state and cancer
type of each sample. Furthermore, it can predict miRNA transcription profile of
each sample, which enables performing miRNA expression research when only mRNA
transcriptome data are available. We also show this system is very robust
against noise and missing values. Collectively, our results highlight
applications of artificial intelligence in molecular cancer pathology and
oncological research.
</summary>
    <author>
      <name>Behrooz Azarkhalili</name>
    </author>
    <author>
      <name>Ali Saberi</name>
    </author>
    <author>
      <name>Hamidreza Chitsaz</name>
    </author>
    <author>
      <name>Ali Sharifi-Zarchi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02227v1</id>
    <updated>2018-08-07T06:47:01Z</updated>
    <published>2018-08-07T06:47:01Z</published>
    <title>Hierarchical Clustering better than Average-Linkage</title>
    <summary>  Hierarchical Clustering (HC) is a widely studied problem in exploratory data
analysis, usually tackled by simple agglomerative procedures like
average-linkage, single-linkage or complete-linkage. In this paper we focus on
two objectives, introduced recently to give insight into the performance of
average-linkage clustering: a similarity based HC objective proposed by
[Moseley and Wang, 2017] and a dissimilarity based HC objective proposed by
[Cohen-Addad et al., 2018]. In both cases, we present tight counterexamples
showing that average-linkage cannot obtain better than 1/3 and 2/3
approximations respectively (in the worst-case), settling an open question
raised in [Moseley and Wang, 2017]. This matches the approximation ratio of a
random solution, raising a natural question: can we beat average-linkage for
these objectives? We answer this in the affirmative, giving two new algorithms
based on semidefinite programming with provably better guarantees.
</summary>
    <author>
      <name>Moses Charikar</name>
    </author>
    <author>
      <name>Vaggos Chatziafratis</name>
    </author>
    <author>
      <name>Rad Niazadeh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02213v1</id>
    <updated>2018-08-07T05:09:13Z</updated>
    <published>2018-08-07T05:09:13Z</published>
    <title>Importance of the Mathematical Foundations of Machine Learning Methods
  for Scientific and Engineering Applications</title>
    <summary>  There has been a lot of recent interest in adopting machine learning methods
for scientific and engineering applications. This has in large part been
inspired by recent successes and advances in the domains of Natural Language
Processing (NLP) and Image Classification (IC). However, scientific and
engineering problems have their own unique characteristics and requirements
raising new challenges for effective design and deployment of machine learning
approaches. There is a strong need for further mathematical developments on the
foundations of machine learning methods to increase the level of rigor of
employed methods and to ensure more reliable and interpretable results. Also as
reported in the recent literature on state-of-the-art results and indicated by
the No Free Lunch Theorems of statistical learning theory incorporating some
form of inductive bias and domain knowledge is essential to success.
Consequently, even for existing and widely used methods there is a strong need
for further mathematical work to facilitate ways to incorporate prior
scientific knowledge and related inductive biases into learning frameworks and
algorithms. We briefly discuss these topics and discuss some ideas proceeding
in this direction.
</summary>
    <author>
      <name>Paul J. Atzberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position Paper at SciML2018 Workshop, US Department of Energy,
  January 2018, (two-page limit)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04447v1</id>
    <updated>2018-08-07T05:09:11Z</updated>
    <published>2018-08-07T05:09:11Z</published>
    <title>Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological
  and Quantitative Magnetic Resonance Imaging</title>
    <summary>  Obtaining magnetic resonance images (MRI) with high resolution and generating
quantitative image-based biomarkers for assessing tissue biochemistry is
crucial in clinical and research applications. How- ever, acquiring
quantitative biomarkers requires high signal-to-noise ratio (SNR), which is at
odds with high-resolution in MRI, especially in a single rapid sequence. In
this paper, we demonstrate how super-resolution can be utilized to maintain
adequate SNR for accurate quantification of the T2 relaxation time biomarker,
while simultaneously generating high- resolution images. We compare the
efficacy of resolution enhancement using metrics such as peak SNR and
structural similarity. We assess accuracy of cartilage T2 relaxation times by
comparing against a standard reference method. Our evaluation suggests that SR
can successfully maintain high-resolution and generate accurate biomarkers for
accelerating MRI scans and enhancing the value of clinical and research MRI.
</summary>
    <author>
      <name>Akshay Chaudhari</name>
    </author>
    <author>
      <name>Zhongnan Fang</name>
    </author>
    <author>
      <name>Jin Hyung Lee</name>
    </author>
    <author>
      <name>Garry Gold</name>
    </author>
    <author>
      <name>Brian Hargreaves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the Machine Learning for Medical Image Reconstruction
  Workshop at MICCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03184v2</id>
    <updated>2018-08-07T03:58:01Z</updated>
    <published>2018-02-09T09:52:35Z</published>
    <title>Self-Bounded Prediction Suffix Tree via Approximate String Matching</title>
    <summary>  Prediction suffix trees (PST) provide an effective tool for sequence
modelling and prediction. Current prediction techniques for PSTs rely on exact
matching between the suffix of the current sequence and the previously observed
sequence. We present a provably correct algorithm for learning a PST with
approximate suffix matching by relaxing the exact matching condition. We then
present a self-bounded enhancement of our algorithm where the depth of suffix
tree grows automatically in response to the model performance on a training
sequence. Through experiments on synthetic datasets as well as three real-world
datasets, we show that the approximate matching PST results in better
predictive performance than the other variants of PST.
</summary>
    <author>
      <name>Dongwoo Kim</name>
    </author>
    <author>
      <name>Christian Walder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 35th International Conference on Machine Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07712v3</id>
    <updated>2018-08-07T03:04:11Z</updated>
    <published>2018-03-21T01:39:08Z</published>
    <title>Causal Inference on Discrete Data via Estimating Distance Correlations</title>
    <summary>  In this paper, we deal with the problem of inferring causal directions when
the data is on discrete domain. By considering the distribution of the cause
$P(X)$ and the conditional distribution mapping cause to effect $P(Y|X)$ as
independent random variables, we propose to infer the causal direction via
comparing the distance correlation between $P(X)$ and $P(Y|X)$ with the
distance correlation between $P(Y)$ and $P(X|Y)$. We infer "$X$ causes $Y$" if
the dependence coefficient between $P(X)$ and $P(Y|X)$ is smaller. Experiments
are performed to show the performance of the proposed method.
</summary>
    <author>
      <name>Furui Liu</name>
    </author>
    <author>
      <name>Laiwan Chan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00820</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00820" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, Vol. 28, No. 5, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.07712v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07712v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02180v1</id>
    <updated>2018-08-07T01:47:57Z</updated>
    <published>2018-08-07T01:47:57Z</published>
    <title>Instance-Dependent PU Learning by Bayesian Optimal Relabeling</title>
    <summary>  When learning from positive and unlabelled data, it is a strong assumption
that the positive observations are randomly sampled from the distribution of
$X$ conditional on $Y = 1$, where X stands for the feature and Y the label.
Most existing algorithms are optimally designed under the assumption. However,
for many real-world applications, the observed positive examples are dependent
on the conditional probability $P(Y = 1|X)$ and should be sampled biasedly. In
this paper, we assume that a positive example with a higher $P(Y = 1|X)$ is
more likely to be labelled and propose a probabilistic-gap based PU learning
algorithms. Specifically, by treating the unlabelled data as noisy negative
examples, we could automatically label a group positive and negative examples
whose labels are identical to the ones assigned by a Bayesian optimal
classifier with a consistency guarantee. The relabelled examples have a biased
domain, which is remedied by the kernel mean matching technique. The proposed
algorithm is model-free and thus do not have any parameters to tune.
Experimental results demonstrate that our method works well on both generated
and real-world datasets.
</summary>
    <author>
      <name>Fengxiang He</name>
    </author>
    <author>
      <name>Tongliang Liu</name>
    </author>
    <author>
      <name>Geoffrey I Webb</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09075v2</id>
    <updated>2018-08-07T01:44:58Z</updated>
    <published>2016-10-28T04:06:59Z</published>
    <title>Missing Data Imputation for Supervised Learning</title>
    <summary>  Missing data imputation can help improve the performance of prediction models
in situations where missing data hide useful information. This paper compares
methods for imputing missing categorical data for supervised classification
tasks. We experiment on two machine learning benchmark datasets with missing
categorical data, comparing classifiers trained on non-imputed (i.e., one-hot
encoded) or imputed data with different levels of additional missing-data
perturbation. We show imputation methods can increase predictive accuracy in
the presence of missing-data perturbation, which can actually improve
prediction accuracy by regularizing the classifier. We achieve the
state-of-the-art on the Adult dataset with missing-data perturbation and
k-nearest-neighbors (k-NN) imputation.
</summary>
    <author>
      <name>Jason Poulos</name>
    </author>
    <author>
      <name>Rafael Valle</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09075v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09075v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02174v1</id>
    <updated>2018-08-07T01:18:57Z</updated>
    <published>2018-08-07T01:18:57Z</published>
    <title>Test without Trust: Optimal Locally Private Distribution Testing</title>
    <summary>  We study the problem of distribution testing when the samples can only be
accessed using a locally differentially private mechanism and focus on two
representative testing questions of identity (goodness-of-fit) and independence
testing for discrete distributions. We are concerned with two settings: First,
when we insist on using an already deployed, general-purpose locally
differentially private mechanism such as the popular RAPPOR or the recently
introduced Hadamard Response for collecting data, and must build our tests
based on the data collected via this mechanism; and second, when no such
restriction is imposed, and we can design a bespoke mechanism specifically for
testing. For the latter purpose, we introduce the Randomized Aggregated Private
Testing Optimal Response (RAPTOR) mechanism which is remarkably simple and
requires only one bit of communication per sample.
  We propose tests based on these mechanisms and analyze their sample
complexities. Each proposed test can be implemented efficiently. In each case
(barring one), we complement our performance bounds for algorithms with
information-theoretic lower bounds and establish sample optimality of our
proposed algorithm. A peculiar feature that emerges is that our sample-optimal
algorithm based on RAPTOR uses public-coins, and any test based on RAPPOR or
Hadamard Response, which are both private-coin mechanisms, requires
significantly more samples.
</summary>
    <author>
      <name>Jayadev Acharya</name>
    </author>
    <author>
      <name>Clément L. Canonne</name>
    </author>
    <author>
      <name>Cody Freitag</name>
    </author>
    <author>
      <name>Himanshu Tyagi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02169v1</id>
    <updated>2018-08-07T00:49:24Z</updated>
    <published>2018-08-07T00:49:24Z</published>
    <title>Fast Variance Reduction Method with Stochastic Batch Size</title>
    <summary>  In this paper we study a family of variance reduction methods with randomized
batch size---at each step, the algorithm first randomly chooses the batch size
and then selects a batch of samples to conduct a variance-reduced stochastic
update. We give the linear convergence rate for this framework for composite
functions, and show that the optimal strategy to achieve the optimal
convergence rate per data access is to always choose batch size of 1, which is
equivalent to the SAGA algorithm. However, due to the presence of cache/disk IO
effect in computer architecture, the number of data access cannot reflect the
running time because of 1) random memory access is much slower than sequential
access, 2) when data is too big to fit into memory, disk seeking takes even
longer time. After taking these into account, choosing batch size of $1$ is no
longer optimal, so we propose a new algorithm called SAGA++ and show how to
calculate the optimal average batch size theoretically. Our algorithm
outperforms SAGA and other existing batched and stochastic solvers on real
datasets. In addition, we also conduct a precise analysis to compare different
update rules for variance reduction methods, showing that SAGA++ converges
faster than SVRG in theory.
</summary>
    <author>
      <name>Xuanqing Liu</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01550v2</id>
    <updated>2018-08-07T00:47:43Z</updated>
    <published>2018-08-05T02:01:31Z</published>
    <title>Designing Adaptive Neural Networks for Energy-Constrained Image
  Classification</title>
    <summary>  As convolutional neural networks (CNNs) enable state-of-the-art computer
vision applications, their high energy consumption has emerged as a key
impediment to their deployment on embedded and mobile devices. Towards
efficient image classification under hardware constraints, prior work has
proposed adaptive CNNs, i.e., systems of networks with different accuracy and
computation characteristics, where a selection scheme adaptively selects the
network to be evaluated for each input image. While previous efforts have
investigated different network selection schemes, we find that they do not
necessarily result in energy savings when deployed on mobile systems. The key
limitation of existing methods is that they learn only how data should be
processed among the CNNs and not the network architectures, with each network
being treated as a blackbox.
  To address this limitation, we pursue a more powerful design paradigm where
the architecture settings of the CNNs are treated as hyper-parameters to be
globally optimized. We cast the design of adaptive CNNs as a hyper-parameter
optimization problem with respect to energy, accuracy, and communication
constraints imposed by the mobile device. To efficiently solve this problem, we
adapt Bayesian optimization to the properties of the design space, reaching
near-optimal configurations in few tens of function evaluations. Our method
reduces the energy consumed for image classification on a mobile device by up
to 6x, compared to the best previously published work that uses CNNs as
blackboxes. Finally, we evaluate two image classification practices, i.e.,
classifying all images locally versus over the cloud under energy and
communication constraints.
</summary>
    <author>
      <name>Dimitrios Stamoulis</name>
    </author>
    <author>
      <name>Ting-Wu Chin</name>
    </author>
    <author>
      <name>Anand Krishnan Prakash</name>
    </author>
    <author>
      <name>Haocheng Fang</name>
    </author>
    <author>
      <name>Sribhuvan Sajja</name>
    </author>
    <author>
      <name>Mitchell Bognar</name>
    </author>
    <author>
      <name>Diana Marculescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This conference paper will appear in the proceedings of ICCAD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01550v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01550v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02123v1</id>
    <updated>2018-08-06T21:27:05Z</updated>
    <published>2018-08-06T21:27:05Z</published>
    <title>Structure Learning for Relational Logistic Regression: An Ensemble
  Approach</title>
    <summary>  We consider the problem of learning Relational Logistic Regression (RLR).
Unlike standard logistic regression, the features of RLRs are first-order
formulae with associated weight vectors instead of scalar weights. We turn the
problem of learning RLR to learning these vector-weighted formulae and develop
a learning algorithm based on the recently successful functional-gradient
boosting methods for probabilistic logic models. We derive the functional
gradients and show how weights can be learned simultaneously in an efficient
manner. Our empirical evaluation on standard and novel data sets demonstrates
the superiority of our approach over other methods for learning RLR.
</summary>
    <author>
      <name>Nandini Ramanan</name>
    </author>
    <author>
      <name>Gautam Kunapuli</name>
    </author>
    <author>
      <name>Tushar Khot</name>
    </author>
    <author>
      <name>Bahare Fatemi</name>
    </author>
    <author>
      <name>Seyed Mehran Kazemi</name>
    </author>
    <author>
      <name>David Poole</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <author>
      <name>Sriraam Natarajan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02026v1</id>
    <updated>2018-08-06T21:21:48Z</updated>
    <published>2018-08-06T21:21:48Z</published>
    <title>Active Learning based on Data Uncertainty and Model Sensitivity</title>
    <summary>  Robots can rapidly acquire new skills from demonstrations. However, during
generalisation of skills or transitioning across fundamentally different
skills, it is unclear whether the robot has the necessary knowledge to perform
the task. Failing to detect missing information often leads to abrupt movements
or to collisions with the environment. Active learning can quantify the
uncertainty of performing the task and, in general, locate regions of missing
information. We introduce a novel algorithm for active learning and demonstrate
its utility for generating smooth trajectories. Our approach is based on deep
generative models and metric learning in latent spaces. It relies on the
Jacobian of the likelihood to detect non-smooth transitions in the latent
space, i.e., transitions that lead to abrupt changes in the movement of the
robot. When non-smooth transitions are detected, our algorithm asks for an
additional demonstration from that specific region. The newly acquired
knowledge modifies the data manifold and allows for learning a latent
representation for generating smooth movements. We demonstrate the efficacy of
our approach on generalising elementary skills, transitioning across different
skills, and implicitly avoiding collisions with the environment. For our
experiments, we use a simulated pendulum where we observe its motion from
images and a 7-DoF anthropomorphic arm.
</summary>
    <author>
      <name>Nutan Chen</name>
    </author>
    <author>
      <name>Alexej Klushyn</name>
    </author>
    <author>
      <name>Alexandros Paraschos</name>
    </author>
    <author>
      <name>Djalel Benbouzid</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published on 2018 IEEE/RSJ International Conference on Intelligent
  Robots and System</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02113v1</id>
    <updated>2018-08-06T21:05:55Z</updated>
    <published>2018-08-06T21:05:55Z</published>
    <title>Paying Attention to Attention: Highlighting Influential Samples in
  Sequential Analysis</title>
    <summary>  In (Yang et al. 2016), a hierarchical attention network (HAN) is created for
document classification. The attention layer can be used to visualize text
influential in classifying the document, thereby explaining the model's
prediction. We successfully applied HAN to a sequential analysis task in the
form of real-time monitoring of turn taking in conversations. However, we
discovered instances where the attention weights were uniform at the stopping
point (indicating all turns were equivalently influential to the classifier),
preventing meaningful visualization for real-time human review or classifier
improvement. We observed that attention weights for turns fluctuated as the
conversations progressed, indicating turns had varying influence based on
conversation state. Leveraging this observation, we develop a method to create
more informative real-time visuals (as confirmed by human reviewers) in cases
of uniform attention weights using the changes in turn importance as a
conversation progresses over time.
</summary>
    <author>
      <name>Cynthia Freeman</name>
    </author>
    <author>
      <name>Jonathan Merriman</name>
    </author>
    <author>
      <name>Abhinav Aggarwal</name>
    </author>
    <author>
      <name>Ian Beaver</name>
    </author>
    <author>
      <name>Abdullah Mueen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02093v1</id>
    <updated>2018-08-06T20:10:27Z</updated>
    <published>2018-08-06T20:10:27Z</published>
    <title>Learning to Share and Hide Intentions using Information Regularization</title>
    <summary>  Learning to cooperate with friends and compete with foes is a key component
of multi-agent reinforcement learning. Typically to do so, one requires access
to either a model of or interaction with the other agent(s). Here we show how
to learn effective strategies for cooperation and competition in an asymmetric
information game with no such model or interaction. Our approach is to
encourage an agent to reveal or hide their intentions using an
information-theoretic regularizer. We consider both the mutual information
between goal and action given state, as well as the mutual information between
goal and state. We show how to stochastically optimize these regularizers in a
way that is easy to integrate with policy gradient reinforcement learning.
Finally, we demonstrate that cooperative (competitive) policies learned with
our approach lead to more (less) reward for a second agent in two simple
asymmetric information games.
</summary>
    <author>
      <name>DJ Strouse</name>
    </author>
    <author>
      <name>Max Kleiman-Weiner</name>
    </author>
    <author>
      <name>Josh Tenenbaum</name>
    </author>
    <author>
      <name>Matt Botvinick</name>
    </author>
    <author>
      <name>David Schwab</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02083v1</id>
    <updated>2018-08-06T19:37:56Z</updated>
    <published>2018-08-06T19:37:56Z</published>
    <title>Efficient Principal Subspace Projection of Streaming Data Through Fast
  Similarity Matching</title>
    <summary>  Big data problems frequently require processing datasets in a streaming
fashion, either because all data are available at once but collectively are
larger than available memory or because the data intrinsically arrive one data
point at a time and must be processed online. Here, we introduce a
computationally efficient version of similarity matching, a framework for
online dimensionality reduction that incrementally estimates the top
K-dimensional principal subspace of streamed data while keeping in memory only
the last sample and the current iterate. To assess the performance of our
approach, we construct and make public a test suite containing both a synthetic
data generator and the infrastructure to test online dimensionality reduction
algorithms on real datasets, as well as performant implementations of our
algorithm and competing algorithms with similar aims. Among the algorithms
considered we find our approach to be competitive, performing among the best on
both synthetic and real data.
</summary>
    <author>
      <name>Andrea Giovannucci</name>
    </author>
    <author>
      <name>Victor Minden</name>
    </author>
    <author>
      <name>Cengiz Pehlevan</name>
    </author>
    <author>
      <name>Dmitri B. Chklovskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05096v2</id>
    <updated>2018-08-06T19:36:59Z</updated>
    <published>2018-06-13T14:58:49Z</published>
    <title>Introducing user-prescribed constraints in Markov chains for nonlinear
  dimensionality reduction</title>
    <summary>  Stochastic kernel based dimensionality reduction approaches have become
popular in the last decade. The central component of many of these methods is a
symmetric kernel that quantifies the vicinity between pairs of data points and
a kernel-induced Markov chain on the data. Typically, the Markov chain is fully
specified by the kernel through row normalization. However, in many cases, it
is desirable to impose user-specified stationary-state and dynamical
constraints on the Markov chain. Unfortunately, no systematic framework exists
to impose such user-defined constraints. Here, we introduce a path entropy
maximization based approach to derive the transition probabilities of Markov
chains using a kernel and additional user-specified constraints. We illustrate
the usefulness of these Markov chains with examples.
</summary>
    <author>
      <name>Purushottam D. Dixit</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05096v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05096v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02078v1</id>
    <updated>2018-08-06T19:28:26Z</updated>
    <published>2018-08-06T19:28:26Z</published>
    <title>Unbiased Implicit Variational Inference</title>
    <summary>  We develop unbiased implicit variational inference (UIVI), a method that
expands the applicability of variational inference by defining an expressive
variational family. UIVI considers an implicit variational distribution
obtained in a hierarchical manner using a simple reparameterizable distribution
whose variational parameters are defined by arbitrarily flexible deep neural
networks. Unlike previous works, UIVI directly optimizes the evidence lower
bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on
several models, including Bayesian multinomial logistic regression and
variational autoencoders, and show that UIVI achieves both tighter ELBO and
better predictive performance than existing approaches at a similar
computational cost.
</summary>
    <author>
      <name>Michalis K. Titsias</name>
    </author>
    <author>
      <name>Francisco J. R. Ruiz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02061v1</id>
    <updated>2018-08-06T18:36:12Z</updated>
    <published>2018-08-06T18:36:12Z</published>
    <title>Semblance: A Rank-Based Kernel on Probability Spaces for Niche Detection</title>
    <summary>  Kernel methods provide a principled approach for detecting nonlinear
relations using well understood linear algorithms. In exploratory data analyses
when the underlying structure of the data's probability space is unclear, the
choice of kernel is often arbitrary. Here, we present a novel kernel,
Semblance, on a probability feature space. The advantage of Semblance lies in
its distribution free formulation and its ability to detect niche features by
placing greater emphasis on similarity between observation pairs that fall at
the tail ends of a distribution, as opposed to those that fall towards the
mean. We prove that Semblance is a valid Mercer kernel and illustrate its
applicability through simulations and real world examples.
</summary>
    <author>
      <name>Divyansh Agarwal</name>
    </author>
    <author>
      <name>Nancy Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Figures, 4 Supplementary Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01992v1</id>
    <updated>2018-08-06T16:58:42Z</updated>
    <published>2018-08-06T16:58:42Z</published>
    <title>Simultaneous Edge Alignment and Learning</title>
    <summary>  Edge detection is among the most fundamental vision problems for its role in
perceptual grouping and its wide applications. Recent advances in
representation learning have led to considerable improvements in this area.
Many state of the art edge detection models are learned with fully
convolutional networks (FCNs). However, FCN-based edge learning tends to be
vulnerable to misaligned labels due to the delicate structure of edges. While
such problem was considered in evaluation benchmarks, similar issue has not
been explicitly addressed in general edge learning. In this paper, we show that
label misalignment can cause considerably degraded edge learning quality, and
address this issue by proposing a simultaneous edge alignment and learning
framework. To this end, we formulate a probabilistic model where edge alignment
is treated as latent variable optimization, and is learned end-to-end during
network training. Experiments show several applications of this work, including
improved edge detection with state of the art performance, and automatic
refinement of noisy annotations.
</summary>
    <author>
      <name>Zhiding Yu</name>
    </author>
    <author>
      <name>Weiyang Liu</name>
    </author>
    <author>
      <name>Yang Zou</name>
    </author>
    <author>
      <name>Chen Feng</name>
    </author>
    <author>
      <name>Srikumar Ramalingam</name>
    </author>
    <author>
      <name>B. V. K. Vijaya Kumar</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01990v1</id>
    <updated>2018-08-06T16:51:36Z</updated>
    <published>2018-08-06T16:51:36Z</published>
    <title>Hashing with Binary Matrix Pursuit</title>
    <summary>  We propose theoretical and empirical improvements for two-stage hashing
methods. We first provide a theoretical analysis on the quality of the binary
codes and show that, under mild assumptions, a residual learning scheme can
construct binary codes that fit any neighborhood structure with arbitrary
accuracy. Secondly, we show that with high-capacity hash functions such as
CNNs, binary code inference can be greatly simplified for many standard
neighborhood definitions, yielding smaller optimization problems and more
robust codes. Incorporating our findings, we propose a novel two-stage hashing
method that significantly outperforms previous hashing studies on widely used
image retrieval benchmarks.
</summary>
    <author>
      <name>Fatih Cakir</name>
    </author>
    <author>
      <name>Kun He</name>
    </author>
    <author>
      <name>Stan Sclaroff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures. In Proceedings of European Conference on
  Computer Vision (ECCV), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01976v1</id>
    <updated>2018-08-06T16:13:43Z</updated>
    <published>2018-08-06T16:13:43Z</published>
    <title>Adversarial Vision Challenge</title>
    <summary>  The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate
measurable progress towards robust machine vision models and more generally
applicable adversarial attacks. This document is an updated version of our
competition proposal that was accepted in the competition track of 32nd
Conference on Neural Information Processing Systems (NIPS 2018).
</summary>
    <author>
      <name>Wieland Brendel</name>
    </author>
    <author>
      <name>Jonas Rauber</name>
    </author>
    <author>
      <name>Alexey Kurakin</name>
    </author>
    <author>
      <name>Nicolas Papernot</name>
    </author>
    <author>
      <name>Behar Veliqi</name>
    </author>
    <author>
      <name>Marcel Salathé</name>
    </author>
    <author>
      <name>Sharada P. Mohanty</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://www.crowdai.org/challenges/adversarial-vision-challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01974v1</id>
    <updated>2018-08-06T16:06:43Z</updated>
    <published>2018-08-06T16:06:43Z</published>
    <title>A Survey on Deep Transfer Learning</title>
    <summary>  As a new classification platform, deep learning has recently received
increasing attention from researchers and has been successfully applied to many
domains. In some domains, like bioinformatics and robotics, it is very
difficult to construct a large-scale well-annotated dataset due to the expense
of data acquisition and costly annotation, which limits its development.
Transfer learning relaxes the hypothesis that the training data must be
independent and identically distributed (i.i.d.) with the test data, which
motivates us to use transfer learning to solve the problem of insufficient
training data. This survey focuses on reviewing the current researches of
transfer learning by using deep neural network and its applications. We defined
deep transfer learning, category and review the recent research works based on
the techniques used in deep transfer learning.
</summary>
    <author>
      <name>Chuanqi Tan</name>
    </author>
    <author>
      <name>Fuchun Sun</name>
    </author>
    <author>
      <name>Tao Kong</name>
    </author>
    <author>
      <name>Wenchang Zhang</name>
    </author>
    <author>
      <name>Chao Yang</name>
    </author>
    <author>
      <name>Chunfang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 27th International Conference on Artificial Neural Networks
  (ICANN 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01960v1</id>
    <updated>2018-08-06T15:22:13Z</updated>
    <published>2018-08-06T15:22:13Z</published>
    <title>Distributional Multivariate Policy Evaluation and Exploration with the
  Bellman GAN</title>
    <summary>  The recently proposed distributional approach to reinforcement learning
(DiRL) is centered on learning the distribution of the reward-to-go, often
referred to as the value distribution. In this work, we show that the
distributional Bellman equation, which drives DiRL methods, is equivalent to a
generative adversarial network (GAN) model. In this formulation, DiRL can be
seen as learning a deep generative model of the value distribution, driven by
the discrepancy between the distribution of the current value, and the
distribution of the sum of current reward and next value. We use this insight
to propose a GAN-based approach to DiRL, which leverages the strengths of GANs
in learning distributions of high-dimensional data. In particular, we show that
our GAN approach can be used for DiRL with multivariate rewards, an important
setting which cannot be tackled with prior methods. The multivariate setting
also allows us to unify learning the distribution of values and state
transitions, and we exploit this idea to devise a novel exploration method that
is driven by the discrepancy in estimating both values and states.
</summary>
    <author>
      <name>Dror Freirich</name>
    </author>
    <author>
      <name>Ron Meir</name>
    </author>
    <author>
      <name>Aviv Tamar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02375v2</id>
    <updated>2018-08-06T15:12:45Z</updated>
    <published>2018-06-01T03:57:56Z</published>
    <title>Understanding Batch Normalization</title>
    <summary>  Batch normalization is a ubiquitous deep learning technique that normalizes
activations in intermediate layers. It is associated with improved accuracy and
faster learning, but despite its enormous success there is little consensus
regarding why it works. We aim to rectify this and take an empirical approach
to understanding batch normalization. Our primary observation is that the
higher learning rates that batch normalization enables have a regularizing
effect that dramatically improves generalization of normalized networks, which
is both demonstrated empirically and motivated theoretically. We show how
activations become large and how the convolutional channels become increasingly
ill-behaved for layers deep in unnormalized networks, and how this results in
larger input-independent gradients. Beyond just gradient scaling, we
demonstrate how the learning rate in unnormalized networks is further limited
by the magnitude of activations growing exponentially with network depth for
large parameter updates, a problem batch normalization trivially avoids.
Motivated by recent results in random matrix theory, we argue that
ill-conditioning of the activations is due to fluctuations in random
initialization, shedding new light on classical initialization schemes and
their consequences.
</summary>
    <author>
      <name>Johan Bjorck</name>
    </author>
    <author>
      <name>Carla Gomes</name>
    </author>
    <author>
      <name>Bart Selman</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02375v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02375v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01944v1</id>
    <updated>2018-08-06T14:51:33Z</updated>
    <published>2018-08-06T14:51:33Z</published>
    <title>V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial
  Segmentation</title>
    <summary>  Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder
that causes changes in the anatomy of the atria. A better characterization of
these changes is desirable for the definition of clinical biomarkers, and thus
there is a need of its fully automatic segmentation from clinical images. In
this work we present an architecture based in 3D-convolution kernels, a
Volumetric Fully Convolution Neural Network (V-FCNN), able to segment the
entire volume in one-shot, and consequently integrate the implicit spatial
redundancy present in high resolution images. A loss function based on the
mixture of both Mean Square Error (MSE) and Dice Loss (DL) is used, in an
attempt to combine the ability to capture the bulk shape and the reduction of
local errors products by over segmentation. Results demonstrate a reasonable
performance in the middle region of the atria, and the impact of the challenges
of capturing the variability of the pulmonary veins or the identification of
the valve plane that separates the atria to the ventricle.
</summary>
    <author>
      <name>Nicoló Savioli</name>
    </author>
    <author>
      <name>Giovanni Montana</name>
    </author>
    <author>
      <name>Pablo Lamata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, In Proceedings of MICCAI 2018 Atrial Segmentation
  Challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02024v1</id>
    <updated>2018-08-06T14:15:46Z</updated>
    <published>2018-08-06T14:15:46Z</published>
    <title>Outlier detection on network flow analysis</title>
    <summary>  It is important to be able to detect and classify malicious network traffic
flows such as DDoS attacks from benign flows. Normally the task is performed by
using supervised classification algorithms. In this paper we analyze the usage
of outlier detection algorithms for the network traffic classification problem.
</summary>
    <author>
      <name>Quang-Vinh Dang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01916v1</id>
    <updated>2018-08-06T14:00:40Z</updated>
    <published>2018-08-06T14:00:40Z</published>
    <title>Residual Memory Networks: Feed-forward approach to learn long temporal
  dependencies</title>
    <summary>  Training deep recurrent neural network (RNN) architectures is complicated due
to the increased network complexity. This disrupts the learning of higher order
abstracts using deep RNN. In case of feed-forward networks training deep
structures is simple and faster while learning long-term temporal information
is not possible. In this paper we propose a residual memory neural network
(RMN) architecture to model short-time dependencies using deep feed-forward
layers having residual and time delayed connections. The residual connection
paves way to construct deeper networks by enabling unhindered flow of gradients
and the time delay units capture temporal information with shared weights. The
number of layers in RMN signifies both the hierarchical processing depth and
temporal depth. The computational complexity in training RMN is significantly
less when compared to deep recurrent networks. RMN is further extended as
bi-directional RMN (BRMN) to capture both past and future information.
Experimental analysis is done on AMI corpus to substantiate the capability of
RMN in learning long-term information and hierarchical information. Recognition
performance of RMN trained with 300 hours of Switchboard corpus is compared
with various state-of-the-art LVCSR systems. The results indicate that RMN and
BRMN gains 6 % and 3.8 % relative improvement over LSTM and BLSTM networks.
</summary>
    <author>
      <name>Murali Karthick Baskar</name>
    </author>
    <author>
      <name>Martin Karafiat</name>
    </author>
    <author>
      <name>Lukas Burget</name>
    </author>
    <author>
      <name>Karel Vesely</name>
    </author>
    <author>
      <name>Frantisek Grezl</name>
    </author>
    <author>
      <name>Jan Honza Cernocky</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01857v1</id>
    <updated>2018-08-06T12:54:04Z</updated>
    <published>2018-08-06T12:54:04Z</published>
    <title>Statistical Windows in Testing for the Initial Distribution of a
  Reversible Markov Chain</title>
    <summary>  We study the problem of hypothesis testing between two discrete
distributions, where we only have access to samples after the action of a known
reversible Markov chain, playing the role of noise. We derive
instance-dependent minimax rates for the sample complexity of this problem, and
show how its dependence in time is related to the spectral properties of the
Markov chain. We show that there exists a wide statistical window, in terms of
sample complexity for hypothesis testing between different pairs of initial
distributions. We illustrate these results in several concrete examples.
</summary>
    <author>
      <name>Quentin Berthet</name>
    </author>
    <author>
      <name>Varun Kanade</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01842v1</id>
    <updated>2018-08-06T12:23:42Z</updated>
    <published>2018-08-06T12:23:42Z</published>
    <title>Beyond $1/2$-Approximation for Submodular Maximization on Massive Data
  Streams</title>
    <summary>  Many tasks in machine learning and data mining, such as data diversification,
non-parametric learning, kernel machines, clustering etc., require extracting a
small but representative summary from a massive dataset. Often, such problems
can be posed as maximizing a submodular set function subject to a cardinality
constraint. We consider this question in the streaming setting, where elements
arrive over time at a fast pace and thus we need to design an efficient,
low-memory algorithm. One such method, proposed by Badanidiyuru et al. (2014),
always finds a $0.5$-approximate solution. Can this approximation factor be
improved? We answer this question affirmatively by designing a new algorithm
SALSA for streaming submodular maximization. It is the first low-memory,
single-pass algorithm that improves the factor $0.5$, under the natural
assumption that elements arrive in a random order. We also show that this
assumption is necessary, i.e., that there is no such algorithm with better than
$0.5$-approximation when elements arrive in arbitrary order. Our experiments
demonstrate that SALSA significantly outperforms the state of the art in
applications related to exemplar-based clustering, social graph analysis, and
recommender systems.
</summary>
    <author>
      <name>Ashkan Norouzi-Fard</name>
    </author>
    <author>
      <name>Jakub Tarnawski</name>
    </author>
    <author>
      <name>Slobodan Mitrović</name>
    </author>
    <author>
      <name>Amir Zandieh</name>
    </author>
    <author>
      <name>Aida Mousavifar</name>
    </author>
    <author>
      <name>Ola Svensson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10206v3</id>
    <updated>2018-08-06T11:40:15Z</updated>
    <published>2018-06-26T20:39:13Z</published>
    <title>Deep Feature Factorization For Concept Discovery</title>
    <summary>  We propose Deep Feature Factorization (DFF), a method capable of localizing
similar semantic concepts within an image or a set of images. We use DFF to
gain insight into a deep convolutional neural network's learned features, where
we detect hierarchical cluster structures in feature space. This is visualized
as heat maps, which highlight semantically matching regions across a set of
images, revealing what the network `perceives' as similar. DFF can also be used
to perform co-segmentation and co-localization, and we report state-of-the-art
results on these tasks.
</summary>
    <author>
      <name>Edo Collins</name>
    </author>
    <author>
      <name>Radhakrishna Achanta</name>
    </author>
    <author>
      <name>Sabine Süsstrunk</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10206v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10206v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03915v2</id>
    <updated>2018-08-06T11:14:31Z</updated>
    <published>2018-07-11T01:13:13Z</published>
    <title>Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment
  Analysis</title>
    <summary>  Multimodal machine learning is a core research area spanning the language,
visual and acoustic modalities. The central challenge in multimodal learning
involves learning representations that can process and relate information from
multiple modalities. In this paper, we propose two methods for unsupervised
learning of joint multimodal representations using sequence to sequence
(Seq2Seq) methods: a \textit{Seq2Seq Modality Translation Model} and a
\textit{Hierarchical Seq2Seq Modality Translation Model}. We also explore
multiple different variations on the multimodal inputs and outputs of these
seq2seq models. Our experiments on multimodal sentiment analysis using the
CMU-MOSI dataset indicate that our methods learn informative multimodal
representations that outperform the baselines and achieve improved performance
on multimodal sentiment analysis, specifically in the Bimodal case where our
model is able to improve F1 Score by twelve points. We also discuss future
directions for multimodal Seq2Seq methods.
</summary>
    <author>
      <name>Hai Pham</name>
    </author>
    <author>
      <name>Thomas Manzini</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages of content, 11 pages total, 2 figures. Published as a
  workshop paper at ACL 2018, Proceedings of Grand Challenge and Workshop on
  Human Multimodal Language (Challenge-HML). 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03915v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03915v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01813v1</id>
    <updated>2018-08-06T10:34:40Z</updated>
    <published>2018-08-06T10:34:40Z</published>
    <title>Regret Bounds for Reinforcement Learning via Markov Chain Concentration</title>
    <summary>  We give a simple optimistic algorithm for which it is easy to derive regret
bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ after $T$ steps in uniformly
ergodic MDPs with $S$ states, $A$ actions, and mixing time parameter $t_{\rm
mix}$. These bounds are the first regret bounds in the general, non-episodic
setting with an optimal dependence on all given parameters. They could only be
improved by using an alternative mixing time parameter.
</summary>
    <author>
      <name>Ronald Ortner</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11582v2</id>
    <updated>2018-08-06T10:17:55Z</updated>
    <published>2018-07-27T11:10:03Z</published>
    <title>A Hierarchical Approach to Neural Context-Aware Modeling</title>
    <summary>  We present a new recurrent neural network topology to enhance
state-of-the-art machine learning systems by incorporating a broader context.
Our approach overcomes recent limitations with extended narratives through a
multi-layered computational approach to generate an abstract context
representation. Therefore, the developed system captures the narrative on
word-level, sentence-level, and context-level. Through the hierarchical set-up,
our proposed model summarizes the most salient information on each level and
creates an abstract representation of the extended context. We subsequently use
this representation to enhance neural language processing systems on the task
of semantic error detection. To show the potential of the newly introduced
topology, we compare the approach against a context-agnostic set-up including a
standard neural language model and a supervised binary classification network.
The performance measures on the error detection task show the advantage of the
hierarchical context-aware topologies, improving the baseline by 12.75%
relative for unsupervised models and 20.37% relative for supervised models.
</summary>
    <author>
      <name>Patrick Huber</name>
    </author>
    <author>
      <name>Jan Niehues</name>
    </author>
    <author>
      <name>Alex Waibel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11582v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11582v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06823v2</id>
    <updated>2018-08-06T09:48:28Z</updated>
    <published>2018-02-19T19:39:59Z</published>
    <title>Entropy-Isomap: Manifold Learning for High-dimensional Dynamic Processes</title>
    <summary>  Scientific and engineering processes deliver massive high-dimensional data
sets that are generated as non-linear transformations of an initial state and
few process parameters. Mapping such data to a low-dimensional manifold
facilitates better understanding of the underlying processes, and enables their
optimization. In this paper, we first show that off-the-shelf non-linear
spectral dimensionality reduction methods, e.g., Isomap, fail for such data,
primarily due to the presence of strong temporal correlations. Then, we propose
a novel method, Entropy-Isomap, to address the issue. The proposed method is
successfully applied to large data describing a fabrication process of organic
materials. The resulting low-dimensional representation correctly captures
process control variables, allows for low-dimensional visualization of the
material morphology evolution, and provides key insights to improve the
process.
</summary>
    <author>
      <name>Frank Schoeneman</name>
    </author>
    <author>
      <name>Varun Chandola</name>
    </author>
    <author>
      <name>Nils Napp</name>
    </author>
    <author>
      <name>Olga Wodo</name>
    </author>
    <author>
      <name>Jaroslaw Zola</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06823v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06823v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07888v4</id>
    <updated>2018-08-06T08:42:35Z</updated>
    <published>2017-04-25T19:52:52Z</published>
    <title>Stochastic Optimization from Distributed, Streaming Data in Rate-limited
  Networks</title>
    <summary>  Motivated by machine learning applications in networks of sensors,
internet-of-things (IoT) devices, and autonomous agents, we propose techniques
for distributed stochastic convex learning from high-rate data streams. The
setup involves a network of nodes---each one of which has a stream of data
arriving at a constant rate---that solve a stochastic convex optimization
problem by collaborating with each other over rate-limited communication links.
To this end, we present and analyze two algorithms---termed distributed
stochastic approximation mirror descent (D-SAMD) and accelerated distributed
stochastic approximation mirror descent (AD-SAMD)---that are based on two
stochastic variants of mirror descent and in which nodes collaborate via
approximate averaging of the local, noisy subgradients using distributed
consensus. Our main contributions are (i) bounds on the convergence rates of
D-SAMD and AD-SAMD in terms of the number of nodes, network topology, and ratio
of the data streaming and communication rates, and (ii) sufficient conditions
for order-optimum convergence of these algorithms. In particular, we show that
for sufficiently well-connected networks, distributed learning schemes can
obtain order-optimum convergence even if the communications rate is small.
Further we find that the use of accelerated methods significantly enlarges the
regime in which order-optimum convergence is achieved; this is in contrast to
the centralized setting, where accelerated methods usually offer only a modest
improvement. Finally, we demonstrate the effectiveness of the proposed
algorithms using numerical experiments.
</summary>
    <author>
      <name>Matthew Nokleby</name>
    </author>
    <author>
      <name>Waheed U. Bajwa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 6 figures; Accepted for publication in IEEE Transactions on
  Signal and Information Processing over Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07888v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07888v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01753v1</id>
    <updated>2018-08-06T07:26:44Z</updated>
    <published>2018-08-06T07:26:44Z</published>
    <title>Gray-box Adversarial Training</title>
    <summary>  Adversarial samples are perturbed inputs crafted to mislead the machine
learning systems. A training mechanism, called adversarial training, which
presents adversarial samples along with clean samples has been introduced to
learn robust models. In order to scale adversarial training for large datasets,
these perturbations can only be crafted using fast and simple methods (e.g.,
gradient ascent). However, it is shown that adversarial training converges to a
degenerate minimum, where the model appears to be robust by generating weaker
adversaries. As a result, the models are vulnerable to simple black-box
attacks. In this paper we, (i) demonstrate the shortcomings of existing
evaluation policy, (ii) introduce novel variants of white-box and black-box
attacks, dubbed gray-box adversarial attacks" based on which we propose novel
evaluation method to assess the robustness of the learned models, and (iii)
propose a novel variant of adversarial training, named Graybox Adversarial
Training" that uses intermediate versions of the models to seed the
adversaries. Experimental evaluation demonstrates that the models trained using
our method exhibit better robustness compared to both undefended and
adversarially trained model
</summary>
    <author>
      <name>Vivek B. S.</name>
    </author>
    <author>
      <name>Konda Reddy Mopuri</name>
    </author>
    <author>
      <name>R. Venkatesh Babu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01743v1</id>
    <updated>2018-08-06T06:28:35Z</updated>
    <published>2018-08-06T06:28:35Z</published>
    <title>NIMFA: A Python Library for Nonnegative Matrix Factorization</title>
    <summary>  NIMFA is an open-source Python library that provides a unified interface to
nonnegative matrix factorization algorithms. It includes implementations of
state-of-the-art factorization methods, initialization approaches, and quality
scoring. It supports both dense and sparse matrix representation. NIMFA's
component-based implementation and hierarchical design should help the users to
employ already implemented techniques or design and code new strategies for
matrix factorization tasks.
</summary>
    <author>
      <name>Marinka Zitnik</name>
    </author>
    <author>
      <name>Blaz Zupan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 13 (2012) 849-853</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.01743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01739v1</id>
    <updated>2018-08-06T06:02:15Z</updated>
    <published>2018-08-06T06:02:15Z</published>
    <title>Concentration bounds for empirical conditional value-at-risk: The
  unbounded case</title>
    <summary>  In several real-world applications involving decision making under
uncertainty, the traditional expected value objective may not be suitable, as
it may be necessary to control losses in the case of a rare but extreme event.
Conditional Value-at-Risk (CVaR) is a popular risk measure for modeling the
aforementioned objective. We consider the problem of estimating CVaR from
i.i.d. samples of an unbounded random variable, which is either sub-Gaussian or
sub-exponential. We derive a novel one-sided concentration bound for a natural
sample-based CVaR estimator in this setting. Our bound relies on a
concentration result for a quantile-based estimator for Value-at-Risk (VaR),
which may be of independent interest.
</summary>
    <author>
      <name>Ravi Kumar Kolla</name>
    </author>
    <author>
      <name>Prashanth L. A.</name>
    </author>
    <author>
      <name>Sanjay P. Bhat</name>
    </author>
    <author>
      <name>Krishna Jagannathan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11414v2</id>
    <updated>2018-08-06T01:41:19Z</updated>
    <published>2018-07-30T16:06:26Z</published>
    <title>Faster Convergence &amp; Generalization in DNNs</title>
    <summary>  Deep neural networks have gained tremendous popularity in last few years.
They have been applied for the task of classification in almost every domain.
Despite the success, deep networks can be incredibly slow to train for even
moderate sized models on sufficiently large datasets. Additionally, these
networks require large amounts of data to be able to generalize. The importance
of speeding up convergence, and generalization in deep networks can not be
overstated. In this work, we develop an optimization algorithm based on
generalized-optimal updates derived from minibatches that lead to faster
convergence. Towards the end, we demonstrate on two benchmark datasets that the
proposed method achieves two orders of magnitude speed up over traditional
back-propagation, and is more robust to noise/over-fitting.
</summary>
    <author>
      <name>Gaurav Singh</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11414v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11414v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03679v1</id>
    <updated>2018-08-06T01:15:51Z</updated>
    <published>2018-08-06T01:15:51Z</published>
    <title>Machine Learning Promoting Extreme Simplification of Spectroscopy
  Equipment</title>
    <summary>  The spectroscopy measurement is one of main pathways for exploring and
understanding the nature. Today, it seems that racing artificial intelligence
will remould its styles. The algorithms contained in huge neural networks are
capable of substituting many of expensive and complex components of spectrum
instruments. In this work, we presented a smart machine learning strategy on
the measurement of absorbance curves, and also initially verified that an
exceedingly-simplified equipment is sufficient to meet the needs for this
strategy. Further, with its simplicity, the setup is expected to infiltrate
into many scientific areas in versatile forms.
</summary>
    <author>
      <name>Jianchao Lee</name>
    </author>
    <author>
      <name>Qiannan Duan</name>
    </author>
    <author>
      <name>Sifan Bi</name>
    </author>
    <author>
      <name>Ruen Luo</name>
    </author>
    <author>
      <name>Yachao Lian</name>
    </author>
    <author>
      <name>Hanqiang Liu</name>
    </author>
    <author>
      <name>Ruixing Tian</name>
    </author>
    <author>
      <name>Jiayuan Chen</name>
    </author>
    <author>
      <name>Guodong Ma</name>
    </author>
    <author>
      <name>Jinhong Gao</name>
    </author>
    <author>
      <name>Zhaoyi Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06167v5</id>
    <updated>2018-08-06T00:22:53Z</updated>
    <published>2018-02-17T01:04:53Z</published>
    <title>CapsuleGAN: Generative Adversarial Capsule Network</title>
    <summary>  We present Generative Adversarial Capsule Network (CapsuleGAN), a framework
that uses capsule networks (CapsNets) instead of the standard convolutional
neural networks (CNNs) as discriminators within the generative adversarial
network (GAN) setting, while modeling image data. We provide guidelines for
designing CapsNet discriminators and the updated GAN objective function, which
incorporates the CapsNet margin loss, for training CapsuleGAN models. We show
that CapsuleGAN outperforms convolutional-GAN at modeling image data
distribution on MNIST and CIFAR-10 datasets, evaluated on the generative
adversarial metric and at semi-supervised image classification.
</summary>
    <author>
      <name>Ayush Jaiswal</name>
    </author>
    <author>
      <name>Wael AbdAlmageed</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Premkumar Natarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV Workshop on Brain Driven Computer Vision (BDCV) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06167v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06167v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00876v2</id>
    <updated>2018-08-05T23:21:54Z</updated>
    <published>2018-08-02T15:57:57Z</published>
    <title>Normalization Before Shaking Toward Learning Symmetrically Distributed
  Representation Without Margin in Speech Emotion Recognition</title>
    <summary>  Regularization is crucial to the success of many practical deep learning
models, in particular in a more often than not scenario where there are only a
few to a moderate number of accessible training samples. In addition to weight
decay, data augmentation and dropout, regularization based on multi-branch
architectures, such as Shake-Shake regularization, has been proven successful
in many applications and attracted more and more attention. However, beyond
model-based representation augmentation, it is unclear how Shake-Shake
regularization helps to provide further improvement on classification tasks,
let alone the baffling interaction between batch normalization and shaking. In
this work, we present our investigation on Shake-Shake regularization, drawing
connections to the vicinal risk minimization principle and discriminative
feature learning in verification tasks. Furthermore, we identify a strong
resemblance between batch normalized residual blocks and batch normalized
recurrent neural networks, where both of them share a similar convergence
behavior, which could be mitigated by a proper initialization of batch
normalization. Based on the findings, our experiments on speech emotion
recognition demonstrate simultaneously an improvement on the classification
accuracy and a reduction on the generalization gap both with statistical
significance.
</summary>
    <author>
      <name>Che-Wei Huang</name>
    </author>
    <author>
      <name>Shrikanth S. Narayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submission to The IEEE Transactions</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00876v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00876v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01687v1</id>
    <updated>2018-08-05T21:28:10Z</updated>
    <published>2018-08-05T21:28:10Z</published>
    <title>Hybrid Subspace Learning for High-Dimensional Data</title>
    <summary>  The high-dimensional data setting, in which p &gt;&gt; n, is a challenging
statistical paradigm that appears in many real-world problems. In this setting,
learning a compact, low-dimensional representation of the data can
substantially help distinguish signal from noise. One way to achieve this goal
is to perform subspace learning to estimate a small set of latent features that
capture the majority of the variance in the original data. Most existing
subspace learning models, such as PCA, assume that the data can be fully
represented by its embedding in one or more latent subspaces. However, in this
work, we argue that this assumption is not suitable for many high-dimensional
datasets; often only some variables can easily be projected to a
low-dimensional space. We propose a hybrid dimensionality reduction technique
in which some features are mapped to a low-dimensional subspace while others
remain in the original space. Our model leads to more accurate estimation of
the latent space and lower reconstruction error. We present a simple
optimization procedure for the resulting biconvex problem and show synthetic
data results that demonstrate the advantages of our approach over existing
methods. Finally, we demonstrate the effectiveness of this method for
extracting meaningful features from both gene expression and video background
subtraction datasets.
</summary>
    <author>
      <name>Micol Marchetti-Bowick</name>
    </author>
    <author>
      <name>Benjamin J. Lengerich</name>
    </author>
    <author>
      <name>Ankur P. Parikh</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01686v1</id>
    <updated>2018-08-05T21:27:32Z</updated>
    <published>2018-08-05T21:27:32Z</published>
    <title>Too many secants: a hierarchical approach to secant-based dimensionality
  reduction on large data sets</title>
    <summary>  A fundamental question in many data analysis settings is the problem of
discerning the "natural" dimension of a data set. That is, when a data set is
drawn from a manifold (possibly with noise), a meaningful aspect of the data is
the dimension of that manifold. Various approaches exist for estimating this
dimension, such as the method of Secant-Avoidance Projection (SAP).
Intuitively, the SAP algorithm seeks to determine a projection which best
preserves the lengths of all secants between points in a data set; by applying
the algorithm to find the best projections to vector spaces of various
dimensions, one may infer the dimension of the manifold of origination. That
is, one may learn the dimension at which it is possible to construct a
diffeomorphic copy of the data in a lower-dimensional Euclidean space. Using
Whitney's embedding theorem, we can relate this information to the natural
dimension of the data. A drawback of the SAP algorithm is that a data set with
$T$ points has $O(T^2)$ secants, making the computation and storage of all
secants infeasible for very large data sets. In this paper, we propose a novel
algorithm that generalizes the SAP algorithm with an emphasis on addressing
this issue. That is, we propose a hierarchical secant-based
dimensionality-reduction method, which can be employed for data sets where
explicitly calculating all secants is not feasible.
</summary>
    <author>
      <name>Henry Kvinge</name>
    </author>
    <author>
      <name>Elin Farnell</name>
    </author>
    <author>
      <name>Michael Kirby</name>
    </author>
    <author>
      <name>Chris Peterson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the 2018 IEEE High Performance
  Extreme Computing Conference, Waltham, MA USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06919v2</id>
    <updated>2018-08-05T21:09:36Z</updated>
    <published>2018-07-18T13:28:59Z</published>
    <title>Backplay: "Man muss immer umkehren"</title>
    <summary>  A long-standing problem in model free reinforcement learning (RL) is that it
requires a large number of trials to learn a good policy, especially in
environments with sparse rewards. We explore a method to increase the sample
efficiency of RL when we have access to demonstrations. Our approach, which we
call Backplay, uses a single demonstration to construct a curriculum for a
given task. Rather than starting each training episode in the environment's
fixed initial state, we start the agent near the end of the demonstration and
move the starting point backwards during the course of training until we reach
the initial state. We perform experiments in a competitive four player game
(Pommerman) and a path-finding maze game. We find that this weak form of
guidance provides significant gains in sample complexity with a stark advantage
in sparse reward environments. In some cases, standard RL did not yield any
improvement while Backplay reached success rates greater than 50% and
generalized to unseen initial conditions in the same amount of training time.
Additionally, we see that agents trained via Backplay can learn policies
superior to those of the original demonstration.
</summary>
    <author>
      <name>Cinjon Resnick</name>
    </author>
    <author>
      <name>Roberta Raileanu</name>
    </author>
    <author>
      <name>Sanyam Kapoor</name>
    </author>
    <author>
      <name>Alex Peysakhovich</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06919v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06919v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01684v1</id>
    <updated>2018-08-05T21:06:05Z</updated>
    <published>2018-08-05T21:06:05Z</published>
    <title>Missing Value Imputation Based on Deep Generative Models</title>
    <summary>  Missing values widely exist in many real-world datasets, which hinders the
performing of advanced data analytics. Properly filling these missing values is
crucial but challenging, especially when the missing rate is high. Many
approaches have been proposed for missing value imputation (MVI), but they are
mostly heuristics-based, lacking a principled foundation and do not perform
satisfactorily in practice. In this paper, we propose a probabilistic framework
based on deep generative models for MVI. Under this framework, imputing the
missing entries amounts to seeking a fixed-point solution between two
conditional distributions defined on the missing entries and latent variables
respectively. These distributions are parameterized by deep neural networks
(DNNs) which possess high approximation power and can capture the nonlinear
relationships between missing entries and the observed values. The learning of
weight parameters of DNNs is performed by maximizing an approximation of the
log-likelihood of observed values. We conducted extensive evaluation on 13
datasets and compared with 11 baselines methods, where our methods largely
outperforms the baselines.
</summary>
    <author>
      <name>Hongbao Zhang</name>
    </author>
    <author>
      <name>Pengtao Xie</name>
    </author>
    <author>
      <name>Eric Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10059v2</id>
    <updated>2018-08-05T19:54:58Z</updated>
    <published>2017-10-27T10:24:00Z</published>
    <title>Direction of arrival estimation for multiple sound sources using
  convolutional recurrent neural network</title>
    <summary>  This paper proposes a deep neural network for estimating the directions of
arrival (DOA) of multiple sound sources. The proposed stacked convolutional and
recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS)
along with the DOA estimates in both azimuth and elevation. We avoid any
explicit feature extraction step by using the magnitudes and phases of the
spectrograms of all the channels as input to the network. The proposed DOAnet
is evaluated by estimating the DOAs of multiple concurrently present sources in
anechoic, matched and unmatched reverberant conditions. The results show that
the proposed DOAnet is capable of estimating the number of sources and their
respective DOAs with good precision and generate SPS with high signal-to-noise
ratio.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Archontis Politis</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EUSIPCO 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10059v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10059v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00482v2</id>
    <updated>2018-08-05T19:02:05Z</updated>
    <published>2018-07-02T06:27:42Z</published>
    <title>Tap-based User Authentication for Smartwatches</title>
    <summary>  This paper presents TapMeIn, an eyes-free, two-factor authentication method
for smartwatches. It allows users to tap a memorable melody (tap-password) of
their choice anywhere on the touchscreen to unlock their watch. A user is
verified based on the tap-password as well as her physiological and behavioral
characteristics when tapping. Results from preliminary experiments with 41
participants show that TapMeIn could achieve an accuracy of 98.7% with a False
Positive Rate of only 0.98%. In addition, TapMeIn retains its performance in
different conditions such as sitting and walking. In terms of speed, TapMeIn
has an average authentication time of 2 seconds. A user study with the System
Usability Scale (SUS) tool suggests that TapMeIn has a high usability score.
</summary>
    <author>
      <name>Toan Nguyen</name>
    </author>
    <author>
      <name>Nasir Memon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cose.2018.07.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cose.2018.07.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer &amp; Security, Volume 78, September 2018, Pages 174-186</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.00482v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00482v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01664v1</id>
    <updated>2018-08-05T18:06:37Z</updated>
    <published>2018-08-05T18:06:37Z</published>
    <title>Structured Adversarial Attack: Towards General Implementation and Better
  Interpretability</title>
    <summary>  When generating adversarial examples to attack deep neural networks (DNNs),
$\ell_p$ norm of the added perturbation is usually used to measure the
similarity between original image and adversarial example. However, such
adversarial attacks may fail to capture key infomation hidden in the input.
This work develops a more general attack model i.e., the structured attack that
explores group sparsity in adversarial perturbations by sliding a mask through
images aiming for extracting key structures. An ADMM (alternating direction
method of multipliers)-based framework is proposed that can split the original
problem into a sequence of analytically solvable subproblems and can be
generalized to implement other state-of-the-art attacks. Strong group sparsity
is achieved in adversarial perturbations even with the same level of distortion
in terms of $\ell_p$ norm as the state-of-the-art attacks. Extensive
experimental results on MNIST, CIFAR-10 and ImageNet show that our attack could
be much stronger (in terms of smaller $\ell_0$ distortion) than the existing
ones, and its better interpretability from group sparse structures aids in
uncovering the origins of adversarial examples.
</summary>
    <author>
      <name>Kaidi Xu</name>
    </author>
    <author>
      <name>Sijia Liu</name>
    </author>
    <author>
      <name>Pu Zhao</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Huan Zhang</name>
    </author>
    <author>
      <name>Deniz Erdogmus</name>
    </author>
    <author>
      <name>Yanzhi Wang</name>
    </author>
    <author>
      <name>Xue Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01650v1</id>
    <updated>2018-08-05T16:44:25Z</updated>
    <published>2018-08-05T16:44:25Z</published>
    <title>Combining Graph-based Dependency Features with Convolutional Neural
  Network for Answer Triggering</title>
    <summary>  Answer triggering is the task of selecting the best-suited answer for a given
question from a set of candidate answers if exists. In this paper, we present a
hybrid deep learning model for answer triggering, which combines several
dependency graph based alignment features, namely graph edit distance,
graph-based similarity and dependency graph coverage, with dense vector
embeddings from a Convolutional Neural Network (CNN). Our experiments on the
WikiQA dataset show that such a combination can more accurately trigger a
candidate answer compared to the previous state-of-the-art models. Comparative
study on WikiQA dataset shows 5.86% absolute F-score improvement at the
question level.
</summary>
    <author>
      <name>Deepak Gupta</name>
    </author>
    <author>
      <name>Sarah Kohail</name>
    </author>
    <author>
      <name>Pushpak Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Conference on Computational Linguistics and
  Intelligent Text Processing (CICLing 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01642v1</id>
    <updated>2018-08-05T16:19:56Z</updated>
    <published>2018-08-05T16:19:56Z</published>
    <title>Multi-Objective Cognitive Model: a supervised approach for multi-subject
  fMRI analysis</title>
    <summary>  In order to decode the human brain, Multivariate Pattern (MVP) classification
generates cognitive models by using functional Magnetic Resonance Imaging
(fMRI) datasets. As a standard pipeline in the MVP analysis, brain patterns in
multi-subject fMRI dataset must be mapped to a shared space and then a
classification model is generated by employing the mapped patterns. However,
the MVP models may not provide stable performance on a new fMRI dataset because
the standard pipeline uses disjoint steps for generating these models. Indeed,
each step in the pipeline includes an objective function with independent
optimization approach, where the best solution of each step may not be optimum
for the next steps. For tackling the mentioned issue, this paper introduces the
Multi-Objective Cognitive Model (MOCM) that utilizes an integrated objective
function for MVP analysis rather than just using those disjoint steps. For
solving the integrated problem, we proposed a customized multi-objective
optimization approach, where all possible solutions are firstly generated, and
then our method ranks and selects the robust solutions as the final results.
Empirical studies confirm that the proposed method can generate superior
performance in comparison with other techniques.
</summary>
    <author>
      <name>Muhammad Yousefnezhad</name>
    </author>
    <author>
      <name>Daoqiang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroinformatics, Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00701v3</id>
    <updated>2018-08-05T15:49:20Z</updated>
    <published>2018-06-02T20:31:30Z</published>
    <title>On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional
  Neural Networks</title>
    <summary>  Parsimonious representations in data modeling are ubiquitous and central for
processing information. Motivated by the recent Multi-Layer Convolutional
Sparse Coding (ML-CSC) model, we herein generalize the traditional Basis
Pursuit regression problem to a multi-layer setting, introducing similar sparse
enforcing penalties at different representation layers in a symbiotic relation
between synthesis and analysis sparse priors. We propose and analyze different
iterative algorithms to solve this new problem in practice. We prove that the
presented multi-layer Iterative Soft Thresholding (ML-ISTA) and multi-layer
Fast ISTA (ML-FISTA) converge to the global optimum of our multi-layer
formulation at a rate of $\mathcal{O}(1/k)$ and $\mathcal{O}(1/k^2)$,
respectively and independently of the number of layers. We further show how
these algorithms effectively implement particular recurrent neural networks
that generalize feed-forward architectures without any increase in the number
of parameters. We present different architectures that result from unfolding
the iterations of the proposed multi-layer pursuit algorithms, providing a
principled way to construct deep recurrent CNNs from feed-forward ones. We
demonstrate the emerging constructions by training them in an end-to-end
manner, consistently improving the performance of classical networks without
introducing extra filters or parameters.
</summary>
    <author>
      <name>Jeremias Sulam</name>
    </author>
    <author>
      <name>Aviad Aberdam</name>
    </author>
    <author>
      <name>Michael Elad</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00701v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00701v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01616v1</id>
    <updated>2018-08-05T14:06:18Z</updated>
    <published>2018-08-05T14:06:18Z</published>
    <title>Predicting Learning Status in MOOCs using LSTM</title>
    <summary>  Real-time and open online course resources of MOOCs have attracted a large
number of learners in recent years. However, many new questions were emerging
about the high dropout rate of learners. For MOOCs platform, predicting the
learning status of MOOCs learners in real time with high accuracy is the
crucial task, and it also help improve the quality of MOOCs teaching. The
prediction task in this paper is inherently a time series prediction problem,
and can be treated as time series classification problem, hence this paper
proposed a prediction model based on RNNLSTMs and optimization techniques which
can be used to predict learners' learning status. Using datasets provided by
Chinese University MOOCs as the inputs of model, the average accuracy of
model's outputs was about 90%.
</summary>
    <author>
      <name>Zhemin Liu</name>
    </author>
    <author>
      <name>Feng Xiong</name>
    </author>
    <author>
      <name>Kaifa Zou</name>
    </author>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1402.1128 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01614v1</id>
    <updated>2018-08-05T13:40:22Z</updated>
    <published>2018-08-05T13:40:22Z</published>
    <title>Using Machine Learning Safely in Automotive Software: An Assessment and
  Adaption of Software Process Requirements in ISO 26262</title>
    <summary>  The use of machine learning (ML) is on the rise in many sectors of software
development, and automotive software development is no different. In
particular, Advanced Driver Assistance Systems (ADAS) and Automated Driving
Systems (ADS) are two areas where ML plays a significant role. In automotive
development, safety is a critical objective, and the emergence of standards
such as ISO 26262 has helped focus industry practices to address safety in a
systematic and consistent way. Unfortunately, these standards were not designed
to accommodate technologies such as ML or the type of functionality that is
provided by an ADS and this has created a conflict between the need to innovate
and the need to improve safety. In this report, we take steps to address this
conflict by doing a detailed assessment and adaption of ISO 26262 for ML,
specifically in the context of supervised learning. First we analyze the key
factors that are the source of the conflict. Then we assess each software
development process requirement (Part 6 of ISO 26262) for applicability to ML.
Where there are gaps, we propose new requirements to address the gaps. Finally
we discuss the application of this adapted and extended variant of Part 6 to ML
development scenarios.
</summary>
    <author>
      <name>Rick Salay</name>
    </author>
    <author>
      <name>Krzysztof Czarnecki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01595v1</id>
    <updated>2018-08-05T11:05:23Z</updated>
    <published>2018-08-05T11:05:23Z</published>
    <title>Spherical Harmonic Residual Network for Diffusion Signal Harmonization</title>
    <summary>  Diffusion imaging is an important method in the field of neuroscience, as it
is sensitive to changes within the tissue microstructure of the human brain.
However, a major challenge when using MRI to derive quantitative measures is
that the use of different scanners, as used in multi-site group studies,
introduces measurement variability. This can lead to an increased variance in
quantitative metrics, even if the same brain is scanned.
  Contrary to the assumption that these characteristics are comparable and
similar, small changes in these values are observed in many clinical studies,
hence harmonization of the signals is essential.
  In this paper, we present a method that does not require additional
preprocessing, such as segmentation or registration, and harmonizes the signal
based on a deep learning residual network. For this purpose, a training
database is required, which consist of the same subjects, scanned on different
scanners.
  The results show that harmonized signals are significantly more similar to
the ground truth signal compared to no harmonization, but also improve in
comparison to another deep learning method. The same effect is also
demonstrated in commonly used metrics derived from the diffusion MRI signal.
</summary>
    <author>
      <name>Simon Koppers</name>
    </author>
    <author>
      <name>Luke Bloy</name>
    </author>
    <author>
      <name>Jeffrey I. Berman</name>
    </author>
    <author>
      <name>Chantal M. W. Tax</name>
    </author>
    <author>
      <name>J. Christopher Edgar</name>
    </author>
    <author>
      <name>Dorit Merhof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03154v3</id>
    <updated>2018-08-05T10:05:13Z</updated>
    <published>2018-04-09T18:00:08Z</published>
    <title>Cauchy noise loss for stochastic optimization of random matrix models
  via free deterministic equivalents</title>
    <summary>  For random matrix models, the parameter estimation based on the traditional
likelihood is not straightforward in particular when there is only one sample
matrix. We introduce a new parameter optimization method of random matrix
models which works even in such a case not based on the traditional likelihood,
instead based on the spectral distribution. We use the spectral distribution
perturbed by Cauchy noises because the free deterministic equivalent, which is
a tool in free probability theory, allows us to approximate it by a smooth and
accessible density function.
  Moreover, we study an asymptotic property of a determination gap, which has a
similar role as the generalization gap. In addition, we propose a new
dimensionality recovery method for the signal-plus-noise model, and
experimentally demonstrate that it recovers the rank of the signal part even if
the rank is not low. It is a simultaneous rank selection and parameter
estimation procedure.
</summary>
    <author>
      <name>Tomohiro Hayase</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 13 figures, v3: minor correction. Submitted. Our simulation
  code is available at https://github.com/ThayaFluss/cnl</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03154v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03154v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01591v1</id>
    <updated>2018-08-05T09:50:47Z</updated>
    <published>2018-08-05T09:50:47Z</published>
    <title>LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse
  Semantic Accumulation and Example to Pattern Transformation</title>
    <summary>  Recurrent neural networks (RNNs) are temporal networks and cumulative in
nature that have shown promising results in various natural language processing
tasks. Despite their success, it still remains a challenge to understand their
hidden behavior. In this work, we analyze and interpret the cumulative nature
of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation
(LISA) for explaining decisions and detecting the most likely (i.e., saliency)
patterns that the network relies on while decision making. We demonstrate (1)
LISA: "How an RNN accumulates or builds semantics during its sequential
processing for a given text example and expected response" (2) Example2pattern:
"How the saliency patterns look like for each category in the data according to
the network in decision making". We analyse the sensitiveness of RNNs about
different inputs to check the increase or decrease in prediction scores and
further extract the saliency patterns learned by the network. We employ two
relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to
explain RNN predictions via the LISA and example2pattern.
</summary>
    <author>
      <name>Pankaj Gupta</name>
    </author>
    <author>
      <name>Hinrich Schütze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2018 Conference on Empirical Methods in Natural Language Processing
  (EMNLP2018) workshop on Analyzing and Interpreting Neural Networks for NLP
  (BlackBoxNLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04326v3</id>
    <updated>2018-08-05T07:53:09Z</updated>
    <published>2018-06-12T04:21:53Z</published>
    <title>Differentiable Compositional Kernel Learning for Gaussian Processes</title>
    <summary>  The generalization properties of Gaussian processes depend heavily on the
choice of kernel, and this choice remains a dark art. We present the Neural
Kernel Network (NKN), a flexible family of kernels represented by a neural
network. The NKN architecture is based on the composition rules for kernels, so
that each unit of the network corresponds to a valid kernel. It can compactly
approximate compositional kernel structures such as those used by the Automatic
Statistician (Lloyd et al., 2014), but because the architecture is
differentiable, it is end-to-end trainable with gradient-based optimization. We
show that the NKN is universal for the class of stationary kernels. Empirically
we demonstrate pattern discovery and extrapolation abilities of NKN on several
tasks that depend crucially on identifying the underlying structure, including
time series and texture extrapolation, as well as Bayesian optimization.
</summary>
    <author>
      <name>Shengyang Sun</name>
    </author>
    <author>
      <name>Guodong Zhang</name>
    </author>
    <author>
      <name>Chaoqi Wang</name>
    </author>
    <author>
      <name>Wenyuan Zeng</name>
    </author>
    <author>
      <name>Jiaman Li</name>
    </author>
    <author>
      <name>Roger Grosse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018; update proof</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01574v1</id>
    <updated>2018-08-05T07:45:06Z</updated>
    <published>2018-08-05T07:45:06Z</published>
    <title>Autoencoder Based Sample Selection for Self-Taught Learning</title>
    <summary>  Self-taught learning is a technique that uses a large number of unlabeled
data as source samples to improve the task performance on target samples.
Compared with other transfer learning techniques, self-taught learning can be
applied to a broader set of scenarios due to the loose restrictions on source
data. However, knowledge transferred from source samples that are not
sufficiently related to the target domain may negatively influence the target
learner, which is referred to as negative transfer. In this paper, we propose a
metric for the relevance between a source sample and target samples. To be more
specific, both source and target samples are reconstructed through a
single-layer autoencoder with a linear relationship between source samples and
target samples simultaneously enforced. An l_{2,1}-norm sparsity constraint is
imposed on the transformation matrix to identify source samples relevant to the
target domain. Source domain samples that are deemed relevant are assigned
pseudo-labels reflecting their relevance to target domain samples, and are
combined with target samples in order to provide an expanded training set for
classifier training. Local data structures are also preserved during source
sample selection through spectral graph analysis. Promising results in
extensive experiments show the advantages of the proposed approach.
</summary>
    <author>
      <name>Siwei Feng</name>
    </author>
    <author>
      <name>Marco F. Duarte</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05294v3</id>
    <updated>2018-08-05T06:48:43Z</updated>
    <published>2016-09-17T08:17:36Z</published>
    <title>Sparse Boltzmann Machines with Structure Learning as Applied to Text
  Analysis</title>
    <summary>  We are interested in exploring the possibility and benefits of structure
learning for deep models. As the first step, this paper investigates the matter
for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated
Softmax, a variant of RBMs for unsupervised text analysis. We present a method
for learning what we call Sparse Boltzmann Machines, where each hidden unit is
connected to a subset of the visible units instead of all of them. Empirical
results show that the method yields models with significantly improved model
fit and interpretability as compared with RBMs where each hidden unit is
connected to all visible units.
</summary>
    <author>
      <name>Zhourong Chen</name>
    </author>
    <author>
      <name>Nevin L. Zhang</name>
    </author>
    <author>
      <name>Dit-Yan Yeung</name>
    </author>
    <author>
      <name>Peixian Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05294v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05294v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01552v1</id>
    <updated>2018-08-05T02:28:27Z</updated>
    <published>2018-08-05T02:28:27Z</published>
    <title>Smart City Development with Urban Transfer Learning</title>
    <summary>  The rapid development of big data techniques has offered great opportunities
to develop smart city services in public safety, transportation management,
city planning, etc. Meanwhile, the smart city development levels of different
cities are still unbalanced. For a large of number of cities which just start
development, the governments will face a critical cold-start problem, 'how to
develop a new smart city service suffering from data scarcity?'. To address
this problem, transfer learning is recently leveraged to accelerate the smart
city development, which we term the urban transfer learning paradigm. This
article investigates the common process of urban transfer learning, aiming to
provide city governors and relevant practitioners with guidelines of applying
this novel learning paradigm. Our guidelines include common transfer strategies
to take, general steps to follow, and case studies to refer. We also summarize
a few future research opportunities in urban transfer learning, and expect this
article can attract more researchers into this promising area.
</summary>
    <author>
      <name>Leye Wang</name>
    </author>
    <author>
      <name>Bin Guo</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06221v2</id>
    <updated>2018-08-05T02:15:32Z</updated>
    <published>2016-11-18T20:54:03Z</published>
    <title>Theoretical Aspects of Cyclic Structural Causal Models</title>
    <summary>  Structural causal models (SCMs), also known as (non-parametric) structural
equation models (SEMs), are widely used for causal modeling purposes. A large
body of theoretical results is available for the special case in which cycles
are absent (i.e., acyclic SCMs, also known as recursive SEMs). However, in many
application domains cycles are abundantly present, for example in the form of
feedback loops. In this paper, we provide a general and rigorous theory of
cyclic SCMs. The paper consists of two parts: the first part gives a rigorous
treatment of structural causal models, dealing with measure-theoretic and other
complications that arise in the presence of cycles. In contrast with the
acyclic case, in cyclic SCMs solutions may no longer exist, or if they exist,
they may no longer be unique, or even measurable in general. We give several
sufficient and necessary conditions for the existence of (unique) measurable
solutions. We show how causal reasoning proceeds in these models and how this
differs from the acyclic case. Moreover, we give an overview of the Markov
properties that hold for cyclic SCMs. In the second part, we address the
question of how one can marginalize an SCM (possibly with cycles) to a subset
of the endogenous variables. We show that under a certain condition, one can
effectively remove a subset of the endogenous variables from the model, leading
to a more parsimonious marginal SCM that preserves the causal and
counterfactual semantics of the original SCM on the remaining variables.
Moreover, we show how the marginalization relates to the latent projection and
to latent confounders, i.e. latent common causes.
</summary>
    <author>
      <name>Stephan Bongers</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <author>
      <name>Joris M. Mooij</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Will probably be submitted to The Annals of Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01535v1</id>
    <updated>2018-08-04T21:10:03Z</updated>
    <published>2018-08-04T21:10:03Z</published>
    <title>Triplet Network with Attention for Speaker Diarization</title>
    <summary>  In automatic speech processing systems, speaker diarization is a crucial
front-end component to separate segments from different speakers. Inspired by
the recent success of deep neural networks (DNNs) in semantic inferencing,
triplet loss-based architectures have been successfully used for this problem.
However, existing work utilizes conventional i-vectors as the input
representation and builds simple fully connected networks for metric learning,
thus not fully leveraging the modeling power of DNN architectures. This paper
investigates the importance of learning effective representations from the
sequences directly in metric learning pipelines for speaker diarization. More
specifically, we propose to employ attention models to learn embeddings and the
metric jointly in an end-to-end fashion. Experiments are conducted on the
CALLHOME conversational speech corpus. The diarization results demonstrate
that, besides providing a unified model, the proposed approach achieves
improved performance when compared against existing approaches.
</summary>
    <author>
      <name>Huan Song</name>
    </author>
    <author>
      <name>Megan Willi</name>
    </author>
    <author>
      <name>Jayaraman J. Thiagarajan</name>
    </author>
    <author>
      <name>Visar Berisha</name>
    </author>
    <author>
      <name>Andreas Spanias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01531v1</id>
    <updated>2018-08-04T20:47:44Z</updated>
    <published>2018-08-04T20:47:44Z</published>
    <title>Global Convergence to the Equilibrium of GANs using Variational
  Inequalities</title>
    <summary>  In optimization, the negative gradient of a function denotes the direction of
steepest descent. Furthermore, traveling in any direction orthogonal to the
gradient maintains the value of the function. In this work, we show that these
orthogonal directions that are ignored by gradient descent can be critical in
equilibrium problems. Equilibrium problems have drawn heightened attention in
machine learning due to the emergence of the Generative Adversarial Network
(GAN). We use the framework of Variational Inequalities to analyze popular
training algorithms for a fundamental GAN variant: the Wasserstein
Linear-Quadratic GAN. We show that the steepest descent direction causes
divergence from the equilibrium, and guaranteed convergence to the equilibrium
is achieved through following a particular orthogonal direction. We call this
successful technique Crossing-the-Curl, named for its mathematical derivation
as well as its intuition: identify the game's axis of rotation and move
"across" space in the direction towards smaller "curling".
</summary>
    <author>
      <name>Ian Gemp</name>
    </author>
    <author>
      <name>Sridhar Mahadevan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07756v4</id>
    <updated>2018-08-04T20:38:14Z</updated>
    <published>2018-02-21T19:09:17Z</published>
    <title>Determining the best classifier for predicting the value of a boolean
  field on a blood donor database using genetic algorithms</title>
    <summary>  Motivation: Thanks to digitization, we often have access to large databases,
consisting of various fields of information, ranging from numbers to texts and
even boolean values. Such databases lend themselves especially well to machine
learning, classification and big data analysis tasks. We are able to train
classifiers, using already existing data and use them for predicting the values
of a certain field, given that we have information regarding the other fields.
Most specifically, in this study, we look at the Electronic Health Records
(EHRs) that are compiled by hospitals. These EHRs are convenient means of
accessing data of individual patients, but there processing as a whole still
remains a task. However, EHRs that are composed of coherent, well-tabulated
structures lend themselves quite well to the application to machine language,
via the usage of classifiers. In this study, we look at a Blood Transfusion
Service Center Data Set (Data taken from the Blood Transfusion Service Center
in Hsin-Chu City in Taiwan). We used scikit-learn machine learning in python.
From Support Vector Machines(SVM), we use Support Vector Classification(SVC),
from the linear model we import Perceptron. We also used the
K.neighborsclassifier and the decision tree classifiers. Furthermore, we use
the TPOT library to find an optimized pipeline using genetic algorithms. Using
the above classifiers, we score each one of them using k fold cross-validation.
  Contact: ritabratamaiti@hiretrex.com GitHub Repository:
https://github.com/ritabratamaiti/Blooddonorprediction
</summary>
    <author>
      <name>Ritabrata Maiti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1336304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1336304" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GitHub Repository here:
  https://github.com/ritabratamaiti/Blooddonorprediction</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07756v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07756v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01527v1</id>
    <updated>2018-08-04T20:13:35Z</updated>
    <published>2018-08-04T20:13:35Z</published>
    <title>Deep Reinforcement One-Shot Learning for Artificially Intelligent
  Classification Systems</title>
    <summary>  In recent years there has been a sharp rise in networking applications, in
which significant events need to be classified but only a few training
instances are available. These are known as cases of one-shot learning.
Examples include analyzing network traffic under zero-day attacks, and computer
vision tasks by sensor networks deployed in the field. To handle this
challenging task, organizations often use human analysts to classify events
under high uncertainty. Existing algorithms use a threshold-based mechanism to
decide whether to classify an object automatically or send it to an analyst for
deeper inspection. However, this approach leads to a significant waste of
resources since it does not take the practical temporal constraints of system
resources into account. Our contribution is threefold. First, we develop a
novel Deep Reinforcement One-shot Learning (DeROL) framework to address this
challenge. The basic idea of the DeROL algorithm is to train a deep-Q network
to obtain a policy which is oblivious to the unseen classes in the testing
data. Then, in real-time, DeROL maps the current state of the one-shot learning
process to operational actions based on the trained deep-Q network, to maximize
the objective function. Second, we develop the first open-source software for
practical artificially intelligent one-shot classification systems with limited
resources for the benefit of researchers in related fields. Third, we present
an extensive experimental study using the OMNIGLOT dataset for computer vision
tasks and the UNSW-NB15 dataset for intrusion detection tasks that demonstrates
the versatility and efficiency of the DeROL framework.
</summary>
    <author>
      <name>Anton Puzanov</name>
    </author>
    <author>
      <name>Kobi Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01524v1</id>
    <updated>2018-08-04T19:34:39Z</updated>
    <published>2018-08-04T19:34:39Z</published>
    <title>Learning disentangled representation from 12-lead electrograms:
  application in localizing the origin of Ventricular Tachycardia</title>
    <summary>  The increasing availability of electrocardiogram (ECG) data has motivated the
use of data-driven models for automating various clinical tasks based on ECG
data. The development of subject-specific models are limited by the cost and
difficulty of obtaining sufficient training data for each individual. The
alternative of population model, however, faces challenges caused by the
significant inter-subject variations within the ECG data. We address this
challenge by investigating for the first time the problem of learning
representations for clinically-informative variables while disentangling other
factors of variations within the ECG data. In this work, we present a
conditional variational autoencoder (VAE) to extract the subject-specific
adjustment to the ECG data, conditioned on task-specific representations
learned from a deterministic encoder. To encourage the representation for
inter-subject variations to be independent from the task-specific
representation, maximum mean discrepancy is used to match all the moments
between the distributions learned by the VAE conditioning on the code from the
deterministic encoder. The learning of the task-specific representation is
regularized by a weak supervision in the form of contrastive regularization. We
apply the proposed method to a novel yet important clinical task of classifying
the origin of ventricular tachycardia (VT) into pre-defined segments,
demonstrating the efficacy of the proposed method against the standard VAE.
</summary>
    <author>
      <name>Prashnna K Gyawali</name>
    </author>
    <author>
      <name>B. Milan Horacek</name>
    </author>
    <author>
      <name>John L. Sapp</name>
    </author>
    <author>
      <name>Linwei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04390v4</id>
    <updated>2018-08-04T18:53:49Z</updated>
    <published>2015-10-15T03:50:01Z</published>
    <title>Dual Principal Component Pursuit</title>
    <summary>  We consider the problem of learning a linear subspace from data corrupted by
outliers. Classical approaches are typically designed for the case in which the
subspace dimension is small relative to the ambient dimension. Our approach
works with a dual representation of the subspace and hence aims to find its
orthogonal complement; as such, it is particularly suitable for subspaces whose
dimension is close to the ambient dimension (subspaces of high relative
dimension). We pose the problem of computing normal vectors to the inlier
subspace as a non-convex $\ell_1$ minimization problem on the sphere, which we
call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical
guarantees under which every global solution to DPCP is a vector in the
orthogonal complement of the inlier subspace. Moreover, we relax the non-convex
DPCP problem to a recursion of linear programs whose solutions are shown to
converge in a finite number of steps to a vector orthogonal to the subspace. In
particular, when the inlier subspace is a hyperplane, the solutions to the
recursion of linear programs converge to the global minimum of the non-convex
DPCP problem in a finite number of steps. We also propose algorithms based on
alternating minimization and iteratively re-weighted least squares, which are
suitable for dealing with large-scale data. Experiments on synthetic data show
that the proposed methods are able to handle more outliers and higher relative
dimensions than current state-of-the-art methods, while experiments in the
context of the three-view geometry problem in computer vision suggest that the
proposed methods can be a useful or even superior alternative to traditional
RANSAC-based approaches for computer vision and other applications.
</summary>
    <author>
      <name>Manolis C. Tsakiris</name>
    </author>
    <author>
      <name>Rene Vidal</name>
    </author>
    <link href="http://arxiv.org/abs/1510.04390v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04390v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01517v1</id>
    <updated>2018-08-04T18:26:24Z</updated>
    <published>2018-08-04T18:26:24Z</published>
    <title>DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging</title>
    <summary>  DELIMIT is a framework extension for deep learning in diffusion imaging,
which extends the basic framework PyTorch towards spherical signals. Based on
several novel layers, deep learning can be applied to spherical diffusion
imaging data in a very convenient way. First, two spherical harmonic
interpolation layers are added to the extension, which allow to transform the
signal from spherical surface space into the spherical harmonic space, and vice
versa. In addition, a local spherical convolution layer is introduced that adds
the possibility to include gradient neighborhood information within the
network. Furthermore, these extensions can also be utilized for the
preprocessing of diffusion signals.
</summary>
    <author>
      <name>Simon Koppers</name>
    </author>
    <author>
      <name>Dorit Merhof</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02017v1</id>
    <updated>2018-08-04T17:50:41Z</updated>
    <published>2018-08-04T17:50:41Z</published>
    <title>Withholding aggressive treatments may not accelerate time to death among
  dying ICU patients</title>
    <summary>  Critically ill patients may die despite aggressive treatment. In this study,
we examine trends in the application of two such treatments over a decade, as
well as the impact of these trends on survival durations in patients who die
within a month of ICU admission. We considered observational data available
from the MIMIC-III open-access ICU database, collected from June 2001 to
October 2012: These data comprise almost 60,000 hospital admissions for a total
of 38,645 unique adults.
  We explored two hypotheses: (i) administration of aggressive treatment during
the ICU stay immediately preceding end-of-life would decrease over the study
time period and (ii) time-to-death from ICU admission would also decrease due
to the decrease in aggressive treatment administration. Tests for significant
trends were performed and a p-value threshold of 0.05 was used to assess
statistical significance. We found that aggressive treatments in this
population were employed with decreasing frequency over the study period
duration, and also that reducing aggressive treatments for such patients may
not result in shorter times to death. The latter finding has implications for
end of life discussions that involve the possible use or non-use of such
treatments in those patients with very poor prognosis.
</summary>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <author>
      <name>Peter Clardy</name>
    </author>
    <author>
      <name>Leo Anthony Celi</name>
    </author>
    <author>
      <name>David J. Stone</name>
    </author>
    <author>
      <name>Robert S. Rudin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01486v1</id>
    <updated>2018-08-04T14:03:10Z</updated>
    <published>2018-08-04T14:03:10Z</published>
    <title>Spatial Deep Learning for Wireless Scheduling</title>
    <summary>  The optimal scheduling of interfering links in a dense wireless network with
full frequency reuse is a challenging task. The traditional method involves
first estimating all the interfering channel strengths then optimizing the
scheduling based on the model. This model-based method is however resource and
computationally intensive, because channel estimation is expensive in dense
networks; further, finding even a locally optimal solution of the resulting
optimization problem may be computationally complex. This paper shows that by
using a deep learning approach, it is possible to bypass channel estimation and
to schedule links efficiently based solely on the geographic locations of
transmitters and receivers. This is accomplished by using locally optimal
schedules generated using a fractional programming method for randomly deployed
device-to-device networks as training data, and by using a novel neural network
architecture that takes the geographic spatial convolutions of the interfering
or interfered neighboring nodes as input over multiple feedback stages to learn
the optimum solution. The resulting neural network gives near-optimal
performance for sum-rate maximization and is capable of generalizing to larger
deployment areas and to deployments of different link densities. Finally, this
paper proposes a novel scheduling approach that utilizes the sum-rate optimal
scheduling heuristics over judiciously chosen subsets of links to provide fair
scheduling across the network.
</summary>
    <author>
      <name>Wei Cui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <author>
      <name>Kaiming Shen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <author>
      <name>Wei Yu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is the full version of the paper to be presented at IEEE
  Global Communications Conference 2018. It includes 30 pages and 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00089v2</id>
    <updated>2018-08-04T13:37:51Z</updated>
    <published>2018-04-30T20:34:16Z</published>
    <title>Concolic Testing for Deep Neural Networks</title>
    <summary>  Concolic testing combines program execution and symbolic analysis to explore
the execution paths of a software program. This paper presents the first
concolic testing approach for Deep Neural Networks (DNNs). More specifically,
we formalise coverage criteria for DNNs that have been studied in the
literature, and then develop a coherent method for performing concolic testing
to increase test coverage. Our experimental results show the effectiveness of
the concolic testing approach in both achieving high coverage and finding
adversarial examples.
</summary>
    <author>
      <name>Youcheng Sun</name>
    </author>
    <author>
      <name>Min Wu</name>
    </author>
    <author>
      <name>Wenjie Ruan</name>
    </author>
    <author>
      <name>Xiaowei Huang</name>
    </author>
    <author>
      <name>Marta Kwiatkowska</name>
    </author>
    <author>
      <name>Daniel Kroening</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00089v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00089v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03258v1</id>
    <updated>2018-08-04T13:37:51Z</updated>
    <published>2018-08-04T13:37:51Z</published>
    <title>Application of Bounded Total Variation Denoising in Urban Traffic
  Analysis</title>
    <summary>  While it is believed that denoising is not always necessary in many big data
applications, we show in this paper that denoising is helpful in urban traffic
analysis by applying the method of bounded total variation denoising to the
urban road traffic prediction and clustering problem. We propose two
easy-to-implement methods to estimate the noise strength parameter in the
denoising algorithm, and apply the denoising algorithm to GPS-based traffic
data from Beijing taxi system. For the traffic prediction problem, we combine
neural network and history matching method for roads randomly chosen from an
urban area of Beijing. Numerical experiments show that the predicting accuracy
is improved significantly by applying the proposed bounded total variation
denoising algorithm. We also test the algorithm on clustering problem, where a
recently developed clustering analysis method is applied to more than one
hundred urban road segments in Beijing based on their velocity profiles. Better
clustering result is obtained after denoising.
</summary>
    <author>
      <name>Shanshan Tang</name>
    </author>
    <author>
      <name>Haijun Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06867v2</id>
    <updated>2018-08-04T09:34:16Z</updated>
    <published>2017-11-18T14:27:18Z</published>
    <title>From Common to Special: When Multi-Attribute Learning Meets Personalized
  Opinions</title>
    <summary>  Visual attributes, which refer to human-labeled semantic annotations, have
gained increasing popularity in a wide range of real world applications.
Generally, the existing attribute learning methods fall into two categories:
one focuses on learning user-specific labels separately for different
attributes, while the other one focuses on learning crowd-sourced global labels
jointly for multiple attributes. However, both categories ignore the joint
effect of the two mentioned factors: the personal diversity with respect to the
global consensus; and the intrinsic correlation among multiple attributes. To
overcome this challenge, we propose a novel model to learn user-specific
predictors across multiple attributes. In our proposed model, the diversity of
personalized opinions and the intrinsic relationship among multiple attributes
are unified in a common-to-special manner. To this end, we adopt a
three-component decomposition. Specifically, our model integrates a common
cognition factor, an attribute-specific bias factor and a user-specific bias
factor. Meanwhile Lasso and group Lasso penalties are adopted to leverage
efficient feature selection. Furthermore, theoretical analysis is conducted to
show that our proposed method could reach reasonable performance. Eventually,
the empirical study carried out in this paper demonstrates the effectiveness of
our proposed method.
</summary>
    <author>
      <name>Zhiyong Yang</name>
    </author>
    <author>
      <name>Qianqian Xu</name>
    </author>
    <author>
      <name>Xiaochun Cao</name>
    </author>
    <author>
      <name>Qingming Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1711.06867v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06867v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02771v4</id>
    <updated>2018-08-04T09:19:51Z</updated>
    <published>2017-04-10T09:20:47Z</published>
    <title>Group Importance Sampling for Particle Filtering and MCMC</title>
    <summary>  Bayesian methods and their implementations by means of sophisticated Monte
Carlo techniques have become very popular in signal processing over the last
years. Importance Sampling (IS) is a well-known Monte Carlo technique that
approximates integrals involving a posterior distribution by means of weighted
samples. In this work, we study the assignation of a single weighted sample
which compresses the information contained in a population of weighted samples.
Part of the theory that we present as Group Importance Sampling (GIS) has been
employed implicitly in different works in the literature. The provided analysis
yields several theoretical and practical consequences. For instance, we discuss
the application of GIS into the Sequential Importance Resampling framework and
show that Independent Multiple Try Metropolis schemes can be interpreted as a
standard Metropolis-Hastings algorithm, following the GIS approach. We also
introduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.
The first one, named Group Metropolis Sampling method, produces a Markov chain
of sets of weighted samples. All these sets are then employed for obtaining a
unique global estimator. The second one is the Distributed Particle
Metropolis-Hastings technique, where different parallel particle filters are
jointly used to drive an MCMC algorithm. Different resampled trajectories are
compared and then tested with a proper acceptance probability. The novel
schemes are tested in different numerical experiments such as learning the
hyperparameters of Gaussian Processes, two localization problems in a wireless
sensor network (with synthetic and real data) and the tracking of vegetation
parameters given satellite observations, where they are compared with several
benchmark Monte Carlo techniques. Three illustrative Matlab demos are also
provided.
</summary>
    <author>
      <name>L. Martino</name>
    </author>
    <author>
      <name>V. Elvira</name>
    </author>
    <author>
      <name>G. Camps-Valls</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Digital Signal Processing. Related Matlab demos are
  provided at https://github.com/lukafree/GIS.git</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02771v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02771v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.10119v6</id>
    <updated>2018-08-04T07:12:58Z</updated>
    <published>2018-01-30T18:07:01Z</published>
    <title>An Incremental Path-Following Splitting Method for Linearly Constrained
  Nonconvex Nonsmooth Programs</title>
    <summary>  The stationary point of Problem 2 is NOT the stationary point of Problem 1.
We are sorry and we are working on fixing this error.
</summary>
    <author>
      <name>Linbo Qiao</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Steven Hoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is an error in Theorem 9</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.10119v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.10119v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03708v2</id>
    <updated>2018-08-04T05:56:19Z</updated>
    <published>2018-07-10T15:24:36Z</published>
    <title>Generalized deterministic policy gradient algorithms</title>
    <summary>  We study a setting of reinforcement learning, where the state transition is a
convex combination of a stochastic continuous function and a deterministic
function. Such a setting include as a special case the stochastic state
transition setting, namely the setting of deterministic policy gradient (DPG).
We firstly give a simple example to illustrate that the deterministic policy
gradient may not exist under deterministic state transitions, and introduce a
theoretical technique to prove the existence of the policy gradient in this
generalized setting. Using this technique, we prove that the deterministic
policy gradient indeed exists for a certain set of discount factors, and
further prove two conditions that guarantee the existence for all discount
factors. We then derive a closed form of the policy gradient whenever exists.
Interestingly, the form of the policy gradient in such setting is equivalent to
that in DPG. Furthermore, to overcome the challenge of high sample complexity
of DPG in this setting, we propose the Generalized Deterministic Policy
Gradient (GDPG) algorithm. The main innovation of the algorithm is to optimize
a weighted objective of the original Markov decision process (MDP) and an
augmented MDP that simplifies the original one, and serves as its lower bound.
To solve the augmented MDP, we make use of the model-based methods which enable
fast convergence. We finally conduct extensive experiments comparing GDPG with
state-of-the-art methods on several standard benchmarks. Results demonstrate
that GDPG substantially outperforms other baselines in terms of both
convergence and long-term rewards.
</summary>
    <author>
      <name>Qingpeng Cai</name>
    </author>
    <author>
      <name>Ling Pan</name>
    </author>
    <author>
      <name>Pingzhong Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03708v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03708v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01410v1</id>
    <updated>2018-08-04T02:21:07Z</updated>
    <published>2018-08-04T02:21:07Z</published>
    <title>Predicting Expressive Speaking Style From Text In End-To-End Speech
  Synthesis</title>
    <summary>  Global Style Tokens (GSTs) are a recently-proposed method to learn latent
disentangled representations of high-dimensional data. GSTs can be used within
Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to
uncover expressive factors of variation in speaking style. In this work, we
introduce the Text-Predicted Global Style Token (TP-GST) architecture, which
treats GST combination weights or style embeddings as "virtual" speaking style
labels within Tacotron. TP-GST learns to predict stylistic renderings from text
alone, requiring neither explicit labels during training nor auxiliary inputs
for inference. We show that, when trained on a dataset of expressive speech,
our system generates audio with more pitch and energy variation than two
state-of-the-art baseline models. We further demonstrate that TP-GSTs can
synthesize speech with background noise removed, and corroborate these analyses
with positive results on human-rated listener preference audiobook tasks.
Finally, we demonstrate that multi-speaker TP-GST models successfully factorize
speaker identity and speaking style. We provide a website with audio samples
for each of our findings.
</summary>
    <author>
      <name>Daisy Stanton</name>
    </author>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>RJ Skerry-Ryan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01400v1</id>
    <updated>2018-08-04T01:26:07Z</updated>
    <published>2018-08-04T01:26:07Z</published>
    <title>code2seq: Generating Sequences from Structured Representations of Code</title>
    <summary>  The ability to generate natural language sequences from source code snippets
can be used for code summarization, documentation, and retrieval.
Sequence-to-sequence (seq2seq) models, adopted from neural machine translation
(NMT), have achieved state-of-the-art performance on these tasks by treating
source code as a sequence of tokens. We present ${\rm {\scriptsize CODE2SEQ}}$:
an alternative approach that leverages the syntactic structure of programming
languages to better encode source code. Our model represents a code snippet as
the set of paths in its abstract syntax tree (AST) and uses attention to select
the relevant paths during decoding, much like contemporary NMT models. We
demonstrate the effectiveness of our approach for two tasks, two programming
languages, and four datasets of up to 16M examples. Our model significantly
outperforms previous models that were specifically designed for programming
languages, as well as general state-of-the-art NMT models.
</summary>
    <author>
      <name>Uri Alon</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <author>
      <name>Eran Yahav</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06288v3</id>
    <updated>2018-08-04T01:25:57Z</updated>
    <published>2016-03-20T22:58:43Z</published>
    <title>Multi-fidelity Gaussian Process Bandit Optimisation</title>
    <summary>  In many scientific and engineering applications, we are tasked with the
maximisation of an expensive to evaluate black box function $f$. Traditional
settings for this problem assume just the availability of this single function.
However, in many cases, cheap approximations to $f$ may be obtainable. For
example, the expensive real world behaviour of a robot can be approximated by a
cheap computer simulation. We can use these approximations to eliminate low
function value regions cheaply and use the expensive evaluations of $f$ in a
small but promising region and speedily identify the optimum. We formalise this
task as a \emph{multi-fidelity} bandit problem where the target function and
its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a
novel method based on upper confidence bound techniques. In our theoretical
analysis we demonstrate that it exhibits precisely the above behaviour, and
achieves better regret than strategies which ignore multi-fidelity information.
Empirically, MF-GP-UCB outperforms such naive strategies and other
multi-fidelity methods on several synthetic and real experiments.
</summary>
    <author>
      <name>Kirthevasan Kandasamy</name>
    </author>
    <author>
      <name>Gautam Dasarathy</name>
    </author>
    <author>
      <name>Junier B. Oliva</name>
    </author>
    <author>
      <name>Jeff Schneider</name>
    </author>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version appeared at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06288v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06288v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07249v2</id>
    <updated>2018-08-04T01:14:30Z</updated>
    <published>2017-12-19T22:46:37Z</published>
    <title>Probabilistic Learning of Torque Controllers from Kinematic and Force
  Constraints</title>
    <summary>  When learning skills from demonstrations, one is often required to think in
advance about the appropriate task representation (usually in either
operational or configuration space). We here propose a probabilistic approach
for simultaneously learning and synthesizing torque control commands which take
into account task space, joint space and force constraints. We treat the
problem by considering different torque controllers acting on the robot, whose
relevance is learned probabilistically from demonstrations. This information is
used to combine the controllers by exploiting the properties of Gaussian
distributions, generating new torque commands that satisfy the important
features of the task. We validate the approach in two experimental scenarios
using 7-DoF torquecontrolled manipulators, with tasks that require the
consideration of different controllers to be properly executed.
</summary>
    <author>
      <name>João Silvério</name>
    </author>
    <author>
      <name>Yanlong Huang</name>
    </author>
    <author>
      <name>Leonel Rozo</name>
    </author>
    <author>
      <name>Sylvain Calinon</name>
    </author>
    <author>
      <name>Darwin G. Caldwell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at 2018 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.07249v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07249v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09045v3</id>
    <updated>2018-08-04T01:13:03Z</updated>
    <published>2018-05-23T10:43:56Z</published>
    <title>When Simple Exploration is Sample Efficient: Identifying Sufficient
  Conditions for Random Exploration to Yield PAC RL Algorithms</title>
    <summary>  Efficient exploration is one of the key challenges for reinforcement learning
(RL) algorithms. Most traditional sample efficiency bounds require strategic
exploration. Recently many deep RL algorithms with simple heuristic exploration
strategies that have few formal guarantees, achieve surprising success in many
domains. These results pose an important question about understanding these
exploration strategies such as $e$-greedy, as well as understanding what
characterize the difficulty of exploration in MDPs. In this work we propose
problem specific sample complexity bounds of $Q$ learning with random walk
exploration that rely on several structural properties. We also link our
theoretical results to some empirical benchmark domains, to illustrate if our
bound gives polynomial sample complexity in these domains and how that is
related with the empirical performance.
</summary>
    <author>
      <name>Yao Liu</name>
    </author>
    <author>
      <name>Emma Brunskill</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09045v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09045v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01701v4</id>
    <updated>2018-08-03T22:51:24Z</updated>
    <published>2017-04-06T04:02:35Z</published>
    <title>Learning Certifiably Optimal Rule Lists for Categorical Data</title>
    <summary>  We present the design and implementation of a custom discrete optimization
technique for building rule lists over a categorical feature space. Our
algorithm produces rule lists with optimal training performance, according to
the regularized empirical risk, with a certificate of optimality. By leveraging
algorithmic bounds, efficient data structures, and computational reuse, we
achieve several orders of magnitude speedup in time and a massive reduction of
memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is
possible to construct optimal sparse rule lists that are approximately as
accurate as the COMPAS proprietary risk prediction tool on data from Broward
County, Florida, but that are completely interpretable. This framework is a
novel alternative to CART and other decision tree methods for interpretable
modeling.
</summary>
    <author>
      <name>Elaine Angelino</name>
    </author>
    <author>
      <name>Nicholas Larus-Stone</name>
    </author>
    <author>
      <name>Daniel Alabi</name>
    </author>
    <author>
      <name>Margo Seltzer</name>
    </author>
    <author>
      <name>Cynthia Rudin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this work appeared in KDD '17 as "Learning
  Certifiably Optimal Rule Lists"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 18(234):1-78, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.01701v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01701v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04706v7</id>
    <updated>2018-08-03T22:13:06Z</updated>
    <published>2016-04-16T07:26:58Z</published>
    <title>DS-MLR: Exploiting Double Separability for Scaling up Distributed
  Multinomial Logistic Regression</title>
    <summary>  Scaling multinomial logistic regression to datasets with very large number of
data points and classes is challenging. This is primarily because one needs to
compute the log-partition function on every data point. This makes distributing
the computation hard. In this paper, we present a distributed stochastic
gradient descent based optimization method (DS-MLR) for scaling up multinomial
logistic regression problems to massive scale datasets without hitting any
storage constraints on the data and model parameters. Our algorithm exploits
double-separability, an attractive property that allows us to achieve both data
as well as model parallelism simultaneously. In addition, we introduce a
non-blocking and asynchronous variant of our algorithm that avoids
bulk-synchronization. We demonstrate the versatility of DS-MLR to various
scenarios in data and model parallelism, through an extensive empirical study
using several real-world datasets. In particular, we demonstrate the
scalability of DS-MLR by solving an extreme multi-class classification problem
on the Reddit dataset (159 GB data, 358 GB parameters) where, to the best of
our knowledge, no other existing methods apply.
</summary>
    <author>
      <name>Parameswaran Raman</name>
    </author>
    <author>
      <name>Sriram Srinivasan</name>
    </author>
    <author>
      <name>Shin Matsushima</name>
    </author>
    <author>
      <name>Xinhua Zhang</name>
    </author>
    <author>
      <name>Hyokun Yun</name>
    </author>
    <author>
      <name>S. V. N. Vishwanathan</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04706v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04706v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01345v1</id>
    <updated>2018-08-03T20:22:57Z</updated>
    <published>2018-08-03T20:22:57Z</published>
    <title>Multi-objective optimization to explicitly account for model complexity
  when learning Bayesian Networks</title>
    <summary>  Bayesian Networks have been widely used in the last decades in many fields,
to describe statistical dependencies among random variables. In general,
learning the structure of such models is a problem with considerable
theoretical interest that still poses many challenges. On the one hand, this is
a well-known NP-complete problem, which is practically hardened by the huge
search space of possible solutions. On the other hand, the phenomenon of
I-equivalence, i.e., different graphical structures underpinning the same set
of statistical dependencies, may lead to multimodal fitness landscapes further
hindering maximum likelihood approaches to solve the task. Despite all these
difficulties, greedy search methods based on a likelihood score coupled with a
regularization term to account for model complexity, have been shown to be
surprisingly effective in practice. In this paper, we consider the formulation
of the task of learning the structure of Bayesian Networks as an optimization
problem based on a likelihood score. Nevertheless, our approach do not adjust
this score by means of any of the complexity terms proposed in the literature;
instead, it accounts directly for the complexity of the discovered solutions by
exploiting a multi-objective optimization procedure. To this extent, we adopt
NSGA-II and define the first objective function to be the likelihood of a
solution and the second to be the number of selected arcs. We thoroughly
analyze the behavior of our method on a wide set of simulated data, and we
discuss the performance considering the goodness of the inferred solutions both
in terms of their objective functions and with respect to the retrieved
structure. Our results show that NSGA-II can converge to solutions
characterized by better likelihood and less arcs than classic approaches,
although paradoxically frequently characterized by a lower similarity to the
target network.
</summary>
    <author>
      <name>Paolo Cazzaniga</name>
    </author>
    <author>
      <name>Marco S. Nobile</name>
    </author>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08676v2</id>
    <updated>2018-08-03T20:16:37Z</updated>
    <published>2017-04-27T17:40:22Z</published>
    <title>Learning the structure of Bayesian Networks: A quantitative assessment
  of the effect of different algorithmic schemes</title>
    <summary>  One of the most challenging tasks when adopting Bayesian Networks (BNs) is
the one of learning their structure from data. This task is complicated by the
huge search space of possible solutions, and by the fact that the problem is
NP-hard. Hence, full enumeration of all the possible solutions is not always
feasible and approximations are often required. However, to the best of our
knowledge, a quantitative analysis of the performance and characteristics of
the different heuristics to solve this problem has never been done before.
  For this reason, in this work, we provide a detailed comparison of many
different state-of-the-arts methods for structural learning on simulated data
considering both BNs with discrete and continuous variables, and with different
rates of noise in the data. In particular, we investigate the performance of
different widespread scores and algorithmic approaches proposed for the
inference and the statistical pitfalls within them.
</summary>
    <author>
      <name>Stefano Beretta</name>
    </author>
    <author>
      <name>Mauro Castelli</name>
    </author>
    <author>
      <name>Ivo Goncalves</name>
    </author>
    <author>
      <name>Roberto Henriques</name>
    </author>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <link href="http://arxiv.org/abs/1704.08676v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08676v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02334v1</id>
    <updated>2018-08-03T19:10:39Z</updated>
    <published>2018-08-03T19:10:39Z</published>
    <title>A novel topology design approach using an integrated deep learning
  network architecture</title>
    <summary>  Topology design optimization offers tremendous opportunity in design and
manufacturing freedoms by designing and producing a part from the ground-up
without a meaningful initial design as required by conventional shape design
optimization approaches. Ideally, with adequate problem statements, to
formulate and solve the topology design problem using a standard topology
optimization process, such as SIMP (Simplified Isotropic Material with
Penalization) is possible. In reality, an estimated over thousands of design
iterations is often required for just a few design variables, the conventional
optimization approach is in general impractical or computationally unachievable
for real world applications significantly diluting the development of the
topology optimization technology. There is, therefore, a need for a different
approach that will be able to optimize the initial design topology effectively
and rapidly. Therefore, this work presents a new topology design procedure to
generate optimal structures using an integrated Generative Adversarial Networks
(GANs) and convolutional neural network architecture.
</summary>
    <author>
      <name>Sharad Rawat</name>
    </author>
    <author>
      <name>M. H. Herman Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 Figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06567v2</id>
    <updated>2018-08-03T17:41:19Z</updated>
    <published>2018-03-17T20:13:28Z</published>
    <title>A Dual Approach to Scalable Verification of Deep Networks</title>
    <summary>  This paper addresses the problem of formally verifying desirable properties
of neural networks, i.e., obtaining provable guarantees that neural networks
satisfy specifications relating their inputs and outputs (robustness to bounded
norm adversarial perturbations, for example). Most previous work on this topic
was limited in its applicability by the size of the network, network
architecture and the complexity of properties to be verified. In contrast, our
framework applies to a general class of activation functions and specifications
on neural network inputs and outputs. We formulate verification as an
optimization problem (seeking to find the largest violation of the
specification) and solve a Lagrangian relaxation of the optimization problem to
obtain an upper bound on the worst case violation of the specification being
verified. Our approach is anytime i.e. it can be stopped at any time and a
valid bound on the maximum violation can be obtained. We develop specialized
verification algorithms with provable tightness guarantees under special
assumptions and demonstrate the practical significance of our general
verification approach on a variety of verification tasks.
</summary>
    <author>
      <name> Krishnamurthy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dj</arxiv:affiliation>
    </author>
    <author>
      <name> Dvijotham</name>
    </author>
    <author>
      <name>Robert Stanforth</name>
    </author>
    <author>
      <name>Sven Gowal</name>
    </author>
    <author>
      <name>Timothy Mann</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06567v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06567v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10454v2</id>
    <updated>2018-08-03T16:57:47Z</updated>
    <published>2018-07-27T06:50:43Z</published>
    <title>From Adversarial Training to Generative Adversarial Networks</title>
    <summary>  In this paper, we are interested in two seemingly different concepts:
\textit{adversarial training} and \textit{generative adversarial networks
(GANs)}. Particularly, how these techniques help to improve each other. To this
end, we analyze the limitation of adversarial training as the defense method,
starting from questioning how well the robustness of a model can generalize.
Then, we successfully improve the generalizability via data augmentation by the
"fake" images sampled from generative adversarial networks. After that, we are
surprised to see that the resulting robust classifier leads to a better
generator, for free. We intuitively explain this interesting phenomenon and
leave the theoretical analysis for future work. Motivated by these
observations, we propose a system that combines generator, discriminator, and
adversarial attacker in a single network. After end-to-end training and fine
tuning, our method can simultaneously improve the robustness of classifiers,
measured by accuracy under strong adversarial attacks; and the quality of
generators, evaluated both aesthetically and quantitatively. In terms of the
classifier, we achieve better robustness than the state-of-the-art adversarial
training algorithm proposed in (Madry etla., 2017), while our generator
achieves competitive performance compared with SN-GAN (Miyato and Koyama,
2018). Source code is publicly available online at
\url{https://github.com/anonymous}.
</summary>
    <author>
      <name>Xuanqing Liu</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2018 submission, under review. v2: More experiments on comparing
  inception score, release code and some minor fixes</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10454v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10454v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04446v1</id>
    <updated>2018-08-03T14:32:02Z</updated>
    <published>2018-08-03T14:32:02Z</published>
    <title>Visual Reasoning with Multi-hop Feature Modulation</title>
    <summary>  Recent breakthroughs in computer vision and natural language processing have
spurred interest in challenging multi-modal tasks such as visual
question-answering and visual dialogue. For such tasks, one successful approach
is to condition image-based convolutional network computation on language via
Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and
shifting. We propose to generate the parameters of FiLM layers going up the
hierarchy of a convolutional network in a multi-hop fashion rather than all at
once, as in prior work. By alternating between attending to the language input
and generating FiLM layer parameters, this approach is better able to scale to
settings with longer input sequences such as dialogue. We demonstrate that
multi-hop FiLM generation achieves state-of-the-art for the short input
sequence task ReferIt --- on-par with single-hop FiLM generation --- while also
significantly outperforming prior state-of-the-art and single-hop FiLM
generation on the GuessWhat?! visual dialogue task.
</summary>
    <author>
      <name>Florian Strub</name>
    </author>
    <author>
      <name>Mathieu Seurin</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Harm de Vries</name>
    </author>
    <author>
      <name>Jérémie Mary</name>
    </author>
    <author>
      <name>Philippe Preux</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Olivier Pietquin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc of ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02303v3</id>
    <updated>2018-08-03T13:44:20Z</updated>
    <published>2018-07-06T08:14:27Z</published>
    <title>A survey on policy search algorithms for learning robot controllers in a
  handful of trials</title>
    <summary>  Most policy search algorithms require thousands of training episodes to find
an effective policy, which is often infeasible with a physical robot. This
survey article focuses on the extreme other end of the spectrum: how can a
robot adapt with only a handful of trials (a dozen) and a few minutes? By
analogy with the word "big-data", we refer to this challenge as "micro-data
reinforcement learning". We show that a first strategy is to leverage prior
knowledge on the policy structure (e.g., dynamic movement primitives), on the
policy parameters (e.g., demonstrations), or on the dynamics (e.g.,
simulators). A second strategy is to create data-driven surrogate models of the
expected reward (e.g., Bayesian optimization) or the dynamical model (e.g.,
model-based policy search), so that the policy optimizer queries the model
instead of the real system. Overall, all successful micro-data algorithms
combine these two strategies by varying the kind of model and prior knowledge.
The current scientific challenges essentially revolve around scaling up to
complex robots (e.g., humanoids), designing generic priors, and optimizing the
computing time.
</summary>
    <author>
      <name>Konstantinos Chatzilygeroudis</name>
    </author>
    <author>
      <name>Vassilis Vassiliades</name>
    </author>
    <author>
      <name>Freek Stulp</name>
    </author>
    <author>
      <name>Sylvain Calinon</name>
    </author>
    <author>
      <name>Jean-Baptiste Mouret</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02303v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02303v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01184v1</id>
    <updated>2018-08-03T13:36:38Z</updated>
    <published>2018-08-03T13:36:38Z</published>
    <title>Structured Neural Network Dynamics for Model-based Control</title>
    <summary>  We present a structured neural network architecture that is inspired by
linear time-varying dynamical systems. The network is designed to mimic the
properties of linear dynamical systems which makes analysis and control simple.
The architecture facilitates the integration of learned system models with
gradient-based model predictive control algorithms, and removes the requirement
of computing potentially costly derivatives online. We demonstrate the efficacy
of this modeling technique in computing autonomous control policies through
evaluation in a variety of standard continuous control domains.
</summary>
    <author>
      <name>Alexander Broad</name>
    </author>
    <author>
      <name>Ian Abraham</name>
    </author>
    <author>
      <name>Todd Murphey</name>
    </author>
    <author>
      <name>Brenna Argall</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01181v1</id>
    <updated>2018-08-03T13:25:39Z</updated>
    <published>2018-08-03T13:25:39Z</published>
    <title>Robust Spectral Filtering and Anomaly Detection</title>
    <summary>  We consider a setting, where the output of a linear dynamical system (LDS)
is, with an unknown but fixed probability, replaced by noise. There, we present
a robust method for the prediction of the outputs of the LDS and identification
of the samples of noise, and prove guarantees on its statistical performance.
One application lies in anomaly detection: the samples of noise, unlikely to
have been generated by the dynamics, can be flagged to operators of the system
for further study.
</summary>
    <author>
      <name>Jakub Marecek</name>
    </author>
    <author>
      <name>Tigran Tchrakian</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01175v1</id>
    <updated>2018-08-03T12:57:15Z</updated>
    <published>2018-08-03T12:57:15Z</published>
    <title>Content-driven, unsupervised clustering of news articles through
  multiscale graph partitioning</title>
    <summary>  The explosion in the amount of news and journalistic content being generated
across the globe, coupled with extended and instantaneous access to information
through online media, makes it difficult and time-consuming to monitor news
developments and opinion formation in real time. There is an increasing need
for tools that can pre-process, analyse and classify raw text to extract
interpretable content; specifically, identifying topics and content-driven
groupings of articles. We present here such a methodology that brings together
powerful vector embeddings from Natural Language Processing with tools from
Graph Theory that exploit diffusive dynamics on graphs to reveal natural
partitions across scales. Our framework uses a recent deep neural network text
analysis methodology (Doc2vec) to represent text in vector form and then
applies a multi-scale community detection method (Markov Stability) to
partition a similarity graph of document vectors. The method allows us to
obtain clusters of documents with similar content, at different levels of
resolution, in an unsupervised manner. We showcase our approach with the
analysis of a corpus of 9,000 news articles published by Vox Media over one
year. Our results show consistent groupings of documents according to content
without a priori assumptions about the number or type of clusters to be found.
The multilevel clustering reveals a quasi-hierarchy of topics and subtopics
with increased intelligibility and improved topic coherence as compared to
external taxonomy services and standard topic detection methods.
</summary>
    <author>
      <name>M. Tarik Altuncu</name>
    </author>
    <author>
      <name>Sophia N. Yaliraki</name>
    </author>
    <author>
      <name>Mauricio Barahona</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; 5 figures; To present at KDD 2018: Data Science, Journalism
  &amp; Media workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01174v1</id>
    <updated>2018-08-03T12:57:12Z</updated>
    <published>2018-08-03T12:57:12Z</published>
    <title>Generalization Error in Deep Learning</title>
    <summary>  Deep learning models have lately shown great performance in various fields
such as computer vision, speech recognition, speech translation, and natural
language processing. However, alongside their state-of-the-art performance, it
is still generally unclear what is the source of their generalization ability.
Thus, an important question is what makes deep neural networks able to
generalize well from the training set to new data. In this article, we provide
an overview of the existing theory and bounds for the characterization of the
generalization error of deep neural networks, combining both classical and more
recent theoretical and empirical results.
</summary>
    <author>
      <name>Daniel Jakubovitz</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Miguel R. D. Rodrigues</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07987v2</id>
    <updated>2018-08-03T11:27:28Z</updated>
    <published>2018-07-20T18:20:34Z</published>
    <title>Deep Learning</title>
    <summary>  Deep learning (DL) is a high dimensional data reduction technique for
constructing high-dimensional predictors in input-output models. DL is a form
of machine learning that uses hierarchical layers of latent features. In this
article, we review the state-of-the-art of deep learning from a modeling and
algorithmic perspective. We provide a list of successful areas of applications
in Artificial Intelligence (AI), Image Processing, Robotics and Automation.
Deep learning is predictive in its nature rather then inferential and can be
viewed as a black-box methodology for high-dimensional function estimation.
</summary>
    <author>
      <name>Nicholas G. Polson</name>
    </author>
    <author>
      <name>Vadim O. Sokolov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1602.06561</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01918v2</id>
    <updated>2018-08-03T11:05:06Z</updated>
    <published>2018-06-05T20:05:51Z</published>
    <title>A Framework for the construction of upper bounds on the number of affine
  linear regions of ReLU feed-forward neural networks</title>
    <summary>  In this work we present a new framework to derive upper bounds on the number
regions of feed-forward neural nets with ReLU activation functions. We derive
all existing such bounds as special cases, however in a different
representation in terms of matrices. This provides new insight and allows a
more detailed analysis of the corresponding bounds. In particular, we provide a
Jordan-like decomposition for the involved matrices and present new tighter
results for an asymptotic setting. Moreover, new even stronger bounds may be
obtained from our framework.
</summary>
    <author>
      <name>Peter Hinz</name>
    </author>
    <author>
      <name>Sara van de Geer</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01918v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01918v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
