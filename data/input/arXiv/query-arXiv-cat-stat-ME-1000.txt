<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dcat%3Astat.ME%26id_list%3D%26start%3D0%26max_results%3D1000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=cat:stat.ME&amp;id_list=&amp;start=0&amp;max_results=1000</title>
  <id>http://arxiv.org/api/xTNXaPS75b1BX8nw9UTQuNDSsrc</id>
  <updated>2018-09-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">9826</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/1809.01606v1</id>
    <updated>2018-09-05T16:22:18Z</updated>
    <published>2018-09-05T16:22:18Z</published>
    <title>Determining the Dependence Structure of Multivariate Extremes</title>
    <summary>  In multivariate extreme value analysis, the nature of the extremal dependence
between variables should be considered when selecting appropriate statistical
models. Interest often lies with determining which subsets of variables can
take their largest values simultaneously, while the others are of smaller
order. Our approach is based on exploiting hidden regular variation properties
on a collection of non-standard cones. This provides a new set of indices that
reveal aspects of the extremal dependence structure not available through any
existing measures of dependence. We derive theoretical properties of these
indices, demonstrate their value through a series of examples, and develop
methods of inference which also estimate the proportion of extremal mass
associated with each cone. We consider two inferential approaches: in the
first, we approximate the cones via a truncation of the variables; the second
involves partitioning the simplex associated with their radial-angular
components. We apply the methods to UK river flows, estimating the
probabilities of different subsets of sites being simultaneously large.
</summary>
    <author>
      <name>Emma S. Simpson</name>
    </author>
    <author>
      <name>Jennifer L. Wadsworth</name>
    </author>
    <author>
      <name>Jonathan A. Tawn</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08445v2</id>
    <updated>2018-09-05T14:06:03Z</updated>
    <published>2017-05-23T17:59:32Z</published>
    <title>Stratification as a general variance reduction method for Markov chain
  Monte Carlo</title>
    <summary>  The Eigenvector Method for Umbrella Sampling (EMUS) belongs to a popular
class of methods in statistical mechanics which adapt the principle of
stratified survey sampling to the computation of free energies. By theoretical
analysis and numerical experiments, we demonstrate that EMUS is an efficient
general method for computing averages with respect to arbitrary target
distributions. We show that EMUS can be dramatically more efficient than direct
MCMC when the target distribution is multimodal or when the goal is to compute
tail probabilities.
</summary>
    <author>
      <name>Aaron R. Dinner</name>
    </author>
    <author>
      <name>Erik Thiede</name>
    </author>
    <author>
      <name>Brian Van Koten</name>
    </author>
    <author>
      <name>Jonathan Weare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08445v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08445v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05, 65C60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00084v4</id>
    <updated>2018-09-05T08:36:22Z</updated>
    <published>2015-10-01T01:24:20Z</published>
    <title>A Direct Approach for Sparse Quadratic Discriminant Analysis</title>
    <summary>  Quadratic discriminant analysis (QDA) is a standard tool for classification
due to its simplicity and flexibility. Because the number of its parameters
scales quadratically with the number of the variables, QDA is not practical,
however, when the dimensionality is relatively large. To address this, we
propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional
data. Formulated in a simple and coherent framework, DA-QDA aims to directly
estimate the key quantities in the Bayes discriminant function including
quadratic interactions and a linear index of the variables for classification.
Under appropriate sparsity assumptions, we establish consistency results for
estimating the interactions and the linear index, and further demonstrate that
the misclassification rate of our procedure converges to the optimal Bayes
risk, even when the dimensionality is exponentially high with respect to the
sample size. An efficient algorithm based on the alternating direction method
of multipliers (ADMM) is developed for finding interactions, which is much
faster than its competitor in the literature. The promising performance of
DA-QDA is illustrated via extensive simulation studies and the analysis of four
real datasets.
</summary>
    <author>
      <name>Binyan Jiang</name>
    </author>
    <author>
      <name>Xiangyu Wang</name>
    </author>
    <author>
      <name>Chenlei Leng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated to the JMLR format</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00084v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00084v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01322v1</id>
    <updated>2018-09-05T04:55:24Z</updated>
    <published>2018-09-05T04:55:24Z</published>
    <title>Preferential sampling for presence/absence data and for fusion of
  presence/absence data with presence-only data</title>
    <summary>  Presence/absence data and presence-only data are the two customary sources
for learning about species distributions over a region. We illuminate the
fundamental modeling differences between the two types of data. Most simply,
locations are considered as fixed under presence/absence data; locations are
random under presence-only data. The definition of "probability of presence" is
incompatible between the two. So, we take issue with modeling strategies in the
literature which ignore this incompatibility, which assume that
presence/absence modeling can be induced from presence-only specifications and
therefore, that fusion of presence-only and presence/absence data sources is
routine. We argue that presence/absence data should be modeled at point level.
That is, we need to specify a surface which provides the probability of
presence at any location in the region. A realization from this surface is a
binary map yielding the results of Bernoulli trials across all locations.
Presence-only data should be modeled as a point pattern driven by specification
of an intensity function. We further argue that, with just presence/absence
data, preferential sampling, using a shared process perspective, can improve
our estimated presence/absence surface and prediction of presence. We also
argue that preferential sampling can enable a probabilistically coherent fusion
of the two data types. We illustrate with two real datasets, one
presence/absence, one presence-only for invasive species presence in New
England in the United States. We demonstrate that potential bias in sampling
locations can affect inference with regard to presence/absence and show that
inference can be improved with preferential sampling ideas. We also provide a
probabilistically coherent fusion of the two datasets to again improve
inference with regard to presence/absence.
</summary>
    <author>
      <name>Alan. E. Gelfand</name>
    </author>
    <author>
      <name>Shinichiro Shirota</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01319v1</id>
    <updated>2018-09-05T04:44:55Z</updated>
    <published>2018-09-05T04:44:55Z</published>
    <title>Cross validation residuals for generalised least squares and other
  correlated data models</title>
    <summary>  Cross validation residuals are well known for the ordinary least squares
model. Here leave-M-out cross validation is extended to generalised least
squares. The relationship between cross validation residuals and Cook's
distance is demonstrated, in terms of an approximation to the difference in the
generalised residual sum of squares for a model fit to all the data (training
and test) and a model fit to a reduced dataset (training data only). For
generalised least squares, as for ordinary least squares, there is no need to
refit the model to reduced size datasets as all the values for K fold cross
validation are available after fitting the model to all the data.
</summary>
    <author>
      <name>Ingrid Annette Baade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01291v1</id>
    <updated>2018-09-05T01:45:44Z</updated>
    <published>2018-09-05T01:45:44Z</published>
    <title>An Online Updating Approach for Testing the Proportional Hazards
  Assumption with Streams of Big Survival Data</title>
    <summary>  The Cox model, which remains as the first choice in analyzing time-to-event
data even for large datasets, relies on the proportional hazards assumption.
When the data size exceeds the computer memory, the standard statistics for
testing the proportional hazards assumption can no longer b e easily
calculated. We propose an online up dating approach with minimal storage
requirement that up dates the standard test statistic as each new block of data
becomes available. Under the null hypothesis of proportional hazards, the
proposed statistic is shown to have the same asymptotic distribution as the
standard version if it could be computed with a super computer. In simulation
studies, the test and its variant based on most recent data blocks maintain
their sizes when the proportional hazards assumption holds and have substantial
power to detect different violations of the proportional hazards assumption.
The approach is illustrated with the survival analysis of patients with
lymphoma cancer from the Surveillance, Epidemiology, and End Results Program.
The proposed test promptly identified deviation from the proportional hazards
assumption that was not captured by the test based on the entire data.
</summary>
    <author>
      <name>Yishu Xue</name>
    </author>
    <author>
      <name>HaiYing Wang</name>
    </author>
    <author>
      <name>Jun Yan</name>
    </author>
    <author>
      <name>Elizabeth D. Schifano</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01278v1</id>
    <updated>2018-09-05T00:35:03Z</updated>
    <published>2018-09-05T00:35:03Z</published>
    <title>Two-sample aggregate data meta-analysis of medians</title>
    <summary>  We consider the problem of meta-analyzing two-group studies that report the
median of the outcome. Often, these studies are excluded from meta-analysis
because there are no well-established statistical methods to pool the
difference of medians. To include these studies in meta-analysis, several
authors have recently proposed methods to estimate the sample mean and standard
deviation from the median, sample size, and several commonly reported measures
of spread. Researchers frequently apply these methods to estimate the
difference of means and its variance for each primary study and pool the
difference of means using inverse variance weighting. In this work, we develop
several methods to directly meta-analyze the difference of medians. We conduct
a simulation study evaluating the performance of the proposed median-based
methods and the competing transformation-based methods. The simulation results
show that the median-based methods outperform the transformation-based methods
when meta-analyzing studies that report the median of the outcome, especially
when the outcome is skewed. Moreover, we illustrate the various methods on a
real-life data set.
</summary>
    <author>
      <name>Sean McGrath</name>
    </author>
    <author>
      <name>Hojoon Sohn</name>
    </author>
    <author>
      <name>Russell Steele</name>
    </author>
    <author>
      <name>Andrea Benedetti</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03230v2</id>
    <updated>2018-09-04T16:20:07Z</updated>
    <published>2018-08-09T16:46:51Z</published>
    <title>Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal
  densities?</title>
    <summary>  Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of
Markov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity
of HMC algorithms is their excellent performance as the dimension $d$ of the
target becomes large: under conditions that are satisfied for many common
statistical models, optimally-tuned HMC algorithms have a running time that
scales like $d^{0.25}$. In stark contrast, the running time of the usual
Random-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This
superior scaling of the HMC algorithm with dimension is attributed to the fact
that it, unlike RWM, incorporates the gradient information in the proposal
distribution. In this paper, we investigate a different scaling question: does
HMC beat RWM for highly $\textit{multimodal}$ targets? We find that the answer
is often $\textit{no}$. We compute the spectral gaps for both the algorithms
for a specific class of multimodal target densities, and show that they are
identical. The key reason is that, within one mode, the gradient is effectively
ignorant about other modes, thus negating the advantage the HMC algorithm
enjoys in unimodal targets. We also give heuristic arguments suggesting that
the above observation may hold quite generally. Our main tool for answering
this question is a novel simple formula for the conductance of HMC using
Liouville's theorem. This result allows us to compute the spectral gap of HMC
algorithms, for both the classical HMC with isotropic momentum and the recent
Riemannian HMC, for multimodal targets.
</summary>
    <author>
      <name>Oren Mangoubi</name>
    </author>
    <author>
      <name>Natesh S. Pillai</name>
    </author>
    <author>
      <name>Aaron Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03230v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03230v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01038v1</id>
    <updated>2018-09-04T15:21:08Z</updated>
    <published>2018-09-04T15:21:08Z</published>
    <title>Shape-Enforcing Operators for Point and Interval Estimators</title>
    <summary>  A common problem in statistics is to estimate and make inference on functions
that satisfy shape restrictions. For example, distribution functions are
nondecreasing and range between zero and one, height growth charts are
nondecreasing in age, and production functions are nondecreasing and
quasi-concave in input quantities. We propose a method to enforce these
restrictions ex post on point and interval estimates of the target function by
applying functional operators. If an operator satisfies certain properties that
we make precise, the shape-enforced point estimates are closer to the target
function than the original point estimates and the shape-enforced interval
estimates have greater coverage and shorter length than the original interval
estimates. We show that these properties hold for six different operators that
cover commonly used shape restrictions in practice: range, convexity,
monotonicity, monotone convexity, quasi-convexity, and monotone
quasi-convexity. We illustrate the results with an empirical application to the
estimation of a height growth chart for infants in India.
</summary>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <author>
      <name>Scott Kostyshak</name>
    </author>
    <author>
      <name>Ye Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F10, 62F25, 62G05, 62G15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.02704v4</id>
    <updated>2018-09-04T15:06:06Z</updated>
    <published>2018-03-07T15:08:08Z</published>
    <title>A deterministic balancing score algorithm to avoid common pitfalls of
  propensity score matching</title>
    <summary>  Propensity score matching (PSM) is the de-facto standard for estimating
causal effects in observational studies. We show that PSM and its
implementations are susceptible to several major drawbacks and illustrate these
findings using a case study with $17,427$ patients. We derive four formal
properties an optimal statistical matching algorithm should meet, and propose
Deterministic Balancing Score exact Matching (DBSeM) which meets the
aforementioned properties for an exact matching. Furthermore, we investigate
one of the main problems of PSM, that is that common PSM results in one valid
set of matched pairs or a bootstrapped PSM in a selection of possible valid
sets of matched pairs. For exact matchings we provide the mathematical proof,
that DBSeM, as a result, delivers the expected value of all valid sets of
matched pairs for the investigated dataset.
</summary>
    <author>
      <name>Felix Bestehorn</name>
    </author>
    <author>
      <name>Maike Bestehorn</name>
    </author>
    <author>
      <name>Markus Bestehorn</name>
    </author>
    <author>
      <name>Christian Kirches</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02704v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02704v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01028v1</id>
    <updated>2018-09-04T14:49:36Z</updated>
    <published>2018-09-04T14:49:36Z</published>
    <title>Determining the Number of Communities in Degree-corrected Stochastic
  Block Models</title>
    <summary>  We propose to estimate the number of communities in degree-corrected
stochastic block models based on a pseudo likelihood ratio. For estimation, we
consider a spectral clustering together with binary segmentation method. This
approach guarantees an upper bound for the pseudo likelihood ratio statistic
when the model is over-fitted. We also derive its limiting distribution when
the model is under-fitted. Based on these properties, we establish the
consistency of our estimator for the true number of communities. Developing
these theoretical properties require a mild condition on the average degree:
growing at a rate faster than log(n), where n is the number of nodes. Our
proposed method is further illustrated by simulation studies and analysis of
real-world networks. The numerical results show that our approach has
satisfactory performance when the network is sparse and/or has unbalanced
communities.
</summary>
    <author>
      <name>Shujie Ma</name>
    </author>
    <author>
      <name>Liangjun Su</name>
    </author>
    <author>
      <name>Yichong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">55 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00946v3</id>
    <updated>2018-09-04T14:20:02Z</updated>
    <published>2017-04-04T10:33:10Z</published>
    <title>Approximation results regarding the multiple-output mixture of linear
  experts model</title>
    <summary>  Mixture of experts (MoE) models are a class of artificial neural networks
that can be used for functional approximation and probabilistic modeling. An
important class of MoE models is the class of mixture of linear experts (MoLE)
models, where the expert functions map to real topological output spaces. There
are a number of powerful approximation results regarding MoLE models, when the
output space is univariate. These results guarantee the ability of MoLE mean
functions to approximate arbitrary continuous functions, and MoLE models
themselves to approximate arbitrary conditional probability density functions.
We utilize and extend upon the univariate approximation results in order to
prove a pair of useful results for situations where the output spaces are
multivariate.
</summary>
    <author>
      <name>Hien D. Nguyen</name>
    </author>
    <author>
      <name>Faicel Chamroukhi</name>
    </author>
    <author>
      <name>Florence Forbes</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00946v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00946v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06127v3</id>
    <updated>2018-09-04T13:56:25Z</updated>
    <published>2016-12-19T11:21:09Z</published>
    <title>Controlling the Size of Autocorrelation Robust Tests</title>
    <summary>  Autocorrelation robust tests are notorious for suffering from size
distortions and power problems. We investigate under which conditions the size
of autocorrelation robust tests can be controlled by an appropriate choice of
critical value.
</summary>
    <author>
      <name>Benedikt M. Pötscher</name>
    </author>
    <author>
      <name>David Preinerstorfer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor changes including correction of a minor error</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06127v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06127v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J05, 62E15, 62F03, 62G10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03519v2</id>
    <updated>2018-09-04T13:38:41Z</updated>
    <published>2017-10-10T11:33:19Z</published>
    <title>Volatility estimation for stochastic PDEs using high-frequency
  observations</title>
    <summary>  We study the parameter estimation for parabolic, linear, second order,
stochastic partial differential equations (SPDEs) observing a mild solution on
a discrete grid in time and space. A high-frequency regime is considered where
the mesh of the grid in the time variable goes to zero. Focusing on volatility
estimation, we provide an explicit and easy to implement method of moments
estimator based on squared increments. The estimator is consistent and admits a
central limit theorem. This is established moreover for the estimation of the
integrated volatility in a semi-parametric framework and for the joint
estimation of the volatility and an unknown parameter in the differential
operator. Starting from a representation of the solution as an infinite factor
model and exploiting mixing-type properties of time series, the theory
considerably differs from the statistics for semi-martingales literature. The
performance of the method is illustrated in a simulation study.
</summary>
    <author>
      <name>Markus Bibinger</name>
    </author>
    <author>
      <name>Mathias Trabs</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03519v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03519v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10 (Primary), 60H15 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07874v2</id>
    <updated>2018-09-04T13:12:17Z</updated>
    <published>2018-07-20T14:41:26Z</published>
    <title>On a Loss-based prior for the number of components in mixture models</title>
    <summary>  We propose a prior distribution for the number of components of a finite
mixture model. The novelty is that the prior distribution is obtained by
considering the loss one would incur if the true value representing the number
of components were not considered. The prior has an elegant and easy to
implement structure, which allows to naturally include any prior information
one may have as well as to opt for a default solution in cases where this
information is not available. The performance of the prior, and comparison with
existing alternatives, is studied through the analysis of both real and
simulated data.
</summary>
    <author>
      <name>Clara Grazian</name>
    </author>
    <author>
      <name>Cristiano Villa</name>
    </author>
    <author>
      <name>Brunero Liseo</name>
    </author>
    <link href="http://arxiv.org/abs/1807.07874v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07874v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01104v2</id>
    <updated>2018-09-04T10:35:30Z</updated>
    <published>2018-05-03T03:55:43Z</published>
    <title>Deep Factor Alpha</title>
    <summary>  Does a factor model exist to absorb all existing anomalies? We provide a deep
learning automated solution to generate long-short factors using a
high-dimensional firm characteristics. Sorting securities on firm
characteristics is a common practice in finance and a nonlinear activation
function built into deep learning. Our algorithm performs a nonlinear search
and finds the optimal transformation of characteristics used for security
sorting, with one asset pricing objective: minimizing alphas. Our deep factors,
hidden neurons in the neural network, are trained greedily with the backward
propagation feedback from the loss function that considers both time series and
cross-sectional variations. Our conditional forecast generalizes a benchmark,
such as CAPM, and includes Fama-French type models as special cases. We have
designed a train-validation-test study for monthly U.S. equity returns from
1975 to 2017 and 57 published firm characteristics. In an out-of-sample
evaluation, the conditional deep factor model shows a forecasting improvement
over the benchmark with factors that offer significant alphas. The conclusion
is the improvement of insignificant alphas for some anomalies as well as sorted
portfolios.
</summary>
    <author>
      <name>Guanhao Feng</name>
    </author>
    <author>
      <name>Nicholas G. Polson</name>
    </author>
    <author>
      <name>Jianeng Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05570v2</id>
    <updated>2018-09-04T10:11:37Z</updated>
    <published>2018-02-14T14:59:32Z</published>
    <title>Optimal Transport: Fast Probabilistic Approximation with Exact Solvers</title>
    <summary>  We propose a simple subsampling scheme for fast randomized approximate
computation of optimal transport distances. This scheme operates on a random
subset of the full data and can use any exact algorithm as a black-box
back-end, including state-of-the-art solvers and entropically penalized
versions. It is based on averaging the exact distances between empirical
measures generated from independent samples from the original measures and can
easily be tuned towards higher accuracy or shorter computation times. To this
end, we give non-asymptotic deviation bounds for its accuracy in the case of
discrete optimal transport problems. In particular, we show that in many
important instances, including images (2D-histograms), the approximation error
is independent of the size of the full problem. We present numerical
experiments that demonstrate that a very good approximation in typical
applications can be obtained in a computation time that is several orders of
magnitude smaller than what is required for exact computation of the full
problem.
</summary>
    <author>
      <name>Max Sommerfeld</name>
    </author>
    <author>
      <name>Jörn Schrieber</name>
    </author>
    <author>
      <name>Yoav Zemel</name>
    </author>
    <author>
      <name>Axel Munk</name>
    </author>
    <link href="http://arxiv.org/abs/1802.05570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00737v1</id>
    <updated>2018-09-03T22:23:22Z</updated>
    <published>2018-09-03T22:23:22Z</published>
    <title>Wavelet estimation of the dimensionality of curve time series</title>
    <summary>  Functional data analysis is ubiquitous in most areas of sciences and
engineering. Several paradigms are proposed to deal with the dimensionality
problem which is inherent to this type of data. Sparseness, penalization,
thresholding, among other principles, have been used to tackle this issue. We
discuss here a solution based on a finite-dimensional functional space. We
employ wavelet representation of the functionals to estimate this finite
dimension, and successfully model a time series of curves. The proposed method
is shown to have nice asymptotic properties. Moreover, the wavelet
representation permits the use of several bootstrap procedures, and it results
in faster computing algorithms. Besides the theoretical and computational
properties, some simulation studies and an application to real data are
provided.
</summary>
    <author>
      <name>Rodney V. Fonseca</name>
    </author>
    <author>
      <name>Aluísio Pinheiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05, 62G20, 62G99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03627v3</id>
    <updated>2018-09-03T20:00:05Z</updated>
    <published>2018-02-10T17:57:25Z</published>
    <title>Detecting Multiple Change Points Using Adaptive Regression Splines with
  Application to Neural Recordings</title>
    <summary>  Time series, as frequently the case in neuroscience, are rarely stationary,
but often exhibit abrupt changes due to attractor transitions or bifurcations
in the dynamical systems producing them. A plethora of methods for detecting
such change points in time series statistics have been developed over the
years, in addition to test criteria to evaluate their significance. Issues to
consider when developing change point analysis methods include computational
demands, difficulties arising from either limited amount of data or a large
number of covariates, and arriving at statistical tests with sufficient power
to detect as many changes as contained in potentially high-dimensional time
series. Here, a general method called Paired Adaptive Regressors for Cumulative
Sum is developed for detecting multiple change points in the mean of
multivariate time series. The method's advantages over alternative approaches
are demonstrated through a series of simulation experiments. This is followed
by a real data application to neural recordings from rat medial prefrontal
cortex during learning. Finally, the method's flexibility to incorporate useful
features from state-of-the-art change point detection techniques is discussed,
along with potential drawbacks and suggestions to remedy them.
</summary>
    <author>
      <name>Hazem Toutounji</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Theoretical Neuroscience, Bernstein Center for Computational Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Durstewitz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Theoretical Neuroscience, Bernstein Center for Computational Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 9 figures, 2 tables, 3 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03627v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03627v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00694v1</id>
    <updated>2018-09-03T19:23:11Z</updated>
    <published>2018-09-03T19:23:11Z</published>
    <title>Proper likelihood ratio based ROC curves for general binary
  classification problems</title>
    <summary>  Everybody writes that ROC curves, a very common tool in binary classification
problems, should be optimal, and in particular concave, non-decreasing and
above the 45-degree line. Everybody uses ROC curves, theoretical and especially
empirical, which are not so. This work is an attempt to correct this
schizophrenic behavior. Optimality stems from the Neyman-Pearson lemma, which
prescribes using likelihood-ratio based ROC curves. Starting from there, we
give the most general definition of a likelihood-ratio based classification
procedure, which encompasses finite, continuous and even more complex data
types. We point out a strict relationship with a general notion of
concentration of two probability measures. We give some nontrivial examples of
situations with non-monotone and non-continuous likelihood ratios. Finally, we
propose the ROC curve of a likelihood ratio based Gaussian kernel flexible
Bayes classifier as a proper default alternative to the usual empirical ROC
curve.
</summary>
    <author>
      <name>M. Gasparini</name>
    </author>
    <author>
      <name>L. Sacchetto</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30 (primary), 62H20 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00652v1</id>
    <updated>2018-09-03T16:44:37Z</updated>
    <published>2018-09-03T16:44:37Z</published>
    <title>Minimum Description Length codes are critical</title>
    <summary>  Learning from the data, in Minimum Description Length (MDL), is equivalent to
an optimal coding problem. We show that the codes that achieve optimal
compression in MDL are critical in a very precise sense. First, when they are
taken as generative models of samples, they generate samples with broad
empirical distributions and with an high value of the relevance, defined as the
entropy of the empirical frequencies. These results are derived for different
statistical models (Dirichlet model, independent and pairwise dependent spin
models, and restricted Boltzmann machines). Second, MDL codes sit precisely at
a second order phase transition point where the symmetry between the sampled
outcomes is spontaneously broken. The order parameter controlling the phase
transition is the coding cost of the samples. The phase transition is a
manifestation of the optimality of MDL codes, and it arises because codes that
achieve a higher compression do not exist. These results suggest a clear
interpretation of the widespread occurrence of statistical criticality as a
characterization of samples which are maximally informative on the underlying
generative process.
</summary>
    <author>
      <name>Ryan Cubero</name>
    </author>
    <author>
      <name>Matteo Marsili</name>
    </author>
    <author>
      <name>Yasser Roudi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00463v1</id>
    <updated>2018-09-03T06:45:33Z</updated>
    <published>2018-09-03T06:45:33Z</published>
    <title>Shrinkage for Covariance Estimation: Asymptotics, Confidence Intervals,
  Bounds and Applications in Sensor Monitoring and Finance</title>
    <summary>  When shrinking a covariance matrix towards (a multiple) of the identity
matrix, the trace of the covariance matrix arises naturally as the optimal
scaling factor for the identity target. The trace also appears in other
context, for example when measuring the size of a matrix or the amount of
uncertainty.
  Of particular interest is the case when the dimension of the covariance
matrix is large. Then the problem arises that the sample covariance matrix is
singular if the dimension is larger than the sample size. Another issue is that
usually the estimation has to based on correlated time series data. We study
the estimation of the trace functional allowing for a high-dimensional time
series model, where the dimension is allowed to grow with the sample size -
without any constraint. Based on a recent result, we investigate a confidence
interval for the trace, which also allows us to propose lower and upper bounds
for the shrinkage covariance estimator as well as bounds for the variance of
projections. In addition, we provide a novel result dealing with shrinkage
towards a diagonal target.
  We investigate the accuracy of the confidence interval by a simulation study,
which indicates good performance, and analyze three stock market data sets to
illustrate the proposed bounds, where the dimension (number of stocks) ranges
between $32$ and $475$. Especially, we apply the results to portfolio
optimization and determine bounds for the risk associated to the
variance-minimizing portfolio.
</summary>
    <author>
      <name>Ansgar Steland</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G15, 62G05, 62P05, 62P30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00420v1</id>
    <updated>2018-09-03T01:29:21Z</updated>
    <published>2018-09-03T01:29:21Z</published>
    <title>Network estimation via graphon with node features</title>
    <summary>  Estimating the probabilities of linkages in a network has gained increasing
interest in recent years. One popular model for network analysis is the
exchangeable graph model (ExGM) characterized by a two-dimensional function
known as a graphon. Estimating an underlying graphon becomes the key of such
analysis. Several nonparametric estimation methods have been proposed, and some
are provably consistent. However, if certain useful features of the nodes
(e.g., age and schools in social network context) are available, none of these
methods was designed to incorporate this source of information to help with the
estimation. This paper develops a consistent graphon estimation method that
integrates the information from both the adjacency matrix itself and node
features. We show that properly leveraging the features can improve the
estimation. A cross-validation method is proposed to automatically select the
tuning parameter of the method.
</summary>
    <author>
      <name>Yi Su</name>
    </author>
    <author>
      <name>Raymond K. W. Wong</name>
    </author>
    <author>
      <name>Thomas C. M. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00399v1</id>
    <updated>2018-09-02T21:48:13Z</updated>
    <published>2018-09-02T21:48:13Z</published>
    <title>Flexible sensitivity analysis for observational studies without
  observable implications</title>
    <summary>  A fundamental challenge in observational causal inference is that assumptions
about unconfoundedness are not testable from data. Assessing sensitivity to
such assumptions is therefore important in practice. Unfortunately, existing
model-based sensitivity analysis approaches typically force investigators to
make a choice between a restrictive model for the data or a well-defined
sensitivity analysis. To address this issue, we propose a framework that allows
(1) arbitrary, flexible models for the observed data and (2) clean separation
of the identified and unidentified parts of the sensitivity model. Our approach
treats the causal inference problem as a missing data problem, and applies a
factorization of missing data densities first attributed to John Tukey and more
recently termed "the extrapolation factorization." This name arises because we
extrapolate from the observed potential outcome distributions to the missing
potential outcome distributions by proposing unidentified selection functions.
We demonstrate the flexibility of this approach for estimating both average
treatment effects and quantile treatment effects using Bayesian nonparametric
models for the observed data. We provide a procedure for interpreting and
calibrating the sensitivity parameters and apply this approach to two examples.
</summary>
    <author>
      <name>Alexander Franks</name>
    </author>
    <author>
      <name>Alexander D'Amour</name>
    </author>
    <author>
      <name>Avi Feller</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00232v2</id>
    <updated>2018-09-02T20:09:20Z</updated>
    <published>2017-09-01T10:18:17Z</published>
    <title>Estimating functions for jump-diffusions</title>
    <summary>  Asymptotic theory for approximate martingale estimating functions is
generalised to diffusions with finite-activity jumps, when the sampling
frequency and terminal sampling time go to infinity. Rate optimality and
efficiency are of particular concern. Under mild assumptions, it is shown that
estimators of drift, diffusion, and jump parameters are consistent and
asymptotically normal, as well as rate-optimal for the drift and jump
parameters. Additional conditions are derived, which ensure rate-optimality for
the diffusion parameter as well as efficiency for all parameters. The findings
indicate a potentially fruitful direction for the further development of
estimation for jump-diffusions.
</summary>
    <author>
      <name>Nina Munkholt Jakobsen</name>
    </author>
    <author>
      <name>Michael Sørensen</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09739v2</id>
    <updated>2018-09-02T17:17:38Z</updated>
    <published>2018-01-29T20:10:38Z</published>
    <title>Model selection in sparse high-dimensional vine copula models with
  application to portfolio risk</title>
    <summary>  Vine copulas allow to build flexible dependence models for an arbitrary
number of variables using only bivariate building blocks. The number of
parameters in a vine copula model increases quadratically with the dimension,
which poses new challenges in high-dimensional applications. To alleviate the
computational burden and risk of overfitting, we propose a modified Bayesian
information criterion (BIC) tailored to sparse vine copula models. We show that
the criterion can consistently distinguish between the true and alternative
models under less stringent conditions than the classical BIC. The new
criterion can be used to select the hyper-parameters of sparse model classes,
such as truncated and thresholded vine copulas. We propose a computationally
efficient implementation and illustrate the benefits of the new concepts with a
case study where we model the dependence in a large stock stock portfolio.
</summary>
    <author>
      <name>Thomas Nagler</name>
    </author>
    <author>
      <name>Christian Bumann</name>
    </author>
    <author>
      <name>Claudia Czado</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09739v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09739v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03240v2</id>
    <updated>2018-09-02T16:42:44Z</updated>
    <published>2018-05-08T19:23:21Z</published>
    <title>Spatial shrinkage via the product independent Gaussian process prior</title>
    <summary>  We study the problem of sparse signal detection on a spatial domain. We
propose a novel approach to model continuous signals that are sparse and
piecewise smooth as product of independent Gaussian processes (PING) with a
smooth covariance kernel. The smoothness of the PING process is ensured by the
smoothness of the covariance kernels of Gaussian components in the product, and
sparsity is controlled by the number of components. The bivariate kurtosis of
the PING process shows more components in the product results in thicker tail
and sharper peak at zero. The simulation results demonstrate the improvement in
estimation using the PING prior over Gaussian process (GP) prior for different
image regressions. We apply our method to a longitudinal MRI dataset to detect
the regions that are affected by multiple sclerosis (MS) in the greatest
magnitude through an image-on-scalar regression model. Due to huge
dimensionality of these images, we transform the data into the spectral domain
and develop methods to conduct computation in this domain. In our MS imaging
study, the estimates from the PING model are more informative than those from
the GP model.
</summary>
    <author>
      <name>Arkaprava Roy</name>
    </author>
    <author>
      <name>Brian J. Reich</name>
    </author>
    <author>
      <name>Joseph Guinness</name>
    </author>
    <author>
      <name>Russell T. Shinohara</name>
    </author>
    <author>
      <name>Ana-Maria Staicu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00358v1</id>
    <updated>2018-09-02T15:31:14Z</updated>
    <published>2018-09-02T15:31:14Z</published>
    <title>Sequential Detection of Regime Changes in Neural Data</title>
    <summary>  The problem of detecting changes in firing patterns in neural data is
studied. The problem is formulated as a quickest change detection problem.
Important algorithms from the literature are reviewed. A new algorithmic
technique is discussed to detect deviations from learned baseline behavior. The
algorithms studied can be applied to both spike and local field potential data.
The algorithms are applied to mice spike data to verify the presence of
behavioral learning.
</summary>
    <author>
      <name>Taposh Banerjee</name>
    </author>
    <author>
      <name>Stephen Allsop</name>
    </author>
    <author>
      <name>Kay M. Tye</name>
    </author>
    <author>
      <name>Demba Ba</name>
    </author>
    <author>
      <name>Vahid Tarokh</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00269v1</id>
    <updated>2018-09-01T23:57:12Z</updated>
    <published>2018-09-01T23:57:12Z</published>
    <title>Matching Algorithms for Causal Inference with Multiple Treatments</title>
    <summary>  Randomized clinical trials (RCTs) are ideal for estimating causal effects,
because the distributions of background covariates are similar in expectation
across treatment groups. When estimating causal effects using observational
data, matching is a commonly used method to replicate the covariate balance
achieved in a RCT. Matching algorithms have a rich history dating back to the
mid-1900s, but have been used mostly to estimate causal effects between two
treatment groups. When there are more than two treatments, estimating causal
effects requires additional assumptions and techniques. We propose matching
algorithms that address the drawbacks of the current methods, and we use
simulations to compare current and new methods. All of the methods display
improved covariate balance in the matched sets relative to the pre-matched
cohorts. In addition, we provide advice to investigators on which matching
algorithms are preferred for different covariate distributions.
</summary>
    <author>
      <name>Anthony D. Scotina</name>
    </author>
    <author>
      <name>Roee Gutman</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00268v1</id>
    <updated>2018-09-01T23:56:23Z</updated>
    <published>2018-09-01T23:56:23Z</published>
    <title>Matching Estimators for Causal Effects of Multiple Treatments</title>
    <summary>  Matching estimators for average treatment effects are widely used in the
binary treatment setting, in which missing potential outcomes are imputed as
the average of observed outcomes of all matches for each unit. With more than
two treatment groups, however, estimation using matching requires additional
techniques. In this paper, we propose a nearest-neighbors matching estimator
for use with multiple, nominal treatments, and use simulations to show that
this method is precise and has coverage levels that are close to nominal.
</summary>
    <author>
      <name>Anthony D. Scotina</name>
    </author>
    <author>
      <name>Roee Gutman</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00266v1</id>
    <updated>2018-09-01T23:39:07Z</updated>
    <published>2018-09-01T23:39:07Z</published>
    <title>Function-on-Scalar Quantile Regression with Application to Mass
  Spectrometry Proteomics Data</title>
    <summary>  Mass spectrometry proteomics, characterized by spiky, spatially heterogeneous
functional data, can be used to identify potential cancer biomarkers. Existing
mass spectrometry analyses utilize mean regression to detect spectral regions
that are differentially expressed across groups. However, given the
inter-patient heterogeneity that is a key hallmark of cancer, many biomarkers
are only present at aberrant levels for a subset of, not all, cancer samples.
Differences in these biomarkers can easily be missed by mean regression, but
might be more easily detected by quantile-based approaches. Thus, we propose a
unified Bayesian framework to perform quantile regression on functional
responses. Our approach utilizes an asymmetric Laplace working likelihood,
represents the functional coefficients with basis representations which enable
borrowing of strength from nearby locations, and places a global-local
shrinkage prior on the basis coefficients to achieve adaptive regularization.
Different types of basis transform and continuous shrinkage priors can be used
in our framework. An efficient Gibbs sampler is developed to generate posterior
samples that can be used to perform Bayesian estimation and inference while
accounting for multiple testing. Our framework performs quantile regression and
coefficient regularization in a unified manner, allowing them to inform each
other and leading to improvement in performance over competing methods as
demonstrated by simulation studies. We apply this model to identify proteomic
biomarkers of pancreatic cancer missed by previous mean-regression based
approaches. Supplementary materials for this article are available online.
</summary>
    <author>
      <name>Yusha Liu</name>
    </author>
    <author>
      <name>Meng Li</name>
    </author>
    <author>
      <name>Jeffrey S. Morris</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00236v1</id>
    <updated>2018-09-01T18:48:54Z</updated>
    <published>2018-09-01T18:48:54Z</published>
    <title>Optimal Bandwidth Choice for Robust Bias Corrected Inference in
  Regression Discontinuity Designs</title>
    <summary>  Modern empirical work in Regression Discontinuity (RD) designs employs local
polynomial estimation and inference with a mean square error (MSE) optimal
bandwidth choice. This bandwidth yields an MSE-optimal RD treatment effect
estimator, but is by construction invalid for inference. Robust bias corrected
(RBC) inference methods are valid when using the MSE-optimal bandwidth, but we
show they yield suboptimal confidence intervals in terms of coverage error. We
establish valid coverage error expansions for RBC confidence interval
estimators and use these results to propose new inference-optimal bandwidth
choices for forming these intervals. We find that the standard MSE-optimal
bandwidth for the RD point estimator must be shrank when the goal is to
construct RBC confidence intervals with the smaller coverage error rate. We
further optimize the constant terms behind the coverage error to derive new
optimal choices for the auxiliary bandwidth required for RBC inference. Our
expansions also establish that RBC inference yields higher-order refinements
(relative to traditional undersmoothing) in the context of RD designs. Our main
results cover sharp and sharp kink RD designs under conditional
heteroskedasticity, and we discuss extensions to fuzzy and other RD designs,
clustered sampling, and pre-intervention covariates adjustments. The
theoretical findings are illustrated with a Monte Carlo experiment and an
empirical application, and the main methodological results are available in
\texttt{R} and \texttt{Stata} packages.
</summary>
    <author>
      <name>Sebastian Calonico</name>
    </author>
    <author>
      <name>Matias D. Cattaneo</name>
    </author>
    <author>
      <name>Max H. Farrell</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06881v5</id>
    <updated>2018-09-01T10:03:15Z</updated>
    <published>2015-12-21T21:34:01Z</published>
    <title>A dynamic Bayesian Markov model for health economic evaluations of
  interventions in infectious disease</title>
    <summary>  Background. Health economic evaluations of interventions against infectious
diseases are commonly based on the predictions of ordinary differential
equation (ODE) systems or Markov models (MMs). Standard MMs are static, whereas
ODE systems are usually dynamic and account for herd immunity which is crucial
to prevent overestimation of infection prevalence. Complex ODE systems
including probabilistic model parameters are computationally intensive. Thus,
mainly ODE-based models including deterministic parameters are presented in the
literature. These do not account for parameter uncertainty. As a consequence,
probabilistic sensitivity analysis (PSA), a crucial component of health
economic evaluations, cannot be conducted straightforwardly.
  Methods. We present a dynamic MM under a Bayesian framework. We extend a
static MM by incorporating the force of infection into the state allocation
algorithm. The corresponding output is based on dynamic changes in prevalence
and thus accounts for herd immunity. In contrast to deterministic ODE-based
models, PSA can be conducted straightforwardly. We introduce a case study of a
fictional sexually transmitted infection and compare our dynamic Bayesian MM to
a deterministic and a Bayesian ODE system. The models are calibrated to time
series data.
  Results. By means of the case study, we show that our methodology produces
outcome which is comparable to the "gold standard" of the Bayesian ODE system.
  Conclusions. In contrast to ODE systems in the literature, the setting of the
dynamic MM is probabilistic at manageable computational effort (including
calibration). The run time of the Bayesian ODE system is 44 times longer.
</summary>
    <author>
      <name>Katrin Haeussler</name>
    </author>
    <author>
      <name>Ardo van den Hout</name>
    </author>
    <author>
      <name>Gianluca Baio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">doi.org/10.1186/s12874-018-0541-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/doi.org/10.1186/s12874-018-0541-7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BMC Medical Research Methodology(2018) 18:82</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.06881v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06881v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00100v1</id>
    <updated>2018-09-01T02:18:46Z</updated>
    <published>2018-09-01T02:18:46Z</published>
    <title>A simulation-based approach to estimate joint model of longitudinal and
  event-time data with many missing longitudinal observations</title>
    <summary>  Joint models of longitudinal and event-time data have been extensively
studied and applied in many different fields. Estimation of joint models is
challenging, most present procedures are computational expensive and have a
strict requirement on data quality. In this study, a novel simulation-based
procedure is proposed to estimate a general family of joint models, which
include many widely-applied joint models as special cases. Our procedure can
easily handle low-quality data where longitudinal observations are
systematically missed for some of the covariate dimensions. In addition, our
estimation procedure is compatible with parallel computing framework when
combining with stochastic descending algorithm, it is perfectly applicable to
massive data and therefore suitable for many financial applications.
Consistency and asymptotic normality of our estimator are proved, a simulation
study is conducted to illustrate its effectiveness. Finally, as an application,
the procedure is applied to estimate pre-payment probability of a massive
consumer-loan dataset drawn from one biggest P2P loan platform of China.
</summary>
    <author>
      <name>Yanqiao Zheng</name>
    </author>
    <author>
      <name>Xiaobing Zhao</name>
    </author>
    <author>
      <name>Xiaoqi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00083v1</id>
    <updated>2018-08-31T23:38:41Z</updated>
    <published>2018-08-31T23:38:41Z</published>
    <title>Predicting protein inter-residue contacts using composite likelihood
  maximization and deep learning</title>
    <summary>  Accurate prediction of inter-residue contacts of a protein is important to
calcu- lating its tertiary structure. Analysis of co-evolutionary events among
residues has been proved effective to inferring inter-residue contacts. The
Markov ran- dom field (MRF) technique, although being widely used for contact
prediction, suffers from the following dilemma: the actual likelihood function
of MRF is accurate but time-consuming to calculate, in contrast, approximations
to the actual likelihood, say pseudo-likelihood, are efficient to calculate but
inaccu- rate. Thus, how to achieve both accuracy and efficiency simultaneously
remains a challenge. In this study, we present such an approach (called clmDCA)
for contact prediction. Unlike plmDCA using pseudo-likelihood, i.e., the
product of conditional probability of individual residues, our approach uses
composite- likelihood, i.e., the product of conditional probability of all
residue pairs. Com- posite likelihood has been theoretically proved as a better
approximation to the actual likelihood function than pseudo-likelihood.
Meanwhile, composite likelihood is still efficient to maximize, thus ensuring
the efficiency of clmDCA. We present comprehensive experiments on popular
benchmark datasets, includ- ing PSICOV dataset and CASP-11 dataset, to show
that: i) clmDCA alone outperforms the existing MRF-based approaches in
prediction accuracy. ii) When equipped with deep learning technique for
refinement, the prediction ac- curacy of clmDCA was further significantly
improved, suggesting the suitability of clmDCA for subsequent refinement
procedure. We further present successful application of the predicted contacts
to accurately build tertiary structures for proteins in the PSICOV dataset.
  Accessibility: The software clmDCA and a server are publicly accessible
through http://protein.ict.ac.cn/clmDCA/.
</summary>
    <author>
      <name>Haicang Zhang</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <author>
      <name>Fusong Ju</name>
    </author>
    <author>
      <name>Jianwei Zhu</name>
    </author>
    <author>
      <name>Shiwei Sun</name>
    </author>
    <author>
      <name>Yujuan Gao</name>
    </author>
    <author>
      <name>Ziwei Xie</name>
    </author>
    <author>
      <name>Minghua Deng</name>
    </author>
    <author>
      <name>Shiwei Sun</name>
    </author>
    <author>
      <name>Wei-Mou Zheng</name>
    </author>
    <author>
      <name>Dongbo Bu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00145v3</id>
    <updated>2018-08-31T19:15:13Z</updated>
    <published>2017-08-01T03:21:11Z</published>
    <title>Least Squares Estimation in a Single Index Model with Convex Lipschitz
  link</title>
    <summary>  We consider estimation and inference in a single index regression model with
an unknown convex link function. We propose a Lipschitz constrained least
squares estimator (LLSE) for both the parametric and the nonparametric
components given independent and identically distributed observations. We prove
the consistency and find the rates of convergence of the LLSE when the errors
are assumed to have only $q \ge 2$ moments and are allowed to depend on the
covariates. In fact, we prove a general theorem which can be used to find the
rates of convergence of LSEs in a variety of nonparametric/semiparametric
regression problems under the same assumptions on the errors. Moreover when
$q\ge 5$, we establish $n^{-1/2}$-rate of convergence and asymptotic normality
of the estimator of the parametric component. Moreover the LLSE is proved to be
semiparametrically efficient if the errors happen to be homoscedastic.
Furthermore, we develop the R package \texttt{simest} to compute the proposed
estimator.
</summary>
    <author>
      <name>Arun K. Kuchibhotla</name>
    </author>
    <author>
      <name>Rohit K. Patra</name>
    </author>
    <author>
      <name>Bodhisattva Sen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">107 pages, 6 Figures, and 3 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00145v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00145v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00484v2</id>
    <updated>2018-08-31T18:43:29Z</updated>
    <published>2017-11-01T18:01:26Z</published>
    <title>Spatial Statistical Downscaling for Constructing High-Resolution Nature
  Runs in Global Observing System Simulation Experiments</title>
    <summary>  Observing system simulation experiments (OSSEs) have been widely used as a
rigorous and cost-effective way to guide development of new observing systems,
and to evaluate the performance of new data assimilation algorithms. Nature
runs (NRs), which are outputs from deterministic models, play an essential role
in building OSSE systems for global atmospheric processes because they are used
both to create synthetic observations at high spatial resolution, and to
represent the "true" atmosphere against which the forecasts are verified.
However, most NRs are generated at resolutions coarser than actual
observations. Here, we propose a principled statistical downscaling framework
to construct high-resolution NRs via conditional simulation from
coarse-resolution numerical model output. We use nonstationary spatial
covariance function models that have basis function representations. This
approach not only explicitly addresses the change-of-support problem, but also
allows fast computation with large volumes of numerical model output. We also
propose a data-driven algorithm to select the required basis functions
adaptively, in order to increase the flexibility of our nonstationary
covariance function models. In this article we demonstrate these techniques by
downscaling a coarse-resolution physical NR at a native resolution of
$1^{\circ} \text{ latitude} \times 1.25^{\circ} \text{ longitude}$ of global
surface $\text{CO}_2$ concentrations to 655,362 equal-area hexagons.
</summary>
    <author>
      <name>Pulong Ma</name>
    </author>
    <author>
      <name>Emily L. Kang</name>
    </author>
    <author>
      <name>Amy Braverman</name>
    </author>
    <author>
      <name>Hai Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted version in Technometrics</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00484v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00484v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10868v1</id>
    <updated>2018-08-31T17:53:10Z</updated>
    <published>2018-08-31T17:53:10Z</published>
    <title>Generalized probabilistic principal component analysis of correlated
  data</title>
    <summary>  Principal component analysis (PCA) is a well-established tool in machine
learning and data processing. \cite{tipping1999probabilistic} proposed a
probabilistic formulation of PCA (PPCA) by showing that the principal axes in
PCA are equivalent to the maximum marginal likelihood estimator of the factor
loading matrix in a latent factor model for the observed data, assuming that
the latent factors are independently distributed as standard normal
distributions. However, the independence assumption may be unrealistic for many
scenarios such as modeling multiple time series, spatial processes, and
functional data, where the output variables are correlated.
  In this paper, we introduce the generalized probabilistic principal component
analysis (GPPCA) to study the latent factor model of multiple correlated
outcomes, where each factor is modeled by a Gaussian process. The proposed
method provides a probabilistic solution of the latent factor model with the
scalable computation. In particular, we derive the maximum marginal likelihood
estimator of the factor loading matrix and the predictive distribution of the
output. Based on the explicit expression of the precision matrix in the
marginal likelihood, the number of the computational operations is linear to
the number of output variables. Moreover, with the use of the Mat{\'e}rn
covariance function, the number of the computational operations is also linear
to the number of time points for modeling the multiple time series without any
approximation to the likelihood function. We discuss the connection of the
GPPCA with other approaches such as the PCA and PPCA, and highlight the
advantage of GPPCA in terms of the practical relevance, estimation accuracy and
computational convenience. Numerical studies confirm the excellent
finite-sample performance of the proposed approach.
</summary>
    <author>
      <name>Mengyang Gu</name>
    </author>
    <author>
      <name>Weining Shen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10770v1</id>
    <updated>2018-08-31T14:26:28Z</updated>
    <published>2018-08-31T14:26:28Z</published>
    <title>Improved Chebyshev inequality: new probability bounds with known
  supremum of PDF</title>
    <summary>  In this paper, we derive new probability bounds for Chebyshev's inequality if
the supremum of the probability density function is known. This result holds
for one-dimensional or multivariate continuous probability distributions with
finite mean and variance (covariance matrix). We also show that the similar
result holds for specific discrete probability distributions.
</summary>
    <author>
      <name>Tomohiro Nishiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10593v1</id>
    <updated>2018-08-31T04:16:58Z</updated>
    <published>2018-08-31T04:16:58Z</published>
    <title>Asymptotic Seed Bias in Respondent-driven Sampling</title>
    <summary>  Respondent-driven sampling (RDS) collects a sample of individuals in a
networked population by incentivizing the sampled individuals to refer their
contacts into the sample. This iterative process is initialized from some seed
node(s). Sometimes, this selection creates a large amount of seed bias. Other
times, the seed bias is small. This paper gains a deeper understanding of this
bias by characterizing its effect on the limiting distribution of various RDS
estimators. Using classical tools and results from multi-type branching
processes (Kesten and Stigum, 1966), we show that the seed bias is negligible
for the Generalized Least Squares (GLS) estimator and non-negligible for both
the inverse probability weighted and Volz-Heckathorn (VH) estimators. In
particular, we show that (i) above a critical threshold, VH converge to a
non-trivial mixture distribution, where the mixture component depends on the
seed node, and the mixture distribution is possibly multi-modal. Moreover, (ii)
GLS converges to a Gaussian distribution independent of the seed node, under a
certain condition on the Markov process. Numerical experiments with both
simulated data and empirical social networks suggest that these results appear
to hold beyond the Markov conditions of the theorems.
</summary>
    <author>
      <name>Yuling Yan</name>
    </author>
    <author>
      <name>Bret Hanlon</name>
    </author>
    <author>
      <name>Sebastien Roch</name>
    </author>
    <author>
      <name>Karl Rohe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05142v5</id>
    <updated>2018-08-31T02:19:14Z</updated>
    <published>2016-08-18T00:46:37Z</published>
    <title>Generic Inference on Quantile and Quantile Effect Functions for Discrete
  Outcomes</title>
    <summary>  Quantile and quantile effect functions are important tools for descriptive
and causal analyses due to their natural and intuitive interpretation. Existing
inference methods for these functions do not apply to discrete random
variables. This paper offers a simple, practical construction of simultaneous
confidence bands for quantile and quantile effect functions of possibly
discrete random variables. It is based on a natural transformation of
simultaneous confidence bands for distribution functions, which are readily
available for many problems. The construction is generic and does not depend on
the nature of the underlying problem. It works in conjunction with parametric,
semiparametric, and nonparametric modeling methods for observed and
counterfactual distributions, and does not depend on the sampling scheme. We
apply our method to characterize the distributional impact of insurance
coverage on health care utilization and obtain the distributional decomposition
of the racial test score gap. We find that universal insurance coverage
increases the number of doctor visits across the entire distribution, and that
the racial test score gap is small at early ages but grows with age due to
socio economic factors affecting child development especially at the top of the
distribution. These are new, interesting empirical findings that complement
previous analyses that focused on mean effects only. In both applications, the
outcomes of interest are discrete rendering existing inference methods invalid
for obtaining uniform confidence bands for observed and counterfactual quantile
functions and for their difference -- the quantile effects functions.
</summary>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <author>
      <name>Blaise Melly</name>
    </author>
    <author>
      <name>Kaspar Wüthrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 6 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05142v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05142v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F25, 62G15, 62P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10558v1</id>
    <updated>2018-08-31T00:35:50Z</updated>
    <published>2018-08-31T00:35:50Z</published>
    <title>An explicit mean-covariance parameterization for multivariate response
  linear regression</title>
    <summary>  We develop a new method to fit the multivariate response linear regression
model that exploits a parametric link between the regression coefficient matrix
and the error covariance matrix. Specifically, we assume that the correlations
between entries in the multivariate error random vector are proportional to the
cosines of the angles between their corresponding regression coefficient matrix
columns, so as the angle between two regression coefficient matrix columns
decreases, the correlation between the corresponding errors increases. This
assumption can be motivated through an error-in-variables formulation. We
propose a novel non-convex weighted residual sum of squares criterion which
exploits this parameterization and admits a new class of penalized estimators.
The optimization is solved with an accelerated proximal gradient descent
algorithm. Extensions to scenarios where responses are missing or some
covariates are measured without error are also proposed. We use our method to
study the association between gene expression and copy-number variations
measured on patients with glioblastoma multiforme. An R package implementing
our method, MCMVR, is available online.
</summary>
    <author>
      <name>Aaron J. Molstad</name>
    </author>
    <author>
      <name>Guangwei Weng</name>
    </author>
    <author>
      <name>Charles R. Doss</name>
    </author>
    <author>
      <name>Adam J. Rothman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10541v1</id>
    <updated>2018-08-30T22:51:24Z</updated>
    <published>2018-08-30T22:51:24Z</published>
    <title>Gaussian process regression for survival time prediction with
  genome-wide gene expression</title>
    <summary>  Predicting the survival time of a cancer patient based on his/her genome-wide
gene expression remains a challenging problem. For certain types of cancer, the
effects of gene expression on survival are both weak and abundant, so
identifying nonzero effects with reasonable accuracy is difficult. As an
alternative to methods that use variable selection, we propose a Gaussian
process accelerated failure time model to predict survival time using
genome-wide or pathway-wide gene expression data. Using a Monte Carlo EM
algorithm, we jointly impute censored log-survival time and estimate model
parameters. We demonstrate the performance of our method and its advantage over
existing methods in both simulations and real data analysis. The real data that
we analyze were collected from 513 patients with kidney renal clear cell
carcinoma and include survival time, demographic/clinical variables, and
expression of more than 20,000 genes. Our method is widely applicable as it can
accommodate right, left, and interval censored outcomes; and provides a natural
way to combine multiple types of high-dimensional -omics data. An R package
implementing our method is available online.
</summary>
    <author>
      <name>Aaron J. Molstad</name>
    </author>
    <author>
      <name>Li Hsu</name>
    </author>
    <author>
      <name>Wei Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10532v1</id>
    <updated>2018-08-30T21:53:06Z</updated>
    <published>2018-08-30T21:53:06Z</published>
    <title>Uniform Inference in High-Dimensional Gaussian Graphical Models</title>
    <summary>  Graphical models have become a very popular tool for representing
dependencies within a large set of variables and are key for representing
causal structures. We provide results for uniform inference on high-dimensional
graphical models with the number of target parameters being possible much
larger than sample size. This is in particular important when certain features
or structures of a causal model should be recovered. Our results highlight how
in high-dimensional settings graphical models can be estimated and recovered
with modern machine learning methods in complex data sets. We also demonstrate
in simulation study that our procedure has good small sample properties.
</summary>
    <author>
      <name>Sven Klaassen</name>
    </author>
    <author>
      <name>Jannis Kück</name>
    </author>
    <author>
      <name>Martin Spindler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 2 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H15, 62J07," scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07563v2</id>
    <updated>2018-08-30T21:42:35Z</updated>
    <published>2018-08-22T20:57:38Z</published>
    <title>Optimizing the tie-breaker regression discontinuity design</title>
    <summary>  Motivated by customer loyalty plans, we study tie-breaker designs which are
hybrids of randomized controlled trials (RCTs) and regression discontinuity
designs (RDDs). We quantify the statistical efficiency of a tie-breaker design
in which a proportion $\Delta$ of observed customers are in the RCT. In a two
line regression, statistical efficiency increases monotonically with $\Delta$,
so efficiency is maximized by an RCT. That same regression model quantifies the
short term value of the treatment allocation and this comparison favors smaller
$\Delta$ with the RDD being best. We solve for the optimal tradeoff between
these exploration and exploitation goals. The usual tie-breaker design
experiments on the middle $\Delta$ subjects as ranked by the running variable.
We quantify the efficiency of other designs such as experimenting only in the
second decile from the top. We also consider more general models such as
quadratic regressions.
</summary>
    <author>
      <name>Art B. Owen</name>
    </author>
    <author>
      <name>Hal Varian</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07563v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07563v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10522v1</id>
    <updated>2018-08-30T21:11:04Z</updated>
    <published>2018-08-30T21:11:04Z</published>
    <title>Bayesian Model Averaging for Model Implied Instrumental Variable Two
  Stage Least Squares Estimators</title>
    <summary>  Model-Implied Instrumental Variable Two-Stage Least Squares (MIIV-2SLS) is a
limited information, equation-by-equation, non-iterative estimator for latent
variable models. Associated with this estimator are equation specific tests of
model misspecification. We propose an extension to the existing MIIV-2SLS
estimator that utilizes Bayesian model averaging which we term Model-Implied
Instrumental Variable Two-Stage Bayesian Model Averaging (MIIV-2SBMA).
MIIV-2SBMA accounts for uncertainty in optimal instrument set selection, and
provides powerful instrument specific tests of model misspecification and
instrument strength. We evaluate the performance of MIIV-2SBMA against
MIIV-2SLS in a simulation study and show that it has comparable performance in
terms of parameter estimation. Additionally, our instrument specific
overidentification tests developed within the MIIV-2SBMA framework show
increased power to detect model misspecification over the traditional equation
level tests of model misspecification. Finally, we demonstrate the use of
MIIV-2SBMA using an empirical example.
</summary>
    <author>
      <name>Teague R. Henry</name>
    </author>
    <author>
      <name>Zachary F. Fisher</name>
    </author>
    <author>
      <name>Kenneth A. Bollen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 8 figures, supplementary materials available upon request</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01453v2</id>
    <updated>2018-08-30T20:51:24Z</updated>
    <published>2018-06-05T01:21:31Z</published>
    <title>Calibration for computer experiments with binary responses and
  application to cell adhesion study</title>
    <summary>  Calibration refers to the estimation of unknown parameters which are present
in computer experiments but not available in physical experiments. An accurate
estimation of these parameters is important because it provides a scientific
understanding of the underlying system which is not available in physical
experiments. Most of the work in the literature is limited to the analysis of
continuous responses. Motivated by a study of cell adhesion experiments, we
propose a new calibration framework for binary responses. Its application to
the T cell adhesion data provides insight into the unknown values of the
kinetic parameters which are difficult to determine by physical experiments due
to the limitation of the existing experimental techniques.
</summary>
    <author>
      <name>Chih-Li Sung</name>
    </author>
    <author>
      <name>Ying Hung</name>
    </author>
    <author>
      <name>William Rittase</name>
    </author>
    <author>
      <name>Cheng Zhu</name>
    </author>
    <author>
      <name>C. F. Jeff Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10506v1</id>
    <updated>2018-08-30T20:28:43Z</updated>
    <published>2018-08-30T20:28:43Z</published>
    <title>Maximum Entropy Principle Analysis in Network Systems with Short-time
  Recordings</title>
    <summary>  In many realistic systems, maximum entropy principle (MEP) analysis provides
an effective characterization of the probability distribution of network
states. However, to implement the MEP analysis, a sufficiently long-time data
recording in general is often required, e.g., hours of spiking recordings of
neurons in neuronal networks. The issue of whether the MEP analysis can be
successfully applied to network systems with data from short recordings has yet
to be fully addressed. In this work, we investigate relationships underlying
the probability distributions, moments, and effective interactions in the MEP
analysis and then show that, with short recordings of network dynamics, the MEP
analysis can be applied to reconstructing probability distributions of network
states under the condition of asynchronous activity of nodes in the network.
Using spike trains obtained from both Hodgkin-Huxley neuronal networks and
electrophysiological experiments, we verify our results and demonstrate that
MEP analysis provides a tool to investigate the neuronal population coding
properties, even for short recordings.
</summary>
    <author>
      <name>Zhi-Qin John Xu</name>
    </author>
    <author>
      <name>Jennifer Crodelle</name>
    </author>
    <author>
      <name>Douglas Zhou</name>
    </author>
    <author>
      <name>David Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B15, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03476v2</id>
    <updated>2018-08-30T20:15:36Z</updated>
    <published>2017-02-12T01:25:56Z</published>
    <title>Powerful statistical inference for nested data using sufficient summary
  statistics</title>
    <summary>  Hierarchically-organized data arise naturally in many psychology and
neuroscience studies. As the standard assumption of independent and identically
distributed samples does not hold for such data, two important problems are to
accurately estimate group-level effect sizes, and to obtain powerful
statistical tests against group-level null hypotheses. A common approach is to
summarize subject-level data by a single quantity per subject, which is often
the mean or the difference between class means, and treat these as samples in a
group-level t-test. This 'naive' approach is, however, suboptimal in terms of
statistical power, as it ignores information about the intra-subject variance.
To address this issue, we review several approaches to deal with nested data,
with a focus on methods that are easy to implement. With what we call the
sufficient-summary-statistic approach, we highlight a computationally efficient
technique that can improve statistical power by taking into account
within-subject variances, and we provide step-by-step instructions on how to
apply this approach to a number of frequently-used measures of effect size. The
properties of the reviewed approaches and the potential benefits over a
group-level t-test are quantitatively assessed on simulated data and
demonstrated on EEG data from a simulated-driving experiment.
</summary>
    <author>
      <name>Irene Dowding</name>
    </author>
    <author>
      <name>Stefan Haufe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fnhum.2018.00103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fnhum.2018.00103" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dowding, I., &amp; Haufe, S. (2018). Powerful Statistical Inference
  for Nested Data Using Sufficient Summary Statistics. Frontiers in human
  neuroscience, 12, 103</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.03476v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03476v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10483v1</id>
    <updated>2018-08-30T18:57:04Z</updated>
    <published>2018-08-30T18:57:04Z</published>
    <title>Permutation tests of non-exchangeable null models</title>
    <summary>  Generalizations to the permutation test are introduced to allow for
situations in which the null model is not exchangeable. It is shown that the
generalized permutation tests are exact, and a partial converse: that any test
function that is exact on all probability densities coincides with a
generalized permutation test on a particular region, is established. A most
powerful generalized permutation test is derived in closed form. Approximations
to the most powerful generalized permutation test are proposed to reduce the
computational burden required to compute the complete test. In particular, an
explicit form for the approximate test is derived in terms of a multinomial
Bernstein polynomial approximation, and its convergence to the most powerful
generalized permutation test is demonstrated. In the case where the
determination of p-values is of greater interest than testing of hypotheses,
two approaches to estimation of significance are analyzed. Bounds on the
deviation from significance of the exact most powerful test are given in terms
of sample size. For both estimators, as sample size approaches infinity, the
estimator converges to the significance of the most powerful generalized
permutation test under mild conditions. Applications of generalized permutation
testing to linear mixed models are provided.
</summary>
    <author>
      <name>Jeffrey Roach</name>
    </author>
    <author>
      <name>William Valdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10477v1</id>
    <updated>2018-08-30T18:27:24Z</updated>
    <published>2018-08-30T18:27:24Z</published>
    <title>Simulation-Selection-Extrapolation: Estimation in High-Dimensional
  Errors-in-Variables Models</title>
    <summary>  This paper considers errors-in-variables models in a high-dimensional setting
where the number of covariates can be much larger than the sample size, and
there are only a small number of non-zero covariates. The presence of
measurement error in the covariates can result in severely biased parameter
estimates, and also affects the ability of penalized methods such as the lasso
to recover the true sparsity pattern. A new estimation procedure called
SIMSELEX (SIMulation-SELection-EXtrapolation) is proposed. This procedure
augments the traditional SIMEX approach with a variable selection step based on
the group lasso. The SIMSELEX estimator is shown to perform well in variable
selection, and has significantly lower estimation error than naive estimators
that ignore measurement error. SIMSELEX can be applied in a variety of
errors-in-variables settings, including linear models, generalized linear
models, and Cox survival models. It is furthermore shown how SIMSELEX can be
applied to spline-based regression models. SIMSELEX estimators are compared to
the corrected lasso and the conic programming estimator for a linear model, and
to the conditional scores lasso for a logistic regression model. Finally, the
method is used to analyze a microarray dataset that contains gene expression
measurements of favorable histology Wilms tumors.
</summary>
    <author>
      <name>Linh Nghiem</name>
    </author>
    <author>
      <name>Cornelis Potgieter</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10415v1</id>
    <updated>2018-08-30T17:36:03Z</updated>
    <published>2018-08-30T17:36:03Z</published>
    <title>Accelerating Parallel Tempering: Quantile Tempering Algorithm (QuanTA)</title>
    <summary>  Using MCMC to sample from a target distribution, $\pi(x)$ on a
$d$-dimensional state space can be a difficult and computationally expensive
problem. Particularly when the target exhibits multimodality, then the
traditional methods can fail to explore the entire state space and this results
in a bias sample output. Methods to overcome this issue include the parallel
tempering algorithm which utilises an augmented state space approach to help
the Markov chain traverse regions of low probability density and reach other
modes. This method suffers from the curse of dimensionality which dramatically
slows the transfer of mixing information from the auxiliary targets to the
target of interest as $d \rightarrow \infty$. This paper introduces a novel
prototype algorithm, QuanTA, that uses a Gaussian motivated transformation in
an attempt to accelerate the mixing through the temperature schedule of a
parallel tempering algorithm. This new algorithm is accompanied by a
comprehensive theoretical analysis quantifying the improved efficiency and
scalability of the approach; concluding that under weak regularity conditions
the new approach gives accelerated mixing through the temperature schedule.
Empirical evidence of the effectiveness of this new algorithm is illustrated on
canonical examples.
</summary>
    <author>
      <name>Nicholas G. Tawn</name>
    </author>
    <author>
      <name>Gareth O. Roberts</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10777v1</id>
    <updated>2018-08-30T15:36:18Z</updated>
    <published>2018-08-30T15:36:18Z</published>
    <title>Tests de bondad de ajuste para la distribución Poisson bivariante</title>
    <summary>  The objective of this text is to propose and study goodness-of-fit tests for
DBP, which are consistent. Since the probability generating function (fgp)
characterizes the distribution of a random vector and can be estimated
consistently by the empirical probability generating function (fgpe), the tests
we propose are functions of the fgpe. The first statistical test compares the
fgpe of the data with an estimator of the fgp of the DPB. Then, we show that
the fgp of the DPB is the only fgp that satisfies a certain system of partial
differential equations, which leads us to propose two statistical tests based
on the empirical analogy of this system, one of them Cramer-von Mises type and
the other is based on the coefficients of the polynomials of the empirical
version. The tests we propose can be seen as extensions to the bivariate case
of some goodness of fit tests designed for the univariate case.
</summary>
    <author>
      <name>Francisco Novoa-Muñoz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">120 pages, in Spanish, 10 tables, book</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03729v2</id>
    <updated>2018-08-30T13:46:18Z</updated>
    <published>2016-06-12T15:12:43Z</published>
    <title>Robust instrumental variable methods using multiple candidate
  instruments with application to Mendelian randomization</title>
    <summary>  Mendelian randomization is the use of genetic variants to make causal
inferences from observational data. The field is currently undergoing a
revolution fuelled by increasing numbers of genetic variants demonstrated to be
associated with exposures in genome-wide association studies, and the public
availability of summarized data on genetic associations with exposures and
outcomes from large consortia. A Mendelian randomization analysis with many
genetic variants can be performed relatively simply using summarized data.
However, a causal interpretation is only assured if each genetic variant
satisfies the assumptions of an instrumental variable. To provide some
protection against failure of these assumptions, robust methods for
instrumental variable analysis have been proposed. Here, we develop three
extensions to instrumental variable methods using: i) robust regression, ii)
the penalization of weights from candidate instruments with heterogeneous
causal estimates, and iii) L1 penalization. Results from a wide variety of
robust methods, including the recently-proposed MR-Egger and median-based
methods, are compared in an extensive simulation study. We demonstrate that two
methods, robust regression in an inverse-variance weighted method and a simple
median of the causal estimates from the individual variants, have considerably
improved Type 1 error rates compared with conventional methods in a wide
variety of scenarios when up to 30% of the genetic variants are invalid
instruments. While the MR-Egger method gives unbiased estimates when its
assumptions are satisfied, these estimates are less efficient than those from
other methods and are highly sensitive to violations of the assumptions.
Methods that make different assumptions should be used routinely to assess the
robustness of findings from applied Mendelian randomization investigations with
multiple genetic variants.
</summary>
    <author>
      <name>Stephen Burgess</name>
    </author>
    <author>
      <name>Jack Bowden</name>
    </author>
    <author>
      <name>Frank Dudbridge</name>
    </author>
    <author>
      <name>Simon G Thompson</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03729v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03729v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10188v1</id>
    <updated>2018-08-30T08:57:48Z</updated>
    <published>2018-08-30T08:57:48Z</published>
    <title>Optimal shrinkage covariance matrix estimation under random sampling
  from elliptical distributions</title>
    <summary>  This paper considers the problem of estimating a high-dimensional (HD)
covariance matrix when the sample size is smaller, or not much larger, than the
dimensionality of the data, which could potentially be very large. We develop a
regularized sample covariance matrix (RSCM) estimator which can be applied in
commonly occurring sparse data problems. The proposed RSCM estimator is based
on estimators of the unknown optimal (oracle) shrinkage parameters that yield
the minimum mean squared error (MMSE) between the RSCM and the true covariance
matrix when the data is sampled from an unspecified elliptically symmetric
distribution. We propose two variants of the RSCM estimator which differ in the
approach in which they estimate the underlying sphericity parameter involved in
the theoretical optimal shrinkage parameter. The performance of the proposed
RSCM estimators are evaluated with numerical simulation studies. In particular
when the sample sizes are low, the proposed RSCM estimators often show a
significant improvement over the conventional RSCM estimator by Ledoit and Wolf
(2004). We further evaluate the performance of the proposed estimators in
classification and portfolio optimization problems with real data wherein the
proposed methods are able to outperform the benchmark methods.
</summary>
    <author>
      <name>Esa Ollila</name>
    </author>
    <author>
      <name>Elias Raninen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01932v4</id>
    <updated>2018-08-30T01:04:09Z</updated>
    <published>2018-04-05T16:12:37Z</published>
    <title>Density estimation on small datasets</title>
    <summary>  How might a smooth probability distribution be estimated, with accurately
quantified uncertainty, from a limited amount of sampled data? Here we describe
a field-theoretic approach that addresses this problem remarkably well in one
dimension, providing an exact nonparametric Bayesian posterior without relying
on tunable parameters or large-data approximations. Strong non-Gaussian
constraints, which require a non-perturbative treatment, are found to play a
major role in reducing distribution uncertainty. A software implementation of
this method is provided.
</summary>
    <author>
      <name>Wei-Chia Chen</name>
    </author>
    <author>
      <name>Ammar Tareen</name>
    </author>
    <author>
      <name>Justin B. Kinney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Includes main text (5 pages, 3 figures) and Supplemental Information
  (10 pages, 4 figures). Same as version 3 but with Feynman diagrams properly
  rendered</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01932v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01932v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10019v1</id>
    <updated>2018-08-29T19:36:30Z</updated>
    <published>2018-08-29T19:36:30Z</published>
    <title>Adaptative significance levels in normal mean hypothesis testing</title>
    <summary>  The Full Bayesian Significance Test (FBST) for precise hypotheses was
presented by Pereira and Stern (1999) as a Bayesian alternative instead of the
traditional significance test based on p-value. The FBST uses the evidence in
favor of the null hypothesis ($H_0$) calculated as the complement of the
posterior probability of the highest posterior density region, which is tangent
to the set defined by $H_0$. An important practical issue for the
implementation of the FBST is the determination of how large the evidence must
be in order to decide for its rejection. In the Classical significance tests,
the most used measure for rejecting a hypothesis is p-value. It is known that
p-value decreases as sample size increases, so by setting a single significance
level, it usually leads $H_0$ rejection. In the FBST procedure, the evidence in
favor of $H_0$ exhibits the same behavior as the p-value when the sample size
increases. This suggests that the cut-off point to define the rejection of
$H_0$ in the FBST should be a sample size function. In this work, we focus on
the case of two-sided normal mean hypothesis testing and present a method to
find a cut-off value for the evidence in the FBST by minimizing the linear
combination of the type I error probability and the expected type II error
probability for a given sample size.
</summary>
    <author>
      <name>Alejandra Estefanía Patiño Hoyos</name>
    </author>
    <author>
      <name>Victor Fossaluza</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09401v3</id>
    <updated>2018-08-29T17:27:30Z</updated>
    <published>2018-06-25T12:00:56Z</published>
    <title>Quasi-likelihood analysis of an ergodic diffusion plus noise</title>
    <summary>  We consider adaptive maximum-likelihood-type estimators and adaptive
Bayes-type ones for discretely observed ergodic diffusion processes with
observation noise whose variance is constant. The quasi-likelihood functions
for the diffusion and drift parameters are introduced and the polynomial-type
large deviation inequalities for those quasi-likelihoods are shown to see the
convergence of moments for those estimators.
</summary>
    <author>
      <name>Shogo H. Nakakita</name>
    </author>
    <author>
      <name>Masayuki Uchida</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09401v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09401v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09758v1</id>
    <updated>2018-08-29T12:29:40Z</updated>
    <published>2018-08-29T12:29:40Z</published>
    <title>Consistency of estimators and variance estimators for two-stage sampling</title>
    <summary>  Two-stage sampling designs are commonly used for household and health
surveys. To produce reliable estimators with assorted confidence intervals,
some basic statistical properties like consistency and asymptotic normality of
the Horvitz-Thompson estimator are desirable , along with the consistency of
assorted variance estimators. These properties have been mainly studied for
single-stage sampling designs. In this work, we prove the consistency of the
Horvitz-Thompson es-timator and of associated variance estimators for a general
class of two-stage sampling designs, under mild assumptions. We also study
two-stage sampling with a large entropy sampling design at the first stage, and
prove that the Horvitz-Thompson estimator is asymptot-ically normally
distributed through a coupling argument. When the 1 first-stage sampling
fraction is negligible, simplified variance estima-tors which do not require
estimating the variance within the Primary Sampling Units are proposed, and
shown to be consistent.
</summary>
    <author>
      <name>Guillaume Chauvet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRMAR</arxiv:affiliation>
    </author>
    <author>
      <name>Audrey-Anne Vallée</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08153v2</id>
    <updated>2018-08-29T09:04:41Z</updated>
    <published>2018-08-24T14:22:18Z</published>
    <title>Spectral thresholding for the estimation of Markov chain transition
  operators</title>
    <summary>  We consider estimation of the transition operator $P$ of a Markov chain and
its transition density $p$ where the eigenvalues of $P$ are assumed to decay
exponentially fast. This is for instance the case for periodised
multi-dimensional diffusions observed in low frequency. We investigate the
performance of a spectral hard thresholded Galerkin-type estimator for $P$ and
${p}$, discarding most of the estimated eigenpairs. We show its statistical
optimality by establishing matching minimax upper and lower bounds in
$L^2$-loss. Particularly, the effect of the dimension $d$ on the nonparametric
rate improves from $2d$ to $d$ compared to the case without eigenvalue decay.
</summary>
    <author>
      <name>Matthias Löffler</name>
    </author>
    <author>
      <name>Antoine Picard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08153v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08153v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10570v2</id>
    <updated>2018-08-29T01:29:31Z</updated>
    <published>2018-05-27T02:34:16Z</published>
    <title>Efficient Signal Inclusion With Genomic Applications</title>
    <summary>  This paper addresses the challenge of efficiently capturing a high proportion
of true signals for subsequent data analyses when sample sizes are relatively
limited with respect to data dimension. We propose the signal missing rate as a
new measure for false negative control to account for the variability of false
negative proportion. Novel data-adaptive procedures are developed to control
signal missing rate without incurring many unnecessary false positives under
dependence. We justify the efficiency and adaptivity of the proposed methods
via theory and simulation. The proposed methods are applied to GWAS on human
height to effectively remove irrelevant SNPs while retaining a high proportion
of relevant SNPs for subsequent polygenic analysis.
</summary>
    <author>
      <name>X. Jessie Jeng</name>
    </author>
    <author>
      <name>Teng Zhang</name>
    </author>
    <author>
      <name>Jung-Ying Tzeng</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09521v1</id>
    <updated>2018-08-28T20:11:16Z</updated>
    <published>2018-08-28T20:11:16Z</published>
    <title>Bounds on the conditional and average treatment effect in the presence
  of unobserved confounders</title>
    <summary>  The causal effect of an intervention can not be consistently estimated when
the treatment assignment is influenced by unknown confounding factors. However,
we can still study the causal effect when the dependence of treatment
assignment on unobserved confounding factors is bounded by performing a
sensitivity analysis. In such a case, the treatment effect is partially
identifiable in that the bound of the treatment effect is still estimable based
on the observed data. Here, we propose a sensitivity analysis approach to bound
the conditional average treatment effect over observed covariates under bounded
selection on unobservables. Additionally, we propose a semi-parametric method
to estimate bounds on the average treatment effect and derive confidence
intervals for these bounds. Combining the confidence intervals of the lower and
upper bound gives a confidence region that includes the average treatment
effect when the bounded selection on unobservables holds. This method scales to
settings where the dimension of observed covariates is too high to apply a
traditional sensitivity analysis based on covariate matching. Finally, we
provide evidence from simulations and real data to illustrate the accuracy of
the confidence intervals and value of our approach in practical finite sample
regimes.
</summary>
    <author>
      <name>Steve Yadlowsky</name>
    </author>
    <author>
      <name>Hongseok Namkoong</name>
    </author>
    <author>
      <name>Sanjay Basu</name>
    </author>
    <author>
      <name>John Duchi</name>
    </author>
    <author>
      <name>Lu Tian</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09507v1</id>
    <updated>2018-08-28T19:31:28Z</updated>
    <published>2018-08-28T19:31:28Z</published>
    <title>Tree-Based Bayesian Treatment Effect Analysis</title>
    <summary>  The inclusion of the propensity score as a covariate in Bayesian regression
trees for causal inference can reduce the bias in treatment effect estimations,
which occurs due to the regularization-induced confounding phenomenon. This
study advocate for the use of the propensity score by evaluating it under a
full-Bayesian variable selection setting, and the use of Individual Conditional
Expectation Plots, which is a graphical tool that can improve treatment effect
analysis on tree-based Bayesian models and others "black box" models. The first
one, even if poorly estimated, can lead to bias reduction on the estimated
treatment effects, while the latter can be used to found groups of individuals
which have different responses to the applied treatment, and analyze the impact
of each variable in the estimated treatment effect.
</summary>
    <author>
      <name>Pedro Henrique Filipini dos Santos</name>
    </author>
    <author>
      <name>Hedibert Freitas Lopes</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00796v2</id>
    <updated>2018-08-28T19:06:44Z</updated>
    <published>2018-02-02T18:47:19Z</published>
    <title>Bayes Calculations from Quantile Implied Likelihood</title>
    <summary>  In statistical practice, a realistic Bayesian model for a given data set can
be defined by a likelihood function that is analytically or computationally
intractable, due to large data sample size, high parameter dimensionality, or
complex likelihood functional form. This in turn poses challenges to the
computation and inference of the posterior distribution of the model
parameters. For such a model, a tractable likelihood function is introduced
which approximates the exact likelihood through its quantile function. It is
defined by an asymptotic chi-square confidence distribution for a pivotal
quantity, which is generated by the asymptotic normal distribution of the
sample quantiles given model parameters. This Quantile Implied Likelihood (QIL)
gives rise to an approximate posterior distribution, which is
fully-interpretable as a confidence distribution, and can be estimated by using
penalized log-likelihood maximization or any suitable Monte Carlo algorithm.
The QIL approach to Bayesian Computation is illustrated through the Bayesian
analysis of simulated and real data sets having sample sizes that reach the
millions, involving various models for univariate or multivariate iid or
non-iid data, with low or high parameter dimensionality, many of which are
defined by intractable likelihoods. Models include the Student's t, g-and-h,
and g-and-k distributions; the Bayesian logit regression model with many
covariates; exponential random graph model, a doubly-intractable model for
networks; the multivariate skew normal model, for robust inference of
inverse-covariance matrix when it is large relative to the sample size; and the
Wallenius distribution model.
</summary>
    <author>
      <name>George Karabatsos</name>
    </author>
    <author>
      <name>Fabrizio Leisen</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00796v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00796v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09379v1</id>
    <updated>2018-08-28T16:04:08Z</updated>
    <published>2018-08-28T16:04:08Z</published>
    <title>A transport-based multifidelity preconditioner for Markov chain Monte
  Carlo</title>
    <summary>  Markov chain Monte Carlo (MCMC) sampling of posterior distributions arising
in Bayesian inverse problems is challenging when evaluations of the forward
model are computationally expensive. Replacing the forward model with a
low-cost, low-fidelity model often significantly reduces computational cost;
however, employing a low-fidelity model alone means that the stationary
distribution of the MCMC chain is the posterior distribution corresponding to
the low-fidelity model, rather than the original posterior distribution
corresponding to the high-fidelity model. We propose a multifidelity approach
that combines, rather than replaces, the high-fidelity model with a
low-fidelity model. First, the low-fidelity model is used to construct a
transport map that deterministically couples a reference Gaussian distribution
with an approximation of the low-fidelity posterior. Then, the high-fidelity
posterior distribution is explored using a non-Gaussian proposal distribution
derived from the transport map. This multifidelity "preconditioned" MCMC
approach seeks efficient sampling via a proposal that is explicitly tailored to
the posterior at hand and that is constructed efficiently with the low-fidelity
model. By relying on the low-fidelity model only to construct the proposal
distribution, our approach guarantees that the stationary distribution of the
MCMC chain is the high-fidelity posterior. In our numerical examples, our
multifidelity approach achieves significant speedups compared to
single-fidelity MCMC sampling methods.
</summary>
    <author>
      <name>Benjamin Peherstorfer</name>
    </author>
    <author>
      <name>Youssef Marzouk</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09448v1</id>
    <updated>2018-08-28T14:32:58Z</updated>
    <published>2018-08-28T14:32:58Z</published>
    <title>Estimating the distribution and thinning parameters of a homogeneous
  multimode Poisson process</title>
    <summary>  In this paper we propose estimators of the distribution of events of
different kinds in a multimode Poisson process. We give the explicit solution
for the maximum likelihood estimator, and derive its strong consistency and
asymptotic normality. We also provide an order restricted estimator and derive
its consistency and asymptotic distribution. We discuss the application of the
estimator to the detection of neutrons in a novel detector being developed at
the European Spallation Source in Lund, Sweden. The inference problem gives
rise to Sylvester-Ramanujan system of equations.
</summary>
    <author>
      <name>Dragi Anevski</name>
    </author>
    <author>
      <name>Vladimir Pastukhov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09262v1</id>
    <updated>2018-08-28T12:58:35Z</updated>
    <published>2018-08-28T12:58:35Z</published>
    <title>The Sparse Latent Position Model for nonnegative weighted networks</title>
    <summary>  This paper introduces a new methodology to analyse bipartite and unipartite
networks with nonnegative edge values. The proposed approach combines and
adapts a number of ideas from the literature on latent variable network models.
The resulting framework is a new type of latent position model which exhibits
great flexibility, and is able to capture important features that are generally
exhibited by observed networks, such as sparsity and heavy tailed degree
distributions. A crucial advantage of the proposed method is that the number of
latent dimensions is automatically deduced from the data in one single
algorithmic framework. In addition, the model attaches a weight to each of the
latent dimensions, hence providing a measure of their relative importance. A
fast variational Bayesian algorithm is proposed to estimate the parameters of
the model. Finally, applications of the proposed methodology are illustrated on
both artificial and real datasets, and comparisons with other existing
procedures are provided.
</summary>
    <author>
      <name>Riccardo Rastelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09200v1</id>
    <updated>2018-08-28T09:54:54Z</updated>
    <published>2018-08-28T09:54:54Z</published>
    <title>Bayesian model-based spatiotemporal survey design for log-Gaussian Cox
  process</title>
    <summary>  In geostatistics, the design for data collection is central for accurate
prediction and parameter inference. One important class of geostatistical
models is log-Gaussian Cox process (LGCP) which is used extensively, for
example, in ecology. However, there are no formal analyses on optimal designs
for LGCP models. In this work, we develop a novel model-based experimental
design for LGCP modeling of spatiotemporal point process data. We propose a new
spatially balanced rejection sampling design which directs sampling to
spatiotemporal locations that are a priori expected to provide most
information. We compare the rejection sampling design to traditional balanced
and uniform random designs using the average predictive variance loss function
and the Kullback-Leibler divergence between prior and posterior for the LGCP
intensity function. Our results show that the rejection sampling method
outperforms the corresponding balanced and uniform random sampling designs for
LGCP whereas the latter work better for models with Gaussian models. We perform
a case study applying our new sampling design to plan a survey for species
distribution modeling on larval areas of two commercially important fish stocks
on Finnish coastal areas. The case study results show that rejection sampling
designs give considerable benefit compared to traditional designs. Results show
also that best performing designs may vary considerably between target species.
</summary>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Jarno Vanhatalo</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09152v1</id>
    <updated>2018-08-28T07:38:43Z</updated>
    <published>2018-08-28T07:38:43Z</published>
    <title>On the Continuous Limit of Weak GARCH</title>
    <summary>  We prove that the symmetric weak GARCH limit is a geometric mean-reverting
stochastic volatility process with diffusion determined by kurtosis of physical
log returns; this provides an improved fit to implied volatility surfaces. When
log returns are normal the limit coincides with Nelson's limit. The limit is
unique, unlike strong GARCH limits, because assumptions about convergence of
model parameters is unnecessary -- parameter convergence is uniquely determined
by time-aggregation of the weak GARCH process.
</summary>
    <author>
      <name>Carol Alexander</name>
    </author>
    <author>
      <name>Emese Lazar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09107v1</id>
    <updated>2018-08-28T04:09:50Z</updated>
    <published>2018-08-28T04:09:50Z</published>
    <title>Robust Factor Number Specification for Large-dimensional Factor Model</title>
    <summary>  The accurate specification of the number of factors is critical to the
validity of factor models and the topic almost occupies the central position in
factor analysis. Plenty of estimators are available under the restrictive
condition that the fourth moments of the factors and idiosyncratic errors are
bounded. In this paper we propose efficient and robust estimators for the
factor number via considering a more general static Elliptical Factor Model
(EFM) framework. We innovatively propose to exploit the multivariate Kendall's
tau matrix, which captures the correlation structure of elliptical random
vectors. Theoretically we show that the proposed estimators are consistent
without exerting any moment condition when both cross-sections N and time
dimensions T go to infinity. Simulation study shows that the new estimators
perform much better in heavy-tailed data setting while performing comparably
with the state-of-the-art methods in the light-tailed Gaussian setting. At
last, a real macroeconomic data example is given to illustrate its empirical
advantages and usefulness.
</summary>
    <author>
      <name>Long Yu</name>
    </author>
    <author>
      <name>Yong He</name>
    </author>
    <author>
      <name>Xinsheng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02920v2</id>
    <updated>2018-08-28T03:34:14Z</updated>
    <published>2018-02-08T15:28:46Z</published>
    <title>Spectral State Compression of Markov Processes</title>
    <summary>  Model reduction of the Markov process is a basic problem in modeling
state-transition systems. Motivated by the state aggregation approach rooted in
control theory, we study the statistical state compression of a finite-state
Markov chain from empirical trajectories. Through the lens of spectral
decomposition, we study the rank and features of Markov processes, as well as
properties like representability, aggregatability, and lumpability. We develop
a class of spectral state compression methods for three tasks: (1) estimate the
transition matrix of a low-rank Markov model, (2) estimate the leading subspace
spanned by Markov features, and (3) recover latent structures of the state
space like state aggregation and lumpable partition. The proposed methods
provide an unsupervised learning framework for identifying Markov features and
clustering states. We provide upper bounds for the estimation errors and nearly
matching minimax lower bounds. Numerical studies are performed on synthetic
data and a dataset of New York City taxi trips.
</summary>
    <author>
      <name>Anru Zhang</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02920v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02920v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02365v2</id>
    <updated>2018-08-28T01:16:17Z</updated>
    <published>2017-08-08T04:04:02Z</published>
    <title>Indirect Inference with a Non-Smooth Criterion Function</title>
    <summary>  Indirect inference requires simulating realisations of endogenous variables
from the model under study. When the endogenous variables are discontinuous
functions of the model parameters, the resulting indirect inference criterion
function is discontinuous and does not permit the use of derivative-based
optimisation routines. Using a change of variables technique, we propose a
novel simulation algorithm that alleviates the underlying discontinuities
inherent in such indirect inference criterion functions, and permits the
application of derivative-based optimisation routines to estimate the unknown
model parameters. Unlike competing approaches, this approach does not rely on
kernel smoothing or bandwidth parameters. Several Monte Carlo examples that
have featured in the literature on indirect inference with discontinuous
outcomes illustrate the approach. These examples demonstrate that this new
method gives superior performance over existing alternatives in terms of bias
and variance.
</summary>
    <author>
      <name>David T. Frazier</name>
    </author>
    <author>
      <name>Tatsushi Oka</name>
    </author>
    <author>
      <name>Dan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is a revision of arXiv:1708.02365 and supersedes the
  earlier arXiv paper "Derivative-Based Optimization with a Non-Smooth
  Simulated Criterion"</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02365v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02365v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10173v2</id>
    <updated>2018-08-27T21:15:33Z</updated>
    <published>2018-07-26T14:47:52Z</published>
    <title>Differential Analysis of Directed Networks</title>
    <summary>  We developed a novel statistical method to identify structural differences
between networks characterized by structural equation models. We propose to
reparameterize the model to separate the differential structures from common
structures, and then design an algorithm with calibration and construction
stages to identify these differential structures. The calibration stage serves
to obtain consistent prediction by building the L2 regularized regression of
each endogenous variables against pre-screened exogenous variables, correcting
for potential endogeneity issue. The construction stage consistently selects
and estimates both common and differential effects by undertaking L1
regularized regression of each endogenous variable against the predicts of
other endogenous variables as well as its anchoring exogenous variables. Our
method allows easy parallel computation at each stage. Theoretical results are
obtained to establish nonasymptotic error bounds of predictions and estimates
at both stages, as well as the consistency of identified common and
differential effects. Our studies on synthetic data demonstrated that our
proposed method performed much better than independently constructing the
networks. A real data set is analyzed to illustrate the applicability of our
method.
</summary>
    <author>
      <name>Min Ren</name>
    </author>
    <author>
      <name>Dabao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, Proceedings of the 34th Conference on Uncertainty in
  Artificial Intelligence (UAI), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10173v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10173v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09011v1</id>
    <updated>2018-08-27T19:38:16Z</updated>
    <published>2018-08-27T19:38:16Z</published>
    <title>Cauchy combination test: a powerful test with analytic p-value
  calculation under arbitrary dependency structures</title>
    <summary>  Combining individual p-values to aggregate multiple small effects has a
long-standing interest in statistics, dating back to the classic Fisher's
combination test. In modern large-scale data analysis, correlation and sparsity
are common features and efficient computation is a necessary requirement for
dealing with massive data. To overcome these challenges, we propose a new test
that takes advantage of the Cauchy distribution. Our test statistic has a very
simple form and is defined as a weighted sum of Cauchy transformation of
individual p-values. We prove a non-asymptotic result that the tail of the null
distribution of our proposed test statistic can be well approximated by a
Cauchy distribution under arbitrary dependency structures. Based on this
theoretical result, the p-value calculation of our proposed test is not only
accurate, but also as simple as the classic z-test or t-test, making our test
well suited for analyzing massive data. We further show that the power of the
proposed test is asymptotically optimal in a strong sparsity setting. Extensive
simulations demonstrate that the proposed test has both strong power against
sparse alternatives and a good accuracy with respect to p-value calculations,
especially for very small p-values. The proposed test has also been applied to
a genome-wide association study of Crohn's disease and compared with several
existing tests.
</summary>
    <author>
      <name>Yaowu Liu</name>
    </author>
    <author>
      <name>Jun Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08986v1</id>
    <updated>2018-08-27T18:24:26Z</updated>
    <published>2018-08-27T18:24:26Z</published>
    <title>The Behrens-Fisher Problem with Covariates and Baseline Adjustments</title>
    <summary>  The Welch-Satterthwaite t-test is one of the most prominent and often used
statistical inference method in applications. The method is, however, not
flexible with respect to adjustments for baseline values or other covariates,
which may impact the response variable. Existing analysis of covariance methods
are typically based on the assumption of equal variances across the groups.
This assumption is hard to justify in real data applications and the methods
tend to not control the type-1 error rate satisfactorily under variance
heteroscedasticity. In the present paper, we tackle this problem and develop
unbiased variance estimators of group specific variances, and especially of the
variance of the estimated adjusted treatment effect in a general analysis of
covariance model. These results are used to generalize the Welch-Satterthwaite
t-test to covariates adjustments. Extensive simulation studies show that the
method accurately controls the nominal type-1 error rate, even for very small
sample sizes, moderately skewed distributions and under variance
heteroscedasticity. A real data set motivates and illustrates the application
of the proposed methods.
</summary>
    <author>
      <name>Cong Cao</name>
    </author>
    <author>
      <name>Markus Pauly</name>
    </author>
    <author>
      <name>Frank Konietschke</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08793v1</id>
    <updated>2018-08-27T11:45:42Z</updated>
    <published>2018-08-27T11:45:42Z</published>
    <title>Empirical likelihood for linear models with spatial errors</title>
    <summary>  For linear models with spatial errors, the empirical likelihood ratio
statistics are constructed for the parameters of the models. It is shown that
the limiting distributions of the empirical likelihood ratio statistics are
chi-squared distributions, which are used to construct confidence regions for
the parameters of the models.
</summary>
    <author>
      <name>Yongsong Qin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08778v1</id>
    <updated>2018-08-27T10:48:39Z</updated>
    <published>2018-08-27T10:48:39Z</published>
    <title>Dynamical systems theory for causal inference with application to
  synthetic control methods</title>
    <summary>  To estimate treatment effects in panel data, suitable control units need to
be selected to generate counterfactual outcomes. To guard against
cherry-picking of potential controls, which is an important concern in
practice, we leverage results from dynamical systems theory. Specifically, key
results on delay embeddings in dynamical systems~\citep{Takens1981} show that
under fairly general assumptions a dynamical system can be reconstructed up to
a one-to-one mapping from scalar observations of the system. This suggests a
quantified measure of strength of the dynamical relationship between any two
time series variables. The key idea in this paper is to use this measure to
ensure that selected control units are dynamically related to treated units,
and thus guard against cherry-picking of controls. We illustrate our approach
on the synthetic control methodology of~\citet{Abadie2003}, which generates
counterfactuals using a model of treated unit outcomes fitted on outcomes from
control units. In this setting, we propose to screen out control units that
have a weak dynamical relationship to the single treated unit before the model
is fit. In simulated studies, we show that the standard synthetic control
methodology can be biased towards any desirable direction by adversarially
creating artificial control units, but the bias is largely mitigated if we
apply the aforementioned screening. In real-world applications, the proposed
approach contributes to more reliable control selection, and thus more robust
estimation of treatment effects.
</summary>
    <author>
      <name>Yi Ding</name>
    </author>
    <author>
      <name>Panos Toulis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08764v1</id>
    <updated>2018-08-27T10:04:24Z</updated>
    <published>2018-08-27T10:04:24Z</published>
    <title>Intrinsic wavelet regression for surfaces of Hermitian positive definite
  matrices</title>
    <summary>  This paper develops intrinsic wavelet denoising methods for surfaces of
Hermitian positive definite matrices, with in mind the application to
nonparametric estimation of the time-varying spectral matrix of a multivariate
locally stationary time series. First, we construct intrinsic
average-interpolating wavelet transforms acting directly on surfaces of
Hermitian positive definite matrices in a curved Riemannian manifold with
respect to an affine-invariant metric. Second, we derive the wavelet
coefficient decay and linear wavelet thresholding convergence rates of
intrinsically smooth surfaces of Hermitian positive definite matrices, and
investigate practical nonlinear thresholding of wavelet coefficients based on
their trace in the context of intrinsic signal plus noise models in the
Riemannian manifold. The finite-sample performance of nonlinear tree-structured
trace thresholding is assessed by means of simulated data, and the proposed
intrinsic wavelet methods are used to estimate the time-varying spectral matrix
of a nonstationary multivariate electroencephalography (EEG) time series
recorded during an epileptic brain seizure.
</summary>
    <author>
      <name>Joris Chau</name>
    </author>
    <author>
      <name>Rainer von Sachs</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M15, 62G08" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05005v3</id>
    <updated>2018-08-27T08:45:06Z</updated>
    <published>2018-02-14T09:41:59Z</published>
    <title>Using Longitudinal Targeted Maximum Likelihood Estimation in Complex
  Settings with Dynamic Interventions</title>
    <summary>  Longitudinal targeted maximum likelihood estimation (LTMLE) has hardly ever
been used to estimate dynamic treatment effects in the context of
time-dependent confounding affected by prior treatment when faced with long
follow-up times, multiple time-varying confounders, and complex associational
relationships simultaneously. Reasons for this include the potential
computational burden, technical challenges, restricted modeling options for
long follow-up times, and limited practical guidance in the literature.
However, LTMLE has desirable asymptotic properties, i.e. it is doubly robust,
and can yield valid inference when used in conjunction with machine learning.
We use a topical and sophisticated question from HIV treatment research to show
that LTMLE can be used successfully in complex realistic settings and compare
results to competing estimators. Our example illustrates the following
practical challenges common to many epidemiological studies 1) long follow-up
time (30 months), 2) gradually declining sample size 3) limited support for
some intervention rules of interest 4) a high-dimensional set of potential
adjustment variables, increasing both the need and the challenge of integrating
appropriate machine learning methods 5) consideration of collider bias. Our
analyses, as well as simulations, shed new light on the application of LTMLE in
complex and realistic settings: we show that (i) LTMLE can yield stable and
good estimates, even when confronted with small samples and limited modeling
options; (ii) machine learning utilized with a small set of simple learners (if
more complex ones can't be fitted) can outperform a single, complex model,
which is tailored to incorporate prior clinical knowledge; (iii) performance
can vary considerably depending on interventions and their support in the data,
and therefore critical quality checks should accompany every LTMLE analysis.
</summary>
    <author>
      <name>Michael Schomaker</name>
    </author>
    <author>
      <name>Miguel Angel Luque-Fernandez</name>
    </author>
    <author>
      <name>Valeriane Leroy</name>
    </author>
    <author>
      <name>Mary-Ann Davies</name>
    </author>
    <link href="http://arxiv.org/abs/1802.05005v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05005v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08683v1</id>
    <updated>2018-08-27T04:23:44Z</updated>
    <published>2018-08-27T04:23:44Z</published>
    <title>Regression Adjustments for Estimating the Global Treatment Effect in
  Experiments with Interference</title>
    <summary>  Standard estimators of the global average treatment effect can be biased in
the presence of interference. This paper proposes regression adjustment
estimators for removing bias due to interference in Bernoulli randomized
experiments. We use a fitted model to predict the counterfactual outcomes of
global control and global treatment. Our work differs from standard regression
adjustments in that the adjustment variables are constructed from functions of
the treatment assignment vector, and that we allow the researcher to use a
collection of any functions correlated with the response, turning the problem
of detecting interference into a feature engineering problem. We characterize
the distribution of the proposed estimator in a linear model setting and
connect the results to the standard theory of regression adjustments under
SUTVA. We then propose an estimator that allows for flexible machine learning
estimators to be used for fitting a nonlinear interference functional form,
borrowing ideas from the double machine learning literature. We propose
conducting statistical inference via bootstrap and resampling methods, which
allow us to sidestep the complicated dependences implied by interference and
instead rely on empirical covariance structures. Such variance estimation
relies on an exogeneity assumption akin to the standard unconfoundedness
assumption invoked in observational studies. In simulation experiments, our
methods are better at debiasing estimates than existing inverse propensity
weighted estimators based on neighborhood exposure modeling. We use our method
to reanalyze an experiment concerning weather insurance adoption conducted on a
collection of villages in rural China.
</summary>
    <author>
      <name>Alex Chin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07318v3</id>
    <updated>2018-08-27T00:36:45Z</updated>
    <published>2018-01-22T20:57:39Z</published>
    <title>Variable Prioritization in Nonlinear Black Box Methods: A Genetic
  Association Case Study</title>
    <summary>  The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the "RelATive cEntrality" (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other "black box" methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
</summary>
    <author>
      <name>Lorin Crawford</name>
    </author>
    <author>
      <name>Seth R. Flaxman</name>
    </author>
    <author>
      <name>Daniel E. Runcie</name>
    </author>
    <author>
      <name>Mike West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures, 1 tables; Supplementary Material</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07318v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07318v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08551v1</id>
    <updated>2018-08-26T13:28:21Z</updated>
    <published>2018-08-26T13:28:21Z</published>
    <title>Doubly Robust Sure Screening for Elliptical Copula Regression Model</title>
    <summary>  Regression analysis has always been a hot research topic in statistics. We
propose a very flexible semi-parametric regression model called Elliptical
Copula Regression (ECR) model, which covers a large class of linear and
nonlinear regression models such as additive regression model,single index
model. Besides, ECR model can capture the heavy-tail characteristic and tail
dependence between variables, thus it could be widely applied in many areas
such as econometrics and finance. In this paper we mainly focus on the feature
screening problem for ECR model in ultra-high dimensional setting. We propose a
doubly robust sure screening procedure for ECR model, in which two types of
correlation coefficient are involved: Kendall tau correlation and Canonical
correlation. Theoretical analysis shows that the procedure enjoys sure
screening property, i.e., with probability tending to 1, the screening
procedure selects out all important variables and substantially reduces the
dimensionality to a moderate size against the sample size. Thorough numerical
studies are conducted to illustrate its advantage over existing sure
independence screening methods and thus it can be used as a safe replacement of
the existing procedures in practice. At last, the proposed procedure is applied
on a gene-expression real data set to show its empirical usefulness.
</summary>
    <author>
      <name>Yong He</name>
    </author>
    <author>
      <name>Liang Zhang</name>
    </author>
    <author>
      <name>Jiadong JI</name>
    </author>
    <author>
      <name>Xinsheng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08507v1</id>
    <updated>2018-08-26T05:42:14Z</updated>
    <published>2018-08-26T05:42:14Z</published>
    <title>Finite and infinite Mallows ranking models, maximum likelihood
  estimator, and regeneration</title>
    <summary>  In this paper we are concerned with various Mallows ranking models. First we
study the statistical properties of the MLE of Mallows' $\phi$ model:
$\mathbb{P}_{\theta, \pi_0}(\pi) \propto \exp(-\theta \, inv(\pi \circ
\pi_0^{-1}))$, where $\theta$ is the dispersion parameter and $\pi_0$ is the
central ranking. We prove that (1). the MLE $\widehat{\theta}$ is biased
upwards for both Mallows' $\phi$ model and the single parameter IGM model; (2).
the MLE $\widehat{\pi}_0$ converges exponentially for Mallows' $\phi$ model. We
also make connections of various Mallows ranking models, encompassing the work
of Gnedin and Olshanski, and Pitman and Tang. Motivated by the infinite top-$t$
ranking model of Meil\v{a} and Bao, we propose an algorithm to select the model
size $t$ automatically. The key idea relies on the regenerative property of
such an infinite permutation. Finally, we apply our algorithm to several data
sets including the APA election data and the University homepage search data.
</summary>
    <author>
      <name>Wenpin Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 5 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08478v1</id>
    <updated>2018-08-25T22:42:46Z</updated>
    <published>2018-08-25T22:42:46Z</published>
    <title>Network Inference from Temporal-Dependent Grouped Observations</title>
    <summary>  In social network analysis, the observed data is usually some social
behavior, such as the formation of groups, rather than an explicit network
structure. Zhao and Weko (2017) propose a model-based approach called the hub
model to infer implicit networks from grouped observations. The hub model
assumes independence between groups, which sometimes is not valid in practice.
In this article, we generalize the idea of the hub model into the case of
grouped observations with temporal dependence. As in the hub model, we assume
that the group at each time point is gathered by one leader. Unlike in the hub
model, the group leaders are not sampled independently but follow a Markov
chain, and other members in adjacent groups can also be correlated.
  An expectation-maximization (EM) algorithm is developed for this model and a
polynomial-time algorithm is proposed for the E-step. The performance of the
new model is evaluated under different simulation settings. We apply this model
to a data set of the Kibale Chimpanzee Project.
</summary>
    <author>
      <name>Yunpeng Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08440v1</id>
    <updated>2018-08-25T15:30:08Z</updated>
    <published>2018-08-25T15:30:08Z</published>
    <title>Causes of Effects via a Bayesian Model Selection Procedure</title>
    <summary>  In causal inference, and specifically in the \textit{Causes of Effects}
problem, one is interested in how to use statistical evidence to understand
causation in an individual case, and so how to assess the so-called {\em
probability of causation} (PC).
  The answer relies on the potential responses, which can incorporate
information about what would have happened to the outcome as we had observed a
different value of the exposure. However, even given the best possible
statistical evidence for the association between exposure and outcome, we can
typically only provide bounds for the PC. Dawid et al. (2016) highlighted some
fundamental conditions, namely, exogeneity, comparability, and sufficiency,
required to obtain such bounds, based on experimental data. The aim of the
present paper is to provide methods to find, in specific cases, the best
subsample of the reference dataset to satisfy such requirements. To this end,
we introduce a new variable, expressing the desire to be exposed or not, and we
set the question up as a model selection problem. The best model will be
selected using the marginal probability of the responses and a suitable prior
proposal over the model space. An application in the educational field is
presented.
</summary>
    <author>
      <name>Fabio Corradi</name>
    </author>
    <author>
      <name>Monica Musio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08400v1</id>
    <updated>2018-08-25T10:09:33Z</updated>
    <published>2018-08-25T10:09:33Z</published>
    <title>Tree-based Particle Smoothing Algorithms in a Hidden Markov Model</title>
    <summary>  We provide a new strategy built on the divide-and-conquer approach by
Lindsten et al. (2017) to investigate the smoothing problem in a hidden Markov
model. We employ this approach to decompose a hidden Markov model into
sub-models with intermediate target distributions based on an auxiliary tree
structure and produce independent samples from the sub-models at the leaf nodes
towards the original model of interest at the root. We review the target
distribution in the sub-models suggested by Lindsten et al. and propose two new
classes of target distributions, which are the estimates of the (joint)
filtering distributions and the (joint) smoothing distributions. The first
proposed type is straightforwardly constructible by running a filtering
algorithm in advance. The algorithm using the second type of target
distributions has an advantage of roughly retaining the marginals of all random
variables invariant at all levels of the tree at the cost of approximating the
marginal smoothing distributions in advance. We further propose the
constructions of these target distributions using pre-generated Monte Carlo
samples. We show empirically the algorithms with the proposed intermediate
target distributions give stable and comparable results as the conventional
smoothing methods in a linear Gaussian model and a non-linear model.
</summary>
    <author>
      <name>Dong Ding</name>
    </author>
    <author>
      <name>Axel Gandy</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08326v1</id>
    <updated>2018-08-24T22:23:45Z</updated>
    <published>2018-08-24T22:23:45Z</published>
    <title>A Bayesian Approach to Restricted Latent Class Models for
  Scientifically-Structured Clustering of Multivariate Binary Outcomes</title>
    <summary>  In this paper, we propose a general framework for combining evidence of
varying quality to estimate underlying binary latent variables in the presence
of restrictions imposed to respect the scientific context. The resulting
algorithms cluster the multivariate binary data in a manner partly guided by
prior knowledge. The primary model assumptions are that 1) subjects belong to
classes defined by unobserved binary states, such as the true presence or
absence of pathogens in epidemiology, or of antibodies in medicine, or the
"ability" to correctly answer test questions in psychology, 2) a binary design
matrix $\Gamma$ specifies relevant features in each class, and 3) measurements
are independent given the latent class but can have different error rates.
Conditions ensuring parameter identifiability from the likelihood function are
discussed and inform the design of a novel posterior inference algorithm that
simultaneously estimates the number of clusters, design matrix $\Gamma$, and
model parameters. In finite samples and dimensions, we propose prior
assumptions so that the posterior distribution of the number of clusters and
the patterns of latent states tend to concentrate on smaller values and sparser
patterns, respectively. The model readily extends to studies where some
subjects' latent classes are known or important prior knowledge about
differential measurement accuracy is available from external sources. The
methods are illustrated with an analysis of protein data to detect clusters
representing auto-antibody classes among scleroderma patients.
</summary>
    <author>
      <name>Zhenke Wu</name>
    </author>
    <author>
      <name>Livia Casciola-Rosen</name>
    </author>
    <author>
      <name>Antony Rosen</name>
    </author>
    <author>
      <name>Scott L. Zeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages and three figures in Main Paper, 24 pages in Supplementary
  Materials</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06970v2</id>
    <updated>2018-08-24T19:32:51Z</updated>
    <published>2018-05-17T21:11:34Z</published>
    <title>Global and Simultaneous Hypothesis Testing for High-Dimensional Logistic
  Regression Models</title>
    <summary>  High-dimensional logistic regression is widely used in analyzing data with
binary outcomes. In this paper, global testing and large-scale multiple testing
for the regression coefficients are considered in both single- and
two-regression settings. A test statistic for testing the global null
hypothesis is constructed using a generalized low-dimensional projection for
bias correction and its asymptotic null distribution is derived. A minimax
lower bound for the global testing is established, which shows that the
proposed test is asymptotically minimax optimal. For testing the individual
coefficients simultaneously, multiple testing procedures are proposed and shown
to control the false discovery rate (FDR) and falsely discovered variables
(FDV) asymptotically. Simulation studies are carried out to examine the
numerical performance of the proposed tests and their superiority over existing
methods. The testing procedures are also illustrated by analyzing a
metabolomics study that investigates the association between fecal metabolites
and pediatric Crohn's disease and the effects of treatment on such
associations.
</summary>
    <author>
      <name>Rong Ma</name>
    </author>
    <author>
      <name>T. Tony Cai</name>
    </author>
    <author>
      <name>Hongzhe Li</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06970v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06970v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04982v2</id>
    <updated>2018-08-24T16:44:13Z</updated>
    <published>2018-07-13T09:31:46Z</published>
    <title>Generalized Simultaneous Component Analysis of Binary and Quantitative
  data</title>
    <summary>  In the current era of systems biological research there is a need for the
integrative analysis of binary and quantitative genomics data sets measured on
the same objects. One standard tool of exploring the underlying dependence
structure present in multiple quantitative data sets is simultaneous component
analysis (SCA) model. However, it does not have any provisions when a part of
the data are binary. To this end, we propose the generalized SCA (GSCA) model,
which takes into account the distinct mathematical properties of binary and
quantitative measurements in the maximum likelihood framework. Like in the SCA
model, a common low dimensional subspace is assumed to represent the shared
information between these two distinct types of measurements. However, the GSCA
model can easily be overfitted when a rank larger than one is used, leading to
some of the estimated parameters to become very large. To achieve a low rank
solution and combat overfitting, we propose to use a concave variant of the
nuclear norm penalty. An efficient majorization algorithm is developed to fit
this model with different concave penalties. Realistic simulations (low
signal-to-noise ratio and highly imbalanced binary data) are used to evaluate
the performance of the proposed model in recovering the underlying structure.
Also, a missing value based cross validation procedure is implemented for model
selection. We illustrate the usefulness of the GSCA model for exploratory data
analysis of quantitative gene expression and binary copy number aberration
(CNA) measurements obtained from the GDSC1000 data sets.
</summary>
    <author>
      <name>Yipeng Song</name>
    </author>
    <author>
      <name>Johan A. Westerhuis</name>
    </author>
    <author>
      <name>Nanne Aben</name>
    </author>
    <author>
      <name>Lodewyk F. A. Wessels</name>
    </author>
    <author>
      <name>Patrick J. F. Groenen</name>
    </author>
    <author>
      <name>Age K. Smilde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04982v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04982v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00564v2</id>
    <updated>2018-08-24T13:35:09Z</updated>
    <published>2016-02-01T15:32:30Z</published>
    <title>Conditional Estimation in Two-stage Adaptive Designs</title>
    <summary>  We consider conditional estimation in two-stage sample size adjustable
designs and the following bias. More specifically, we consider a design which
permits raising the sample size when interim results look rather promising,
and, which keeps the originally planned sample size when results look very
promising. The estimation procedures reported comprise the unconditional
maximum likelihood, the conditionally unbiased Rao-Blackwell estimator, the
conditional median unbiased estimator, and the conditional maximum likelihood
with and without bias correction. We compare these estimators based on
analytical results and by a simulation study. We show in a real clinical trial
setting how they can be applied.
</summary>
    <author>
      <name>Per Broberg</name>
    </author>
    <author>
      <name>Frank Miller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/biom.12642</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/biom.12642" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Broberg P, Miller F (2017). Conditional estimation in two-stage
  adaptive designs. Biometrics, 73, 895-904</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.00564v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00564v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08104v1</id>
    <updated>2018-08-24T12:26:18Z</updated>
    <published>2018-08-24T12:26:18Z</published>
    <title>A Bayesian nonparametric approach for generalized Bradley-Terry models
  in random environment</title>
    <summary>  This paper deals with the estimation of the unknown distribution of hidden
random variables from the observation of pairwise comparisons between these
variables. This problem is inspired by recent developments on Bradley-Terry
models in random environment since this framework happens to be relevant to
predict for instance the issue of a championship from the observation of a few
contests per team. This paper provides three contributions on a Bayesian
nonparametric approach to solve this problem. First, we establish contraction
rates of the posterior distribution. We also propose a Markov Chain Monte Carlo
algorithm to approximately sample from this posterior distribution inspired
from a recent Bayesian nonparametric method for hidden Markov models. Finally,
the performance of this algorithm are appreciated by comparing predictions on
the issue of a championship based on the actual values of the teams and those
obtained by sampling from the estimated posterior distribution.
</summary>
    <author>
      <name>Sylvain Le Corff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LMO</arxiv:affiliation>
    </author>
    <author>
      <name>Matthieu Lerasle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LM-Orsay</arxiv:affiliation>
    </author>
    <author>
      <name>Elodie Vernet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CMAP</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1808.08104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00642v2</id>
    <updated>2018-08-24T11:16:30Z</updated>
    <published>2017-04-03T15:34:11Z</published>
    <title>Local nearest neighbour classification with applications to
  semi-supervised learning</title>
    <summary>  We derive a new asymptotic expansion for the global excess risk of a local
$k$-nearest neighbour classifier, where the choice of $k$ may depend upon the
test point. This expansion elucidates conditions under which the dominant
contribution to the excess risk comes from the locus of points at which each
class label is equally likely to occur, but we also show that if these
conditions are not satisfied, the dominant contribution may arise from the
tails of the marginal distribution of the features. Moreover, we prove that,
provided the $d$-dimensional marginal distribution of the features has a finite
$\rho$th moment for some $\rho &gt; 4$ (as well as other regularity conditions), a
local choice of $k$ can yield a rate of convergence of the excess risk of
$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard
$k$-nearest neighbour classifier, our theory would require $d \geq 5$ and $\rho
&gt; 4d/(d-4)$ finite moments to achieve this rate. Our results motivate a new
$k$-nearest neighbour classifier for semi-supervised learning problems, where
the unlabelled data are used to obtain an estimate of the marginal feature
density, and fewer neighbours are used for classification when this density
estimate is small. The potential improvements over the standard $k$-nearest
neighbour classifier are illustrated both through our theory and via a
simulation study.
</summary>
    <author>
      <name>Timothy I. Cannings</name>
    </author>
    <author>
      <name>Thomas B. Berrett</name>
    </author>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">59 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08086v1</id>
    <updated>2018-08-24T10:55:07Z</updated>
    <published>2018-08-24T10:55:07Z</published>
    <title>Bayesian Multi--Dipole Modeling in the Frequency Domain</title>
    <summary>  Background: Magneto- and Electro-encephalography record the electromagnetic
field generated by neural currents with high temporal frequency and good
spatial resolution, and are therefore well suited for source localization in
the time and in the frequency domain. In particular, localization of the
generators of neural oscillations is very important in the study of cognitive
processes in the healthy and in the pathological brain.
  New method: We introduce the use of a Bayesian multi-dipole localization
method in the frequency domain. The algorithm is a sequential Monte Carlo
algorithm that approximates numerically the posterior distribution with a set
of weighted samples.
  Results: We use synthetic data to show that the proposed method behaves well
under a wide range of experimental conditions, including low signal-to-noise
ratios, correlated sources. We use dipole clusters to mimic the effect of
extended sources. In addition, we test the algorithm on real MEG data to
confirm its feasibility.
  Comparison with existing method(s): Throughout the whole study, DICS (Dynamic
Imaging of Coherent Sources) is used systematically as a benchmark. The two
methods provide similar general pictures, however, the posterior distributions
of the Bayesian approach contain richer information.
  Conclusions: The Bayesian method described in this paper represents a
reliable approach for localization of multiple dipoles in the frequency domain.
</summary>
    <author>
      <name>Gianvittorio Luria</name>
    </author>
    <author>
      <name>Dunja Duran</name>
    </author>
    <author>
      <name>Elisa Visani</name>
    </author>
    <author>
      <name>Sara Sommariva</name>
    </author>
    <author>
      <name>Fabio Rotondi</name>
    </author>
    <author>
      <name>Davide Rossi Sebastiano</name>
    </author>
    <author>
      <name>Ferruccio Panzica</name>
    </author>
    <author>
      <name>Michele Piana</name>
    </author>
    <author>
      <name>Alberto Sorrentino</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92C55, 65C05, 62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07995v3</id>
    <updated>2018-08-24T07:02:23Z</updated>
    <published>2018-02-22T11:52:56Z</published>
    <title>Multidimensional multiscale scanning in Exponential Families: Limit
  theory and statistical consequences</title>
    <summary>  We consider the problem of finding anomalies in a $d$-dimensional field of
independent random variables $\{Y_i\}_{i \in \left\{1,...,n\right\}^d}$, each
distributed according to a one-dimensional natural exponential family $\mathcal
F = \left\{F_\theta\right\}_{\theta \in\Theta}$. Given some baseline parameter
$\theta_0 \in\Theta$, the field is scanned using local likelihood ratio tests
to detect from a (large) given system of regions $\mathcal{R}$ those regions $R
\subset \left\{1,...,n\right\}^d$ with $\theta_i \neq \theta_0$ for some $i \in
R$. We provide a unified methodology which controls the overall family wise
error (FWER) to make a wrong detection at a given error rate.
  Fundamental to our method is a Gaussian approximation of the distribution of
the underlying multiscale test statistic with explicit rate of convergence.
From this, we obtain a weak limit theorem which can be seen as a generalized
weak invariance principle to non identically distributed data and is of
independent interest. Furthermore, we give an asymptotic expansion of the
procedures power, which yields minimax optimality in case of Gaussian
observations.
</summary>
    <author>
      <name>Claudia König</name>
    </author>
    <author>
      <name>Axel Munk</name>
    </author>
    <author>
      <name>Frank Werner</name>
    </author>
    <link href="http://arxiv.org/abs/1802.07995v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07995v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60F17, 62H10 (Primary), 60G50, 62F03 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04778v2</id>
    <updated>2018-08-23T21:06:03Z</updated>
    <published>2016-09-15T18:54:57Z</published>
    <title>False Discovery Rate Control for High-Dimensional Networks of Quantile
  Associations Conditioning on Covariates</title>
    <summary>  Motivated by the gene co-expression pattern analysis, we propose a novel
sample quantile-based contingency (squac) statistic to infer quantile
associations conditioning on covariates. It features enhanced flexibility in
handling variables with both arbitrary distributions and complex association
patterns conditioning on covariates. We first derive its asymptotic null
distribution, and then develop a multiple testing procedure based on squac to
simultaneously test the independence between one pair of variables conditioning
on covariates for all $p(p-1)/2$ pairs. Here, $p$ is the length of the outcomes
and could exceed the sample size. The testing procedure does not require
resampling or perturbation, and thus is computationally efficient. We prove by
theory and numerical experiments that this testing method asymptotically
controls the false discovery rate (\FDR). It outperforms all alternative
methods when the complex association panterns exist. Applied to a gastric
cancer data, this testing method successfully inferred the gene co-expression
networks of early and late stage patients. It identified more changes in the
networks which are associated with cancer survivals. We extend our method to
the case that both the length of the outcomes and the length of covariates
exceed the sample size, and show that the asymptotic theory still holds.
</summary>
    <author>
      <name>Jichun Xie</name>
    </author>
    <author>
      <name>Ruosha Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/rssb.12288</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/rssb.12288" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04778v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04778v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07932v1</id>
    <updated>2018-08-23T20:24:36Z</updated>
    <published>2018-08-23T20:24:36Z</published>
    <title>Structural-Factor Modeling of High-Dimensional Time Series: Another Look
  at Approximate Factor Models with Diverging Eigenvalues</title>
    <summary>  This article proposes a new approach to modeling high-dimensional time series
data by providing a simple and natural way to understand the mechanism of
factor models. We treat a $p$-dimensional time series as a nonsingular linear
transformation of certain common factors and structured idiosyncratic
components. Unlike the approximate factor models, we allow the largest
eigenvalues of the covariance matrix of the idiosyncratic components to diverge
as the dimension $p$ increases, which is reasonable in the high-dimensional
setting. A white noise testing procedure for high-dimensional random vectors is
proposed to determine the number of common factors under the assumption that
the idiosyncratic term is a vector white noise. We also introduce a projected
Principal Component Analysis (PCA) to eliminate the diverging effect of the
noises. Asymptotic properties of the proposed method are established for both
fixed $p$ and diverging $p$ as the sample size $n$ tends to infinity. Both
simulated and real examples are used to assess the performance of the proposed
method. We also compare our method with two commonly used methods in the
literature and find that the proposed approach not only provides interpretable
results, but also performs well in out-of-sample forecasting.
</summary>
    <author>
      <name>Zhaoxing Gao</name>
    </author>
    <author>
      <name>Ruey S Tsay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07861v1</id>
    <updated>2018-08-23T17:48:00Z</updated>
    <published>2018-08-23T17:48:00Z</published>
    <title>On model selection criteria for climate change impact studies</title>
    <summary>  Climate change impact studies inform policymakers on the estimated damages of
future climate change on economic, health and other outcomes. In most studies,
an annual outcome variable is observed, e.g. annual mortality rate, along with
higher-frequency regressors, e.g. daily temperature and precipitation.
Practitioners use summaries of the higher-frequency regressors in fixed effects
panel models. The choice over summary statistics amounts to model selection.
Some practitioners use Monte Carlo cross-validation (MCCV) to justify a
particular specification. However, conventional implementation of MCCV with
fixed testing-to-full sample ratios tends to select over-fit models. This paper
presents conditions under which MCCV, and also information criteria, can
deliver consistent model selection. Previous work has established that the
Bayesian information criterion (BIC) can be inconsistent for non-nested
selection. We illustrate that the BIC can also be inconsistent in our
framework, when all candidate models are misspecified. Our results have
practical implications for empirical conventions in climate change impact
studies. Specifically, they highlight the importance of a priori information
provided by the scientific literature to guide the models considered for
selection. We emphasize caution in interpreting model selection results in
settings where the scientific literature does not specify the relationship
between the outcome and the weather variables.
</summary>
    <author>
      <name>Xiaomeng Cui</name>
    </author>
    <author>
      <name>Dalia Ghanem</name>
    </author>
    <author>
      <name>Todd Kuffner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Additional simulation results available from authors by request</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M99 (Primary) 62P12, 62F99 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09169v3</id>
    <updated>2018-08-23T16:52:27Z</updated>
    <published>2015-09-30T13:38:31Z</published>
    <title>Lecture notes on ridge regression</title>
    <summary>  The linear regression model cannot be fitted to high-dimensional data, as the
high-dimensionality brings about empirical non-identifiability. Penalized
regression overcomes this non-identifiability by augmentation of the loss
function by a penalty (i.e. a function of regression coefficients). The ridge
penalty is the sum of squared regression coefficients, giving rise to ridge
regression. Here many aspect of ridge regression are reviewed e.g. moments,
mean squared error, its equivalence to constrained estimation, and its relation
to Bayesian regression. Finally, its behaviour and use are illustrated in
simulation and on omics data. Subsequently, ridge regression is generalized to
allow for a more general penalty. The ridge penalization framework is then
translated to logistic regression and its properties are shown to carry over.
To contrast ridge penalized estimation, the final chapter introduces its lasso
counterpart.
</summary>
    <author>
      <name>Wessel N. van Wieringen</name>
    </author>
    <link href="http://arxiv.org/abs/1509.09169v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09169v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03198v2</id>
    <updated>2018-08-23T15:25:49Z</updated>
    <published>2017-12-08T17:49:28Z</published>
    <title>Using simulation studies to evaluate statistical methods</title>
    <summary>  Simulation studies are computer experiments that involve creating data by
pseudorandom sampling. The key strength of simulation studies is the ability to
understand the behaviour of statistical methods because some 'truth' (usually
some parameter/s of interest) is known from the process of generating the data.
This allows us to consider properties of methods, such as bias. While widely
used, simulation studies are often poorly designed, analysed and reported. This
tutorial outlines the rationale for using simulation studies and offers
guidance for design, execution, analysis, reporting and presentation. In
particular, this tutorial provides: a structured approach for planning and
reporting simulation studies, which involves defining aims, data-generating
mechanisms, estimands, methods and performance measures ('ADEMP'); coherent
terminology for simulation studies; guidance on coding simulation studies; a
critical discussion of key performance measures and their estimation; guidance
on structuring tabular and graphical presentation of results; and new graphical
presentations. With a view to describing recent practice, we review 100
articles taken from Volume 34 of Statistics in Medicine that included at least
one simulation study and identify areas for improvement.
</summary>
    <author>
      <name>Tim P Morris</name>
    </author>
    <author>
      <name>Ian R White</name>
    </author>
    <author>
      <name>Michael J Crowther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 8 figures (2 in appendix), 8 tables (1 in appendix)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07996v2</id>
    <updated>2018-08-23T15:15:34Z</updated>
    <published>2018-07-20T18:34:18Z</published>
    <title>Reliable variance propagation for spatial density surface models</title>
    <summary>  Density Surface Models (DSMs) are two-stage models for estimating animal
density from line-transect data. First, detectability is estimated by distance
sampling, and then a Generalized Additive Model (GAM) is used to estimate
animal density as a function of location and/or other spatially-varying
environmental covariates. One criticism of DSMs has been that uncertainty from
the two stages is not usually propagated correctly into the final variance
estimates. Here we show how to reformulate a DSM so that the uncertainty in
detection probability from the distance sampling stage (regardless of its
complexity) is captured as a random effect in the GAM stage. This allows
straightforward computation of the overall variance via standard GAM machinery.
We further extend the formulation to allow for spatial variation in group size,
which can be an important covariate for detectability. We illustrate these
models using line transect survey data of minke whales and harbour porpoise
from the SCANS-II survey in northern Europe.
</summary>
    <author>
      <name>Mark V Bravington</name>
    </author>
    <author>
      <name>David L Miller</name>
    </author>
    <author>
      <name>Sharon L Hedley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07996v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07996v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03447v2</id>
    <updated>2018-08-23T13:36:16Z</updated>
    <published>2018-08-10T08:03:49Z</published>
    <title>Bagging of Density Estimators</title>
    <summary>  In this work we give new density estimators by averaging classical density
estimators such as the histogram, the frequency polygon and the kernel density
estimators obtained over different bootstrap samples of the original data. We
prove the L 2-consistency of these new estimators and compare them to several
similar approaches by extensive simulations. Based on them, we give also a way
to construct non parametric pointwise confidence intervals for the target
density.
</summary>
    <author>
      <name>Mathias Bourel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMERL</arxiv:affiliation>
    </author>
    <author>
      <name>Jairo Cugliari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1808.03447v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03447v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07719v1</id>
    <updated>2018-08-23T12:41:24Z</updated>
    <published>2018-08-23T12:41:24Z</published>
    <title>Joint Models with Multiple Longitudinal Outcomes and a Time-to-Event
  Outcome</title>
    <summary>  Joint models for longitudinal and survival data have garnered a lot of
attention in recent years, with the development of myriad extensions to the
basic model, including those which allow for multivariate longitudinal data,
competing risks and recurrent events. Several software packages are now also
available for their implementation. Although mathematically straightforward,
the inclusion of multiple longitudinal outcomes in the joint model remains
computationally difficult due to the large number of random effects required,
which hampers the practical application of this extension. We present a novel
approach that enables the fitting of such models with more realistic
computational times. The idea behind the approach is to split the estimation of
the joint model in two steps; estimating a multivariate mixed model for the
longitudinal outcomes, and then using the output from this model to fit the
survival submodel. So called two-stage approaches have previously been
proposed, and shown to be biased. Our approach differs from the standard
version, in that we additionally propose the application of a correction
factor, adjusting the estimates obtained such that they more closely resemble
those we would expect to find with the multivariate joint model. This
correction is based on importance sampling ideas. Simulation studies show that
this corrected-two-stage approach works satisfactorily, eliminating the bias
while maintaining substantial improvement in computational time, even in more
difficult settings.
</summary>
    <author>
      <name>Katya Mauff</name>
    </author>
    <author>
      <name>Ewout Steyerberg</name>
    </author>
    <author>
      <name>Isabella Kardys</name>
    </author>
    <author>
      <name>Eric Boersma</name>
    </author>
    <author>
      <name>Dimitris Rizopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 12 figures and 3 tables including appendices. Pending
  review in Biometrics</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07704v1</id>
    <updated>2018-08-23T11:36:27Z</updated>
    <published>2018-08-23T11:36:27Z</published>
    <title>Data-adaptive trimming of the Hill estimator and detection of outliers
  in the extremes of heavy-tailed data</title>
    <summary>  We introduce a trimmed version of the Hill estimator for the index of a
heavy-tailed distribution, which is robust to perturbations in the extreme
order statistics. In the ideal Pareto setting, the estimator is essentially
finite-sample efficient among all unbiased estimators with a given strict upper
break-down point. For general heavy-tailed models, we establish the asymptotic
normality of the estimator under second order regular variation conditions and
also show it is minimax rate-optimal in the Hall class of distributions. We
also develop an automatic, data-driven method for the choice of the trimming
parameter which yields a new type of robust estimator that can adapt to the
unknown level of contamination in the extremes. This adaptive robustness
property makes our estimator particularly appealing and superior to other
robust estimators in the setting where the extremes of the data are
contaminated. As an important application of the data-driven selection of the
trimming parameters, we obtain a methodology for the principled identification
of extreme outliers in heavy tailed data. Indeed, the method has been shown to
correctly identify the number of outliers in the previously explored Condroz
data set.
</summary>
    <author>
      <name>Shrijita Bhattacharya</name>
    </author>
    <author>
      <name>Michael Kallitsis</name>
    </author>
    <author>
      <name>Stilian Stoev</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01715v3</id>
    <updated>2018-08-23T07:35:26Z</updated>
    <published>2017-05-04T06:47:30Z</published>
    <title>Directed networks with a noisy bi-degree sequence</title>
    <summary>  Although a lot of approaches are developed to release network data with a
differentially privacy guarantee, inference using noisy data in many network
models is still unknown or not properly explored.In this paper, we release the
bi-degree sequences of directed networks under a general additive noisy
mechanism with the Laplace mechanism as a special case and use the $p_0$ model
for inferring the degree parameters. We show that the estimator of the
parameter without the denoised process is asymptotically consistent and
normally distributed. This is contrast sharply with known results that valid
inference such as the existence and consistency of the estimator needs the
denoised process. Along the way, a new phenomenon is revealed in which an
additional variance factor appears in the asymptotic variance of the estimator
when the noise becomes large. Further, we propose an efficient algorithm for
finding the closet point lying in the set of all graphical bi-degree sequences
under the global $L_1$ optimization problem. The algorithm simultaneously
produces a synthetic directed graph. Numerical studies demonstrate our
theoretical findings.
</summary>
    <author>
      <name>Ting Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 5 figures, new contents added, major revision. We have
  changed the original title to "Directed networks with a noisy bi-degree
  sequence" because new contents are added</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01715v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01715v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06638v3</id>
    <updated>2018-08-23T02:20:15Z</updated>
    <published>2018-08-20T18:18:52Z</published>
    <title>Supervised Kernel PCA For Longitudinal Data</title>
    <summary>  In statistical learning, high covariate dimensionality poses challenges for
robust prediction and inference. To address this challenge, supervised
dimension reduction is often performed, where dependence on the outcome is
maximized for a selected covariate subspace with smaller dimensionality.
Prevalent dimension reduction techniques assume data are $i.i.d.$, which is not
appropriate for longitudinal data comprising multiple subjects with repeated
measurements over time. In this paper, we derive a decomposition of the
Hilbert-Schmidt Independence Criterion as a supervised loss function for
longitudinal data, enabling dimension reduction between and within clusters
separately, and propose a dimensionality-reduction technique, $sklPCA$, that
performs this decomposed dimension reduction. We also show that this technique
yields superior model accuracy compared to the model it extends.
</summary>
    <author>
      <name>Patrick Staples</name>
    </author>
    <author>
      <name>Min Ouyang</name>
    </author>
    <author>
      <name>Robert F. Dougherty</name>
    </author>
    <author>
      <name>Gregory A. Ryslik</name>
    </author>
    <author>
      <name>Paul Dagum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06638v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06638v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07501v1</id>
    <updated>2018-08-22T18:06:38Z</updated>
    <published>2018-08-22T18:06:38Z</published>
    <title>Calibration Scoring Rules for Practical Prediction Training</title>
    <summary>  In situations where forecasters are scored on the quality of their
probabilistic predictions, it is standard to use `proper' scoring rules to
perform such scoring. These rules are desirable because they give forecasters
no incentive to lie about their probabilistic beliefs. However, in the real
world context of creating a training program designed to help people improve
calibration through prediction practice, there are a variety of desirable
traits for scoring rules that go beyond properness. These potentially may have
a substantial impact on the user experience, usability of the program, or
efficiency of learning. The space of proper scoring rules is too broad, in the
sense that most proper scoring rules lack these other desirable properties. On
the other hand, the space of proper scoring rules is potentially also too
narrow, in the sense that we may sometimes choose to give up properness when it
conflicts with other properties that are even more desirable from the point of
view of usability and effective training. We introduce a class of scoring rules
that we call `Practical' scoring rules, designed to be intuitive to users in
the context of `right' vs. `wrong' probabilistic predictions. We also introduce
two specific scoring rules for prediction intervals, the `Distance' and `Order
of magnitude' rules. These rules are designed to satisfy a variety of
properties that, based on user testing, we believe are desirable for applied
calibration training.
</summary>
    <author>
      <name>Spencer Greenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07449v1</id>
    <updated>2018-08-22T17:27:05Z</updated>
    <published>2018-08-22T17:27:05Z</published>
    <title>Robust Spatial Extent Inference with a Semiparametric Bootstrap Joint
  Testing Procedure</title>
    <summary>  Spatial extent inference (SEI) is widely used across neuroimaging modalities
to study brain-phenotype associations that inform our understanding of disease.
Recent studies have shown that Gaussian random field (GRF) based tools can have
inflated family-wise error rates (FWERs). This has led to fervent discussion as
to which preprocessing steps are necessary to control the FWER using GRF-based
SEI. The failure of GRF-based methods is due to unrealistic assumptions about
the covariance function of the imaging data. The permutation procedure is the
most robust SEI tool because it estimates the covariance function from the
imaging data. However, the permutation procedure can fail because its
assumption of exchangeability is violated in many imaging modalities. Here, we
propose the (semi-) parametric bootstrap joint (PBJ; sPBJ) testing procedures
that are designed for SEI of multilevel imaging data. The sPBJ procedure uses a
robust estimate of the covariance function, which yields consistent estimates
of standard errors, even if the covariance model is misspecified. We use our
methods to study the association between performance and executive functioning
in a working fMRI study. The sPBJ procedure is robust to variance
misspecification and maintains nominal FWER in small samples, in contrast to
the GRF methods. The sPBJ also has equal or superior power to the PBJ and
permutation procedures. We provide an R package
https://github.com/simonvandekar/pbj to perform inference using the PBJ and
sPBJ procedures
</summary>
    <author>
      <name>Simon N. Vandekar</name>
    </author>
    <author>
      <name>Theodore D. Satterthwaite</name>
    </author>
    <author>
      <name>Cedric H. Xia</name>
    </author>
    <author>
      <name>Kosha Ruparel</name>
    </author>
    <author>
      <name>Ruben C. Gur</name>
    </author>
    <author>
      <name>Raquel E. Gur</name>
    </author>
    <author>
      <name>Russell T. Shinohara</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07433v1</id>
    <updated>2018-08-22T16:42:04Z</updated>
    <published>2018-08-22T16:42:04Z</published>
    <title>Bayesian Estimation of Sparse Spiked Covariance Matrices in High
  Dimensions</title>
    <summary>  We propose a Bayesian methodology for estimating spiked covariance matrices
with jointly sparse structure in high dimensions. The spiked covariance matrix
is reparametrized in terms of the latent factor model, where the loading matrix
is equipped with a novel matrix spike-and-slab LASSO prior, which is a
continuous shrinkage prior for modeling jointly sparse matrices. We establish
the rate-optimal posterior contraction for the covariance matrix with respect
to the operator norm as well as that for the principal subspace with respect to
the projection operator norm loss. We also study the posterior contraction rate
of the principal subspace with respect to the two-to-infinity norm loss, a
novel loss function measuring the distance between subspaces that is able to
capture element-wise eigenvector perturbations. We show that the posterior
contraction rate with respect to the two-to-infinity norm loss is tighter than
that with respect to the routinely used projection operator norm loss under
certain low-rank and bounded coherence conditions. % on the corresponding
eigenvector matrix. In addition, a point estimator for the principal subspace
is proposed with the rate-optimal risk bound with respect to the projection
operator norm loss. These results are based on a collection of concentration
and large deviation inequalities for the matrix spike-and-slab LASSO prior. The
numerical performance of the proposed methodology is assessed through synthetic
examples and the analysis of a real-world face data example.
</summary>
    <author>
      <name>Fangzheng Xie</name>
    </author>
    <author>
      <name>Yanxun Xu</name>
    </author>
    <author>
      <name>Carey E. Priebe</name>
    </author>
    <author>
      <name>Joshua Cape</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07387v1</id>
    <updated>2018-08-22T14:43:38Z</updated>
    <published>2018-08-22T14:43:38Z</published>
    <title>Sensitivity Analysis using Approximate Moment Condition Models</title>
    <summary>  We consider inference in models defined by approximate moment conditions. We
show that near-optimal confidence intervals (CIs) can be formed by taking a
generalized method of moments (GMM) estimator, and adding and subtracting the
standard error times a critical value that takes into account the potential
bias from misspecification of the moment conditions. In order to optimize
performance under potential misspecification, the weighting matrix for this GMM
estimator takes into account this potential bias, and therefore differs from
the one that is optimal under correct specification. To formally show the
near-optimality of these CIs, we develop asymptotic efficiency bounds for
inference in the locally misspecified GMM setting. These bounds may be of
independent interest, due to their implications for the possibility of using
moment selection procedures when conducting inference in moment condition
models. We apply our methods in an empirical application to automobile demand,
and show that adjusting the weighting matrix can shrink the CIs by a factor of
up to 5 or more.
</summary>
    <author>
      <name>Timothy B. Armstrong</name>
    </author>
    <author>
      <name>Michal Kolesár</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">72 pages, including all tables, figures, and appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09339v1</id>
    <updated>2018-08-22T13:46:40Z</updated>
    <published>2018-08-22T13:46:40Z</published>
    <title>Scheduling a Rescue</title>
    <summary>  Scheduling service order, in a very specific queueing/inventory model with
perishable inventory, is considered. Different strategies are discusses and
results are applied to the tragic cave situation in Thailand in June and July
of 2018.
</summary>
    <author>
      <name>Nian Liu</name>
    </author>
    <author>
      <name>Myron Hlynka</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20, 90B36" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07309v1</id>
    <updated>2018-08-22T10:50:25Z</updated>
    <published>2018-08-22T10:50:25Z</published>
    <title>Doubly Robust Regression Analysis for Data Fusion</title>
    <summary>  This paper investigates the problem of making inference about a parametric
model for the regression of an outcome variable $Y$ on covariates $(V,L)$ when
data are fused from two separate sources, one which contains information only
on $(V, Y)$ while the other contains information only on covariates. This data
fusion setting may be viewed as an extreme form of missing data in which the
probability of observing complete data $(V,L,Y)$ on any given subject is zero.
We have developed a large class of semiparametric estimators, which includes
doubly robust estimators, of the regression coefficients in fused data. The
proposed method is DR in that it is consistent and asymptotically normal if, in
addition to the model of interest, we correctly specify a model for either the
data source process under an ignorability assumption, or the distribution of
unobserved covariates. We evaluate the performance of our various estimators
via an extensive simulation study, and apply the proposed methods to
investigate the relationship between net asset value and total expenditure
among U.S. households in 1998, while controlling for potential confounders
including income and other demographic variables.
</summary>
    <author>
      <name>Katherine Evans</name>
    </author>
    <author>
      <name>BaoLuo Sun</name>
    </author>
    <author>
      <name>James Robins</name>
    </author>
    <author>
      <name>Eric J. Tchetgen Tchetgen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07287v1</id>
    <updated>2018-08-22T09:11:22Z</updated>
    <published>2018-08-22T09:11:22Z</published>
    <title>Comparison of Dynamic Treatment Regimes with An Ordinal Outcome</title>
    <summary>  Sequential multiple assignment randomized trials (SMART) are used to develop
optimal treatment strategies for patients based on their medical histories in
different branches of medical and behavioral sciences where a sequence of
treatments are given to the patients; such sequential treatment strategies are
often called dynamic treatment regimes. In the existing literature, the
majority of the analysis methodologies for SMART studies assume a continuous
primary outcome. However, ordinal outcomes are also quite common in medical
practice; for example, the quality of life (poor, moderate, good) is an ordinal
variable. In this work, first, we develop the notion of dynamic generalized
odds-ratio ($dGOR$) to compare two dynamic treatment regimes embedded in a
2-stage SMART with an ordinal outcome. We propose a likelihood-based approach
to estimate $dGOR$ from SMART data. Next, we discuss some results related to
$dGOR$ and derive the asymptotic properties of it's estimate. We derive the
required sample size formula. Then, we extend the proposed methodology to a
$K$-stage SMART. Finally, we discuss some alternative ways to estimate $dGOR$
using concordant-discordant pairs and multi-sample $U$-statistic. A simulation
study shows the performance of the estimated $dGOR$ in terms of the estimated
power corresponding to the derived sample size. We analyze data from STAR*D, a
multistage randomized clinical trial for treating major depression, to
illustrate the proposed methodology. A freely available online tool using R
statistical software is provided to make the proposed method accessible to
other researchers and practitioners.
</summary>
    <author>
      <name>Palash Ghosh</name>
    </author>
    <author>
      <name>Bibhas Chakraborty</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00918v3</id>
    <updated>2018-08-22T09:03:36Z</updated>
    <published>2017-09-04T12:28:39Z</published>
    <title>Cancer phase I trial design using drug combinations when a fraction of
  dose limiting toxicities is attributable to one or more agents</title>
    <summary>  Drug combination trials are increasingly common nowadays in clinical
research. However, very few methods have been developed to consider toxicity
attributions in the dose escalation process. We are motivated by a trial in
which the clinician is able to identify certain toxicities that can be
attributed to one of the agents. We present a Bayesian adaptive design in which
toxicity attributions are modeled via Copula regression and the maximum
tolerated dose (MTD) curve is estimated as a function of model parameters. The
dose escalation algorithm uses cohorts of two patients, following the continual
reassessment method (CRM) scheme, where at each stage of the trial, we search
for the dose of one agent given the current dose of the other agent. The
performance of the design is studied by evaluating its operating
characteristics when the underlying model is either correctly specified or
misspecified. We show that this method can be extended to accommodate discrete
dose combinations.
</summary>
    <author>
      <name>Jose L. Jimenez</name>
    </author>
    <author>
      <name>Mourad Tighiouart</name>
    </author>
    <author>
      <name>Mauro Gasparini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/bimj.201700166</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/bimj.201700166" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Biometrical Journal (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.00918v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00918v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07194v3</id>
    <updated>2018-08-22T07:53:19Z</updated>
    <published>2017-06-22T07:44:23Z</published>
    <title>From here to infinity - sparse finite versus Dirichlet process mixtures
  in model-based clustering</title>
    <summary>  In model-based-clustering mixture models are used to group data points into
clusters. A useful concept introduced for Gaussian mixtures by Malsiner Walli
et al (2016) are sparse finite mixtures, where the prior distribution on the
weight distribution of a mixture with $K$ components is chosen in such a way
that a priori the number of clusters in the data is random and is allowed to be
smaller than $K$ with high probability. The number of cluster is then inferred
a posteriori from the data.
  The present paper makes the following contributions in the context of sparse
finite mixture modelling. First, it is illustrated that the concept of sparse
finite mixture is very generic and easily extended to cluster various types of
non-Gaussian data, in particular discrete data and continuous multivariate data
arising from non-Gaussian clusters. Second, sparse finite mixtures are compared
to Dirichlet process mixtures with respect to their ability to identify the
number of clusters. For both model classes, a random hyper prior is considered
for the parameters determining the weight distribution. By suitable matching of
these priors, it is shown that the choice of this hyper prior is far more
influential on the cluster solution than whether a sparse finite mixture or a
Dirichlet process mixture is taken into consideration.
</summary>
    <author>
      <name>Sylvia Frühwirth-Schnatter</name>
    </author>
    <author>
      <name>Gertraud Malsiner-Walli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted version</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07194v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07194v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02220v2</id>
    <updated>2018-08-22T07:29:47Z</updated>
    <published>2017-04-07T13:22:38Z</published>
    <title>Semi-Parametric Empirical Best Prediction for small area estimation of
  unemployment indicators</title>
    <summary>  The Italian National Institute for Statistics regularly provides estimates of
unemployment indicators using data from the Labor Force Survey. However, direct
estimates of unemployment incidence cannot be released for Local Labor Market
Areas. These are unplanned domains defined as clusters of municipalities; many
are out-of-sample areas and the majority is characterized by a small sample
size, which render direct estimates inadequate. The Empirical Best Predictor
represents an appropriate, model-based, alternative. However, for non-Gaussian
responses, its computation and the computation of the analytic approximation to
its Mean Squared Error require the solution of (possibly) multiple integrals
that, generally, have not a closed form. To solve the issue, Monte Carlo
methods and parametric bootstrap are common choices, even though the
computational burden is a non trivial task. In this paper, we propose a
Semi-Parametric Empirical Best Predictor for a (possibly) non-linear mixed
effect model by leaving the distribution of the area-specific random effects
unspecified and estimating it from the observed data. This approach is known to
lead to a discrete mixing distribution which helps avoid unverifiable
parametric assumptions and heavy integral approximations. We also derive a
second-order, bias-corrected, analytic approximation to the corresponding Mean
Squared Error. Finite sample properties of the proposed approach are tested via
a large scale simulation study. Furthermore, the proposal is applied to
unit-level data from the 2012 Italian Labor Force Survey to estimate
unemployment incidence for 611 Local Labor Market Areas using auxiliary
information from administrative registers and the 2011 Census.
</summary>
    <author>
      <name>Maria Francesca Marino</name>
    </author>
    <author>
      <name>Maria Giovanna Ranalli</name>
    </author>
    <author>
      <name>Nicola Salvati</name>
    </author>
    <author>
      <name>Marco Alfo'</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02419v4</id>
    <updated>2018-08-22T05:33:38Z</updated>
    <published>2017-06-08T00:47:46Z</published>
    <title>Estimating Mixture Entropy with Pairwise Distances</title>
    <summary>  Mixture distributions arise in many parametric and non-parametric settings --
for example, in Gaussian mixture models and in non-parametric estimation. It is
often necessary to compute the entropy of a mixture, but, in most cases, this
quantity has no closed-form expression, making some form of approximation
necessary. We propose a family of estimators based on a pairwise distance
function between mixture components, and show that this estimator class has
many attractive properties. For many distributions of interest, the proposed
estimators are efficient to compute, differentiable in the mixture parameters,
and become exact when the mixture components are clustered. We prove this
family includes lower and upper bounds on the mixture entropy. The Chernoff
$\alpha$-divergence gives a lower bound when chosen as the distance function,
with the Bhattacharyya distance providing the tightest lower bound for
components that are symmetric and members of a location family. The
Kullback-Leibler divergence gives an upper bound when used as the distance
function. We provide closed-form expressions of these bounds for mixtures of
Gaussians, and discuss their applications to the estimation of mutual
information. We then demonstrate that our bounds are significantly tighter than
well-known existing bounds using numeric simulations. This estimator class is
very useful in optimization problems involving maximization/minimization of
entropy and mutual information, such as MaxEnt and rate distortion problems.
</summary>
    <author>
      <name>Artemy Kolchinsky</name>
    </author>
    <author>
      <name>Brendan D. Tracey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e19070361</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e19070361" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrects several errata in published version, in particular in
  Section V (bounds on mutual information)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Entropy 2017, 19(7), 361</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.02419v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02419v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07140v1</id>
    <updated>2018-08-21T21:35:36Z</updated>
    <published>2018-08-21T21:35:36Z</published>
    <title>Multinomial Models with Linear Inequality Constraints: Overview and
  Improvements of Computational Methods for Bayesian Inference</title>
    <summary>  Many psychological theories can be operationalized as linear inequality
constraints on the parameters of multinomial distributions (e.g., discrete
choice analysis). These constraints can be described in two equivalent ways: 1)
as the solution set to a system of linear inequalities and 2) as the convex
hull of a set of extremal points (vertices). For both representations, we
describe a general Gibbs sampler for drawing posterior samples in order to
carry out Bayesian analyses. We also summarize alternative sampling methods for
estimating Bayes factors for these model representations using the encompassing
Bayes factor method. We introduce the R package multinomineq, which provides an
easily-accessible interface to a computationally efficient C++ implementation
of these techniques.
</summary>
    <author>
      <name>Daniel W. Heck</name>
    </author>
    <author>
      <name>Clintin P. Davis-Stober</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03273v2</id>
    <updated>2018-08-21T20:22:55Z</updated>
    <published>2018-05-08T20:40:51Z</published>
    <title>Seeking evidence of absence: Reconsidering tests of model assumptions</title>
    <summary>  Statistical tests can only reject the null hypothesis, never prove it.
However, when researchers test modeling assumptions, they often interpret the
failure to reject a null of "no violation" as evidence that the assumption
holds. We discuss the statistical and conceptual problems with this approach.
We show that equivalence/non-inferiority tests, while giving correct Type I
error, have low power to rule out many violations that are practically
significant. We suggest sensitivity analyses that may be more appropriate than
hypothesis testing.
</summary>
    <author>
      <name>Alyssa Bilinski</name>
    </author>
    <author>
      <name>Laura A. Hatfield</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03273v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03273v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07086v1</id>
    <updated>2018-08-21T19:03:11Z</updated>
    <published>2018-08-21T19:03:11Z</published>
    <title>An Approximation Scheme for Quasistationary Distributions of Killed
  Diffusions</title>
    <summary>  In this paper we study the asymptotic behavior of the normalized weighted
empirical occupation measures of a diffusion process on a compact manifold
which is also killed at a given rate and regenerated at a random location,
distributed according to the weighted empirical occupation measure. We show
that the weighted occupation measures almost surely comprise an asymptotic
pseudo-trajectory for a certain deterministic measure-valued semiflow, after
suitably rescaling the time, and that with probability one they converge to the
quasistationary distribution of the killed diffusion. These results provide
theoretical justification for a scalable quasistationary Monte Carlo method for
sampling from Bayesian posterior distributions in large data settings.
</summary>
    <author>
      <name>Andi Q. Wang</name>
    </author>
    <author>
      <name>Gareth O. Roberts</name>
    </author>
    <author>
      <name>David Steinsaltz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60B12, 60J60, 37C50 (Primary), 65C05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02424v3</id>
    <updated>2018-08-21T16:20:28Z</updated>
    <published>2017-01-10T03:06:10Z</published>
    <title>Asymptotic convergence in distribution of the area bounded by
  prevalence-weighted Kaplan-Meier curves using empirical process modeling</title>
    <summary>  The Kaplan-Meier product-limit estimator is a simple and powerful tool in
time to event analysis. An extension exists for populations stratified into
cohorts where a population survival curve is generated by weighted averaging of
cohort-level survival curves. For making population-level comparisons using
this statistic, we analyze the statistics of the area between two such weighted
survival curves. We derive the large sample behavior of this statistic based on
an empirical process of product-limit estimators. This estimator was used by an
interdisciplinary NIH-SSA team in the identification of medical conditions to
prioritize for adjudication in disability benefits processing.
</summary>
    <author>
      <name>Aaron Heuser</name>
    </author>
    <author>
      <name>Minh Huynh</name>
    </author>
    <author>
      <name>Joshua C. Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In review</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02424v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02424v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06952v1</id>
    <updated>2018-08-21T14:58:45Z</updated>
    <published>2018-08-21T14:58:45Z</published>
    <title>An ensemble learning method for variable selection: application to high
  dimensional data and missing values</title>
    <summary>  Standard approaches for variable selection in linear models are not tailored
to deal properly with high dimensional and incomplete data. Currently, methods
dedicated to high dimensional data handle missing values by ad-hoc strategies,
like complete case analysis or single imputation, while methods dedicated to
missing values, mainly based on multiple imputation, do not discuss the
imputation method to use with high dimensional data. Consequently, both
approaches appear to be limited for many modern applications.
  With inspiration from ensemble methods, a new variable selection method is
proposed. It extends classical variable selection methods such as stepwise,
lasso or knockoff in the case of high dimensional data with or without missing
data. Theoretical properties are studied and the practical interest is
demonstrated through a simulation study.
  In the low dimensional case without missing values, the performances of the
method can be better than those obtained by standard techniques. Moreover, the
procedure improves the control of the error risks. With missing values, the
method performs better than reference selection methods based on multiple
imputation. Similar performances are obtained in the high-dimensional case with
or without missing values.
</summary>
    <author>
      <name>Avner Bar-Hen</name>
    </author>
    <author>
      <name>Vincent Audigier</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06924v1</id>
    <updated>2018-08-21T14:31:22Z</updated>
    <published>2018-08-21T14:31:22Z</published>
    <title>General hypergeometric distribution: A basic statistical distribution
  for the number of overlapped elements in multiple subsets drawn from a finite
  population</title>
    <summary>  Suppose drawing K balls at random from a finite population of N balls
containing M red and N-M green balls, the classical hypergeometric distribution
(HGD) described the probability that exactly x of the selected balls are red
p(X=x |N,M,K ), here x is the number of overlapped elements (NOE) in two
subsets (containing M and K elements, respectively). However, if the selected
subsets are more than 2, there is no statistical distribution can describe the
NOE, and we define these kinds of distribution as General hypergeometric
distribution (GHGD). There is a huge potential demand of such a theory along
with the accelerated accumulation of big data, especially in biological
research, where the elements (such as genes) with a certain degree of overlap
from several datasets (which are subsets of the genome) were often considered
to be worth for further study. The overlapped elements are often visualized by
Venn diagram method, but the statistical distribution has not been established
yet, mainly because of the difficulty of the problem. Suppose there are totally
T subsets, the elements in these subsets can be overlapped for 2 to T times.
Therefore, besides the population size N and the numbers of elements in each
subset [M[i]](i=0~T-1), the GHGD has an additional parameter: level of overlap
(LO). GHGD described not only the distribution of NOE that are overlapped in
all of the subsets (LO=T), but also the NOE that are overlapped in a portion of
the subsets (LO=t or LO&gt;=t, (0&lt;=t&lt;=T). Here, we developed algorithms to
calculate the GHGD and discovered graceful formulas of the essential statistics
for the GHGD, including mathematical expectation, variance, and high order
moments. In addition, statistical theory to estimate the significance of the
NOE based on these formulas was established by applying Chebyshev's
inequalities.
</summary>
    <author>
      <name>Xing-gang Mao</name>
    </author>
    <author>
      <name>Xiao-yan Xue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 2 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06689v1</id>
    <updated>2018-08-20T20:56:10Z</updated>
    <published>2018-08-20T20:56:10Z</published>
    <title>Bayesian Function-on-Scalars Regression for High Dimensional Data</title>
    <summary>  We develop a fully Bayesian framework for function-on-scalars regression with
many predictors. The functional data response is modeled nonparametrically
using unknown basis functions, which produces a flexible and data-adaptive
functional basis. We incorporate shrinkage priors that effectively remove
unimportant scalar covariates from the model and reduce sensitivity to number
of (unknown) basis functions. For variable selection in functional regression,
we propose a decoupled shrinkage and selection technique, which identifies a
small subset of covariates that retains nearly the predictive accuracy of the
full model. Our novel approach is broadly applicable for Bayesian functional
regression models, and unlike existing methods provides joint rather than
marginal selection of important predictor variables. Computationally scalable
posterior inference is achieved using a Gibbs sampler with linear time
complexity in the number of predictors. The resulting algorithm is empirically
faster than existing frequentist and Bayesian techniques, and provides joint
estimation of model parameters, prediction and imputation of functional
trajectories, and uncertainty quantification via the posterior distribution. A
simulation study demonstrates substantial improvements in estimation accuracy,
uncertainty quantification, and variable selection relative to existing
alternatives. The methodology is applied to actigraphy data to investigate the
association between intraday physical activity and responses to a sleep
questionnaire.
</summary>
    <author>
      <name>Daniel R. Kowal</name>
    </author>
    <author>
      <name>Daniel C. Bourgeois</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04698v2</id>
    <updated>2018-08-20T16:56:54Z</updated>
    <published>2018-08-14T14:03:03Z</published>
    <title>Probabilistic forecasting of heterogeneous consumer transaction-sales
  time series</title>
    <summary>  We present new Bayesian methodology for consumer sales forecasting. With a
focus on multi-step ahead forecasting of daily sales of many supermarket items,
we adapt dynamic count mixture models to forecast individual customer
transactions, and introduce novel dynamic binary cascade models for predicting
counts of items per transaction. These transactions-sales models can
incorporate time-varying trend, seasonal, price, promotion, random effects and
other outlet-specific predictors for individual items. Sequential Bayesian
analysis involves fast, parallel filtering on sets of decoupled items and is
adaptable across items that may exhibit widely varying characteristics. A
multi-scale approach enables information sharing across items with related
patterns over time to improve prediction while maintaining scalability to many
items. A motivating case study in many-item, multi-period, multi-step ahead
supermarket sales forecasting provides examples that demonstrate improved
forecast accuracy in multiple metrics, and illustrates the benefits of full
probabilistic models for forecast accuracy evaluation and comparison.
  Keywords: Bayesian forecasting; decouple/recouple; dynamic binary cascade;
forecast calibration; intermittent demand; multi-scale forecasting; predicting
rare events; sales per transaction; supermarket sales forecasting
</summary>
    <author>
      <name>Lindsay R. Berry</name>
    </author>
    <author>
      <name>Paul Helman</name>
    </author>
    <author>
      <name>Mike West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04698v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04698v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15 (primary), 62M10, 62M20, (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06518v1</id>
    <updated>2018-08-20T15:39:00Z</updated>
    <published>2018-08-20T15:39:00Z</published>
    <title>A Structural-Factor Approach to Modeling High-Dimensional Time Series</title>
    <summary>  This paper considers a structural-factor approach to modeling
high-dimensional time series where individual series are decomposed into trend,
seasonal, and irregular components. For ease in analyzing many time series, we
employ a time polynomial for the trend, a linear combination of trigonometric
series for the seasonal component, and a new factor model for the irregular
components. The new factor model can simplify the modeling process and achieve
parsimony in parameterization. We propose a Bayesian Information Criterion
(BIC) to consistently determine the order of the polynomial trend and the
number of trigonometric functions. A test statistic is used to determine the
number of common factors. The convergence rates for the estimators of the trend
and seasonal components and the limiting distribution of the test statistic are
established under the setting that the number of time series tends to infinity
with the sample size, but at a slower rate. We use simulation to study the
performance of the proposed analysis in finite samples and apply the proposed
approach to two real examples. The first example considers modeling weekly
PM$_{2.5}$ data of 15 monitoring stations in the southern region of Taiwan and
the second example consists of monthly value-weighted returns of 12 industrial
portfolios.
</summary>
    <author>
      <name>Zhaoxing Gao</name>
    </author>
    <author>
      <name>Ruey S Tsay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1310.1990 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06418v1</id>
    <updated>2018-08-20T12:29:43Z</updated>
    <published>2018-08-20T12:29:43Z</published>
    <title>Spillover Effects in Cluster Randomized Trials with Noncompliance</title>
    <summary>  Clustered randomized trials (CRTs) are popular in the social sciences to
evaluate the efficacy of a new policy or program by randomly assigning one set
of clusters to the new policy and the other set to the usual policy. Often,
many individuals within a cluster fail to take advantage of the new policy,
resulting in noncompliance behaviors. Also, individuals within a cluster may
influence each other through treatment spillovers where those who comply with
the new policy may affect the outcomes of those who do not. Here, we study the
identification of causal effects in CRTs when both noncompliance and treatment
spillovers are present. We first show that the standard analysis of CRT data
with noncompliance using instrumental variables does not identify the usual
complier average causal effect under treatment spillovers. We extend this
result and show that no analysis of CRT data can unbiasedly estimate local
network causal effects. Finally, we develop bounds for these network causal
effects that only require standard instrumental variables analysis. We
demonstrate these results with an empirical study of a new deworming
intervention in Kenya. We find that given high levels of compliance, we can
place informative bounds on the total effect among compliers and that the
deworming intervention reduced infections among those complied with their
treatment assignment.
</summary>
    <author>
      <name>Hyunseung Kang</name>
    </author>
    <author>
      <name>Luke Keele</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06408v1</id>
    <updated>2018-08-20T12:01:43Z</updated>
    <published>2018-08-20T12:01:43Z</published>
    <title>Semiparametric estimation of structural failure time model in
  continuous-time processes</title>
    <summary>  Structural failure time models are causal models for estimating the effect of
time-varying treatments on a survival outcome. G-estimation and artificial
censoring have been proposed to estimate the model parameters in the presence
of time-dependent confounding and administrative censoring. However, most of
existing methods require preprocessing data into regularly spaced data such as
monthly data, and the computation and inference are challenging due to the
non-smoothness of artificial censoring. We propose a class of continuous-time
structural failure time models and semiparametric estimators, which do not
restrict to regularly spaced data. We show that our estimators are doubly
robust, in the sense that the estimators are consistent if either the model for
the treatment process is correctly specified or the failure time model is
correctly specified, but not necessarily both. Moreover, we propose using
inverse probability of censoring weighting to deal with dependent censoring. In
contrast to artificial censoring, our weighting strategy does not introduce
non-smoothness in estimation and ensures that the resampling methods can be
used to make inference.
</summary>
    <author>
      <name>Shu Yang</name>
    </author>
    <author>
      <name>Karen Pieper</name>
    </author>
    <author>
      <name>Frank Cools</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06399v1</id>
    <updated>2018-08-20T11:45:58Z</updated>
    <published>2018-08-20T11:45:58Z</published>
    <title>Bayesian Regression for a Dirichlet Distributed Response using Stan</title>
    <summary>  For an observed response that is composed by a set - or vector - of positive
values that sum up to 1, the Dirichlet distribution (Bol'shev, 2018) is a
helpful mathematical construction for the quantification of the data-generating
mechanics underlying this process. In applications, these response-sets are
usually denoted as proportions, or compositions of proportions, and by means of
covariates, one wishes to manifest the underlying signal - by changes in the
value of these covariates - leading to differently distributed response
compositions. This article gives a brief introduction into this class of
regression models, and based on a recently developed formulation (Maier, 2014),
illustrates the implementation in the Bayesian inference framework Stan.
</summary>
    <author>
      <name>Holger Sennhenn-Reulen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02199v3</id>
    <updated>2018-08-20T10:25:21Z</updated>
    <published>2016-11-07T18:14:52Z</published>
    <title>Inference for Additive Models in the Presence of Possibly Infinite
  Dimensional Nuisance Parameters</title>
    <summary>  A framework for estimation and hypothesis testing of functional restrictions
against general alternatives is proposed. The parameter space is a reproducing
kernel Hilbert space (RKHS). The null hypothesis does not necessarily define a
parametric model. The test allows us to deal with infinite dimensional nuisance
parameters. The methodology is based on a moment equation similar in spirit to
the construction of the efficient score in semiparametric statistics. The
feasible version of such moment equation requires to consistently estimate
projections in the space of RKHS and it is shown that this is possible using
the proposed approach. This allows us to derive some tractable asymptotic
theory and critical values by fast simulation. Simulation results show that the
finite sample performance of the test is consistent with the asymptotics and
that ignoring the effect of nuisance parameters highly distorts the size of the
tests.
</summary>
    <author>
      <name>Alessio Sancetta</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02199v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02199v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G5, 62G10, 62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02560v2</id>
    <updated>2018-08-20T10:12:21Z</updated>
    <published>2018-08-07T21:43:32Z</published>
    <title>Belief likelihood function for generalised logistic regression</title>
    <summary>  The notion of belief likelihood function of repeated trials is introduced,
whenever the uncertainty for individual trials is encoded by a belief measure
(a finite random set). This generalises the traditional likelihood function,
and provides a natural setting for belief inference from statistical data.
Factorisation results are proven for the case in which conjunctive or
disjunctive combination are employed, leading to analytical expressions for the
lower and upper likelihoods of `sharp' samples in the case of Bernoulli trials,
and to the formulation of a generalised logistic regression framework.
</summary>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures; submitted to UAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02560v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02560v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06310v1</id>
    <updated>2018-08-20T05:29:53Z</updated>
    <published>2018-08-20T05:29:53Z</published>
    <title>Analysis of "Learn-As-You-Go" (LAGO) Studies</title>
    <summary>  In learn-as-you-go (LAGO) adaptive designs, the intervention is a package
consisting of multiple components, and is adapted in stages during the study
based on past outcomes. This formalizes standard practice, and desires for
practice, in public health intervention studies. Typically, an effective
intervention package is sought, while minimizing cost. The main complication
when analyzing data from a learn-as-you-go design is that interventions in
later stages depend upon the outcomes in the previous stages. Therefore,
conditioning on the interventions would lead to effectively conditioning on the
earlier stages' outcomes, which violates common statistical principles. We
develop a method to estimate intervention effects from a learn-as-you-go study.
We prove consistency and asymptotic normality using a novel coupling argument,
and ensure the validity of the test for the hypothesis of no overall
intervention effect. We further develop a confidence set for the optimal
intervention package and confidence bands for the success probabilities under
different package compositions. We illustrate our methods by applying them to
the BetterBirth Study, which aimed to improve maternal and neonatal outcomes in
India.
</summary>
    <author>
      <name>Daniel Nevo</name>
    </author>
    <author>
      <name>Judith J. Lok</name>
    </author>
    <author>
      <name>Donna Spiegelman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09057v2</id>
    <updated>2018-08-19T22:02:50Z</updated>
    <published>2018-02-25T18:37:57Z</published>
    <title>First derivatives at the optimum analysis (\textit{fdao}): An approach
  to estimate the uncertainty in nonlinear regression involving stochastically
  independent variables</title>
    <summary>  An important problem of optimization analysis surges when parameters such as
$ \{\theta_j\}_{j=1,\, \dots \,,k }$, determining a function $
y=f(x\given\{\theta_j\}) $, must be estimated from a set of observables $ \{
x_i,y_i\}_{i=1,\, \dots \,,m} $. Where $ \{x_i\} $ are independent variables
assumed to be uncertainty-free. It is known that analytical solutions are
possible if $ y=f(x\given\theta_j) $ is a linear combination of $
\{\theta_{j=1,\, \dots \,,k} \}.$ Here it is proposed that determining the
uncertainty of parameters that are not \textit{linearly independent} may be
achieved from derivatives $ \tfrac{\partial f(x \given \{\theta_j\})}{\partial
\theta_j} $ at an optimum, if the parameters are \textit{stochastically
independent}.
</summary>
    <author>
      <name>Carlos Sevcik</name>
    </author>
    <link href="http://arxiv.org/abs/1802.09057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02177v5</id>
    <updated>2018-08-19T21:50:15Z</updated>
    <published>2017-03-07T02:14:38Z</published>
    <title>Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t
  Distributions for Model-Based Clustering with Incomplete Data</title>
    <summary>  Robust clustering from incomplete data is an important topic because, in many
practical situations, real data sets are heavy-tailed, asymmetric, and/or have
arbitrary patterns of missing observations. Flexible methods and algorithms for
model-based clustering are presented via mixture of the generalized hyperbolic
distributions and its limiting case, the mixture of multivariate skew-t
distributions. An analytically feasible EM algorithm is formulated for
parameter estimation and imputation of missing values for mixture models
employing missing at random mechanisms. The proposed methodologies are
investigated through a simulation study with varying proportions of synthetic
missing values and illustrated using a real dataset. Comparisons are made with
those obtained from the traditional mixture of generalized hyperbolic
distribution counterparts by filling in the missing data using the mean
imputation method.
</summary>
    <author>
      <name>Yuhong Wei</name>
    </author>
    <author>
      <name>Yang Tang</name>
    </author>
    <author>
      <name>Paul D. McNicholas</name>
    </author>
    <link href="http://arxiv.org/abs/1703.02177v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02177v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05314v3</id>
    <updated>2018-08-19T20:26:12Z</updated>
    <published>2015-08-21T15:37:57Z</published>
    <title>Asymptotic Efficiency of Goodness-of-fit Tests Based on Too-Lin
  Characterization</title>
    <summary>  In this paper a new class of uniformity tests is proposed. It is shown that
those tests are applicable to the cases of any simple null hypothesis as well
as for the composite null hypothesis of rectangular distributions on arbitrary
support. The asymptotic properties of test statistics are examined. The tests
are compared with some standard and some recent uniformity tests. For each test
the Bahadur efficiencies against some common local alternatives are calculated.
A class of locally optimal alternatives is found for each proposed test. The
power study is also provided. Some applications in time series analysis are
presented.
</summary>
    <author>
      <name>Bojana Milošević</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05314v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05314v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60F10, 62G10, 62G20, 62G30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06222v1</id>
    <updated>2018-08-19T16:10:24Z</updated>
    <published>2018-08-19T16:10:24Z</published>
    <title>The empirical likelihood prior applied to bias reduction of general
  estimating equations</title>
    <summary>  The practice of employing empirical likelihood (EL) components in place of
parametric likelihood functions in the construction of Bayesian-type procedures
has been well-addressed in the modern statistical literature. We rigorously
derive the EL prior, a Jeffreys-type prior, which asymptotically maximizes the
Shannon mutual information between data and the parameters of interest. The
focus of our approach is on an integrated Kullback-Leibler distance between the
EL-based posterior and prior density functions. The EL prior density is the
density function for which the corresponding posterior form is asymptotically
negligibly different from the EL. We show that the proposed result can be used
to develop a methodology for reducing the asymptotic bias of solutions of
general estimating equations and M-estimation schemes by removing the
first-order term. This technique is developed in a similar manner to methods
employed to reduce the asymptotic bias of maximum likelihood estimates via
penalizing the underlying parametric likelihoods by their Jeffreys invariant
priors. A real data example related to a study of myocardial infarction
illustrates the attractiveness of the proposed technique in practical aspects.
  Keywords: Asymptotic bias, Biased estimating equations, Empirical likelihood,
Expected Kullback-Leibler distance, Penalized likelihood, Reference prior.
</summary>
    <author>
      <name>Albert Vexler</name>
    </author>
    <author>
      <name>Li Zou</name>
    </author>
    <author>
      <name>Alan D. Hutson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07949v2</id>
    <updated>2018-08-19T14:35:15Z</updated>
    <published>2016-10-25T16:18:46Z</published>
    <title>Statistical Inference Based on a New Weighted Likelihood Approach</title>
    <summary>  We discuss a new weighted likelihood method for parametric estimation. The
method is motivated by the need for generating a simple estimation strategy
which provides a robust solution that is simultaneously fully efficient when
the model is correctly specified. This is achieved by appropriately weighting
the score function at each observation in the maximum likelihood score
equation. The weight function determines the compatibility of each observation
with the model in relation to the remaining observations and applies a
downweighting only if it is necessary, rather than automatically downweighting
a proportion of the observations all the time. This allows the estimators to
retain full asymptotic efficiency at the model. We establish all the
theoretical properties of the proposed estimators and substantiate the theory
developed through simulation and real data examples. Our approach provides an
alternative to the weighted likelihood method of Markatou et al (1997,1998).
</summary>
    <author>
      <name>Suman Majumder</name>
    </author>
    <author>
      <name>Adhidev Biswas</name>
    </author>
    <author>
      <name>Tania Roy</name>
    </author>
    <author>
      <name>Subir Bhandari</name>
    </author>
    <author>
      <name>Ayanendranath Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.07949v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07949v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06152v1</id>
    <updated>2018-08-19T02:16:40Z</updated>
    <published>2018-08-19T02:16:40Z</published>
    <title>On Design of Problem Token Questions in Quality of Experience Surveys</title>
    <summary>  User surveys for Quality of Experience (QoE) are a critical source of
information. In addition to the common "star rating" used to estimate Mean
Opinion Score (MOS), more detailed survey questions (problem tokens) about
specific areas provide valuable insight into the factors impacting QoE. This
paper explores two aspects of the problem token questionnaire design. First, we
study the bias introduced by fixed question order, and second, we study the
challenge of selecting a subset of questions to keep the token set small. Based
on 900,000 calls gathered using a randomized controlled experiment from a live
system, we find that the order bias can be significantly reduced by randomizing
the display order of tokens. The difference in response rate varies based on
token position and display design. It is worth noting that the users respond to
the randomized-order variant at levels that are comparable to the fixed-order
variant. The effective selection of a subset of token questions is achieved by
extracting tokens that provide the highest information gain over user ratings.
This selection is known to be in the class of NP-hard problems. We apply a
well-known greedy submodular maximization method on our dataset to capture 94%
of the information using just 30% of the questions.
</summary>
    <author>
      <name>Jayant Gupchup</name>
    </author>
    <author>
      <name>Ebrahim Beyrami</name>
    </author>
    <author>
      <name>Martin Ellis</name>
    </author>
    <author>
      <name>Yasaman Hosseinkashi</name>
    </author>
    <author>
      <name>Sam Johnson</name>
    </author>
    <author>
      <name>Ross Cutler</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06040v1</id>
    <updated>2018-08-18T04:52:37Z</updated>
    <published>2018-08-18T04:52:37Z</published>
    <title>Optimal proposals for Approximate Bayesian Computation</title>
    <summary>  We derive the optimal proposal density for Approximate Bayesian Computation
(ABC) using Sequential Monte Carlo (SMC) (or Population Monte Carlo, PMC). The
criterion for optimality is that the SMC/PMC-ABC sampler maximise the effective
number of samples per parameter proposal. The optimal proposal density
represents the optimal trade-off between favoring high acceptance rate and
reducing the variance of the importance weights of accepted samples. We discuss
two convenient approximations of this proposal and show that the optimal
proposal density gives a significant boost in the expected sampling efficiency
compared to standard kernels that are in common use in the ABC literature,
especially as the number of parameters increases.
</summary>
    <author>
      <name>Justin Alsing</name>
    </author>
    <author>
      <name>Benjamin D. Wandelt</name>
    </author>
    <author>
      <name>Stephen M. Feeney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06038v1</id>
    <updated>2018-08-18T03:10:21Z</updated>
    <published>2018-08-18T03:10:21Z</published>
    <title>A general approach to detect gene (G)-environment (E) additive
  interaction leveraging G-E independence in case-control studies</title>
    <summary>  It is increasingly of interest in statistical genetics to test for the
presence of a mechanistic interaction between genetic (G) and environmental (E)
risk factors by testing for the presence of an additive GxE interaction. In
case-control studies involving a rare disease, a statistical test of no
additive interaction typically entails a test of no relative excess risk due to
interaction (RERI). It is also well known that a test of multiplicative
interaction exploiting G-E independence can be dramatically more powerful than
standard logistic regression for case-control data. Likewise, it has recently
been shown that a likelihood ratio test of a null RERI incorporating the G-E
independence assumption (RERI-LRT) outperforms the standard RERI approach. In
this paper, the authors describe a general, yet relatively straightforward
approach to test for GxE additive interaction exploiting G-E independence. The
approach which relies on regression models for G and E is particularly
attractive because, unlike the RERI-LRT, it allows the regression model for the
binary outcome to remain unrestricted. Therefore, the new methodology is
completely robust to possible mis-specification in the outcome regression. This
is particularly important for settings not easily handled by RERI-LRT, such as
when E is a count or a continuous exposure with multiple components, or when
there are several auxiliary covariates in the regression model. While the
proposed approach avoids fitting an outcome regression, it nonetheless still
allows for straightforward covariate adjustment. The methods are illustrated
through an extensive simulation study and an ovarian cancer empirical
application.
</summary>
    <author>
      <name>Eric J. Tchetgen Tchetgen</name>
    </author>
    <author>
      <name>Xu Shi</name>
    </author>
    <author>
      <name>Tamar Sofer</name>
    </author>
    <author>
      <name>Benedict H. W. Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06016v1</id>
    <updated>2018-08-17T22:36:10Z</updated>
    <published>2018-08-17T22:36:10Z</published>
    <title>A Stepwise Approach for High-Dimensional Gaussian Graphical Models</title>
    <summary>  We present a stepwise approach to estimate high dimensional Gaussian
graphical models. We exploit the relation between the partial correlation
coefficients and the distribution of the prediction errors, and parametrize the
model in terms of the Pearson correlation coefficients between the prediction
errors of the nodes' best linear predictors. We propose a novel stepwise
algorithm for detecting pairs of conditionally dependent variables. We show
that the proposed algorithm outperforms existing methods such as the graphical
lasso and CLIME in simulation studies and real life applications. In our
comparison we report different performance measures that look at different
desirable features of the recovered graph and consider several model settings.
</summary>
    <author>
      <name>Ginette Lafit</name>
    </author>
    <author>
      <name>Francisco J. Nogales</name>
    </author>
    <author>
      <name>Marcelo Ruiz</name>
    </author>
    <author>
      <name>Ruben H. Zamar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 5 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03057v3</id>
    <updated>2018-08-17T18:46:14Z</updated>
    <published>2017-07-10T20:42:00Z</published>
    <title>Robust and Accurate Inference via a Mixture of Gaussian and Student's t
  Errors</title>
    <summary>  A Gaussian measurement error assumption, i.e., an assumption that the data
are observed up to Gaussian noise, can bias any parameter estimation in the
presence of outliers. A heavy tailed error assumption based on Student's t
distribution helps reduce the bias. However, it may be less efficient in
estimating parameters if the heavy tailed assumption is uniformly applied to
all of the data when most of them are normally observed. We propose a mixture
error assumption that selectively converts Gaussian errors into Student's t
errors according to latent outlier indicators, leveraging the best of the
Gaussian and Student's t errors; a parameter estimation can be not only robust
but also accurate. Using simulated hospital profiling data and astronomical
time series of brightness data, we demonstrate the potential for the proposed
mixture error assumption to estimate parameters accurately in the presence of
outliers.
</summary>
    <author>
      <name>Hyungsuk Tak</name>
    </author>
    <author>
      <name>Justin A. Ellis</name>
    </author>
    <author>
      <name>Sujit K. Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1707.03057v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03057v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05895v1</id>
    <updated>2018-08-17T15:12:39Z</updated>
    <published>2018-08-17T15:12:39Z</published>
    <title>Estimating and accounting for unobserved covariates in high dimensional
  correlated data</title>
    <summary>  Many high dimensional and high-throughput biological datasets have complex
sample correlation structures, which include longitudinal and multiple tissue
data, as well as data with multiple treatment conditions or related
individuals. These data, as well as nearly all high-throughput `omic' data, are
influenced by technical and biological factors unknown to the researcher,
which, if unaccounted for, can severely obfuscate estimation and inference on
effects due to the known covariate of interest. We therefore developed CBCV and
CorrConf: provably accurate and computationally efficient methods to choose the
number of and estimate latent confounding factors present in high dimensional
data with correlated or nonexchangeable residuals. We demonstrate each method's
superior performance compared to other state of the art methods by analyzing
simulated multi-tissue gene expression data and identifying sex-associated DNA
methylation sites in a real, longitudinal twin study. As far as we are aware,
these are the first methods to estimate the number of and correct for latent
confounding factors in data with correlated or nonexchangeable residuals. An
R-package is available for download at
https://github.com/chrismckennan/CorrConf.
</summary>
    <author>
      <name>Chris McKennan</name>
    </author>
    <author>
      <name>Dan Nicolae</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 slides (54 including the supplement), 3 tables, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05889v1</id>
    <updated>2018-08-17T14:51:09Z</updated>
    <published>2018-08-17T14:51:09Z</published>
    <title>Data Consistency Approach to Model Validation</title>
    <summary>  In scientific inference problems, the underlying statistical modeling
assumptions have a crucial impact on the end results. There exist, however,
only a few automatic means for validating these fundamental modelling
assumptions. The contribution in this paper is a general criterion to evaluate
the consistency of a set of statistical models with respect to observed data.
This is achieved by automatically gauging the models' ability to generate data
that is similar to the observed data. Importantly, the criterion follows from
the model class itself and is therefore directly applicable to a broad range of
inference problems with varying data types. The proposed data consistency
criterion is illustrated and evaluated using three synthetic and two real data
sets.
</summary>
    <author>
      <name>Andreas Svensson</name>
    </author>
    <author>
      <name>Dave Zachariah</name>
    </author>
    <author>
      <name>Petre Stoica</name>
    </author>
    <author>
      <name>Thomas B. Schön</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05878v1</id>
    <updated>2018-08-17T14:25:39Z</updated>
    <published>2018-08-17T14:25:39Z</published>
    <title>Statistical modeling for adaptive trait evolution in randomly evolving
  environment</title>
    <summary>  In past decades, Gaussian processes has been widely applied in studying trait
evolution using phylogenetic comparative analysis. In particular, two members
of Gaussian processes: Brownian motion and Ornstein-Uhlenbeck process, have
been frequently used to describe continuous trait evolution. Under the
assumption of adaptive evolution, several models have been created around
Ornstein-Uhlenbeck process where the optimum $\theta^y_t$ of a single trait
$y_t$ is influenced with predictor $x_t$. Since in general the dynamics of rate
of evolution $\tau^y_t$ of trait could adopt a pertinent process, in this work
we extend models of adaptive evolution by considering the rate of evolution
$\tau_t^y$ following the Cox-Ingersoll-Ross (CIR) process. We provide a
heuristic Monte Carlo simulation scheme to simulate trait along the phylogeny
as a structure of dependence among species. We add a framework to incorporate
multiple regression with interaction between optimum of the trait and its
potential predictors. Since the likelihood function for our models are
intractable, we propose the use of Approximate Bayesian Computation (ABC) for
parameter estimation and inference. Simulation as well as empirical study using
the proposed models are also performed and carried out to validate our models
and for practical applications.
</summary>
    <author>
      <name>Dwueng-Chwuan Jhwueng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 13 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P10, 62J05, 62C05 62P10, 62J05, 62C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07656v4</id>
    <updated>2018-08-17T03:23:22Z</updated>
    <published>2018-05-19T21:15:06Z</published>
    <title>Functional response regression with funBART: an analysis of
  patient-specific stillbirth risk</title>
    <summary>  This article introduces functional BART, a new approach for functional
response regression--that is, estimating a functional mean response f(t) that
depends upon a set of scalar covariates x. Functional BART, or funBART, is
based on the successful Bayesian Additive Regression Trees (BART) model. The
original BART model is an ensemble of regression trees; funBART extends this
model to an ensemble of functional regression trees, in which the terminal
nodes of each tree are parametrized by functions rather than scalar responses.
Just like the original BART model, funBART offers an appealing combination of
flexibility with user friendliness: it captures complex nonlinear relationships
and interactions among the predictors, while eliminating many of the onerous
"researcher degrees of freedom" involved in function-on-scalar regression using
standard tools. In particular, functional BART does not require the user to
specify a functional form or basis set for f(t), to manually choose
interactions, or to use a multi-step approach to select model terms or basis
coefficients. Our model replaces all of these choices by a single smoothing
parameter, which can either be chosen to reflect prior knowledge or tuned in a
data-dependent way.
  After demonstrating the performance of the method on a series of benchmarking
experiments, we then apply funBART to our motivating example:
pregnancy-outcomes data from the National Center for Health Statistics. Here,
the goal is to build a model for stillbirth risk as a function of gestational
age (t), based on maternal and fetal risk factors (x). We show how these
patient-specific estimates of stillbirth risk can be used to inform clinical
management of high-risk pregnancies, with the aim of reducing the risk of
perinatal mortality.
  The R package funbart implements all methods described in this article, and
supplementary materials are available online.
</summary>
    <author>
      <name>Jennifer E. Starling</name>
    </author>
    <author>
      <name>Jared S. Murray</name>
    </author>
    <author>
      <name>Carlos M. Carvalho</name>
    </author>
    <author>
      <name>Radek Bukowski</name>
    </author>
    <author>
      <name>James G. Scott</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07656v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07656v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02999v3</id>
    <updated>2018-08-17T02:44:33Z</updated>
    <published>2017-04-10T18:18:46Z</published>
    <title>Estimating Local Interactions Among Many Agents Who Observe Their
  Neighbors</title>
    <summary>  In various economic environments, people observe those with whom they
strategically interact. We can model such information-sharing relations as an
information network, and the strategic interactions as a game on the network.
When any two agents in the network are connected either directly or indirectly,
empirical modeling using an equilibrium approach is cumbersome, since the
testable implications from an equilibrium generally involve all the players of
the game, whereas a researcher's data set may contain only a fraction of these
players in practice. This paper develops a tractable empirical model of linear
interactions where each agent, after observing part of his neighbors' types,
not knowing the full information network, uses best responses that are linear
in his and other players' types that he observes, based on simple beliefs about
other players' strategies. We provide conditions on information networks and
beliefs such that best responses take an explicit form with multiple intuitive
features. Furthermore, the best responses reveal how local payoff
interdependence among agents is translated into local stochastic dependence of
their actions, allowing the econometrician to perform asymptotic inference
without having to observe all the players in the game or having to know
precisely the sampling process.
</summary>
    <author>
      <name>Nathan Canen</name>
    </author>
    <author>
      <name>Jacob Schwartz</name>
    </author>
    <author>
      <name>Kyungchul Song</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02999v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02999v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P20, 62P25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06539v2</id>
    <updated>2018-08-16T23:54:48Z</updated>
    <published>2018-07-17T16:35:12Z</published>
    <title>On the Beta Prime Prior for Scale Parameters in High-Dimensional
  Bayesian Regression Models</title>
    <summary>  We study high-dimensional Bayesian linear regression with a general beta
prime distribution for the scale parameter. To avoid misspecification of the
hyperparameters and to enable our prior to adapt to both sparse and dense
models, we propose a data-adaptive method for estimating the hyperparameters in
the beta prime density. Our estimation of hyperparameters is based on
maximization of marginal likelihood (MML), and we show how to incorporate our
estimation procedure easily into our approximation of the posterior. Under our
proposed empirical Bayes procedure, the MML estimates are never at risk of
collapsing to zero. We also investigate the theoretical properties of our
prior. We prove that careful selection of the hyperparameters leads to (near)
minimax posterior contraction when $p \gg n$. As an aside, we also derive
conditions for minimax posterior contraction under our prior for the sparse
normal means problem. Finally, we demonstrate our prior's self-adaptivity and
excellent finite sample performance through simulations and analysis of a gene
expression data set.
</summary>
    <author>
      <name>Ray Bai</name>
    </author>
    <author>
      <name>Malay Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">51 pages, 4 figures, 4 tables. We have added the algorithms to
  Appendix A and a new simulation study to Appendix C (Section C.1)</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06539v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06539v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05541v1</id>
    <updated>2018-08-16T15:35:04Z</updated>
    <published>2018-08-16T15:35:04Z</published>
    <title>Switching Regression Models and Causal Inference in the Presence of
  Latent Variables</title>
    <summary>  Given a response $Y$ and a vector $X = (X^1, \dots, X^d)$ of $d$ predictors,
we investigate the problem of inferring direct causes of $Y$ among the vector
$X$. Models for $Y$ that use its causal covariates as predictors enjoy the
property of being invariant across different environments or interventional
settings. Given data from such environments, this property has been exploited
for causal discovery: one collects the models that show predictive stability
across all environments and outputs the set of predictors that are necessary to
obtain stability. If some of the direct causes are latent, however, there may
not exist invariant models for $Y$ based on variables from $X$, and the above
reasoning breaks down. In this paper, we extend the principle of invariant
prediction by introducing a relaxed version of the invariance assumption. This
property can be used for causal discovery in the presence of latent variables
if the latter's influence on $Y$ can be restricted. More specifically, we allow
for latent variables with a low-range discrete influence on the target $Y$.
This assumption gives rise to switching regression models, where each value of
the (unknown) hidden variable corresponds to a different regression
coefficient. We provide sufficient conditions for the existence, consistency
and asymptotic normality of the maximum likelihood estimator in switching
regression models, and construct a test for the equality of such models. Our
results on switching regression models allow us to prove that asymptotic false
discovery control for the causal discovery method is obtained under mild
conditions. We provide an algorithm for the overall method, make available
code, and illustrate the performance of our method on simulated data.
</summary>
    <author>
      <name>Rune Christiansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Copenhagen</arxiv:affiliation>
    </author>
    <author>
      <name>Jonas Peters</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Copenhagen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05528v1</id>
    <updated>2018-08-16T15:03:50Z</updated>
    <published>2018-08-16T15:03:50Z</published>
    <title>Permutation-based simultaneous confidence bounds for the false discovery
  proportion</title>
    <summary>  When multiple hypotheses are tested, interest is often in ensuring that the
proportion of false discoveries (FDP) is small with high confidence. In this
paper, confidence upper bounds for the FDP are constructed, which are
simultaneous over all rejection cut-offs. In particular this allows the user to
select a set of hypotheses post hoc such that the FDP lies below some constant
with high confidence. Our method uses permutations to account for the
dependence structure in the data. So far only Meinshausen provided an exact,
permutation-based and computationally feasible method for simultaneous FDP
bounds. We provide an exact method, which uniformly improves this procedure.
Further, we provide a generalization of this method. It lets the user select
the shape of the simultaneous confidence bounds. This gives the user more
freedom in determining the power properties of the method. Interestingly,
several existing permutation methods, such as Significance Analysis of
Microarrays (SAM) and Westfall and Young's maxT method, are obtained as special
cases.
</summary>
    <author>
      <name>Jesse Hemerik</name>
    </author>
    <author>
      <name>Aldo Solari</name>
    </author>
    <author>
      <name>Jelle J. Goeman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G09, 62H15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05441v1</id>
    <updated>2018-08-16T12:20:49Z</updated>
    <published>2018-08-16T12:20:49Z</published>
    <title>A comparative study of structural similarity and regularization for
  joint inverse problems governed by PDEs</title>
    <summary>  Joint inversion refers to the simultaneous inference of multiple parameter
fields from observations of systems governed by single or multiple forward
models. In many cases these parameter fields reflect different attributes of a
single medium and are thus spatially correlated or structurally similar. By
imposing prior information on their spatial correlations via a joint
regularization term, we seek to improve the reconstruction of the parameter
fields relative to inversion for each field independently. One of the main
challenges is to devise a joint regularization functional that conveys the
spatial correlations or structural similarity between the fields while at the
same time permitting scalable and efficient solvers for the joint inverse
problem. We describe several joint regularizations that are motivated by these
goals: a cross-gradient and a normalized cross-gradient structural similarity
term, the vectorial total variation, and a joint regularization based on the
nuclear norm of the gradients. Based on numerical results from three classes of
inverse problems with piecewise-homogeneous parameter fields, we conclude that
the vectorial total variation functional is preferable to the other methods
considered. Besides resulting in good reconstructions in all experiments, it
allows for scalable, efficient solvers for joint inverse problems governed by
PDE forward models.
</summary>
    <author>
      <name>Benjamin Crestel</name>
    </author>
    <author>
      <name>Georg Stadler</name>
    </author>
    <author>
      <name>Omar Ghattas</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05414v1</id>
    <updated>2018-08-16T11:04:35Z</updated>
    <published>2018-08-16T11:04:35Z</published>
    <title>Functional Outlier Detection and Taxonomy by Sequential Transformations</title>
    <summary>  Functional data analysis can be seriously impaired by abnormal observations,
which can be classified as either magnitude or shape outliers based on their
way of deviating from the bulk of data. Identifying magnitude outliers is
relatively easy, while detecting shape outliers is much more challenging. We
propose turning the shape outliers into magnitude outliers through data
transformation and detecting them using the functional boxplot. Besides easing
the detection procedure, applying several transformations sequentially provides
a reasonable taxonomy for the flagged outliers. A joint functional ranking,
which consists of several transformations, is also defined here. Simulation
studies are carried out to evaluate the performance of the proposed method
using different functional depth notions. Interesting results are obtained in
several practical applications.
</summary>
    <author>
      <name>Wenlin Dai</name>
    </author>
    <author>
      <name>Tomas Mrkvicka</name>
    </author>
    <author>
      <name>Ying Sun</name>
    </author>
    <author>
      <name>Marc G. Genton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 9 figures, and 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06482v1</id>
    <updated>2018-08-16T08:00:34Z</updated>
    <published>2018-08-16T08:00:34Z</published>
    <title>Divergence functions in dually flat spaces and their properties</title>
    <summary>  Dually flat spaces play a key role in the differential geometrical approach
in statistics (information geometry) and many divergences have been studied as
an amount which measures the discrepancy between two probability distributions.
In a dually flat space, there exist dual affine coordinate systems and convex
functions called potential and a canonical divergence is naturally introduced
as the function of the affine coordinates and potentials.The canonical
divergence satisfies a relational expression called triangular relation. This
can be regarded as a generalization of the law of cosines in Euclidean space.
In this paper, we newly introduce two kinds of divergences. The first
divergence is a function of affine coordinates and it is consistent with the
Jeffreys divergence for exponential or mixture families. For this divergence,
we show that more relational equations and theorems similar to Euclidean space
hold in addition to the law of cosines. The second divergences are functions of
potentials and they are consistent with the Bhattacharyya distance for
exponential families and are consistent with the Jensen-Shannon divergence for
mixture families respectively. We derive an inequality between the the first
and the second divergences and show that the inequality is a generalization of
Lin's inequality.
</summary>
    <author>
      <name>Tomohiro Nishiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05362v1</id>
    <updated>2018-08-16T07:15:28Z</updated>
    <published>2018-08-16T07:15:28Z</published>
    <title>Generalized Four Moment Theorem and an Application to CLT for Spiked
  Eigenvalues of Large-dimensional Covariance Matrices</title>
    <summary>  We consider a more generalized spiked covariance matrix $\Sigma$, which is a
general non-definite matrix with the spiked eigenvalues scattered into a few
bulks and the largest ones allowed to tend to infinity. By relaxing the
matching of the 4th moment to a tail probability decay, a {\it Generalized Four
Moment Theorem} (G4MT) is proposed to show the universality of the asymptotic
law for the local spectral statistics of generalized spiked covariance
matrices, which implies the limiting distribution of the spiked eigenvalues of
the generalized spiked covariance matrix is independent of the actual
distributions of the samples satisfying our relaxed assumptions. Moreover, by
applying it to the Central Limit Theorem (CLT) for the spiked eigenvalues of
the generalized spiked covariance matrix, we also extend the result of Bai and
Yao (2012) to a general form of the population covariance matrix, where the 4th
moment is not necessarily required to exist and the spiked eigenvalues are
allowed to be dependent on the non-spiked ones, thus meeting the actual cases
better.
</summary>
    <author>
      <name>Dandan Jiang</name>
    </author>
    <author>
      <name>Zhidong Bai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60B20, 62H25, 60F05, 62H10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05339v1</id>
    <updated>2018-08-16T03:46:56Z</updated>
    <published>2018-08-16T03:46:56Z</published>
    <title>Propensity Score Weighting for Causal Inference with Multi-valued
  Treatments</title>
    <summary>  This article proposes a unified framework, the balancing weights, for
estimating causal effects with multi-valued treatments using propensity score
weighting. These weights incorporate the generalized propensity score to
balance the weighted covariate distribution of each treatment group, all
weighted toward a common pre-specified target population. The class of
balancing weights include several existing approaches such as inverse
probability weights and trimming weights as special cases. Within this
framework, we propose a class of target estimands based on linear contrasts and
their corresponding nonparametric weighting estimators. We further propose the
generalized overlap weights, constructed as the product of the inverse
probability weights and the harmonic mean of the generalized propensity scores,
to focus on the target population with the most overlap in covariates. These
weights are bounded and thus bypass the problem of extreme propensities. We
show that the generalized overlap weights minimize the total asymptotic
variance of the nonparametric estimators for the pairwise contrasts within the
class of balancing weights. We also develop two new balance check criteria and
a sandwich variance estimator for estimating the causal effects with
generalized overlap weights. We illustrate these methods by simulations and
apply them to study the racial disparities in medical expenditure.
</summary>
    <author>
      <name>Fan Li</name>
    </author>
    <author>
      <name>Fan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 4 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05291v1</id>
    <updated>2018-08-15T21:56:29Z</updated>
    <published>2018-08-15T21:56:29Z</published>
    <title>Tensor models for linguistics pitch curve data of native speakers of
  Afrikaans</title>
    <summary>  We use tensor analysis techniques for high-dimensional data to gain insight
into pitch curves, which play an important role in linguistics research. In
particular, we propose that demeaned phonetics pitch curve data can be modeled
as having a Kronecker product inverse covariance structure with sparse factors
corresponding to words and time. Using data from a study of native Afrikaans
speakers, we show that by targeting conditional independence through a
graphical model, we reveal relationships associated with natural properties of
words as studied by linguists. We find that words with long vowels cluster
based on whether the vowel is pronounced at the front or back of the mouth, and
words with short vowels have strong edges associated with the initial
consonant.
</summary>
    <author>
      <name>Michael Hornstein</name>
    </author>
    <author>
      <name>Shuheng Zhou</name>
    </author>
    <author>
      <name>Kerby Shedden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 7 figures, 1 table in main paper; 51 pages, 48 figures, 2
  tables in Appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05260v1</id>
    <updated>2018-08-15T19:14:30Z</updated>
    <published>2018-08-15T19:14:30Z</published>
    <title>Testing for Balance in Social Networks</title>
    <summary>  Friendship and antipathy exist in concert with one another in real social
networks. Despite the role they play in social interactions, antagonistic ties
are poorly understood and infrequently measured. One important theory of
negative ties that has received relatively little empirical evaluation is
balance theory, the codification of the adage `the enemy of my enemy is my
friend' and similar sayings. Unbalanced triangles are those with an odd number
of negative ties, and the theory posits that such triangles are rare. To test
for balance, previous works have utilized a permutation test on the edge signs.
The flaw in this method, however, is that it assumes that negative and positive
edges are interchangeable. In reality, they could not be more different. Here,
we propose a novel test of balance that accounts for this discrepancy and show
that our test is more accurate at detecting balance. Along the way, we prove
asymptotic normality of the test statistic under our null model, which is of
independent interest. Our case study is a novel dataset of signed networks we
collected from 32 isolated, rural villages in Honduras. Contrary to previous
results, we find that there is only marginal evidence for balance in social tie
formation in this setting.
</summary>
    <author>
      <name>Derek Feng</name>
    </author>
    <author>
      <name>Randolf Altmeyer</name>
    </author>
    <author>
      <name>Derek Stafford</name>
    </author>
    <author>
      <name>Nicholas A. Christakis</name>
    </author>
    <author>
      <name>Harrison H. Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05185v1</id>
    <updated>2018-08-15T17:04:36Z</updated>
    <published>2018-08-15T17:04:36Z</published>
    <title>Model-based clustering for random hypergraphs</title>
    <summary>  A probabilistic model for random hypergraphs is introduced to represent
unary, binary and higher order interactions among objects in real-world
problems. This model is an extension of the Latent Class Analysis model, which
captures clustering structures among objects. An EM (expectation maximization)
algorithm with MM (minorization maximization) steps is developed to perform
parameter estimation while a cross validated likelihood approach is employed to
perform model selection. The developed model is applied to three real-world
data sets where interesting results are obtained.
</summary>
    <author>
      <name>Tin Lok James Ng</name>
    </author>
    <author>
      <name>Thomas Brendan Murphy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05126v1</id>
    <updated>2018-08-15T15:20:25Z</updated>
    <published>2018-08-15T15:20:25Z</published>
    <title>Optimal allocation of subjects in a cluster randomized trial with fixed
  number of clusters when the ICCs or costs are heterogeneous over clusters</title>
    <summary>  The intra-cluster correlation coefficient (ICC) plays an important role while
designing the cluster randomized trials (CRTs). Often optimal CRTs are designed
assuming that the magnitude of the ICC is constant across the clusters.
However, this assumption is hardly satisfied. In some applications, the precise
information about the cluster specific correlation is known in advance. In this
article, we propose an optimal design with non-constant ICC across the
clusters. Also in many situations, the cost of sampling of an observation from
a particular cluster may differ from that of some other cluster. An optimal
design in those scenarios is also obtained assuming unequal costs of sampling
from different clusters. The theoretical findings are supplemented by thorough
numerical examples.
</summary>
    <author>
      <name>Satya Prakash Singh</name>
    </author>
    <author>
      <name>Pradeep Yadav</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08695v5</id>
    <updated>2018-08-15T15:19:54Z</updated>
    <published>2017-11-23T13:59:48Z</published>
    <title>Grabit: Gradient Tree Boosted Tobit Models for Default Prediction</title>
    <summary>  In this article, a novel binary classification approach for data with (i)
class imbalance between a minority and a majority class and (ii) a sample size
that is small in relation to the complexity of the decision boundary is
presented. We introduce a novel model - the Grabit model - which is obtained by
applying gradient tree boosting to the Tobit model and show how this model can
leverage auxiliary data to better learn the decision function for imbalanced
data and, thus, obtain increased predictive accuracy. We apply the Grabit model
for predicting defaults on loans made to Swiss small and medium-sized
enterprises (SME) and obtain a large and significant improvement in predictive
performance compared to other state-of-the-art approaches.
</summary>
    <author>
      <name>Fabio Sigrist</name>
    </author>
    <author>
      <name>Christoph Hirnschall</name>
    </author>
    <link href="http://arxiv.org/abs/1711.08695v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08695v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09762v2</id>
    <updated>2018-08-15T15:11:58Z</updated>
    <published>2018-06-26T02:22:13Z</published>
    <title>Boulevard: Regularized Stochastic Gradient Boosted Trees and Their
  Limiting Distribution</title>
    <summary>  This paper examines a novel gradient boosting framework for regression. We
regularize gradient boosted trees by introducing subsampling and employ a
modified shrinkage algorithm so that at every boosting stage the estimate is
given by an average of trees. The resulting algorithm, titled Boulevard, is
shown to converge as the number of trees grows. We also demonstrate a central
limit theorem for this limit, allowing a characterization of uncertainty for
predictions. A simulation study and real world examples provide support for
both the predictive accuracy of the model and its limiting behavior.
</summary>
    <author>
      <name>Yichen Zhou</name>
    </author>
    <author>
      <name>Giles Hooker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 5 figures. Fixed a typo in Theorem 4.2</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09762v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09762v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01955v2</id>
    <updated>2018-08-15T14:57:56Z</updated>
    <published>2016-01-08T17:44:26Z</published>
    <title>Bayesian Crossover Designs for Generalized Linear Models</title>
    <summary>  This article discusses D-optimal Bayesian crossover designs for generalized
linear models. Crossover trials with t treatments and p periods, for $t &lt;= p$,
are considered. The designs proposed in this paper minimize the log determinant
of the variance of the estimated treatment effects over all possible allocation
of the n subjects to the treatment sequences. It is assumed that the p
observations from each subject are mutually correlated while the observations
from different subjects are uncorrelated. Since main interest is in estimating
the treatment effects, the subject effect is assumed to be nuisance, and
generalized estimating equations are used to estimate the marginal means. To
address the issue of parameter dependence a Bayesian approach is employed.
Prior distributions are assumed on the model parameters which are then
incorporated into the D-optimal design criterion by integrating it over the
prior distribution. Three case studies, one with binary outcomes in a
4$\times$4 crossover trial, second one based on count data for a 2$\times$2
trial and a third one with Gamma responses in a 3$times$2 crossover trial are
used to illustrate the proposed method. The effect of the choice of prior
distributions on the designs is also studied.
</summary>
    <author>
      <name>Satya Prakash Singh</name>
    </author>
    <author>
      <name>Siuli Mukhopadhyay</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01955v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01955v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05069v1</id>
    <updated>2018-08-15T13:30:54Z</updated>
    <published>2018-08-15T13:30:54Z</published>
    <title>The Steady-State Behavior of Multivariate Exponentially Weighted Moving
  Average Control Charts</title>
    <summary>  Multivariate Exponentially Weighted Moving Average, MEWMA, charts are
popular, handy and effective procedures to detect distributional changes in a
stream of multivariate data. For doing appropriate performance analysis,
dealing with the steady-state behavior of the MEWMA statistic is essential.
Going beyond early papers, we derive quite accurate approximations of the
respective steady-state densities of the MEWMA statistic. It turns out that
these densities could be rewritten as the product of two functions depending on
one argument only which allows feasible calculation. For proving the related
statements, the presentation of the non-central chisquare density deploying the
confluent hypergeometric limit function is applied. Using the new methods it
was found that for large dimensions, the steady-state behavior becomes
different to what one might expect from the univariate monitoring field. Based
on the integral equation driven methods, steady-state and worst-case average
run lengths are calculated with higher accuracy than before. Eventually,
optimal MEWMA smoothing constants are derived for all considered measures.
</summary>
    <author>
      <name>Sven Knoth</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05465v1</id>
    <updated>2018-08-15T08:10:30Z</updated>
    <published>2018-08-15T08:10:30Z</published>
    <title>Trimmed Ensemble Kalman Filter for Nonlinear and Non-Gaussian Data
  Assimilation Problems</title>
    <summary>  We study the ensemble Kalman filter (EnKF) algorithm for sequential data
assimilation in a general situation, that is, for nonlinear forecast and
measurement models with non-additive and non-Gaussian noises. Such applications
traditionally force us to choose between inaccurate Gaussian assumptions that
permit efficient algorithms (e.g., EnKF), or more accurate direct sampling
methods which scale poorly with dimension (e.g., particle filters, or PF). We
introduce a trimmed ensemble Kalman filter (TEnKF) which can interpolate
between the limiting distributions of the EnKF and PF to facilitate adaptive
control over both accuracy and efficiency. This is achieved by introducing a
trimming function that removes non-Gaussian outliers that introduce errors in
the correlation between the model and observed forecast, which otherwise
prevent the EnKF from proposing accurate forecast updates. We show for specific
trimming functions that the TEnKF exactly reproduces the limiting distributions
of the EnKF and PF. We also develop an adaptive implementation which provides
control of the effective sample size and allows the filter to overcome periods
of increased model nonlinearity. This algorithm allow us to demonstrate
substantial improvements over the traditional EnKF in convergence and
robustness for the nonlinear Lorenz-63 and Lorenz-96 models.
</summary>
    <author>
      <name>Weixuan Li</name>
    </author>
    <author>
      <name>W. Steven Rosenthal</name>
    </author>
    <author>
      <name>Guang Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In revision, SIAM Journal of Uncertainty Quantification</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15, 60H10, 60G35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04945v1</id>
    <updated>2018-08-15T02:00:46Z</updated>
    <published>2018-08-15T02:00:46Z</published>
    <title>A Confounding Bridge Approach for Double Negative Control Inference on
  Causal Effects</title>
    <summary>  Unmeasured confounding is a key challenge for causal inference. Negative
control variables are widely available in observational studies. A negative
control outcome is associated with the confounder but not causally affected by
the exposure in view, and a negative control exposure is correlated with the
primary exposure or the confounder but does not causally affect the outcome of
interest. In this paper, we establish a framework to use them for unmeasured
confounding adjustment. We introduce a confounding bridge function that links
the potential outcome mean and the negative control outcome distribution, and
we incorporate a negative control exposure to identify the bridge function and
the average causal effect. Our approach can be used to repair an invalid
instrumental variable in case it is correlated with the unmeasured confounder.
We also extend our approach by allowing for a causal association between the
primary exposure and the control outcome. We illustrate our approach with
simulations and apply it to a study about the short-term effect of air
pollution. Although a standard analysis shows a significant acute effect of
PM2.5 on mortality, our analysis indicates that this effect may be confounded,
and after double negative control adjustment, the effect is attenuated toward
zero.
</summary>
    <author>
      <name>Wang Miao</name>
    </author>
    <author>
      <name>Eric Tchetgen Tchetgen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04906v1</id>
    <updated>2018-08-14T21:48:13Z</updated>
    <published>2018-08-14T21:48:13Z</published>
    <title>Multiply Robust Causal Inference With Double Negative Control Adjustment
  for Unmeasured Confounding</title>
    <summary>  Unmeasured confounding is a threat to causal inference in observational
studies. In recent years, use of negative controls to address unmeasured
confounding has gained increasing recognition and popularity. Negative controls
have a longstanding tradition in laboratory sciences and epidemiology to rule
out non-causal explanations, although they have been used primarily for bias
detection. Recently, Miao et al. (2017) have described sufficient conditions
under which a pair of negative control exposure-outcome variables can be used
to nonparametrically identify average treatment effect from observational data
subject to uncontrolled confounding. In this paper, building on their results,
we provide a general semiparametric framework for obtaining inferences about
the average treatment effect with double negative control adjustment for
unmeasured confounding, while accounting for a large number of observed
confounding variables. In particular, we derive the semiparametric efficiency
bound under a nonparametric model for the observed data distribution, and we
propose multiply robust locally efficient estimators when nonparametric
estimation may not be feasible. We assess the finite sample performance of our
methods under potential model misspecification in extensive simulation studies.
We illustrate our methods with an application to the evaluation of the effect
of higher education on wage among married working women.
</summary>
    <author>
      <name>Xu Shi</name>
    </author>
    <author>
      <name>Wang Miao</name>
    </author>
    <author>
      <name>Eric Tchetgen Tchetgen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09870v2</id>
    <updated>2018-08-14T20:18:14Z</updated>
    <published>2017-12-28T14:15:26Z</published>
    <title>Indirect Inference for Lévy-driven continuous-time GARCH models</title>
    <summary>  We advocate the use of an Indirect Inference method to estimate the parameter
of a COGARCH(1,1) process for equally spaced observations. This requires that
the true model can be simulated and a reasonable estimation method for an
approximate auxiliary model. We follow previous approaches and use linear
projections leading to an auxiliary autoregressive model for the squared
COGARCH returns. The asymptotic theory of the Indirect Inference estimator
relies {on a uniform SLLN and asymptotic normality of the parameter estimates
of the auxiliary model, which require continuity and differentiability of the
COGARCH process} with respect to its parameter and which we prove via
Kolmogorov's continuity criterion. This leads to consistent and asymptotically
normal Indirect Inference estimates under moment conditions on the driving
L\'evy process. A simulation study shows that the method yields a substantial
finite sample bias reduction compared to previous estimators.
</summary>
    <author>
      <name>Thiago do Rêgo Sousa</name>
    </author>
    <author>
      <name>Stephan Haug</name>
    </author>
    <author>
      <name>Claudia Klüppelberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09870v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09870v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F12, 62M10, 91G70, 37M10, 62P05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01697v2</id>
    <updated>2018-08-14T17:45:13Z</updated>
    <published>2016-10-06T00:30:57Z</published>
    <title>Central Limit Theory for Combined Cross-Section and Time Series</title>
    <summary>  Combining cross-section and time series data is a long and well established
practice in empirical economics. We develop a central limit theory that
explicitly accounts for possible dependence between the two data sets. We focus
on common factors as the mechanism behind this dependence. Using our central
limit theorem (CLT) we establish the asymptotic properties of parameter
estimates of a general class of models based on a combination of
cross-sectional and time series data, recognizing the interdependence between
the two data sources in the presence of aggregate shocks. Despite the
complicated nature of the analysis required to formulate the joint CLT, it is
straightforward to implement the resulting parameter limiting distributions due
to a formal similarity of our approximations with the standard Murphy and
Topel's (1985) formula.
</summary>
    <author>
      <name>Jinyong Hahn</name>
    </author>
    <author>
      <name>Guido Kuersteiner</name>
    </author>
    <author>
      <name>Maurizio Mazzocco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1507.04415</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60F17, 620E20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04780v1</id>
    <updated>2018-08-14T16:17:40Z</updated>
    <published>2018-08-14T16:17:40Z</published>
    <title>Multivariate Density Estimation with Missing Data</title>
    <summary>  Multivariate density estimation is a popular technique in statistics with
wide applications including regression models allowing for heteroskedasticity
in conditional variances. The estimation problems become more challenging when
observations are missing in one or more variables of the multivariate vector. A
flexible class of mixture of tensor products of kernel densities is proposed
which allows for easy implementation of imputation methods using Gibbs sampling
and shown to have superior performance compared to some of the exisiting
imputation methods currently available in literature. Numerical illustrations
are provided using several simulated data scenarios and applications to couple
of case studies are also presented.
</summary>
    <author>
      <name>Zhen Li</name>
    </author>
    <author>
      <name>Lili Wu</name>
    </author>
    <author>
      <name>Weilian Zhou</name>
    </author>
    <author>
      <name>Sujit Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04773v1</id>
    <updated>2018-08-14T16:11:15Z</updated>
    <published>2018-08-14T16:11:15Z</published>
    <title>Probabilistic $K$-mean with local alignment for clustering and motif
  discovery in functional data</title>
    <summary>  We develop a new method to locally cluster curves and discover functional
motifs, i.e. typical "shapes" that may recur several times along and across the
curves capturing important local characteristics. In order to identify these
shared curve portions, our method leverages ideas from functional data analysis
(joint clustering and alignment of curves), bioinformatics (local alignment
through the extension of high similarity seeds) and fuzzy clustering (curves
belonging to more than one cluster, if they contain more than one typical
"shape"). It can employ various dissimilarity measures and incorporate
derivatives in the discovery process, thus exploiting complex facets of shapes.
We demonstrate the performance of our method with an extensive simulation
study, and show how it generalizes other clustering methods for functional
data. Finally, we apply it to the discovery of functional motifs in "Omics"
signals related to mutagenesis and genome dynamics.
</summary>
    <author>
      <name>Marzia A. Cremona</name>
    </author>
    <author>
      <name>Francesca Chiaromonte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures. This work has been presented at CMStatistics
  2017 (London, UK), DSSV 2018 (Vienna, Austria), JSM 2018 (Vancouver, Canada),
  and various other conferences</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04753v1</id>
    <updated>2018-08-14T15:31:56Z</updated>
    <published>2018-08-14T15:31:56Z</published>
    <title>Estimating the size of a hidden finite set: large-sample behavior of
  estimators</title>
    <summary>  A finite set is "hidden" if its elements are not directly enumerable or if
its size cannot be ascertained via a deterministic query. In public health,
epidemiology, demography, ecology and intelligence analysis, researchers have
developed a wide variety of indirect statistical approaches, under different
models for sampling and observation, for estimating the size of a hidden set.
Some methods make use of random sampling with known or estimable sampling
probabilities, and others make structural assumptions about relationships (e.g.
ordering or network information) between the elements that comprise the hidden
set. In this review, we describe models and methods for learning about the size
of a hidden finite set, with special attention to asymptotic properties of
estimators. We study the properties of these methods under two asymptotic
regimes, "infill" in which the number of fixed-size samples increases, but the
population size remains constant, and "outfill" in which the sample size and
population size grow together. Statistical properties under these two regimes
can be dramatically different.
</summary>
    <author>
      <name>Si Cheng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Biostatistics, Yale School of Public Health</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel J. Eck</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Biostatistics, Yale School of Public Health</arxiv:affiliation>
    </author>
    <author>
      <name>Forrest W. Crawford</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Biostatistics, Yale School of Public Health</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Ecology &amp; Evolutionary Biology, Yale University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yale School of Management</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1808.04753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06259v3</id>
    <updated>2018-08-14T15:25:53Z</updated>
    <published>2017-05-17T16:55:59Z</published>
    <title>Joint models for grid point and response processes in longitudinal and
  functional data</title>
    <summary>  The distribution of the grid points at which a response function is observed
in longitudinal or functional data applications is often informative and not
independent of the response process. In this paper we introduce a covariation
model to estimate and make inferences about this interrelation, by treating the
data as replicated realizations of a marked point process. We derive maximum
likelihood estimators, the asymptotic distribution of the estimators, and study
the estimators' behavior by simulation. We apply the model to an online auction
data set and show that there is a strong correlation between bidding patterns
and price trajectories.
</summary>
    <author>
      <name>Daniel Gervini</name>
    </author>
    <author>
      <name>Tyler J. Baur</name>
    </author>
    <link href="http://arxiv.org/abs/1705.06259v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06259v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04045v2</id>
    <updated>2018-08-14T07:49:43Z</updated>
    <published>2018-08-13T02:20:14Z</published>
    <title>A Nonparametric Bayesian Method for Clustering of High-Dimensional Mixed
  Dataset</title>
    <summary>  The paper is motivated from clustering problem in high-throughput mixed
datasets. Clustering of such datasets can provide much insight into biological
associations. An open problem in this context is to simultaneously cluster
high-dimensional mixed dataset. This paper fills that gap and proposes a
nonparametric Bayesian method called Gen-VariScan for biclustering of
high-dimensional mixed dataset.
  Gen-VariScan utilizes Generalized Linear Models (GLM), and latent variable
approaches to integrate mixed dataset. We make use of Poisson Dirichlet Process
(PDP) to identify a lower dimensional structure of mixed covariates. We show
that covariate co-cluster detection is aposteriori consistent, as the number of
subject and covariates grows. The advantage of Gen-VariScan is also
demonstrated through numerical simulation and data analysis. As a byproduct, we
derive a working value approach to perform beta regression. Supplementary
materials for this article are available online.
</summary>
    <author>
      <name>Chetkar Jha</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04045v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04045v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09567v2</id>
    <updated>2018-08-14T04:36:43Z</updated>
    <published>2018-04-25T13:47:00Z</published>
    <title>Symmetric Bilinear Regression for Signal Subgraph Estimation</title>
    <summary>  There is increasing interest in learning a set of small outcome-relevant
subgraphs in network-predictor regression. The extracted signal subgraphs can
greatly improve the interpretation of the association between the network
predictor and the response. In brain connectomics, the brain network for an
individual corresponds to a set of interconnections among brain regions and
there is a strong interest in linking the brain connectome to human cognitive
traits. Modern neuroimaging technology allows a very fine segmentation of the
brain, producing very large structural brain networks. Therefore, accurate and
efficient methods for identifying a set of small predictive subgraphs become
crucial, leading to discovery of key interconnected brain regions related to
the trait and important insights on the mechanism of variation in human
cognitive traits. We propose a symmetric bilinear model with $L_1$ penalty to
search for small clique subgraphs that contain useful information about the
response. A coordinate descent algorithm is developed to estimate the model
where we derive analytical solutions for a sequence of conditional convex
optimizations. Application of this method on human connectome and language
comprehension data shows interesting discovery of relevant interconnections
among several small sets of brain regions and better predictive performance
than competitors.
</summary>
    <author>
      <name>Lu Wang</name>
    </author>
    <author>
      <name>Zhengwu Zhang</name>
    </author>
    <author>
      <name>David Dunson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, double columns, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.09567v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09567v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03662v2</id>
    <updated>2018-08-14T01:31:07Z</updated>
    <published>2017-11-10T01:26:36Z</published>
    <title>A Latent Space Model for Cognitive Social Structures Data</title>
    <summary>  This paper introduces a novel approach for modeling a set of directed, binary
networks in the context of cognitive social structures (CSSs) data. We rely on
a generalized linear model that incorporates a bilinear structure to model
transitivity effects within networks, and a hierarchical specification on the
bilinear effects to borrow information across networks. The model is designed
to allow for a formal evaluation of the accuracy of actors' perception about
their position in social space, and to obtain a consensus representation of the
social space that accounts for differential perceptual acuity among actors. The
performance of the model is evaluated using simulated data as well as two real
CSSs reported in the literature.
</summary>
    <author>
      <name>Juan Sosa</name>
    </author>
    <author>
      <name>Abel Rodriguez</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03662v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03662v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07268v2</id>
    <updated>2018-08-14T01:07:04Z</updated>
    <published>2016-06-23T10:53:05Z</published>
    <title>Semi-supervised Inference: General Theory and Estimation of Means</title>
    <summary>  We propose a general semi-supervised inference framework focused on the
estimation of the population mean. As usual in semi-supervised settings, there
exists an unlabeled sample of covariate vectors and a labeled sample consisting
of covariate vectors along with real-valued responses ("labels"). Otherwise,
the formulation is "assumption-lean" in that no major conditions are imposed on
the statistical or functional form of the data. We consider both the ideal
semi-supervised setting where infinitely many unlabeled samples are available,
as well as the ordinary semi-supervised setting in which only a finite number
of unlabeled samples is available.
  Estimators are proposed along with corresponding confidence intervals for the
population mean. Theoretical analysis on both the asymptotic distribution and
$\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed
estimators, based on a simple form of the least squares method, outperform the
ordinary sample mean. The simple, transparent form of the estimator lends
confidence to the perception that its asymptotic improvement over the ordinary
sample mean also nearly holds even for moderate size samples. The method is
further extended to a nonparametric setting, in which the oracle rate can be
achieved asymptotically. The proposed estimators are further illustrated by
simulation studies and a real data example involving estimation of the homeless
population.
</summary>
    <author>
      <name>Anru Zhang</name>
    </author>
    <author>
      <name>Lawrence D. Brown</name>
    </author>
    <author>
      <name>T. Tony Cai</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07268v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07268v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03989v2</id>
    <updated>2018-08-13T22:53:53Z</updated>
    <published>2018-01-11T21:05:23Z</published>
    <title>Average Power and $λ$-power in Multiple Testing Scenarios when the
  Benjamini-Hochberg False Discovery Rate Procedure is Used</title>
    <summary>  We discuss several approaches to defining power in studies designed around
the Benjamini-Hochberg (BH) false discovery rate (FDR) procedure. We focus
primarily on the \textit{average power} and the $\lambda$-\textit{power}, which
are the expected true positive fraction and the probability that the true
positive fraction exceeds $\lambda$, respectively. We prove results concerning
strong consistency and asymptotic normality for the positive call fraction
(PCF), the true positive fraction (TPF) and false discovery fraction (FDF).
Convergence of their corresponding expected values, including a convergence
result for the average power, follow as a corollaries. After reviewing what is
known about convergence in distribution of the errors of the plugin procedure,
(Genovese, 2004), we prove central limit theorems for fully empirical versions
of the PCF, TPF, and FDF, using a result for stopped stochastic processes. The
central limit theorem (CLT) for the TPF is used to obtain an approximate
expression for the $\lambda$-power, while the CLT for the FDF is used to
introduce an approximate procedure for determining a suitably small nominal FDR
that results in a speicified bound on the FDF with stipulated high probability.
The paper also contains the results of a large simulation study covering a
fairly substantial portion of the space of possible inputs encountered in
application of the results in the design of a biomarker study, a micro-array
experiment and a GWAS study.
</summary>
    <author>
      <name>Grant Izmirlian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, 2 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03989v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03989v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02531v5</id>
    <updated>2018-08-13T21:19:08Z</updated>
    <published>2017-04-08T20:03:01Z</published>
    <title>Three Skewed Matrix Variate Distributions</title>
    <summary>  Three-way data can be conveniently modelled by using matrix variate
distributions. Although there has been a lot of work for the matrix variate
normal distribution, there is little work in the area of matrix skew
distributions. Three matrix variate distributions that incorporate skewness, as
well as other flexible properties such as concentration, are discussed.
Equivalences to multivariate analogues are presented, and moment generating
functions are derived. Maximum likelihood parameter estimation is discussed,
and simulated data is used for illustration.
</summary>
    <author>
      <name>Michael P. B. Gallaugher</name>
    </author>
    <author>
      <name>Paul D. McNicholas</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02531v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02531v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02002v2</id>
    <updated>2018-08-13T20:07:43Z</updated>
    <published>2017-01-08T19:24:59Z</published>
    <title>Smoothing with Couplings of Conditional Particle Filters</title>
    <summary>  In state space models, smoothing refers to the task of estimating a latent
stochastic process given noisy measurements related to the process. We propose
an unbiased estimator of smoothing expectations. The lack-of-bias property has
methodological benefits: independent estimators can be generated in parallel,
and confidence intervals can be constructed from the central limit theorem to
quantify the approximation error. To design unbiased estimators, we combine a
generic debiasing technique for Markov chains with a Markov chain Monte Carlo
algorithm for smoothing. The resulting procedure is widely applicable and we
show in numerical experiments that the removal of the bias comes at a
manageable increase in variance. We establish the validity of the proposed
estimators under mild assumptions. Numerical experiments are provided on toy
models, including a setting of highly-informative observations, and a realistic
Lotka-Volterra model with an intractable transition density.
</summary>
    <author>
      <name>Pierre E. Jacob</name>
    </author>
    <author>
      <name>Fredrik Lindsten</name>
    </author>
    <author>
      <name>Thomas B. Schön</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This document is a self-contained and direct description of the
  smoothing method introduced in Coupling of Particle Filters
  (arXiv:1606.01156). This is the final version. Code is available at
  github.com/pierrejacob/CoupledCPF</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05409v2</id>
    <updated>2018-08-13T20:02:42Z</updated>
    <published>2017-09-15T21:07:46Z</published>
    <title>Gaussian Process Latent Force Models for Learning and Stochastic Control
  of Physical Systems</title>
    <summary>  This article is concerned with learning and stochastic control in physical
systems which contain unknown input signals. These unknown signals are modeled
as Gaussian processes (GP) with certain parametrized covariance structures. The
resulting latent force models (LFMs) can be seen as hybrid models that contain
a first-principles physical model part and a non-parametric GP model part. We
briefly review the statistical inference and learning methods for this kind of
models, introduce stochastic control methodology for the models, and provide
new theoretical observability and controllability results for them.
</summary>
    <author>
      <name>Simo Särkkä</name>
    </author>
    <author>
      <name>Mauricio A. Álvarez</name>
    </author>
    <author>
      <name>Neil D. Lawrence</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05409v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05409v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04416v1</id>
    <updated>2018-08-13T19:38:11Z</updated>
    <published>2018-08-13T19:38:11Z</published>
    <title>Extrapolating Treatment Effects in Multi-Cutoff Regression Discontinuity
  Designs</title>
    <summary>  Regression discontinuity (RD) designs are viewed as one of the most credible
identification strategies for program evaluation and causal inference. However,
RD treatment effect estimands are necessarily local, making the extrapolation
of these effects a critical open question. We introduce a new method for
extrapolation of RD effects that exploits the presence of multiple cutoffs, and
is therefore design-based. Our approach relies on an easy-to-interpret
identifying assumption that mimics the idea of `common trends' in
differences-in-differences designs. We illustrate our methods with a study of
the effect of a cash transfer program for post-education attendance in
Colombia.
</summary>
    <author>
      <name>Matias D. Cattaneo</name>
    </author>
    <author>
      <name>Luke Keele</name>
    </author>
    <author>
      <name>Rocio Titiunik</name>
    </author>
    <author>
      <name>Gonzalo Vazquez-Bare</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04401v1</id>
    <updated>2018-08-13T18:51:39Z</updated>
    <published>2018-08-13T18:51:39Z</published>
    <title>Locally-adaptive Bayesian nonparametric inference for phylodynamics</title>
    <summary>  Phylodynamics is an area of population genetics that uses genetic sequence
data to estimate past population dynamics. Modern state-of-the-art Bayesian
nonparametric methods for phylodynamics use either change-point models or
Gaussian process priors to recover population size trajectories of unknown
form. Change-point models suffer from computational issues when the number of
change-points is unknown and needs to be estimated. Gaussian process-based
methods lack local adaptivity and cannot accurately recover trajectories that
exhibit features such as abrupt changes in trend or varying levels of
smoothness. We propose a novel, locally-adaptive approach to Bayesian
nonparametric phylodynamic inference that has the flexibility to accommodate a
large class of functional behaviors. Local adaptivity results from modeling the
log-transformed effective population size a priori as a horseshoe Markov random
field, a recently proposed statistical model that blends together the best
properties of the change-point and Gaussian process modeling paradigms. We use
simulated data to assess model performance, and find that our proposed method
results in reduced bias and increased precision when compared to contemporary
methods. We also use our models to reconstruct past changes in genetic
diversity of human hepatitis C virus in Egypt and to estimate population size
changes of ancient and modern steppe bison. These analyses show that our new
method captures features of the population size trajectories that were missed
by the state-of-the-art phylodynamic methods.
</summary>
    <author>
      <name>James R. Faulkner</name>
    </author>
    <author>
      <name>Andrew F. Magee</name>
    </author>
    <author>
      <name>Beth Shapiro</name>
    </author>
    <author>
      <name>Vladimir N. Minin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, including supplementary information</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04348v1</id>
    <updated>2018-08-13T17:52:43Z</updated>
    <published>2018-08-13T17:52:43Z</published>
    <title>Multivariate Geometric Anisotropic Cox Processes</title>
    <summary>  This paper introduces a new modelling framework for multivariate anisotropic
Cox processes. Building on recent innovations in multivariate spatial
statistics, we propose a new family of multivariate anisotropic random fields
and construct a family of anisotropic point processes from it. We give
conditions that make the models valid, and we provide additional understanding
of valid point process dependence. We also propose a likelihood-based inference
mechanism for this type of process. Finally we illustrate the utility of the
proposed modelling framework by analysing spatial ecological observations of
plants and trees in the Barro Colorado Island study.
</summary>
    <author>
      <name>James S. Martin</name>
    </author>
    <author>
      <name>David J. Murrell</name>
    </author>
    <author>
      <name>Sofia C. Olhede</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02090v3</id>
    <updated>2018-08-13T16:40:48Z</updated>
    <published>2018-04-06T00:31:11Z</published>
    <title>Microsimulation Model Calibration using Incremental Mixture Approximate
  Bayesian Computation</title>
    <summary>  Microsimulation models (MSMs) are used to predict population-level effects of
health care policies by simulating individual-level outcomes. Simulated
outcomes are governed by unknown parameters that are chosen so that the model
accurately predicts specific targets, a process referred to as model
calibration. Calibration targets can come from randomized controlled trials,
observational studies, and expert opinion, and are typically summary
statistics. A well calibrated model can reproduce a wide range of targets. MSM
calibration generally involves searching a high dimensional parameter space and
predicting many targets through model simulation. This requires efficient
methods for exploring the parameter space and sufficient computational
resources. We develop Incremental Mixture Approximate Bayesian Computation
(IMABC) as a method for MSM calibration and implement it via a high-performance
computing workflow, which provides the necessary computational scale. IMABC
begins with a rejection-based approximate Bayesian computation (ABC) step,
drawing a sample of parameters from the prior distribution and simulating
calibration targets. Next, the sample is iteratively updated by drawing
additional points from a mixture of multivariate normal distributions, centered
at the points that yield simulated targets that are near observed targets.
Posterior estimates are obtained by weighting sampled parameter vectors to
account for the adaptive sampling scheme. We demonstrate IMABC by calibrating a
MSM for the natural history of colorectal cancer to obtain simulated draws from
the joint posterior distribution of model parameters.
</summary>
    <author>
      <name>Carolyn Rutter</name>
    </author>
    <author>
      <name>Jonathan Ozik</name>
    </author>
    <author>
      <name>Maria DeYoreo</name>
    </author>
    <author>
      <name>Nicholson Collier</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02090v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02090v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05248v2</id>
    <updated>2018-08-13T16:21:21Z</updated>
    <published>2017-08-17T12:56:15Z</published>
    <title>A nonparametric test for stationarity in functional time series</title>
    <summary>  We propose a new measure for stationarity of a functional time series, which
is based on an explicit representation of the $L^2$-distance between the
spectral density operator of a non-stationary process and its best
($L^2$-)approximation by a spectral density operator corresponding to a
stationary process. This distance can easily be estimated by sums of
Hilbert-Schmidt inner products of periodogram operators (evaluated at different
frequencies), and asymptotic normality of an appropriately standardized version
of the estimator can be established for the corresponding estimate under the
null hypothesis and alternative. As a result we obtain a simple asymptotic
frequency domain level $\alpha$ test (using the quantiles of the normal
distribution) for the hypothesis of stationarity of functional time series.
Other applications such as asymptotic confidence intervals for a measure of
stationarity or the construction of tests for "relevant deviations from
stationarity", are also briefly mentioned. We demonstrate in a small simulation
study that the new method has very good finite sample properties. Moreover, we
apply our test to annual temperature curves.
</summary>
    <author>
      <name>Anne van Delft</name>
    </author>
    <author>
      <name>Vaidotas Characiejus</name>
    </author>
    <author>
      <name>Holger Dette</name>
    </author>
    <link href="http://arxiv.org/abs/1708.05248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary: 62M15, 62H15, Secondary: 62M10, 62M15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04312v1</id>
    <updated>2018-08-13T16:12:59Z</updated>
    <published>2018-08-13T16:12:59Z</published>
    <title>Analysing Multiple Epidemic Data Sources</title>
    <summary>  Evidence-based knowledge of infectious disease burden, including prevalence,
incidence, severity and transmission, in different population strata and
locations, and possibly in real time, is crucial to the planning and evaluation
of public health policies. Direct observation of a disease process is rarely
possible. However, latent characteristics of an epidemic and its evolution can
often be inferred from the synthesis of indirect information from various
routine data sources, as well as expert opinion. The simultaneous synthesis of
multiple data sources, often conveniently carried out in a Bayesian framework,
poses a number of statistical and computational challenges: the heterogeneity
in type, relevance and granularity of the data, together with selection and
informative observation biases, lead to complex probabilistic models that are
difficult to build and fit, and challenging to criticize. Using motivating case
studies of influenza, this chapter illustrates the cycle of model development
and criticism in the context of Bayesian evidence synthesis, highlighting the
challenges of complex model building, computationally efficient inference, and
conflicting evidence.
</summary>
    <author>
      <name>Daniela De Angelis</name>
    </author>
    <author>
      <name>Anne M. Presanis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is a preprint of a Chapter to appear in the "Handbook
  of Infectious Disease Data Analysis", Held, L., Hens, N., O'Neill, P.D. and
  Wallinga, J. (Eds.). Chapman &amp; Hall/CRC, 2018. Please use the book for
  possible citations</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04280v1</id>
    <updated>2018-08-13T15:02:53Z</updated>
    <published>2018-08-13T15:02:53Z</published>
    <title>Generalized Multivariate Extreme Value Models for Explicit Route Choice
  Sets</title>
    <summary>  This paper analyses a class of route choice models with closed-form
probability expressions, namely, Generalized Multivariate Extreme Value (GMEV)
models. A large group of these models emerge from different utility formulas
that combine systematic utility and random error terms. Twelve models are
captured in a single discrete choice framework. The additive utility formula
leads to the known logit family, being multinomial, path-size, paired
combinatorial and link-nested. For the multiplicative formulation only the
multinomial and path-size weibit models have been identified; this study also
identifies the paired combinatorial and link-nested variations, and generalizes
the path-size variant. Furthermore, a new traveller's decision rule based on
the multiplicative utility formula with a reference route is presented. Here
the traveller chooses exclusively based on the differences between routes. This
leads to four new GMEV models. We assess the models qualitatively based on a
generic structure of route utility with random foreseen travel times, for which
we empirically identify that the variance of utility should be different from
thus far assumed for multinomial probit and logit-kernel models. The expected
travellers' behaviour and model-behaviour under simple network changes are
analysed. Furthermore, all models are estimated and validated on an
illustrative network example with long distance and short distance
origin-destination pairs. The new multiplicative models based on differences
outperform the additive models in both tests.
</summary>
    <author>
      <name>Erik-Sander Smits</name>
    </author>
    <author>
      <name>Adam J. Pel</name>
    </author>
    <author>
      <name>Michiel C. J. Bliemer</name>
    </author>
    <author>
      <name>Bart van Arem</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91B99, 90B06" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04246v1</id>
    <updated>2018-08-13T14:13:02Z</updated>
    <published>2018-08-13T14:13:02Z</published>
    <title>Semiparametric Bayesian causal inference using Gaussian process priors</title>
    <summary>  We develop a semiparametric Bayesian approach for estimating the mean
response in a missing data model with binary outcomes and a nonparametrically
modelled propensity score. Equivalently we estimate the causal effect of a
treatment, correcting nonparametrically for confounding. We show that standard
Gaussian process priors satisfy a semiparametric Bernstein-von Mises theorem
under smoothness conditions. We further propose a novel propensity
score-dependent prior that provides efficient inference under strictly weaker
conditions. We also show that it is theoretically preferable to model the
covariate distribution with a Dirichlet process or Bayesian bootstrap, rather
than modelling the covariate density using a Gaussian process prior.
</summary>
    <author>
      <name>Kolyan Ray</name>
    </author>
    <author>
      <name>Aad van der Vaart</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20 (Primary), 62G15, 62G08 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08830v3</id>
    <updated>2018-08-13T13:10:17Z</updated>
    <published>2016-03-15T14:40:55Z</published>
    <title>Reduced Perplexity: A simplified perspective on assessing probabilistic
  forecasts</title>
    <summary>  A simple, intuitive approach to the assessment of probabilistic inferences is
introduced. The Shannon information metrics are translated to the probability
domain. The translation shows that the negative logarithmic score and the
geometric mean are equivalent measures of the accuracy of a probabilistic
inference. Thus there is both a quantitative reduction in perplexity, which is
the inverse of the geometric mean of the probabilities, as good inference
algorithms reduce the uncertainty and a qualitative reduction due to the
increased clarity between the original set of probabilistic forecasts and their
central tendency, the geometric mean. Further insight is provided by showing
that the R\'enyi and Tsallis entropy functions translated to the probability
domain are both the weighted generalized mean of the distribution. The
generalized mean of probabilistic forecasts forms a spectrum of performance
metrics referred to as a Risk Profile. The arithmetic mean is used to measure
the decisiveness, while the -2/3 mean is used to measure the robustness.
</summary>
    <author>
      <name>Kenric P. Nelson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures, conference paper presented at Recent Advances in
  Info-Metrics, Washington, DC, 2014. Under review for a book chapter in
  "Recent innovations in info-metrics: a cross-disciplinary perspective on
  information and information processing" by Oxford University Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08830v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08830v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03989v2</id>
    <updated>2018-08-13T13:06:34Z</updated>
    <published>2018-04-04T13:31:46Z</published>
    <title>Use of the geometric mean as a statistic for the scale of the coupled
  Gaussian distributions</title>
    <summary>  The geometric mean is shown to be an appropriate statistic for the scale of a
heavy-tailed coupled Gaussian distribution or equivalently the Student's t
distribution. The coupled Gaussian is a member of a family of distributions
parameterized by the nonlinear statistical coupling which is the reciprocal of
the degree of freedom and is proportional to fluctuations in the inverse scale
of the Gaussian. Existing estimators of the scale of the coupled Gaussian have
relied on estimates of the full distribution, and they suffer from problems
related to outliers in heavy-tailed distributions. In this paper, the scale of
a coupled Gaussian is proven to be equal to the product of the generalized mean
and the square root of the coupling. From our numerical computations of the
scales of coupled Gaussians using the generalized mean of random samples, it is
indicated that only samples from a Cauchy distribution (with coupling parameter
one) form an unbiased estimate with diminishing variance for large samples.
Nevertheless, we also prove that the scale is a function of the geometric mean,
the coupling term and a harmonic number. Numerical experiments show that this
estimator is unbiased with diminishing variance for large samples for a broad
range of coupling values.
</summary>
    <author>
      <name>Kenric P. Nelson</name>
    </author>
    <author>
      <name>Mark A. Kon</name>
    </author>
    <author>
      <name>Sabir R. Umarov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03989v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03989v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04139v1</id>
    <updated>2018-08-13T10:28:05Z</updated>
    <published>2018-08-13T10:28:05Z</published>
    <title>A Matching Based Theoretical Framework for Estimating Probability of
  Causation</title>
    <summary>  The concept of Probability of Causation (PC) is critically important in legal
contexts and can help in many other domains. While it has been around since
1986, current operationalizations can obtain only the minimum and maximum
values of PC, and do not apply for purely observational data. We present a
theoretical framework to estimate the distribution of PC from experimental and
from purely observational data. We illustrate additional problems of the
existing operationalizations and show how our method can be used to address
them. We also provide two illustrative examples of how our method is used and
how factors like sample size or rarity of events can influence the distribution
of PC. We hope this will make the concept of PC more widely usable in practice.
</summary>
    <author>
      <name>Tapajit Dey</name>
    </author>
    <author>
      <name>Audris Mockus</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04092v1</id>
    <updated>2018-08-13T07:57:27Z</updated>
    <published>2018-08-13T07:57:27Z</published>
    <title>Detecting deviations from second-order stationarity in locally
  stationary functional time series</title>
    <summary>  A time-domain test for the assumption of second order stationarity of a
functional time series is proposed. The test is based on combining individual
cumulative sum tests which are designed to be sensitive to changes in the mean,
variance and autocovariance operators, respectively. The combination of their
dependent $p$-values relies on a joint dependent block multiplier bootstrap of
the individual test statistics. Conditions under which the proposed combined
testing procedure is asymptotically valid under stationarity are provided. A
procedure is proposed to automatically choose the block length parameter needed
for the construction of the bootstrap. The finite-sample behavior of the
proposed test is investigated in Monte Carlo experiments and an illustration on
a real data set is provided.
</summary>
    <author>
      <name>Axel Bücher</name>
    </author>
    <author>
      <name>Holger Dette</name>
    </author>
    <author>
      <name>Florian Heinrichs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Key words: alpha-mixing, CUSUM-test, auto-covariance operator, block
  multiplier bootstrap, change points</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01667v4</id>
    <updated>2018-08-13T06:55:01Z</updated>
    <published>2017-11-05T22:03:43Z</published>
    <title>Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting</title>
    <summary>  We develop the methodology and a detailed case study in use of a class of
Bayesian predictive synthesis (BPS) models for multivariate time series
forecasting. This extends the recently introduced foundational framework of BPS
to the multivariate setting, with detailed application in the topical and
challenging context of multi-step macroeconomic forecasting in a monetary
policy setting. BPS evaluates-- sequentially and adaptively over time-- varying
forecast biases and facets of miscalibration of individual forecast densities,
and-- critically-- of time-varying inter-dependencies among them over multiple
series. We develop new BPS methodology for a specific subclass of the dynamic
multivariate latent factor models implied by BPS theory. Structured dynamic
latent factor BPS is here motivated by the application context-- sequential
forecasting of multiple US macroeconomic time series with forecasts generated
from several traditional econometric time series models. The case study
highlights the potential of BPS to improve of forecasts of multiple series at
multiple forecast horizons, and its use in learning dynamic relationships among
forecasting models or agents.
</summary>
    <author>
      <name>Kenichiro McAlinn</name>
    </author>
    <author>
      <name>Knut Are Aastveit</name>
    </author>
    <author>
      <name>Jouchi Nakajima</name>
    </author>
    <author>
      <name>Mike West</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01667v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01667v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05307v2</id>
    <updated>2018-08-12T12:47:09Z</updated>
    <published>2016-12-15T23:31:10Z</published>
    <title>Bayesian Robustness to Outliers in Linear Regression and Ratio
  Estimation</title>
    <summary>  Whole robustness is a nice property to have for statistical models. It
implies that the impact of outliers gradually vanishes as they approach plus or
minus infinity. So far, the Bayesian literature provides results that ensure
whole robustness for the location-scale model. In this paper, we make two
contributions. First, we generalise the results to attain whole robustness in
simple linear regression through the origin, which is a necessary step towards
results for general linear regression models. We allow the variance of the
error term to depend on the explanatory variable. This flexibility leads to the
second contribution: we provide a simple Bayesian approach to robustly estimate
finite population means and ratios. The strategy to attain whole robustness is
simple since it lies in replacing the traditional normal assumption on the
error term by a super heavy-tailed distribution assumption. As a result, users
can estimate the parameters as usual, using the posterior distribution.
</summary>
    <author>
      <name>Alain Desgagné</name>
    </author>
    <author>
      <name>Philippe Gagnon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Brazilian Journal of Probability and Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05307v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05307v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62F35, secondary 62J05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05054v2</id>
    <updated>2018-08-12T09:50:12Z</updated>
    <published>2018-05-14T08:15:48Z</published>
    <title>Consistency of Variational Bayes Inference for Estimation and Model
  Selection in Mixtures</title>
    <summary>  Mixture models are widely used in Bayesian statistics and machine learning,
in particular in computational biology, natural language processing and many
other fields. Variational inference, a technique for approximating intractable
posteriors thanks to optimization algorithms, is extremely popular in practice
when dealing with complex models such as mixtures. The contribution of this
paper is two-fold. First, we study the concentration of variational
approximations of posteriors, which is still an open problem for general
mixtures, and we derive consistency and rates of convergence. We also tackle
the problem of model selection for the number of components: we study the
approach already used in practice, which consists in maximizing a numerical
criterion (the Evidence Lower Bound). We prove that this strategy indeed leads
to strong oracle inequalities. We illustrate our theoretical results by
applications to Gaussian and multinomial mixtures.
</summary>
    <author>
      <name>Badr-Eddine Chérief-Abdellatif</name>
    </author>
    <author>
      <name>Pierre Alquier</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05054v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05054v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03889v1</id>
    <updated>2018-08-12T04:34:24Z</updated>
    <published>2018-08-12T04:34:24Z</published>
    <title>Robust high dimensional factor models with applications to statistical
  machine learning</title>
    <summary>  Factor models are a class of powerful statistical models that have been
widely used to deal with dependent measurements that arise frequently from
various applications from genomics and neuroscience to economics and finance.
As data are collected at an ever-growing scale, statistical machine learning
faces some new challenges: high dimensionality, strong dependence among
observed variables, heavy-tailed variables and heterogeneity. High-dimensional
robust factor analysis serves as a powerful toolkit to conquer these
challenges.
  This paper gives a selective overview on recent advance on high-dimensional
factor models and their applications to statistics including Factor-Adjusted
Robust Model selection (FarmSelect) and Factor-Adjusted Robust Multiple testing
(FarmTest). We show that classical methods, especially principal component
analysis (PCA), can be tailored to many new problems and provide powerful tools
for statistical estimation and inference. We highlight PCA and its connections
to matrix perturbation theory, robust statistics, random projection, false
discovery rate, etc., and illustrate through several applications how insights
from these fields yield solutions to modern challenges. We also present
far-reaching connections between factor models and popular statistical learning
problems, including network analysis and low-rank matrix recovery.
</summary>
    <author>
      <name>Jianqing Fan</name>
    </author>
    <author>
      <name>Kaizheng Wang</name>
    </author>
    <author>
      <name>Yiqiao Zhong</name>
    </author>
    <author>
      <name>Ziwei Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10136v4</id>
    <updated>2018-08-12T03:23:54Z</updated>
    <published>2017-03-29T17:00:42Z</published>
    <title>Network Dependence Testing via Diffusion Maps and Distance-Based
  Correlations</title>
    <summary>  Deciphering the associations between network connectivity and nodal
attributes is one of the core problems in network science. The dependency
structure and high-dimensionality of networks pose unique challenges to
traditional dependency tests in terms of theoretical guarantees and empirical
performance. We propose an approach to test network dependence via diffusion
maps and distance-based correlations. We prove that the new method yields a
consistent test statistic under mild distributional assumptions on the graph
structure, and demonstrate that it is able to efficiently identify the most
informative graph embedding with respect to the diffusion time. The testing
performance is illustrated on both simulated and real data.
</summary>
    <author>
      <name>Youjin Lee</name>
    </author>
    <author>
      <name>Cencheng Shen</name>
    </author>
    <author>
      <name>Carey E. Priebe</name>
    </author>
    <author>
      <name>Joshua T. Vogelstein</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10136v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10136v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03829v1</id>
    <updated>2018-08-11T16:23:49Z</updated>
    <published>2018-08-11T16:23:49Z</published>
    <title>On modelling positive continuous data with spatio-temporal dependence</title>
    <summary>  In this paper we aim to propose two models for regression and dependence
analysis when dealing with positive spatial or spatio-temporal continuous data.
Specifically we propose two (possibly non stationary) random processes with
Gamma and Weibull marginals. Both processes stem from the same idea, namely
from the transformation of a sum of independent copies of a squared Gaussian
random process. We provide analytic expression for the bivariate distributions
and we study the associated geometrical and extremal properties. Since maximum
likelihood estimation method is not feasible, even for relatively small
data-set, we suggest to adopt the pairwise likelihood. The effectiveness of our
proposal is illustrated through a simulation study that we supplement with a
new analysis of well-known dataset, the Irish wind speed data
\citep{Haslett:Raftery:1989}. In our example we do not consider a preliminary
transformation of the data differently from the previous studies.
</summary>
    <author>
      <name>M. Bevilacqua</name>
    </author>
    <author>
      <name>C. Caamaño</name>
    </author>
    <author>
      <name>C. Gaetan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09957v3</id>
    <updated>2018-08-11T15:18:51Z</updated>
    <published>2017-12-28T18:07:32Z</published>
    <title>Estimation and prediction of Gaussian processes using generalized Cauchy
  covariance model under fixed domain asymptotics</title>
    <summary>  We study estimation and prediction of Gaussian processes with covariance
model belonging to the generalized Cauchy (GC) family, under fixed domain
asymptotics. Gaussian processes with this kind of covariance function provide
separate characterization of fractal dimension and long range dependence, an
appealing feature in many physical, biological or geological systems. The
results of the paper are classified into three parts. In the first part, we
characterize the equivalence of two Gaussian measures with GC covariance
function. Then we provide sufficient conditions for the equivalence of two
Gaussian measures with Mat{\'e}rn (MT) and GC covariance functions and two
Gaussian measures with Generalized Wendland (GW) and GC covariance functions.
In the second part, we establish strong consistency and asymptotic distribution
of the maximum likelihood estimator of the microergodic parameter associated to
GC covariance model, under fixed domain asymptotics. The last part focuses on
optimal prediction with GC model and specifically, we give conditions for
asymptotic efficiency prediction and asymptotically correct estimation of mean
square error using a misspecified GC, MT or GW model, under fixed domain
asymptotics. Our findings are illustrated through a simulation study: the first
compares the finite sample behavior of the maximum likelihood estimation of the
microergodic parameter of the GC model with the given asymptotic distribution.
We then compare the finite-sample behavior of the prediction and its associated
mean square error when the true model is GC and the prediction is performed
using the true model and a misspecified GW model.
</summary>
    <author>
      <name>Moreno Bevilacqua</name>
    </author>
    <author>
      <name>Tarik Faouzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1607.06921</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09957v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09957v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03813v1</id>
    <updated>2018-08-11T14:44:14Z</updated>
    <published>2018-08-11T14:44:14Z</published>
    <title>Bayesian Bivariate Subgroup Analysis for Risk-Benefit Evaluation</title>
    <summary>  Subgroup analysis is a frequently used tool for evaluating heterogeneity of
treatment effect and heterogeneity in treatment harm across observed baseline
patient characteristics. While treatment efficacy and adverse event measures
are often reported separately for each subgroup, analyzing their
within-subgroup joint distribution is critical for better informed patient
decision-making. In this paper, we describe Bayesian models for performing a
subgroup analysis to compare the joint occurrence of a primary endpoint and an
adverse event between two treatment arms. Our approaches emphasize estimation
of heterogeneity in this joint distribution across subgroups, and our
approaches directly accommodate subgroups with small numbers of observed
primary and adverse event combinations. In addition, we describe several ways
in which our models may be used to generate interpretable summary measures of
benefit-risk tradeoffs for each subgroup. The methods described here are
illustrated throughout using a large cardiovascular trial (N = 9,361)
investigating the efficacy of an intervention for reducing systolic blood
pressure to a lower-than-usual target.
</summary>
    <author>
      <name>Nicholas C. Henderson</name>
    </author>
    <author>
      <name>Ravi Varadhan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05500v2</id>
    <updated>2018-08-11T14:17:41Z</updated>
    <published>2018-06-14T12:29:46Z</published>
    <title>Statistical Aspects of Wasserstein Distances</title>
    <summary>  Wasserstein distances are metrics on probability distributions inspired by
the problem of optimal mass transportation. Roughly speaking, they measure the
minimal effort required to reconfigure the probability mass of one distribution
in order to recover the other distribution. They are ubiquitous in mathematics,
with a long history that has seen them catalyse core developments in analysis,
optimization, and probability. Beyond their intrinsic mathematical richness,
they possess attractive features that make them a versatile tool for the
statistician: they can be used to derive weak convergence and convergence of
moments, and can be easily bounded; they are well-adapted to quantify a natural
notion of perturbation of a probability distribution; and they seamlessly
incorporate the geometry of the domain of the distributions in question, thus
being useful for contrasting complex objects. Consequently, they frequently
appear in the development of statistical theory and inferential methodology,
and have recently become an object of inference in themselves. In this review,
we provide a snapshot of the main concepts involved in Wasserstein distances
and optimal transportation, and a succinct overview of some of their many
statistical aspects.
</summary>
    <author>
      <name>Victor M. Panaretos</name>
    </author>
    <author>
      <name>Yoav Zemel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Annual Review of Statistics and Its Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05500v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05500v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-00 (primary), 62G99, 62M99 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06673v2</id>
    <updated>2018-08-11T13:59:42Z</updated>
    <published>2018-03-18T15:01:22Z</published>
    <title>Damped Anderson acceleration with restarts and monotonicity control for
  accelerating EM and EM-like algorithms</title>
    <summary>  The expectation-maximization (EM) algorithm is a well-known iterative method
for computing maximum likelihood estimates from incomplete data. Despite its
numerous advantages, a main drawback of the EM algorithm is its frequently
observed slow convergence which often hinders the application of EM algorithms
in high-dimensional problems or in other complex settings.To address the need
for more rapidly convergent EM algorithms, we describe a new class of
acceleration schemes that build on the Anderson acceleration technique for
speeding fixed-point iterations. Our approach is effective at greatly
accelerating the convergence of EM algorithms and is automatically scalable to
high dimensional settings. Through the introduction of periodic algorithm
restarts and a damping factor, our acceleration scheme provides faster and more
robust convergence when compared to un-modified Anderson acceleration while
also improving global convergence. Crucially, our method works as an
"off-the-shelf" method in that it may be directly used to accelerate any EM
algorithm without relying on the use of any model-specific features or
insights. Through a series of simulation studies involving five representative
problems, we show that our algorithm is substantially faster than the existing
state-of-art acceleration schemes.
</summary>
    <author>
      <name>Nicholas C. Henderson</name>
    </author>
    <author>
      <name>Ravi Varadhan</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03786v1</id>
    <updated>2018-08-11T10:30:41Z</updated>
    <published>2018-08-11T10:30:41Z</published>
    <title>Improved Methods for Moment Restriction Models with Marginally
  Incompatible Data Combination and an Application to Two-sample Instrumental
  Variable Estimation</title>
    <summary>  Combining information from multiple samples is often needed in biomedical and
economic studies, but the differences between these samples must be
appropriately taken into account in the analysis of the combined data. We study
estimation for moment restriction models with data combination from two samples
under an ignorablility-type assumption but allowing for different marginal
distributions of common variables between the two samples. Suppose that an
outcome regression model and a propensity score model are specified. By
leveraging the semiparametric efficiency theory, we derive an augmented inverse
probability weighted (AIPW) estimator that is locally efficient and doubly
robust with respect to the outcome regression and propensity score models.
Furthermore, we develop calibrated regression and likelihood estimators that
are not only locally efficient and doubly robust, but also intrinsically
efficient in achieving smaller variances than the AIPW estimator when the
propensity score model is correctly specified but the outcome regression model
may be misspecified. As an important application, we study the two-sample
instrumental variable problem and derive the corresponding estimators while
allowing for incompatible distributions of common variables between the two
samples. Finally, we provide a simulation study and an econometric application
on public housing projects to demonstrate the superior performance of our
improved estimators.
</summary>
    <author>
      <name>Heng Shu</name>
    </author>
    <author>
      <name>Zhiqiang Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03750v1</id>
    <updated>2018-08-11T04:47:50Z</updated>
    <published>2018-08-11T04:47:50Z</published>
    <title>Identification and Bayesian inference for heterogeneous treatment
  effects under non-ignorable assignment condition</title>
    <summary>  We provide a sufficient condition for the identification of heterogeneous
treatment effects (HTE) where the missing mechanism is nonignorable and
information is available on the marginal distribution of the untreated outcome.
We also show that under such a condition, the same result holds for the
identification of average treatment effects (ATE). By exposing certain
additivity on the regression function of the assignment probability, we reduce
the identification of HTE to the uniqueness of a solution of some integral
equation, and we discuss this based on the idea borrowed from the
identification of nonparametric instrumental variable models. In addition, our
result is extended to relax several assumptions in data fusion. We propose a
quasi-Bayesian estimation method for HTE and examine its properties through a
simple simulation study.
</summary>
    <author>
      <name>Keisuke Takahata</name>
    </author>
    <author>
      <name>Takahiro Hoshino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first version of the manuscript is found at
  \url{https://ideas.repec.org/p/keo/dpaper/2018-005.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03692v1</id>
    <updated>2018-08-10T20:22:36Z</updated>
    <published>2018-08-10T20:22:36Z</published>
    <title>Estimation of natural indirect effects robust to unmeasured confounding
  and mediator measurement error</title>
    <summary>  The use of causal mediation analysis to evaluate the pathways by which an
exposure affects an outcome is widespread in the social and biomedical
sciences. Recent advances in this area have established formal conditions for
identification and estimation of natural direct and indirect effects. However,
these conditions typically involve stringent no unmeasured confounding
assumptions and that the mediator has been measured without error. These
assumptions may fail to hold in practice where mediation methods are often
applied. The goal of this paper is two-fold. First, we show that the natural
indirect effect can in fact be identified in the presence of unmeasured
exposure-outcome confounding provided there is no additive interaction between
the mediator and unmeasured confounder(s). Second, we introduce a new estimator
of the natural indirect effect that is robust to both classical measurement
error of the mediator and unmeasured confounding of both exposure-outcome and
mediator-outcome relations under certain no interaction assumptions. We provide
formal proofs and a simulation study to demonstrate our results.
</summary>
    <author>
      <name>Isabel R. Fulcher</name>
    </author>
    <author>
      <name>Xu Shi</name>
    </author>
    <author>
      <name>Eric J. Tchetgen Tchetgen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03662v1</id>
    <updated>2018-08-10T18:25:12Z</updated>
    <published>2018-08-10T18:25:12Z</published>
    <title>Multi-Channel Stochastic Variational Inference for the Joint Analysis of
  Heterogeneous Biomedical Data in Alzheimer's Disease</title>
    <summary>  The joint analysis of biomedical data in Alzheimer's Disease (AD) is
important for better clinical diagnosis and to understand the relationship
between biomarkers. However, jointly accounting for heterogeneous measures
poses important challenges related to the modeling of the variability and the
interpretability of the results. These issues are here addressed by proposing a
novel multi-channel stochastic generative model. We assume that a latent
variable generates the data observed through different channels (e.g., clinical
scores, imaging, ...) and describe an efficient way to estimate jointly the
distribution of both latent variable and data generative process. Experiments
on synthetic data show that the multi-channel formulation allows superior data
reconstruction as opposed to the single channel one. Moreover, the derived
lower bound of the model evidence represents a promising model selection
criterion. Experiments on AD data show that the model parameters can be used
for unsupervised patient stratification and for the joint interpretation of the
heterogeneous observations. Because of its general and flexible formulation, we
believe that the proposed method can find important applications as a general
data fusion technique.
</summary>
    <author>
      <name>Luigi Antelmi</name>
    </author>
    <author>
      <name>Nicholas Ayache</name>
    </author>
    <author>
      <name>Philippe Robert</name>
    </author>
    <author>
      <name>Marco Lorenzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for presentation at MLCN 2018 workshop, in Conjunction with
  MICCAI 2018, September 20, Granada, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.10179v3</id>
    <updated>2018-08-10T15:30:12Z</updated>
    <published>2017-06-30T13:02:14Z</published>
    <title>Lasso Meets Horseshoe</title>
    <summary>  The goal of this paper is to contrast and survey the major advances in two of
the most commonly used high-dimensional techniques, namely, the Lasso and
horseshoe regularization. Lasso is a gold standard for predictor selection
while horseshoe is a state-of-the-art Bayesian estimator for sparse signals.
Lasso is fast and scalable and uses convex optimization whilst the horseshoe is
non-convex. Our novel perspective focuses on three aspects: (i) theoretical
optimality in high dimensional inference for the Gaussian sparse model and
beyond, (ii) efficiency and scalability of computation and (iii) methodological
development and performance.
</summary>
    <author>
      <name>Anindya Bhadra</name>
    </author>
    <author>
      <name>Jyotishka Datta</name>
    </author>
    <author>
      <name>Nicholas G. Polson</name>
    </author>
    <author>
      <name>Brandon T. Willard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.10179v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10179v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62J07, 62J05, Secondary 62H15, 62F03" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08626v2</id>
    <updated>2018-08-10T12:38:20Z</updated>
    <published>2017-09-25T08:17:21Z</published>
    <title>A general framework for data-driven uncertainty quantification under
  complex input dependencies using vine copulas</title>
    <summary>  Systems subject to uncertain inputs produce uncertain responses. Uncertainty
quantification (UQ) deals with the estimation of statistics of the system
response, given a computational model of the system and a probabilistic model
of its inputs. In engineering applications it is common to assume that the
inputs are mutually independent or coupled by a Gaussian or elliptical
dependence structure (copula). In this paper we overcome such limitations by
modelling the dependence structure of multivariate inputs as vine copulas. Vine
copulas are models of multivariate dependence built from simpler pair-copulas.
The vine representation is flexible enough to capture complex dependencies.
This paper formalises the framework needed to build vine copula models of
multivariate inputs and to combine them with virtually any UQ method. The
framework allows for a fully automated, data-driven inference of the
probabilistic input model on available input data. The procedure is exemplified
on two finite element models of truss structures, both subject to inputs with
non-Gaussian dependence structures. For each case, we analyse the moments of
the model response (using polynomial chaos expansions), and perform a
structural reliability analysis to calculate the probability of failure of the
system (using the first order reliability method and importance sampling).
Reference solutions are obtained by Monte Carlo simulation. The results show
that, while the Gaussian assumption yields biased statistics, the vine copula
representation achieves significantly more precise estimates, even when its
structure needs to be fully inferred from a limited amount of observations.
</summary>
    <author>
      <name>E. Torre</name>
    </author>
    <author>
      <name>S. Marelli</name>
    </author>
    <author>
      <name>P. Embrechts</name>
    </author>
    <author>
      <name>B. Sudret</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08626v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08626v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00672v4</id>
    <updated>2018-08-10T03:39:13Z</updated>
    <published>2016-09-02T17:21:25Z</published>
    <title>The Inflation Technique for Causal Inference with Latent Variables</title>
    <summary>  The problem of causal inference is to determine if a given probability
distribution on observed variables is compatible with some causal structure.
The difficult case is when the causal structure includes latent variables. We
here introduce the $\textit{inflation technique}$ for tackling this problem. An
inflation of a causal structure is a new causal structure that can contain
multiple copies of each of the original variables, but where the ancestry of
each copy mirrors that of the original. To every distribution of the observed
variables that is compatible with the original causal structure, we assign a
family of marginal distributions on certain subsets of the copies that are
compatible with the inflated causal structure. It follows that compatibility
constraints for the inflation can be translated into compatibility constraints
for the original causal structure. Even if the constraints at the level of
inflation are weak, such as observable statistical independences implied by
disjoint causal ancestry, the translated constraints can be strong. We apply
this method to derive new inequalities whose violation by a distribution
witnesses that distribution's incompatibility with the causal structure (of
which Bell inequalities and Pearl's instrumental inequality are prominent
examples). We describe an algorithm for deriving all such inequalities for the
original causal structure that follow from ancestral independences in the
inflation. For three observed binary variables with pairwise common causes, it
yields inequalities that are stronger in at least some aspects than those
obtainable by existing methods. We also describe an algorithm that derives a
weaker set of inequalities but is more efficient. Finally, we discuss which
inflations are such that the inequalities one obtains from them remain valid
even for quantum (and post-quantum) generalizations of the notion of a causal
model.
</summary>
    <author>
      <name>Elie Wolfe</name>
    </author>
    <author>
      <name>Robert W. Spekkens</name>
    </author>
    <author>
      <name>Tobias Fritz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Notation and content organization have been improved so as to make
  the paper more accessible to non-physicists</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00672v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00672v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09375v2</id>
    <updated>2018-08-09T22:35:48Z</updated>
    <published>2017-06-28T17:35:51Z</published>
    <title>Multilayer Knockoff Filter: Controlled variable selection at multiple
  resolutions</title>
    <summary>  We tackle the problem of selecting from among a large number of variables
those that are 'important' for an outcome. We consider situations where groups
of variables are also of interest in their own right. For example, each
variable might be a genetic polymorphism and we might want to study how a trait
depends on variability in genes, segments of DNA that typically contain
multiple such polymorphisms. Or, variables might quantify various aspects of
the functioning of individual internet servers owned by a company, and we might
be interested in assessing the importance of each server as a whole on the
average download speed for the company's customers. In this context, to
discover that a variable is relevant for the outcome implies discovering that
the larger entity it represents is also important. To guarantee meaningful and
reproducible results, we suggest controlling the rate of false discoveries for
findings at the level of individual variables and at the level of groups.
Building on the knockoff construction of Barber and Candes (2015) and the
multilayer testing framework of Barber and Ramdas (2016), we introduce the
multilayer knockoff filter (MKF). We prove that MKF simultaneously controls the
FDR at each resolution and use simulations to show that it incurs little power
loss compared to methods that provide guarantees only for the discoveries of
individual variables. We apply MKF to analyze a genetic dataset and find that
it successfully reduces the number of false gene discoveries without a
significant reduction in power.
</summary>
    <author>
      <name>Eugene Katsevich</name>
    </author>
    <author>
      <name>Chiara Sabatti</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09375v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09375v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03364v1</id>
    <updated>2018-08-09T22:35:27Z</updated>
    <published>2018-08-09T22:35:27Z</published>
    <title>A Panel Quantile Approach to Attrition Bias in Big Data: Evidence from a
  Randomized Experiment</title>
    <summary>  This paper introduces a quantile regression estimator for panel data models
with individual heterogeneity and attrition. The method is motivated by the
fact that attrition bias is often encountered in Big Data applications. For
example, many users sign-up for the latest program but few remain active users
several months later, making the evaluation of such interventions inherently
very challenging. Building on earlier work by Hausman and Wise (1979), we
provide a simple identification strategy that leads to a two-step estimation
procedure. In the first step, the coefficients of interest in the selection
equation are consistently estimated using parametric or nonparametric methods.
In the second step, standard panel quantile methods are employed on a subset of
weighted observations. The estimator is computationally easy to implement in
Big Data applications with a large number of subjects. We investigate the
conditions under which the parameter estimator is asymptotically Gaussian and
we carry out a series of Monte Carlo simulations to investigate the finite
sample properties of the estimator. Lastly, using a simulation exercise, we
apply the method to the evaluation of a recent Time-of-Day electricity pricing
experiment inspired by the work of Aigner and Hausman (1980).
</summary>
    <author>
      <name>Matthew Harding</name>
    </author>
    <author>
      <name>Carlos Lamarche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">JEL: C21, C23, C25, C55. Keywords: Attrition; Big Data; Quantile
  regression; Individual Effects; Time-of-Day Pricing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02526v2</id>
    <updated>2018-08-09T20:24:31Z</updated>
    <published>2018-08-07T19:17:38Z</published>
    <title>Efficient and Effective $L_0$ Feature Selection</title>
    <summary>  Because of continuous advances in mathematical programing, Mix Integer
Optimization has become a competitive vis-a-vis popular regularization method
for selecting features in regression problems. The approach exhibits
unquestionable foundational appeal and versatility, but also poses important
challenges. We tackle these challenges, reducing computational burden when
tuning the sparsity bound (a parameter which is critical for effectiveness) and
improving performance in the presence of feature collinearity and of signals
that vary in nature and strength. Importantly, we render the approach efficient
and effective in applications of realistic size and complexity - without
resorting to relaxations or heuristics in the optimization, or abandoning
rigorous cross-validation tuning. Computational viability and improved
performance in subtler scenarios is achieved with a multi-pronged blueprint,
leveraging characteristics of the Mixed Integer Programming framework and by
means of whitening, a data pre-processing step.
</summary>
    <author>
      <name>Ana Kenney</name>
    </author>
    <author>
      <name>Francesca Chiaromonte</name>
    </author>
    <author>
      <name>Giovanni Felici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been presented at JSM 2018 (Vancouver, Canada), ISNPS
  2018 (Salerno, Italy), and various other conferences</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02526v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02526v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.6154v4</id>
    <updated>2018-08-09T18:26:55Z</updated>
    <published>2011-05-31T03:15:37Z</published>
    <title>Conditional Quantile Processes based on Series or Many Regressors</title>
    <summary>  Quantile regression (QR) is a principal regression method for analyzing the
impact of covariates on outcomes. The impact is described by the conditional
quantile function and its functionals. In this paper we develop the
nonparametric QR-series framework, covering many regressors as a special case,
for performing inference on the entire conditional quantile function and its
linear functionals. In this framework, we approximate the entire conditional
quantile function by a linear combination of series terms with
quantile-specific coefficients and estimate the function-valued coefficients
from the data. We develop large sample theory for the QR-series coefficient
process, namely we obtain uniform strong approximations to the QR-series
coefficient process by conditionally pivotal and Gaussian processes. Based on
these strong approximations, or couplings, we develop four resampling methods
(pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be
used for inference on the entire QR-series coefficient function.
  We apply these results to obtain estimation and inference methods for linear
functionals of the conditional quantile function, such as the conditional
quantile function itself, its partial derivatives, average partial derivatives,
and conditional average partial derivatives. Specifically, we obtain uniform
rates of convergence and show how to use the four resampling methods mentioned
above for inference on the functionals. All of the above results are for
function-valued parameters, holding uniformly in both the quantile index and
the covariate value, and covering the pointwise case as a by-product. We
demonstrate the practical utility of these results with an example, where we
estimate the price elasticity function and test the Slutsky condition of the
individual demand for gasoline, as indexed by the individual unobserved
propensity for gasoline consumption.
</summary>
    <author>
      <name>Alexandre Belloni</name>
    </author>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Denis Chetverikov</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">131 pages, 2 tables, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.6154v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.6154v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01589v2</id>
    <updated>2018-08-09T16:52:07Z</updated>
    <published>2017-09-05T20:44:32Z</published>
    <title>An active-learning algorithm that combines sparse polynomial chaos
  expansions and bootstrap for structural reliability analysis</title>
    <summary>  Polynomial chaos expansions (PCE) have seen widespread use in the context of
uncertainty quantification. However, their application to structural
reliability problems has been hindered by the limited performance of PCE in the
tails of the model response and due to the lack of local metamodel error
estimates. We propose a new method to provide local metamodel error estimates
based on bootstrap resampling and sparse PCE. An initial experimental design is
iteratively updated based on the current estimation of the limit-state surface
in an active learning algorithm. The greedy algorithm uses the bootstrap-based
local error estimates for the polynomial chaos predictor to identify the best
candidate set of points to enrich the experimental design. We demonstrate the
effectiveness of this approach on a well-known analytical benchmark
representing a series system, on a truss structure and on a complex realistic
frame structure problem.
</summary>
    <author>
      <name>S. Marelli</name>
    </author>
    <author>
      <name>B. Sudret</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.strusafe.2018.06.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.strusafe.2018.06.003" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Structural Safety, 75, pp. 67-74 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.01589v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01589v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03201v1</id>
    <updated>2018-08-09T15:37:14Z</updated>
    <published>2018-08-09T15:37:14Z</published>
    <title>A note on optimal design for hierarchical generalized group testing</title>
    <summary>  Choosing an optimal strategy for hierarchical group testing is an important
problem for practitioners who are interested in disease screening under limited
resources. For example, when screening for infection diseases in large
populations, it is important to use algorithms that minimize the cost of
potentially expensive assays. Black et al.(2015) described this as an
intractable problem unless the number of individuals to screen is small. They
proposed an approximation to an optimal strategy that is difficult to implement
for large population sizes. In this note, we develop an optimal design that can
be obtained using a novel dynamic programming algorithm. We show that this
algorithm is substantially more efficient than the approach proposed by Black
et al.(2015). The resulting algorithm is simple to implement and Matlab code is
presented for applied statistician.
</summary>
    <author>
      <name>Yaakov Malinovsky</name>
    </author>
    <author>
      <name>Paul S. Albert</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C39, 62P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05081v2</id>
    <updated>2018-08-09T11:46:52Z</updated>
    <published>2018-06-13T14:24:06Z</published>
    <title>LASSO-Driven Inference in Time and Space</title>
    <summary>  We consider the estimation and inference in a system of high-dimensional
regression equations allowing for temporal and cross-sectional dependency in
covariates and error processes, covering rather general forms of weak
dependence. A sequence of large-scale regressions with LASSO is applied to
reduce the dimensionality, and an overall penalty level is carefully chosen by
a block multiplier bootstrap procedure to account for multiplicity of the
equations and dependencies in the data. Correspondingly, oracle properties with
a jointly selected tuning parameter are derived. We further provide
high-quality de-biased simultaneous inference on the many target parameters of
the system. We provide bootstrap consistency results of the test procedure,
which are based on a general Bahadur representation for the $Z$-estimators with
dependent data. Simulations demonstrate good performance of the proposed
inference procedure. Finally, we apply the method to quantify spillover effects
of textual sentiment indices in a financial market and to test the
connectedness among sectors.
</summary>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Wolfgang K. Härdle</name>
    </author>
    <author>
      <name>Chen Huang</name>
    </author>
    <author>
      <name>Weining Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05188v3</id>
    <updated>2018-08-09T11:07:30Z</updated>
    <published>2017-11-14T16:53:27Z</published>
    <title>Weak convergence of Galerkin approximations for fractional elliptic
  stochastic PDEs with spatial white noise</title>
    <summary>  The numerical approximation of the solution to a stochastic partial
differential equation with additive spatial white noise on a bounded domain is
considered. The differential operator is assumed to be a fractional power of an
integer order elliptic differential operator. The solution is approximated by
means of a finite element discretization in space and a quadrature
approximation of an integral representation of the fractional inverse from the
Dunford-Taylor calculus.
  For the resulting approximation, a concise analysis of the weak error is
performed. Specifically, for the class of twice continuously Fr\'echet
differentiable functionals with second derivatives of polynomial growth, an
explicit rate of weak convergence is derived, and it is shown that the
component of the convergence rate stemming from the stochasticity is doubled
compared to the corresponding strong rate. Numerical experiments for different
functionals validate the theoretical results.
</summary>
    <author>
      <name>David Bolin</name>
    </author>
    <author>
      <name>Kristin Kirchner</name>
    </author>
    <author>
      <name>Mihály Kovács</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10543-018-0719-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10543-018-0719-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BIT Numerical Mathematics (2018).
  https://doi.org/10.1007/s10543-018-0719-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.05188v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05188v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35S15, 65C30, 65C60, 65N12, 65N30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03500v2</id>
    <updated>2018-08-09T08:00:46Z</updated>
    <published>2015-12-11T01:31:11Z</published>
    <title>Multi-threshold Accelerate Failure Time Model</title>
    <summary>  A two-stage procedure for simultaneously detecting multiple thresholds and
achieving model selection in the segmented accelerate failure time (AFT) model
is developed in this paper. In the first stage, we formulate the threshold
problem as a group model selection problem so that a concave 2-norm group
selection method can be applied. In the second stage, the thresholds are
finalized via a refining method. We establish the strong consistency of the
threshold estimates and regression coefficient estimates under some mild
technical conditions. The proposed procedure performs satisfactorily in our
extensive simulation studies. Its real world applicability is demonstrated via
analyzing a follicular lymphoma data.
</summary>
    <author>
      <name>Jialiang Li</name>
    </author>
    <author>
      <name>Baisuo Jin</name>
    </author>
    <link href="http://arxiv.org/abs/1512.03500v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03500v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05002v2</id>
    <updated>2018-08-09T06:05:40Z</updated>
    <published>2018-05-14T03:35:02Z</published>
    <title>How can the score test be consistent?</title>
    <summary>  The score test statistic using the observed information is easy to compute
numerically. Its large sample distribution under the null hypothesis is well
known and is equivalent to that of the score test based on the expected
information, the likelihood-ratio test and the Wald test. However, several
authors have noted that under the alternative this no longer holds and in
particular the statistic can take negative values. Here we examine the score
test using the observed information in the context of comparing two binomial
proportions under imperfect detection, a common problem in ecology when
studying occurrence of species. We demonstrate through a combination of
simulations and theoretical analysis that a new modified rule which we propose
that rejects the null hypothesis when the observed score statistic is larger
than the usual chi-square cut-off or is negative has power that is mostly
greater to any other test. In addition consistency is largely restored. Our new
test is easy to use and inference is always possible.
</summary>
    <author>
      <name>N. Karavarsamis</name>
    </author>
    <author>
      <name>G. Guillera-Arroita</name>
    </author>
    <author>
      <name>RM Huggins</name>
    </author>
    <author>
      <name>B J T Morgan</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.11354v2</id>
    <updated>2018-08-09T05:49:00Z</updated>
    <published>2018-03-30T06:22:54Z</published>
    <title>Two-stage approaches to the analysis of occupancy data II. The
  heterogeneous model and conditional likelihood</title>
    <summary>  Occupancy models involve both the probability a site is occupied and the
probability occupancy is detected. The homogeneous occupancy model, where the
occupancy and detection probabilities are the same at each site, admits an
orthogonal parameter transformation that yields a two-stage process to
calculate the maximum likelihood estimates so that it is not necessary to
simultaneously estimate the occupancy and detection probabilities. The
two-stage approach is examined here for the heterogeneous occupancy model where
the occupancy and detection probabilities now depend on covariates that may
vary between sites and over time. There is no longer an orthogonal
transformation but this approach effectively reduces the parameter space and
allows fuller use of the R functionality. This permits use of existing vector
generalised linear models methods to fit models for detection and allows the
development of an iterative weighted least squares approach to fit models for
occupancy. Efficiency is examined in a simulation study and the full maximum
likelihood and two-stage approaches are compared on several data sets.
</summary>
    <author>
      <name>N. Karavarsamis</name>
    </author>
    <author>
      <name>R. M. Huggins</name>
    </author>
    <link href="http://arxiv.org/abs/1803.11354v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11354v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02348v2</id>
    <updated>2018-08-09T03:36:34Z</updated>
    <published>2018-04-06T16:33:08Z</published>
    <title>Statistical inference for autoregressive models under heteroscedasticity
  of unknown form</title>
    <summary>  This paper provides an entire inference procedure for the autoregressive
model under (conditional) heteroscedasticity of unknown form with a finite
variance. We first establish the asymptotic normality of the weighted least
absolute deviations estimator (LADE) for the model. Second, we develop the
random weighting (RW) method to estimate its asymptotic covariance matrix,
leading to the implementation of the Wald test. Third, we construct a
portmanteau test for model checking, and use the RW method to obtain its
critical values. As a special weighted LADE, the feasible adaptive LADE (ALADE)
is proposed and proved to have the same efficiency as its infeasible
counterpart. The importance of our entire methodology based on the feasible
ALADE is illustrated by simulation results and the real data analysis on three
U.S. economic data sets.
</summary>
    <author>
      <name>Ke Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02348v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02348v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06988v2</id>
    <updated>2018-08-08T22:03:51Z</updated>
    <published>2017-04-23T21:51:54Z</published>
    <title>Ensemble Kalman methods for high-dimensional hierarchical dynamic
  space-time models</title>
    <summary>  We propose a new class of filtering and smoothing methods for inference in
high-dimensional, nonlinear, non-Gaussian, spatio-temporal state-space models.
The main idea is to combine the ensemble Kalman filter and smoother, developed
in the geophysics literature, with state-space algorithms from the statistics
literature. Our algorithms address a variety of estimation scenarios, including
on-line and off-line state and parameter estimation. We take a Bayesian
perspective, for which the goal is to generate samples from the joint posterior
distribution of states and parameters. The key benefit of our approach is the
use of ensemble Kalman methods for dimension reduction, which allows inference
for high-dimensional state vectors. We compare our methods to existing ones,
including ensemble Kalman filters, particle filters, and particle MCMC. Using a
real data example of cloud motion and data simulated under a number of
nonlinear and non-Gaussian scenarios, we show that our approaches outperform
these existing methods.
</summary>
    <author>
      <name>Matthias Katzfuss</name>
    </author>
    <author>
      <name>Jonathan R. Stroud</name>
    </author>
    <author>
      <name>Christopher K. Wikle</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06988v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06988v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00520v3</id>
    <updated>2018-08-08T13:57:47Z</updated>
    <published>2017-04-03T10:40:15Z</published>
    <title>Efficient acquisition rules for model-based approximate Bayesian
  computation</title>
    <summary>  Approximate Bayesian computation (ABC) is a method for Bayesian inference
when the likelihood is unavailable but simulating from the model is possible.
However, many ABC algorithms require a large number of simulations, which can
be costly. To reduce the computational cost, Bayesian optimisation (BO) and
surrogate models such as Gaussian processes have been proposed. Bayesian
optimisation enables one to intelligently decide where to evaluate the model
next but common BO strategies are not designed for the goal of estimating the
posterior distribution. Our paper addresses this gap in the literature. We
propose to compute the uncertainty in the ABC posterior density, which is due
to a lack of simulations to estimate this quantity accurately, and define a
loss function that measures this uncertainty. We then propose to select the
next evaluation location to minimise the expected loss. Experiments show that
the proposed method often produces the most accurate approximations as compared
to common BO strategies.
</summary>
    <author>
      <name>Marko Järvenpää</name>
    </author>
    <author>
      <name>Michael U. Gutmann</name>
    </author>
    <author>
      <name>Arijus Pleska</name>
    </author>
    <author>
      <name>Aki Vehtari</name>
    </author>
    <author>
      <name>Pekka Marttinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00520v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00520v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02671v1</id>
    <updated>2018-08-08T08:47:50Z</updated>
    <published>2018-08-08T08:47:50Z</published>
    <title>Testing heteroscedasticity for regression models based on projections</title>
    <summary>  In this paper we propose a new test of heteroscedasticity for parametric
regression models and partial linear regression models in high dimensional
settings. When the dimension of covariates is large, existing tests of
heteroscedasticity perform badly due to the \curse of dimensionality". To
attack this problem, we construct a test of heteroscedasticity by using a
projection-based empirical process. We study the asymptotic properties of the
test statistic under the null hypothesis and alternative hypotheses. It is
shown that the test can detect local alternatives departure from the null
hypothesis at the fastest possible rate in hypothesis testing. As the limiting
null distribution of the test statistic is not distribution free, we propose a
residual-based bootstrap. The validity of the bootstrap approximations is
investigated. We present some simulation results to show the finite sample
performances of the test. Two real data analyses are conducted for
illustration.
</summary>
    <author>
      <name>Falong Tan</name>
    </author>
    <author>
      <name>Xuejun Jiang</name>
    </author>
    <author>
      <name>Xu Guo</name>
    </author>
    <author>
      <name>Lixing Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01206v2</id>
    <updated>2018-08-08T06:27:50Z</updated>
    <published>2016-02-03T06:53:51Z</published>
    <title>denoiseR: A Package for Low Rank Matrix Estimation</title>
    <summary>  We introduce denoiseR, an R package that provides a unified implementation of
several state-of-the-art proposals for regularized low rank matrix estimation,
along with automatic selection of the regularization parameters. We also extend
these methods to allow for missing values. The regularization schemes discussed
in this paper are built around singular-value shrinkage and bootstrap-based
stability arguments. We illustrate how to use out package by applying it to
several real and simulated datasets, and highlight strengths and weaknesses of
the different implemented methods.
</summary>
    <author>
      <name>Julie Josse</name>
    </author>
    <author>
      <name>Sylvain Sardy</name>
    </author>
    <author>
      <name>Stefan Wager</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01206v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01206v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02635v1</id>
    <updated>2018-08-08T05:55:36Z</updated>
    <published>2018-08-08T05:55:36Z</published>
    <title>Reconciliation of probabilistic forecasts with an application to wind
  power</title>
    <summary>  New methods are proposed for adjusting probabilistic forecasts to ensure
coherence with the aggregation constraints inherent in temporal hierarchies.
The different approaches nested within this framework include methods that
exploit information at all levels of the hierarchy as well as a novel method
based on cross-validation. The methods are evaluated using real data from two
wind farms in Crete, an application where it is imperative for optimal
decisions related to grid operations and bidding strategies to be based on
coherent probabilistic forecasts of wind power. Empirical evidence is also
presented showing that probabilistic forecast reconciliation improves the
accuracy of both point forecasts and probabilistic forecasts.
</summary>
    <author>
      <name>Jooyoung Jeon</name>
    </author>
    <author>
      <name>Anastasios Panagiotelis</name>
    </author>
    <author>
      <name>Fotios Petropoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01481v2</id>
    <updated>2018-08-07T20:38:34Z</updated>
    <published>2017-08-04T13:09:31Z</published>
    <title>Multivariate Design of Experiments for Engineering Dimensional Analysis</title>
    <summary>  We consider the design of dimensional analysis experiments when there is more
than a single response. We first give a brief overview of dimensional analysis
experiments and the dimensional analysis (DA) procedure. The validity of the DA
method for univariate responses was established by the Buckingham $\Pi$-Theorem
in the early 20th century. We extend the theorem to the multivariate case,
develop basic criteria for multivariate design of DA and give guidelines for
design construction. Finally, we illustrate the construction of designs for DA
experiments for an example involving the design of a heat exchanger.
</summary>
    <author>
      <name>Daniel J. Eck</name>
    </author>
    <author>
      <name>Christopher J. Nachtsheim</name>
    </author>
    <author>
      <name>R. Dennis Cook</name>
    </author>
    <author>
      <name>Thomas A. Albrecht</name>
    </author>
    <link href="http://arxiv.org/abs/1708.01481v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01481v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01018v2</id>
    <updated>2018-08-07T18:00:38Z</updated>
    <published>2018-02-03T19:47:38Z</published>
    <title>Randomization Tests that Condition on Non-Categorical Covariate Balance</title>
    <summary>  A benefit of randomized experiments is that covariate distributions of
treatment and control groups are balanced on average, resulting in simple
unbiased estimators for treatment effects. However, it is possible that a
particular randomization yields covariate imbalances that researchers want to
address in the analysis stage through adjustment or other methods. Here we
present a randomization test that conditions on covariate balance by only
considering treatment assignments that are similar to the observed one in terms
of covariate balance. Previous conditional randomization tests have only
allowed for categorical covariates, while our randomization test allows for any
type of covariate. Through extensive simulation studies, we find that our
conditional randomization test can account for the threat to inference implied
by covariate imbalance and is more powerful than unconditional randomization
tests and other conditional tests. Furthermore, we find that our conditional
randomization test is similar to a randomization test that uses a
model-adjusted test statistic, suggesting a parallel between conditional
randomization-based inference and inference from statistical models such as
linear regression.
</summary>
    <author>
      <name>Zach Branson</name>
    </author>
    <author>
      <name>Luke Miratrix</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.01018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09724v3</id>
    <updated>2018-08-07T16:01:01Z</updated>
    <published>2016-10-30T22:57:43Z</published>
    <title>Likelihood Inference for Large Scale Stochastic Blockmodels with
  Covariates based on a Divide-and-Conquer Parallelizable Algorithm with
  Communication</title>
    <summary>  We consider a stochastic blockmodel equipped with node covariate information,
that is helpful in analyzing social network data. The key objective is to
obtain maximum likelihood estimates of the model parameters. For this task, we
devise a fast, scalable Monte Carlo EM type algorithm based on case-control
approximation of the log-likelihood coupled with a subsampling approach. A key
feature of the proposed algorithm is its parallelizability, by processing
portions of the data on several cores, while leveraging communication of key
statistics across the cores during each iteration of the algorithm. The
performance of the algorithm is evaluated on synthetic data sets and compared
with competing methods for blockmodel parameter estimation. We also illustrate
the model on data from a Facebook derived social network enhanced with node
covariate information.
</summary>
    <author>
      <name>Sandipan Roy</name>
    </author>
    <author>
      <name>Yves Atchadé</name>
    </author>
    <author>
      <name>George Michailidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09724v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09724v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02430v1</id>
    <updated>2018-08-07T15:52:18Z</updated>
    <published>2018-08-07T15:52:18Z</published>
    <title>Granger Causality Analysis Based on Quantized Minimum Error Entropy
  Criterion</title>
    <summary>  Linear regression model (LRM) based on mean square error (MSE) criterion is
widely used in Granger causality analysis (GCA), which is the most commonly
used method to detect the causality between a pair of time series. However,
when signals are seriously contaminated by non-Gaussian noises, the LRM
coefficients will be inaccurately identified. This may cause the GCA to detect
a wrong causal relationship. Minimum error entropy (MEE) criterion can be used
to replace the MSE criterion to deal with the non-Gaussian noises. But its
calculation requires a double summation operation, which brings computational
bottlenecks to GCA especially when sizes of the signals are large. To address
the aforementioned problems, in this study we propose a new method called GCA
based on the quantized MEE (QMEE) criterion (GCA-QMEE), in which the QMEE
criterion is applied to identify the LRM coefficients and the quantized error
entropy is used to calculate the causality indexes. Compared with the
traditional GCA, the proposed GCA-QMEE not only makes the results more
discriminative, but also more robust. Its computational complexity is also not
high because of the quantization operation. Illustrative examples on synthetic
and EEG datasets are provided to verify the desirable performance and the
availability of the GCA-QMEE.
</summary>
    <author>
      <name>Badong Chen</name>
    </author>
    <author>
      <name>Rongjin Ma</name>
    </author>
    <author>
      <name>Siyu Yu</name>
    </author>
    <author>
      <name>Shaoyi Du</name>
    </author>
    <author>
      <name>Jing Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02403v1</id>
    <updated>2018-08-07T14:39:24Z</updated>
    <published>2018-08-07T14:39:24Z</published>
    <title>Log-Contrast Regression with Functional Compositional Predictors:
  Linking Preterm Infant's Gut Microbiome Trajectories in Early Postnatal
  Period to Neurobehavioral Outcome</title>
    <summary>  When compositional data serve as predictors in regression, the log-contrast
model is commonly applied. A prominent feature of the model is that it complies
with the simplex geometry and enables the regression analysis to have various
desirable invariance properties. Motivated by the needs in understanding how
the trajectories of gut microbiome compositions during early postnatal stage
impact later neurobehavioral outcomes among preterm infants, we develop a
sparse log-contrast regression with functional compositional predictors. The
functional simplex structure is preserved by a set of zero-sum constraints on
the parameters, and the compositional predictors are allowed to have sparse,
smoothly varying, and accumulating effects on the outcome through time. Through
basis expansion, the problem boils down to a linearly constrained group lasso
regression, for which we develop an efficient augmented Lagrangian algorithm
and obtain theoretical performance guarantees. The proposed approach yields
interesting results in the preterm infant study. The identified microbiome
markers and the estimated time dynamics of their impact on the neurobehavioral
outcome shed lights on the functional linkage between stress accumulation in
early postnatal stage and neurodevelpomental process of infants.
</summary>
    <author>
      <name>Zhe Sun</name>
    </author>
    <author>
      <name>Wanli Xu</name>
    </author>
    <author>
      <name>Xiaomei Cong</name>
    </author>
    <author>
      <name>Kun Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J02" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09577v2</id>
    <updated>2018-08-07T11:32:11Z</updated>
    <published>2017-12-27T13:23:38Z</published>
    <title>Multivariate Extremes of a Random Number of Maxima</title>
    <summary>  The classical multivariate extreme-value theory concerns the modeling of
extremes in a multivariate random sample. The observations with large values in
at least one component is an example of this. The theory suggests the use of
max-stable distributions. In this work the classical theory is extended to the
case where aggregated data, such as maxima of a random number of observations,
are the actual interest. A new limit theorem concerning the domain of
attraction for the distribution of the aggregated data is derived, which boils
down to a new family of max-stable distributions. A practical implication of
our result is, for instance, the derivation of an approximation of the joint
upper tail probability for the aggregated data. The connection between the
extremal dependence structure of classical max-stable distributions and that of
our new family of max-stable distributions is established. By means of the
so-called inverse problem, a semiparametric composite-estimator for the
extremal dependence of the unobservable data is derived, starting from a
preliminary estimator of the extremal dependence obtained with the aggregated
data. The large-sample theory of the composite-estimator is developed and its
finite-sample performance is illustrated by means of a simulation study.
</summary>
    <author>
      <name>Enkelejd Hashorva</name>
    </author>
    <author>
      <name>Simone A. Padoan</name>
    </author>
    <author>
      <name>Stefano Rizzelli</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09577v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09577v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02193v1</id>
    <updated>2018-08-07T03:17:10Z</updated>
    <published>2018-08-07T03:17:10Z</published>
    <title>Generalized Integrative Principal Component Analysis for Multi-Type Data
  with Block-Wise Missing Structure</title>
    <summary>  High-dimensional multi-source data are encountered in many fields. Despite
recent developments on the integrative dimension reduction of such data, most
existing methods cannot easily accommodate data of multiple types (e.g., binary
or count-valued). Moreover, multi-source data often have block-wise missing
structure, i.e., data in one or more sources may be completely unobserved for a
sample. The heterogeneous data types and presence of block-wise missing data
pose significant challenges to the integration of multi-source data and further
statistical analyses. In this paper, we develop a low-rank method, called
Generalized Integrative Principal Component Analysis (GIPCA), for the
simultaneous dimension reduction and imputation of multi-source block-wise
missing data, where different sources may have different data types. We also
devise an adapted BIC criterion for rank estimation. Comprehensive simulation
studies demonstrate the efficacy of the proposed method in terms of rank
estimation, signal recovery, and missing data imputation. We apply GIPCA to a
mortality study. We achieve accurate block-wise missing data imputation and
identify intriguing latent mortality rate patterns with sociological relevance.
</summary>
    <author>
      <name>Huichen Zhu</name>
    </author>
    <author>
      <name>Gen Li</name>
    </author>
    <author>
      <name>Eric F. Lock</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03513v2</id>
    <updated>2018-08-06T20:33:25Z</updated>
    <published>2017-01-12T21:43:10Z</published>
    <title>Nonparametric imputation by data depth</title>
    <summary>  We present single imputation method for missing values which borrows the idea
of data depth---a measure of centrality defined for an arbitrary point of a
space with respect to a probability distribution or data cloud. This consists
in iterative maximization of the depth of each observation with missing values,
and can be employed with any properly defined statistical depth function. For
each single iteration, imputation reverts to optimization of quadratic, linear,
or quasiconcave functions that are solved analytically by linear programming or
the Nelder-Mead method. As it accounts for the underlying data topology, the
procedure is distribution free, allows imputation close to the data geometry,
can make prediction in situations where local imputation (k-nearest neighbors,
random forest) cannot, and has attractive robustness and asymptotic properties
under elliptical symmetry. It is shown that a special case---when using the
Mahalanobis depth---has direct connection to well-known methods for the
multivariate normal model, such as iterated regression and regularized PCA. The
methodology is extended to multiple imputation for data stemming from an
elliptically symmetric distribution. Simulation and real data studies show good
results compared with existing popular alternatives. The method has been
implemented as an R-package. Supplementary materials for the article are
available online.
</summary>
    <author>
      <name>Pavlo Mozharovskyi</name>
    </author>
    <author>
      <name>Julie Josse</name>
    </author>
    <author>
      <name>Francois Husson</name>
    </author>
    <link href="http://arxiv.org/abs/1701.03513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02061v1</id>
    <updated>2018-08-06T18:36:12Z</updated>
    <published>2018-08-06T18:36:12Z</published>
    <title>Semblance: A Rank-Based Kernel on Probability Spaces for Niche Detection</title>
    <summary>  Kernel methods provide a principled approach for detecting nonlinear
relations using well understood linear algorithms. In exploratory data analyses
when the underlying structure of the data's probability space is unclear, the
choice of kernel is often arbitrary. Here, we present a novel kernel,
Semblance, on a probability feature space. The advantage of Semblance lies in
its distribution free formulation and its ability to detect niche features by
placing greater emphasis on similarity between observation pairs that fall at
the tail ends of a distribution, as opposed to those that fall towards the
mean. We prove that Semblance is a valid Mercer kernel and illustrate its
applicability through simulations and real world examples.
</summary>
    <author>
      <name>Divyansh Agarwal</name>
    </author>
    <author>
      <name>Nancy Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Figures, 4 Supplementary Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02691v2</id>
    <updated>2018-08-06T16:06:38Z</updated>
    <published>2018-02-08T02:14:50Z</published>
    <title>A Bayesian Approach to Multi-State Hidden Markov Models: Application to
  Dementia Progression</title>
    <summary>  People are living longer than ever before, and with this arises new
complications and challenges for humanity. Among the most pressing of these
challenges is of understanding the role of aging in the development of
dementia. This paper is motivated by the Mayo Clinic Study of Aging data for
4742 subjects since 2004, and how it can be used to draw inference on the role
of aging in the development of dementia. We construct a hidden Markov model
(HMM) to represent progression of dementia from states associated with the
buildup of amyloid plaque in the brain, and the loss of cortical thickness. A
hierarchical Bayesian approach is taken to estimate the parameters of the HMM
with a truly time-inhomogeneous infinitesimal generator matrix, and response
functions of the continuous-valued biomarker measurements are cut-point
agnostic. A Bayesian approach with these features could be useful in many
disease progression models. Additionally, an approach is illustrated for
correcting a common bias in delayed enrollment studies, in which some or all
subjects are not observed at baseline. Standard software is incapable of
accounting for this critical feature, so code to perform the estimation of the
model described below is made available online.
</summary>
    <author>
      <name>Jonathan P Williams</name>
    </author>
    <author>
      <name>Curtis B Storlie</name>
    </author>
    <author>
      <name>Terry M Therneau</name>
    </author>
    <author>
      <name>Clifford R Jack Jr</name>
    </author>
    <author>
      <name>Jan Hannig</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02691v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02691v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05591v3</id>
    <updated>2018-08-06T13:43:23Z</updated>
    <published>2018-04-16T10:16:48Z</published>
    <title>Separating diffuse from point-like sources - a Bayesian approach</title>
    <summary>  We present the starblade algorithm, a method to separate superimposed point
sources from auto-correlated, diffuse flux using a Bayesian model. Point
sources are assumed to be independent from each other and to follow a power-law
brightness distribution. The diffuse emission is described as a non-parametric
log-normal model with a priori unknown correlation structure. This model
enforces positivity of the underlying emission and allows for variation in the
order of magnitudes. The correlation structure is recovered non-parametrically
in addition to the diffuse flux and is used for the separation of the point
sources. Additionally many measurement artifacts appear as point-like or
quasi-point-like effects, not compatible with superimposed diffuse emission. An
estimate of the separation uncertainty can be provided as well. We demonstrate
the capabilities of the derived method on synthetic data and data obtained by
the Hubble Space Telescope, emphasizing its effect on instrumental artifacts as
well as physical sources. The performance of this method is compared to the
background estimation of the SExtractor method, as well as to a denoising
auto-encoder.
</summary>
    <author>
      <name>Jakob Knollmüller</name>
    </author>
    <author>
      <name>Philipp Frank</name>
    </author>
    <author>
      <name>Torsten A. Enßlin</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05591v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05591v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01749v1</id>
    <updated>2018-08-06T07:07:21Z</updated>
    <published>2018-08-06T07:07:21Z</published>
    <title>Regularized matrix data clustering and its application to image analysis</title>
    <summary>  In this paper, we propose a regularized mixture probabilistic model to
cluster matrix data and apply it to brain signals. The approach is able to
capture the sparsity (low rank, small/zero values) of the original signals by
introducing regularization terms into the likelihood function. Through a
modified EM algorithm, our method achieves the optimal solution with low
computational cost. Theoretical results are also provided to establish the
consistency of the proposed estimators. Simulations show the advantages of the
proposed method over other existing methods. We also apply the approach to two
real datasets from different experiments. Promising results imply that the
proposed method successfully characterizes signals with different patterns
while yielding insightful scientific interpretation.
</summary>
    <author>
      <name>Xu Gao</name>
    </author>
    <author>
      <name>Weining Shen</name>
    </author>
    <author>
      <name>Hernando Ombao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05041v2</id>
    <updated>2018-08-05T21:27:04Z</updated>
    <published>2018-01-15T21:49:20Z</published>
    <title>Panel Data Quantile Regression with Grouped Fixed Effects</title>
    <summary>  This paper introduces estimation methods for grouped latent heterogeneity in
panel data quantile regression. We assume that the observed individuals come
from a heterogeneous population with a finite number of types. The number of
types and group membership is not assumed to be known in advance and is
estimated by means of a convex optimization problem. We provide conditions
under which group membership is estimated consistently and establish asymptotic
normality of the resulting estimators. Simulations show that the method works
well in finite samples when T is reasonably large. To illustrate the proposed
methodology we study the effects of the adoption of Right-to-Carry concealed
weapon laws on violent crime rates using panel data of 51 U.S. states from 1977
- 2010.
</summary>
    <author>
      <name>Jiaying Gu</name>
    </author>
    <author>
      <name>Stanislav Volgushev</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05041v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05041v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01665v1</id>
    <updated>2018-08-05T18:16:54Z</updated>
    <published>2018-08-05T18:16:54Z</published>
    <title>Diffusion approximations and control variates for MCMC</title>
    <summary>  A new methodology is presented for the construction of control variates to
reduce the variance of additive functionals of Markov Chain Monte Carlo (MCMC)
samplers. Our control variates are defined as linear combinations of functions
whose coefficients are obtained by minimizing a proxy for the asymptotic
variance. The construction is theoretically justified by two new results. We
first show that the asymptotic variances of some well-known MCMC algorithms,
including the Random Walk Metropolis and the (Metropolis) Unadjusted/Adjusted
Langevin Algorithm, are close to the asymptotic variance of the Langevin
diffusion. Second, we provide an explicit representation of the optimal
coefficients minimizing the asymptotic variance of the Langevin diffusion.
Several examples of Bayesian inference problems demonstrate that the
corresponding reduction in the variance is significant, and that in some cases
it can be dramatic.
</summary>
    <author>
      <name>Nicolas Brosse</name>
    </author>
    <author>
      <name>Alain Durmus</name>
    </author>
    <author>
      <name>Sean Meyn</name>
    </author>
    <author>
      <name>Eric Moulines</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01647v1</id>
    <updated>2018-08-05T16:37:04Z</updated>
    <published>2018-08-05T16:37:04Z</published>
    <title>Inverse Conditional Probability Weighting with Clustered Data in Causal
  Inference</title>
    <summary>  Estimating the average treatment causal effect in clustered data often
involves dealing with unmeasured cluster-specific confounding variables. Such
variables may be correlated with the measured unit covariates and outcome. When
the correlations are ignored, the causal effect estimation can be biased. By
utilizing sufficient statistics, we propose an inverse conditional probability
weighting (ICPW) method, which is robust to both (i) the correlation between
the unmeasured cluster-specific confounding variable and the covariates and
(ii) the correlation between the unmeasured cluster-specific confounding
variable and the outcome. Assumptions and conditions for the ICPW method are
presented. We establish the asymptotic properties of the proposed estimators.
Simulation studies and a case study are presented for illustration.
</summary>
    <author>
      <name>Zhulin He</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11641v2</id>
    <updated>2018-08-05T15:31:01Z</updated>
    <published>2018-07-31T02:50:47Z</published>
    <title>Adaptive Non-Parametric Regression With the $K$-NN Fused Lasso</title>
    <summary>  The fused lasso, also known as total-variation denoising, is a
locally-adaptive function estimator over a regular grid of design points. In
this paper, we extend the fused lasso to settings in which the points do not
occur on a regular grid, leading to a new approach for non-parametric
regression. This approach, which we call the $K$-nearest neighbors ($K$-NN)
fused lasso, involves (i) computing the $K$-NN graph of the design points; and
(ii) performing the fused lasso over this $K$-NN graph. We show that this
procedure has a number of theoretical advantages over competing approaches:
specifically, it inherits local adaptivity from its connection to the fused
lasso, and it inherits manifold adaptivity from its connection to the $K$-NN
approach. We show that excellent results are obtained in a simulation study and
on an application to flu data. For completeness, we also study an estimator
that makes use of an $\epsilon$-graph rather than a $K$-NN graph, and contrast
this with the $K$-NN fused lasso.
</summary>
    <author>
      <name>Oscar Hernan Madrid Padilla</name>
    </author>
    <author>
      <name>James Sharpnack</name>
    </author>
    <author>
      <name>Yanzhen Chen</name>
    </author>
    <author>
      <name>Daniela M. Witten</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01557v1</id>
    <updated>2018-08-05T03:55:40Z</updated>
    <published>2018-08-05T03:55:40Z</published>
    <title>A hierarchical independent component analysis model for longitudinal
  Neuroimaging studies</title>
    <summary>  In recent years, longitudinal neuroimaging study has become increasingly
popular in neuroscience research to investigate disease-related changes in
brain functions. In current neuroscience literature, one of the most commonly
used tools to extract and characterize brain functional networks is independent
component analysis (ICA). However, existing ICA methods are not suited for
modelling repeatedly measured imaging data. In this paper, we propose a novel
longitudinal independent component model (L-ICA) which provides a formal
modeling framework for extending ICA to longitudinal studies. By incorporating
subject-specific random effects and visit-specific covariate effects, L-ICA is
able to provide more accurate estimates of changes in brain functional networks
on both the population- and individual-level, borrow information across
repeated scans within the same subject to increase statistical power in
detecting covariate effects on the networks, and allow for model-based
prediction for brain networks changes caused by disease progression, treatment
or neurodevelopment. We develop a fully traceable exact EM algorithm to obtain
maximum likelihood estimates of L-ICA. We further develop a subspace-based
approximate EM algorithm which greatly reduce the computation time while still
retaining high accuracy. Moreover, we present a statistical testing procedure
for examining covariate effects on brain network changes. Simulation results
demonstrate the advantages of our proposed methods. We apply L-ICA to ADNI2
study to investigate changes in brain functional networks in Alzheimer disease.
Results from the L-ICA provide biologically insightful findings which are not
revealed using existing methods.
</summary>
    <author>
      <name>Yikai Wang</name>
    </author>
    <author>
      <name>Ying Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 13 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06221v2</id>
    <updated>2018-08-05T02:15:32Z</updated>
    <published>2016-11-18T20:54:03Z</published>
    <title>Theoretical Aspects of Cyclic Structural Causal Models</title>
    <summary>  Structural causal models (SCMs), also known as (non-parametric) structural
equation models (SEMs), are widely used for causal modeling purposes. A large
body of theoretical results is available for the special case in which cycles
are absent (i.e., acyclic SCMs, also known as recursive SEMs). However, in many
application domains cycles are abundantly present, for example in the form of
feedback loops. In this paper, we provide a general and rigorous theory of
cyclic SCMs. The paper consists of two parts: the first part gives a rigorous
treatment of structural causal models, dealing with measure-theoretic and other
complications that arise in the presence of cycles. In contrast with the
acyclic case, in cyclic SCMs solutions may no longer exist, or if they exist,
they may no longer be unique, or even measurable in general. We give several
sufficient and necessary conditions for the existence of (unique) measurable
solutions. We show how causal reasoning proceeds in these models and how this
differs from the acyclic case. Moreover, we give an overview of the Markov
properties that hold for cyclic SCMs. In the second part, we address the
question of how one can marginalize an SCM (possibly with cycles) to a subset
of the endogenous variables. We show that under a certain condition, one can
effectively remove a subset of the endogenous variables from the model, leading
to a more parsimonious marginal SCM that preserves the causal and
counterfactual semantics of the original SCM on the remaining variables.
Moreover, we show how the marginalization relates to the latent projection and
to latent confounders, i.e. latent common causes.
</summary>
    <author>
      <name>Stephan Bongers</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <author>
      <name>Joris M. Mooij</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Will probably be submitted to The Annals of Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04924v2</id>
    <updated>2018-08-04T21:40:05Z</updated>
    <published>2018-05-13T18:38:51Z</published>
    <title>Emergence and Evolution of Hierarchical Structure in Complex Systems</title>
    <summary>  It is well known that many complex systems, both in technology and nature,
exhibit hierarchical modularity: smaller modules, each of them providing a
certain function, are used within larger modules that perform more complex
functions. What is not well understood however is how this hierarchical
structure (which is fundamentally a network property) emerges, and how it
evolves over time. We propose a modeling framework, referred to as Evo-Lexis,
that provides insight to some fundamental questions about evolving hierarchical
systems. Evo-Lexis models the most elementary modules of the system as symbols
("sources") and the modules at the highest level of the hierarchy as sequences
of those symbols ("targets"). Evo-Lexis computes the optimized adjustment of a
given hierarchy when the set of targets changes over time by additions and
removals (a process referred to as "incremental design"). In this paper we use
computation modeling to show that:
  - Low-cost and deep hierarchies emerge when the population of target
sequences evolves through tinkering and mutation. - Strong selection on the
cost of new candidate targets results in reuse of more complex (longer) nodes
in an optimized hierarchy. - The bias towards reuse of complex nodes results in
an "hourglass architecture" (i.e., few intermediate nodes that cover almost all
source-target paths). - With such bias, the core nodes are conserved for
relatively long time periods although still being vulnerable to major
transitions and punctuated equilibria. - Finally, we analyze the differences in
terms of cost and structure between incrementally designed hierarchies and the
corresponding "clean-slate" hierarchies which result when the system is
designed from scratch after a change.
</summary>
    <author>
      <name>Payam Siyari</name>
    </author>
    <author>
      <name>Bistra Dilkina</name>
    </author>
    <author>
      <name>Constantine Dovrolis</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05725v5</id>
    <updated>2018-08-04T20:00:05Z</updated>
    <published>2018-01-17T16:06:12Z</published>
    <title>Bayesian Estimation of Gaussian Graphical Models with Predictive
  Covariance Selection</title>
    <summary>  Gaussian graphical models are used for determining conditional relationships
between variables. This is accomplished by identifying off-diagonal elements in
the inverse-covariance matrix that are non-zero. When the ratio of variables
(p) to observations (n) approaches one, the maximum likelihood estimator of the
covariance matrix becomes unstable and requires shrinkage estimation. Whereas
several classical (frequentist) methods have been introduced to address this
issue, fully Bayesian methods remain relatively uncommon in practice and
methodological literatures. Here we introduce a Bayesian method for estimating
sparse matrices, in which conditional relationships are determined with
projection predictive selection. With this method, that uses Kullback-Leibler
divergence and cross-validation for neighborhood selection, we reconstruct the
inverse-covariance matrix in both low and high-dimensional settings. Through
simulation and applied examples, we characterized performance compared to
several Bayesian methods and the graphical lasso, in addition to TIGER that
similarly estimates the inverse-covariance matrix with regression. Our results
demonstrate that projection predictive selection not only has superior
performance compared to selecting the most probable model and Bayesian model
averaging, particularly for high-dimensional data, but also compared to the the
Bayesian and classical glasso methods. Further, we show that estimating the
inverse-covariance matrix with multiple regression is often more accurate, with
respect to various loss functions, and efficient than direct estimation. In
low-dimensional settings, we demonstrate that projection predictive selection
also provides competitive performance. We have implemented the projection
predictive method for covariance selection in the R package GGMprojpred
</summary>
    <author>
      <name>Donald R. Williams</name>
    </author>
    <author>
      <name>Juho Piironen</name>
    </author>
    <author>
      <name>Aki Vehtari</name>
    </author>
    <author>
      <name>Philippe Rast</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05725v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05725v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04876v2</id>
    <updated>2018-08-04T15:16:31Z</updated>
    <published>2018-02-13T22:15:10Z</published>
    <title>Uncertainty Quantification for Online Learning and Stochastic
  Approximation via Hierarchical Incremental Gradient Descent</title>
    <summary>  Stochastic gradient descent (SGD) is an immensely popular approach for online
learning in settings where data arrives in a stream or data sizes are very
large. However, despite an ever- increasing volume of work on SGD, much less is
known about the statistical inferential properties of SGD-based predictions.
Taking a fully inferential viewpoint, this paper introduces a novel procedure
termed HiGrad to conduct statistical inference for online learning, without
incurring additional computational cost compared with SGD. The HiGrad procedure
begins by performing SGD updates for a while and then splits the single thread
into several threads, and this procedure hierarchically operates in this
fashion along each thread. With predictions provided by multiple threads in
place, a t-based confidence interval is constructed by decorrelating
predictions using covariance structures given by a Donsker-style extension of
the Ruppert--Polyak averaging scheme, which is a technical contribution of
independent interest. Under certain regularity conditions, the HiGrad
confidence interval is shown to attain asymptotically exact coverage
probability. Finally, the performance of HiGrad is evaluated through extensive
simulation studies and a real data example. An R package higrad has been
developed to implement the method.
</summary>
    <author>
      <name>Weijie J. Su</name>
    </author>
    <author>
      <name>Yuancheng Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Changed the title and polished writing. For more details, please
  visit the HiGrad webpage http://stat.wharton.upenn.edu/~suw/higrad</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.04876v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04876v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02771v4</id>
    <updated>2018-08-04T09:19:51Z</updated>
    <published>2017-04-10T09:20:47Z</published>
    <title>Group Importance Sampling for Particle Filtering and MCMC</title>
    <summary>  Bayesian methods and their implementations by means of sophisticated Monte
Carlo techniques have become very popular in signal processing over the last
years. Importance Sampling (IS) is a well-known Monte Carlo technique that
approximates integrals involving a posterior distribution by means of weighted
samples. In this work, we study the assignation of a single weighted sample
which compresses the information contained in a population of weighted samples.
Part of the theory that we present as Group Importance Sampling (GIS) has been
employed implicitly in different works in the literature. The provided analysis
yields several theoretical and practical consequences. For instance, we discuss
the application of GIS into the Sequential Importance Resampling framework and
show that Independent Multiple Try Metropolis schemes can be interpreted as a
standard Metropolis-Hastings algorithm, following the GIS approach. We also
introduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.
The first one, named Group Metropolis Sampling method, produces a Markov chain
of sets of weighted samples. All these sets are then employed for obtaining a
unique global estimator. The second one is the Distributed Particle
Metropolis-Hastings technique, where different parallel particle filters are
jointly used to drive an MCMC algorithm. Different resampled trajectories are
compared and then tested with a proper acceptance probability. The novel
schemes are tested in different numerical experiments such as learning the
hyperparameters of Gaussian Processes, two localization problems in a wireless
sensor network (with synthetic and real data) and the tracking of vegetation
parameters given satellite observations, where they are compared with several
benchmark Monte Carlo techniques. Three illustrative Matlab demos are also
provided.
</summary>
    <author>
      <name>L. Martino</name>
    </author>
    <author>
      <name>V. Elvira</name>
    </author>
    <author>
      <name>G. Camps-Valls</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Digital Signal Processing. Related Matlab demos are
  provided at https://github.com/lukafree/GIS.git</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02771v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02771v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01151v2</id>
    <updated>2018-08-04T08:28:32Z</updated>
    <published>2017-08-03T14:10:34Z</published>
    <title>Robust causal structure learning with some hidden variables</title>
    <summary>  We introduce a new method to estimate the Markov equivalence class of a
directed acyclic graph (DAG) in the presence of hidden variables, in settings
where the underlying DAG among the observed variables is sparse, and there are
a few hidden variables that have a direct effect on many of the observed ones.
Building on the so-called low rank plus sparse framework, we suggest a
two-stage approach which first removes the effect of the hidden variables, and
then estimates the Markov equivalence class of the underlying DAG under the
assumption that there are no remaining hidden variables. This approach is
consistent in certain high-dimensional regimes and performs favourably when
compared to the state of the art, both in terms of graphical structure recovery
and total causal effect estimation.
</summary>
    <author>
      <name>Benjamin Frot</name>
    </author>
    <author>
      <name>Preetam Nandy</name>
    </author>
    <author>
      <name>Marloes H. Maathuis</name>
    </author>
    <link href="http://arxiv.org/abs/1708.01151v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01151v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04340v3</id>
    <updated>2018-08-04T02:24:30Z</updated>
    <published>2016-09-14T16:45:09Z</published>
    <title>PSI (Ψ): a Private data Sharing Interface</title>
    <summary>  We provide an overview of PSI ("a Private data Sharing Interface"), a system
we are developing to enable researchers in the social sciences and other fields
to share and explore privacy-sensitive datasets with the strong privacy
protections of differential privacy.
</summary>
    <author>
      <name>Marco Gaboardi</name>
    </author>
    <author>
      <name>James Honaker</name>
    </author>
    <author>
      <name>Gary King</name>
    </author>
    <author>
      <name>Jack Murtagh</name>
    </author>
    <author>
      <name>Kobbi Nissim</name>
    </author>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <author>
      <name>Salil Vadhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04340v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04340v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01408v1</id>
    <updated>2018-08-04T01:58:17Z</updated>
    <published>2018-08-04T01:58:17Z</published>
    <title>Improved Estimation of Average Treatment Effects on the Treated: Local
  Efficiency, Double Robustness, and Beyond</title>
    <summary>  Estimation of average treatment effects on the treated (ATT) is an important
topic of causal inference in econometrics and statistics. This problem seems to
be often treated as a simple modification or extension of that of estimating
overall average treatment effects (ATE). However, the propensity score is no
longer ancillary for estimation of ATT, in contrast with estimation of ATE. In
this article, we review semiparametric theory for estimation of ATT and the use
of efficient influence functionsto derive augmented inverse probability
weighted (AIPW) estimators that are locally efficient and doubly robust.
Moreover, we discuss improved estimation over AIPW by developing calibrated
regression and likelihood estimators that are not only locally efficient and
doubly robust, but also intrinsically efficient in achieving smaller variances
than AIPW estimators when a propensity score model is correctly specified but
an outcome regression model may be misspecified. Finally, we present two
simulation studies and an econometric application to demonstrate the advantage
of the proposed methods when compared with existing methods.
</summary>
    <author>
      <name>Heng Shu</name>
    </author>
    <author>
      <name>Zhiqiang Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01332v1</id>
    <updated>2018-08-03T20:03:47Z</updated>
    <published>2018-08-03T20:03:47Z</published>
    <title>Constructing Stabilized Dynamic Treatment Regimes</title>
    <summary>  We propose a new method termed stabilized O-learning for deriving stabilized
dynamic treatment regimes, which are sequential decision rules for individual
patients that not only adapt over the course of the disease progression but
also remain consistent over time in format. The method provides a robust and
efficient learning framework for constructing dynamic treatment regimes by
directly optimizing a doubly robust estimator of the expected long-term
outcome. It can accommodate various types of outcomes, including continuous,
categorical and potentially censored survival outcomes. In addition, the method
is flexible enough to incorporate clinical preferences into a qualitatively
fixed rule, where the parameters indexing the decision rules that are shared
across stages can be estimated simultaneously. We conducted extensive
simulation studies, which demonstrated superior performance of the proposed
method. We analyzed data from the prospective Canary Prostate Cancer Active
Surveillance Study (PASS) using the proposed method.
</summary>
    <author>
      <name>Ying-Qi Zhao</name>
    </author>
    <author>
      <name>Ruoqing Zhu</name>
    </author>
    <author>
      <name>Guanhua Chen</name>
    </author>
    <author>
      <name>Yingye Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01240v1</id>
    <updated>2018-08-03T16:00:41Z</updated>
    <published>2018-08-03T16:00:41Z</published>
    <title>Joint estimation of conditional quantiles in multivariate linear
  regression models. An application to financial distress</title>
    <summary>  This paper proposes a maximum-likelihood approach to jointly estimate
marginal conditional quantiles of multivariate response variables in a linear
regression framework.
  We consider a slight reparameterization of the Multivariate Asymmetric
Laplace distribution proposed by Kotz et al (2001) and exploit its
location-scale mixture representation to implement a new EM algorithm for
estimating model parameters. The idea is to extend the link between the
Asymmetric Laplace distribution and the well-known univariate quantile
regression model to a multivariate context, i.e. when a multivariate dependent
variable is concerned. The approach accounts for association among multiple
responses and study how the relationship between responses and explanatory
variables can vary across different quantiles of the marginal conditional
distribution of the responses. A penalized version of the EM algorithm is also
presented to tackle the problem of variable selection. The validity of our
approach is analyzed in a simulation study, where we also provide evidence on
the efficiency gain of the proposed method compared to estimation obtained by
separate univariate quantile regressions. A real data application is finally
proposed to study the main determinants of financial distress in a sample of
Italian firms.
</summary>
    <author>
      <name>Lea Petrella</name>
    </author>
    <author>
      <name>Valentina Raponi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01236v1</id>
    <updated>2018-08-03T15:56:14Z</updated>
    <published>2018-08-03T15:56:14Z</published>
    <title>Bayesian Change Point Detection for Functional Data</title>
    <summary>  We propose a Bayesian method to detect change points for functional data. We
extract the features of a sequence of functional data by the discrete wavelet
transform (DWT), and treat each sequence of feature independently. We believe
there is potentially a change in each feature at possibly different time
points. The functional data evolves through such changes throughout the
sequences of observations. The change point for this sequence of functional
data is the cumulative effect of changes in all features. We assign the
features with priors which incorporate the characteristic of the wavelet
coefficients. Then we compute the posterior distribution of change point for
each sequence of feature, and define a matrix where each entry is a measure of
similarity between two functional data in this sequence. We compute the ratio
of the mean similarity between groups and within groups for all possible
partitions, and the change point is where the ratio reaches the minimum. We
demonstrate this method using a dataset on climate change.
</summary>
    <author>
      <name>Xiuqi Li</name>
    </author>
    <author>
      <name>Subhashis Ghosal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01217v1</id>
    <updated>2018-08-03T15:09:07Z</updated>
    <published>2018-08-03T15:09:07Z</published>
    <title>Sounding Spider: An Efficient Way for Representing Uncertainties in High
  Dimensions</title>
    <summary>  This article proposes a visualization method for multidimensional data based
on: (i) Animated functional Hypothetical Outcome Plots (f-HOPs); (ii)
3-dimensional Kiviat plot; and (iii) data sonification. In an Uncertainty
Quantification (UQ) framework, such analysis coupled with standard statistical
analysis tools such as Probability Density Functions (PDF) can be used to
augment the understanding of how the uncertainties in the numerical code inputs
translate into uncertainties in the quantity of interest (QoI).
  In contrast with static representation of most advanced techniques such as
functional Highest Density Region (HDR) boxplot or functional boxplot, f-HOPs
is a dynamic visualization that enables the practitioners to infer the dynamics
of the physics and enables to see functional correlations that may exist. While
this technique only allows to represent the QoI, we propose a 3-dimensional
version of the Kiviat plot to encode all input parameters. This new
visualization takes advantage of information from f-HOPs through data
sonification. All in all, this allows to analyse large datasets within a
high-dimensional parameter space and a functional QoI in the same canvas. The
proposed method is assessed and showed its benefits on two related
environmental datasets.
</summary>
    <author>
      <name>Pamphile T. Roy</name>
    </author>
    <author>
      <name>Sophie Ricci</name>
    </author>
    <author>
      <name>Bénédicte Cuenot</name>
    </author>
    <author>
      <name>Jean-Christophe Jouhaud</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01974v2</id>
    <updated>2018-08-03T05:35:22Z</updated>
    <published>2017-08-07T03:10:04Z</published>
    <title>Model Misspecification in ABC: Consequences and Diagnostics</title>
    <summary>  We analyze the behavior of approximate Bayesian computation (ABC) when the
model generating the simulated data differs from the actual data generating
process; i.e., when the data simulator in ABC is misspecified. We demonstrate
both theoretically and in simple, but practically relevant, examples that when
the model is misspecified different versions of ABC can lead to substantially
different results. Our theoretical results demonstrate that under regularity
conditions a version of the accept/reject ABC approach concentrates posterior
mass on an appropriately defined pseudo-true parameter value. However, under
model misspecification the ABC posterior does not yield credible sets with
valid frequentist coverage and has non-standard asymptotic behavior. We also
examine the theoretical behavior of the popular linear regression adjustment to
ABC under model misspecification and demonstrate that this approach
concentrates posterior mass on a completely different pseudo-true value than
that obtained by the accept/reject approach to ABC. Using our theoretical
results, we suggest two approaches to diagnose model misspecification in ABC.
All theoretical results and diagnostics are illustrated in a simple running
example.
</summary>
    <author>
      <name>David T. Frazier</name>
    </author>
    <author>
      <name>Christian P. Robert</name>
    </author>
    <author>
      <name>Judith Rousseau</name>
    </author>
    <link href="http://arxiv.org/abs/1708.01974v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01974v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04233v1</id>
    <updated>2018-08-02T16:47:03Z</updated>
    <published>2018-08-02T16:47:03Z</published>
    <title>Connecting Sharpe ratio and Student t-statistic, and beyond</title>
    <summary>  Sharpe ratio is widely used in asset management to compare and benchmark
funds and asset managers. It computes the ratio of the excess return over the
strategy standard deviation. However, the elements to compute the Sharpe ratio,
namely, the expected returns and the volatilities are unknown numbers and need
to be estimated statistically. This means that the Sharpe ratio used by funds
is subject to be error prone because of statistical estimation error. Lo
(2002), Mertens (2002) derive explicit expressions for the statistical
distribution of the Sharpe ratio using standard asymptotic theory under several
sets of assumptions (independent normally distributed - and identically
distributed returns). In this paper, we provide the exact distribution of the
Sharpe ratio for independent normally distributed return. In this case, the
Sharpe ratio statistic is up to a rescaling factor a non centered Student
distribution whose characteristics have been widely studied by statisticians.
The asymptotic behavior of our distribution provide the result of Lo (2002). We
also illustrate the fact that the empirical Sharpe ratio is asymptotically
optimal in the sense that it achieves the Cramer Rao bound. We then study the
empirical SR under AR(1) assumptions and investigate the effect of compounding
period on the Sharpe (computing the annual Sharpe with monthly data for
instance). We finally provide general formula in this case of
heteroscedasticity and autocorrelation.
</summary>
    <author>
      <name>Eric Benhamou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E10, 62E15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.11459v3</id>
    <updated>2018-08-02T14:55:34Z</updated>
    <published>2018-03-30T13:50:08Z</published>
    <title>Log-moment estimators for the generalized Linnik and Mittag-Leffler
  distributions with applications to financial modeling</title>
    <summary>  We propose formal estimation procedures for the parameters of the
generalized, three-parameter Linnik $gL(\alpha,\mu, \delta)$ and Mittag-Leffler
$gML(\alpha,\mu, \delta)$ distributions. The estimators are derived from the
moments of the log-transformed random variables, and are shown to be
asymptotically unbiased. The estimation algorithms are computationally
efficient and the proposed procedures are tested using the daily S\&amp;P 500 and
Dow Jones index data. The results show that the standard two-parameter Linnik
and Mittag-Leffler models are not flexible enough to accurately model the
current stock market data.
</summary>
    <author>
      <name>Dexter O. Cahoy</name>
    </author>
    <author>
      <name>Wojbor A. Woyczyński</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Mathematics and Statistics 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.11459v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11459v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03864v5</id>
    <updated>2018-08-02T14:37:31Z</updated>
    <published>2017-05-10T17:29:10Z</published>
    <title>A nested expectation-maximization algorithm for latent class models with
  covariates</title>
    <summary>  We develop a nested EM routine for latent class models with covariates which
allows maximization of the full-model log-likelihood and, differently from
current methods, guarantees monotone log-likelihood sequences along with
improved convergence rates.
</summary>
    <author>
      <name>Daniele Durante</name>
    </author>
    <author>
      <name>Antonio Canale</name>
    </author>
    <author>
      <name>Tommaso Rigon</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03864v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03864v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00135v3</id>
    <updated>2018-08-02T14:03:00Z</updated>
    <published>2018-06-30T07:51:30Z</published>
    <title>Robust functional regression based on principal components</title>
    <summary>  Functional data analysis is a fast evolving branch of modern statistics and
the functional linear model has become popular in recent years. However, most
estimation methods for this model rely on generalized least squares procedures
and therefore are sensitive to atypical observations. To remedy this, we
propose a two-step estimation procedure that combines robust functional
principal components and robust linear regression. Moreover, we propose a
transformation that reduces the curvature of the estimators and can be
advantageous in many settings. For these estimators we prove Fisher-consistency
at elliptical distributions and consistency under mild regularity conditions.
The influence function of the estimators is investigated as well. Simulation
experiments show that the proposed estimators have reasonable efficiency,
protect against outlying observations, produce smooth estimates and perform
well in comparison to existing approaches.
</summary>
    <author>
      <name>Ioannis Kalogridis</name>
    </author>
    <author>
      <name>Stefan Van Aelst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, including the appendix and references</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00135v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00135v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08157v4</id>
    <updated>2018-08-02T12:33:18Z</updated>
    <published>2017-08-28T00:37:38Z</published>
    <title>Characteristic and Universal Tensor Product Kernels</title>
    <summary>  Maximum mean discrepancy (MMD), also called energy distance or N-distance in
statistics and Hilbert-Schmidt independence criterion (HSIC), specifically
distance covariance in statistics, are among the most popular and successful
approaches to quantify the difference and independence of random variables,
respectively. Thanks to their kernel-based foundations, MMD and HSIC are
applicable on a wide variety of domains. Despite their tremendous success,
quite little is known about when HSIC characterizes independence and when MMD
with tensor product kernel can discriminate probability distributions. In this
paper, we answer these questions by studying various notions of characteristic
property of the tensor product kernel.
</summary>
    <author>
      <name>Zoltan Szabo</name>
    </author>
    <author>
      <name>Bharath K. Sriperumbudur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">final version appeared in JMLR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 18(233):1-29, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.08157v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08157v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46E22, 94A15, 62G10, 47B32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; H.1.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00723v1</id>
    <updated>2018-08-02T09:22:39Z</updated>
    <published>2018-08-02T09:22:39Z</published>
    <title>High-dimensional regression in practice: an empirical study of
  finite-sample prediction, variable selection and ranking</title>
    <summary>  Penalized likelihood methods are widely used for high-dimensional regression.
Although many methods have been proposed and the associated theory is now
well-developed, the relative efficacy of different methods in finite-sample
settings, as encountered in practice, remains incompletely understood. There is
therefore a need for empirical investigations in this area that can offer
practical insight and guidance to users of these methods. In this paper we
present a large-scale comparison of penalized regression methods. We
distinguish between three related goals: prediction, variable selection and
variable ranking. Our results span more than 1,800 data-generating scenarios,
allowing us to systematically consider the influence of various factors (sample
size, dimensionality, sparsity, signal strength and multicollinearity). We
consider several widely-used methods (Lasso, Elastic Net, Ridge Regression,
SCAD, the Dantzig Selector as well as Stability Selection). We find
considerable variation in performance between methods, with results dependent
on details of the data-generating scenario and the specific goal. Our results
support a `no panacea' view, with no unambiguous winner across all scenarios,
even in this restricted setting where all data align well with the assumptions
underlying the methods. Lasso is well-behaved, performing competitively in many
scenarios, while SCAD is highly variable. Substantial benefits from a
Ridge-penalty are only seen in the most challenging scenarios with strong
multi-collinearity. The results are supported by semi-synthetic analyzes using
gene expression data from cancer samples. Our empirical results complement
existing theory and provide a resource to compare methods across a range of
scenarios and metrics.
</summary>
    <author>
      <name>Fan Wang</name>
    </author>
    <author>
      <name>Sach Mukherjee</name>
    </author>
    <author>
      <name>Sylvia Richardson</name>
    </author>
    <author>
      <name>Steven M. Hill</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04157v3</id>
    <updated>2018-08-02T06:24:54Z</updated>
    <published>2017-03-12T18:29:03Z</published>
    <title>Using Aggregated Relational Data to feasibly identify network structure
  without network data</title>
    <summary>  Social network data is often prohibitively expensive to collect, limiting
empirical network research. Typical economic network mapping requires (1)
enumerating a census, (2) eliciting the names of all network links for each
individual, (3) matching the list of social connections to the census, and (4)
repeating (1)-(3) across many networks. In settings requiring field surveys,
steps (2)-(3) can be very expensive. In other network populations such as
financial intermediaries or high-risk groups, proprietary data and privacy
concerns may render (2)-(3) impossible. Both restrict the accessibility of
high-quality networks research to investigators with considerable resources.
  We propose an inexpensive and feasible strategy for network elicitation using
Aggregated Relational Data (ARD) -- responses to questions of the form "How
many of your social connections have trait k?" Our method uses ARD to recover
the parameters of a general network formation model, which in turn, permits the
estimation of any arbitrary node- or graph-level statistic. The method works
well in simulations and in matching a range of network characteristics in
real-world graphs from 75 Indian villages. Moreover, we replicate the results
of two field experiments that involved collecting network data. We show that
the researchers would have drawn similar conclusions using ARD alone. Finally,
using calculations from J-PAL fieldwork, we show that in rural India, for
example, ARD surveys are 80% cheaper than full network surveys.
</summary>
    <author>
      <name>Emily Breza</name>
    </author>
    <author>
      <name>Arun G. Chandrasekhar</name>
    </author>
    <author>
      <name>Tyler H. McCormick</name>
    </author>
    <author>
      <name>Mengjie Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04157v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04157v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00662v1</id>
    <updated>2018-08-02T04:24:54Z</updated>
    <published>2018-08-02T04:24:54Z</published>
    <title>Bayesian Classification of Multiclass Functional Data</title>
    <summary>  We propose a Bayesian approach to estimating parameters in multiclass
functional models. Unordered multinomial probit, ordered multinomial probit and
multinomial logistic models are considered. We use finite random series priors
based on a suitable basis such as B-splines in these three multinomial models,
and classify the functional data using the Bayes rule. We average over models
based on the marginal likelihood estimated from Markov Chain Monte Carlo (MCMC)
output. Posterior contraction rates for the three multinomial models are
computed. We also consider Bayesian linear and quadratic discriminant analyses
on the multivariate data obtained by applying a functional principal component
technique on the original functional data. A simulation study is conducted to
compare these methods on different types of data. We also apply these methods
to a phoneme dataset.
</summary>
    <author>
      <name>Xiuqi Li</name>
    </author>
    <author>
      <name>Subhashis Ghosal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00647v1</id>
    <updated>2018-08-02T03:03:36Z</updated>
    <published>2018-08-02T03:03:36Z</published>
    <title>Multi-threshold Change Plane Model: Estimation Theory and Applications
  in Subgroup Identification</title>
    <summary>  We propose a multi-threshold change plane regression model which naturally
partitions the observed subjects into subgroups with different covariate
effects. The underlying grouping variable is a linear function of covariates
and thus multiple thresholds form parallel change planes in the covariate
space. We contribute a novel 2-stage approach to estimate the number of
subgroups, the location of thresholds and all other regression parameters. In
the first stage we adopt a group selection principle to consistently identify
the number of subgroups, while in the second stage change point locations and
model parameter estimates are refined by a penalized induced smoothing
technique. Our procedure allows sparse solutions for relatively moderate- or
high-dimensional covariates. We further establish the asymptotic properties of
our proposed estimators under appropriate technical conditions. We evaluate the
performance of the proposed methods by simulation studies and provide
illustration using two medical data. Our proposal for subgroup identification
may lead to an immediate application in personalized medicine.
</summary>
    <author>
      <name>Jialiang Li</name>
    </author>
    <author>
      <name>Yaguang Li</name>
    </author>
    <author>
      <name>Baisuo Jin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04262v6</id>
    <updated>2018-08-01T19:05:16Z</updated>
    <published>2018-03-09T00:39:19Z</published>
    <title>On the Properties of MVR Chain Graphs</title>
    <summary>  Depending on the interpretation of the type of edges, a chain graph can
represent different relations between variables and thereby independence
models. Three interpretations, known by the acronyms LWF, MVR, and AMP, are
prevalent. Multivariate regression chain graphs (MVR CGs) were introduced by
Cox and Wermuth in 1993. We review Markov properties for MVR chain graphs and
propose an alternative global and local Markov property for them. Except for
pairwise Markov properties, we show that for MVR chain graphs all Markov
properties in the literature are equivalent for semi-graphoids. We derive a new
factorization formula for MVR chain graphs which is more explicit than and
different from the proposed factorizations for MVR chain graphs in the
literature. Finally, we provide a summary table comparing different features of
LWF, AMP, and MVR chain graphs.
</summary>
    <author>
      <name>Mohammad Ali Javidian</name>
    </author>
    <author>
      <name>Marco Valtorta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper submitted to a conference. arXiv admin
  note: text overlap with arXiv:0906.2098, arXiv:1406.6764 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04262v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04262v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00424v1</id>
    <updated>2018-08-01T17:09:10Z</updated>
    <published>2018-08-01T17:09:10Z</published>
    <title>Exploration and inference in spatial extremes using empirical basis
  functions</title>
    <summary>  Statistical methods for inference on spatial extremes of large datasets are
yet to be developed. Motivated by standard dimension reduction techniques used
in spatial statistics, we propose an approach based on empirical basis
functions to explore and model spatial extremal dependence. Based on a low-rank
max-stable model we propose a data-driven approach to estimate meaningful basis
functions using empirical pairwise extremal coefficients. These spatial
empirical basis functions can be used to visualize the main trends in extremal
dependence. In addition to exploratory analysis, we describe how these
functions can be used in a Bayesian hierarchical model to model spatial
extremes of large datasets. We illustrate our methods on extreme precipitations
in eastern U.S.
</summary>
    <author>
      <name>Samuel A. Morris</name>
    </author>
    <author>
      <name>Brian J. Reich</name>
    </author>
    <author>
      <name>Emeric Thibaud</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06457v2</id>
    <updated>2018-08-01T17:07:10Z</updated>
    <published>2017-12-18T15:11:48Z</published>
    <title>Discussion Paper: Should statistics rescue mathematical modelling?</title>
    <summary>  Statistics experiences a storm around the perceived misuse and possible abuse
of its methods in the context of the so-called reproducibility crisis. The
methods and styles of quantification practiced in mathematical modelling rarely
make it to the headlines, though modelling practitioners writing in
disciplinary journals flag a host of problems in the field. Technical, cultural
and ethical dimensions are simultaneously at play in the current predicaments
of both statistics and mathematical modelling. Since mathematical modelling is
not a discipline like statistics, its shortcomings risk remaining untreated
longer. We suggest that the tools of statistics and its disciplinary
organisation might offer a remedial contribution to mathematical modelling,
standardising methodologies and disseminating good practices. Statistics could
provide scientists and engineers from all disciplines with a point of anchorage
for sound modelling work. This is a vast and long-term undertaking. A step in
the proposed direction is offered here by focusing on the use of statistical
tools for quality assurance of mathematical models. By way of illustration,
techniques for uncertainty quantification, sensitivity analysis and sensitivity
auditing are suggested for incorporation in statistical syllabuses and
practices.
</summary>
    <author>
      <name>Andrea Saltelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Considerably reworked text from a previous work entitled "Does
  modelling need a Reformation? Ideas for a new grammar of modelling". The text
  now read as a discussion paper for a statistica journal on whether the
  discipline of statistics should take more interest in the methodologies for
  the quality control of mathematical modelling</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62A01, 97M10, 97M50, 97M60, 97M70" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00419v1</id>
    <updated>2018-08-01T17:01:29Z</updated>
    <published>2018-08-01T17:01:29Z</published>
    <title>Mixed effects models for healthcare longitudinal data with an
  informative visiting process: a Monte Carlo simulation study</title>
    <summary>  Electronic health records are being increasingly used in medical research to
answer more relevant and detailed clinical questions; however, they pose new
and significant methodological challenges. For instance, observation times are
likely correlated with the underlying disease severity: patients with worse
conditions utilise health care more and may have worse biomarker values
recorded. Traditional methods for analysing longitudinal data assume
independence between observation times and disease severity; yet, with
healthcare data such assumptions unlikely holds. Through Monte Carlo
simulation, we compare different analytical approaches proposed to account for
an informative visiting process to assess whether they lead to unbiased
results. Furthermore, we formalise a joint model for the observation process
and the longitudinal outcome within an extended joint modelling framework, and
we elicit formal causal considerations. We illustrate our results using data
from a pragmatic trial on enhanced care for individuals with chronic kidney
disease, and we introduce user-friendly software that can be used to fit the
joint model for the observation process and a longitudinal outcome.
</summary>
    <author>
      <name>Alessandro Gasparini</name>
    </author>
    <author>
      <name>Keith R. Abrams</name>
    </author>
    <author>
      <name>Jessica K. Barrett</name>
    </author>
    <author>
      <name>Rupert W. Major</name>
    </author>
    <author>
      <name>Michael J. Sweeting</name>
    </author>
    <author>
      <name>Nigel J. Brunskill</name>
    </author>
    <author>
      <name>Michael J. Crowther</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08862v3</id>
    <updated>2018-08-01T16:19:50Z</updated>
    <published>2018-04-24T06:53:43Z</published>
    <title>Composite Inference for Gaussian Processes</title>
    <summary>  Large-scale Gaussian process models are becoming increasingly important and
widely used in many areas, such as, computer experiments, stochastic
optimization via simulation, and machine learning using Gaussian processes. The
standard methods, such as maximum likelihood estimation (MLE) for parameter
estimation and the best linear unbiased predictor (BLUP) for prediction, are
generally the primary choices in many applications. In spite of their merits,
those methods are not feasible due to intractable computation when the sample
size is huge. A novel method for the purposes of parameter estimation and
prediction is proposed to solve the computational problems of large-scale
Gaussian process based models, by separating the original dataset into
tractable subsets. This method consistently combines parameter estimation and
prediction by making full use of the dependence among conditional densities: a
statistically efficient composite likelihood based on joint distributions of
some well selected conditional densities is developed to estimate parameters
and then "composite inference" is coined to make prediction for an unknown
input point, based on its distributions conditional on each block subset. The
proposed method transforms the intractable BLUP into a tractable convex
optimization problem. It is also shown that the prediction given by the
proposed method, called the best linear unbiased block predictor, has a minimum
variance for a given separation of the dataset.
  Keywords: Large scale, Parallel computing, Composite likelihood, Spatial
process
</summary>
    <author>
      <name>Yongxiang Li</name>
    </author>
    <author>
      <name>Qiang Zhou</name>
    </author>
    <author>
      <name>Kwok Leung Tsui</name>
    </author>
    <author>
      <name>Javier Cabrera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08862v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08862v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10016v2</id>
    <updated>2018-08-01T14:36:09Z</updated>
    <published>2017-11-27T21:56:37Z</published>
    <title>Bayesian model averaging via mixture model estimation</title>
    <summary>  A new approach for Bayesian model averaging (BMA) and selection is proposed,
based on the mixture model approach for hypothesis testing in Kaniav et al.,
2014. Inheriting from the good properties of this approach, it extends BMA to
cases where improper priors are chosen for parameters that are common to all
candidate models.
  From an algorithmic point of view, our approach consists in sampling from the
posterior distribution of the single-datum mixture of all candidate models,
weighted by their prior probabilities. We show that this posterior distribution
is equal to the 'Bayesian-model averaged' posterior distribution over all
candidate models, weighted by their posterior probability. From this BMA
posterior sample, a simple Monte-Carlo estimate of each model's posterior
probability is derived, as well as importance sampling estimates for
expectations under each model's posterior distribution.
</summary>
    <author>
      <name>Merlin Keller</name>
    </author>
    <author>
      <name>Kaniav Kamary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures, submission in preparation</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11908v2</id>
    <updated>2018-08-01T10:00:45Z</updated>
    <published>2018-05-30T11:42:44Z</published>
    <title>Who Learns Better Bayesian Network Structures: Constraint-Based,
  Score-based or Hybrid Algorithms?</title>
    <summary>  The literature groups algorithms to learn the structure of Bayesian networks
from data in three separate classes: constraint-based algorithms, which use
conditional independence tests to learn the dependence structure of the data;
score-based algorithms, which use goodness-of-fit scores as objective functions
to maximise; and hybrid algorithms that combine both approaches. Famously,
Cowell (2001) showed that algorithms in the first two classes learn the same
structures when the topological ordering of the network is known and we use
entropy to assess conditional independence and goodness of fit.
  In this paper we address the complementary question: how do these classes of
algorithms perform outside of the assumptions above? We approach this question
by recognising that structure learning is defined by the combination of a
statistical criterion and an algorithm that determines how the criterion is
applied to the data. Removing the confounding effect of different choices for
the statistical criterion, we find using both simulated and real-world data
that constraint-based algorithms do not appear to be more efficient or more
sensitive to errors than score-based algorithms; and that hybrid algorithms are
not faster or more accurate than constraint-based algorithms. This suggests
that commonly held beliefs on structure learning in the literature are strongly
influenced by the choice of particular statistical criteria rather than just
properties of the algorithms themselves.
</summary>
    <author>
      <name>Marco Scutari</name>
    </author>
    <author>
      <name>Catharina Elisabeth Graafland</name>
    </author>
    <author>
      <name>José Manuel Gutiérrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11908v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11908v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00242v1</id>
    <updated>2018-08-01T09:34:29Z</updated>
    <published>2018-08-01T09:34:29Z</published>
    <title>Wild Bootstrap based Confidence Bands for Multiplicative Hazards Models</title>
    <summary>  We propose new resampling-based approaches to construct asymptotically valid
time simultaneous confidence bands for cumulative hazard functions in
multi-state Cox models. In particular, we exemplify the methodology in detail
for the simple Cox model with time dependent covariates, where the data may be
subject to independent right-censoring or left-truncation. In extensive
simulations we investigate their finite sample behaviour. Finally, the methods
are utilized to analyze an empirical example.
</summary>
    <author>
      <name>Dennis Dobler</name>
    </author>
    <author>
      <name>Markus Pauly</name>
    </author>
    <author>
      <name>Thomas H. Scheike</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 figure, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62N99, 62J99, 60F05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00212v1</id>
    <updated>2018-08-01T08:00:33Z</updated>
    <published>2018-08-01T08:00:33Z</published>
    <title>Model selection by minimum description length: Lower-bound sample sizes
  for the Fisher information approximation</title>
    <summary>  The Fisher information approximation (FIA) is an implementation of the
minimum description length principle for model selection. Unlike information
criteria such as AIC or BIC, it has the advantage of taking the functional form
of a model into account. Unfortunately, FIA can be misleading in finite
samples, resulting in an inversion of the correct rank order of complexity
terms for competing models in the worst case. As a remedy, we propose a
lower-bound $N'$ for the sample size that suffices to preclude such errors. We
illustrate the approach using three examples from the family of multinomial
processing tree models.
</summary>
    <author>
      <name>Daniel W. Heck</name>
    </author>
    <author>
      <name>Morten Moshagen</name>
    </author>
    <author>
      <name>Edgar Erdfelder</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jmp.2014.06.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jmp.2014.06.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Mathematical Psychology (2014) 60, 29-34</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08575v4</id>
    <updated>2018-08-01T07:46:02Z</updated>
    <published>2015-11-27T07:39:05Z</published>
    <title>A Modified Multiple OLS (m$^2$OLS) Algorithm for Signal Recovery in
  Compressive Sensing</title>
    <summary>  Orthogonal least square (OLS) is an important sparse signal recovery
algorithm for compressive sensing, which enjoys superior probability of success
over other well-known recovery algorithms under conditions of correlated
measurement matrices. Multiple OLS (mOLS) is a recently proposed improved
version of OLS which selects multiple candidates per iteration by generalizing
the greedy selection principle used in OLS and enjoys faster convergence than
OLS. In this paper, we present a refined version of the mOLS algorithm where at
each step of the iteration, we first preselect a submatrix of the measurement
matrix suitably and then apply the mOLS computations to the chosen submatrix.
Since mOLS now works only on a submatrix and not on the overall matrix,
computations reduce drastically. Convergence of the algorithm, however,
requires ensuring passage of true candidates through the two stages of
preselection and mOLS based selection successively. This paper presents
convergence conditions for both noisy and noise free signal models. The
proposed algorithm enjoys faster convergence properties similar to mOLS, at a
much reduced computational complexity.
</summary>
    <author>
      <name>Samrat Mukhopadhyay</name>
    </author>
    <author>
      <name>Siddhartha Satpathi</name>
    </author>
    <author>
      <name>Mrityunjoy Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, journal, added new material, changed few
  figures, changed title, some minor changes in writing</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.08575v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08575v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10311v2</id>
    <updated>2018-08-01T07:03:41Z</updated>
    <published>2018-02-28T09:02:26Z</published>
    <title>Fast Maximum Likelihood estimation via Equilibrium Expectation for Large
  Network Data</title>
    <summary>  A major line of contemporary research on complex networks is based on the
development of statistical models that specify the local motifs associated with
macro-structural properties observed in actual networks. This statistical
approach becomes increasingly problematic as network size increases. In the
context of current research on efficient estimation of models for large network
data sets, we propose a fast algorithm for maximum likelihood estimation (MLE)
that afords a signifcant increase in the size of networks amenable to direct
empirical analysis. The algorithm we propose in this paper relies on properties
of Markov chains at equilibrium, and for this reason it is called equilibrium
expectation (EE). We demonstrate the performance of the EE algorithm in the
context of exponential random graphmodels (ERGMs) a family of statistical
models commonly used in empirical research based on network data observed at a
single period in time. Thus far, the lack of efcient computational strategies
has limited the empirical scope of ERGMs to relatively small networks with a
few thousand nodes. The approach we propose allows a dramatic increase in the
size of networks that may be analyzed using ERGMs. This is illustrated in an
analysis of several biological networks and one social network with 104,103
nodes
</summary>
    <author>
      <name>Maksym Byshkin</name>
    </author>
    <author>
      <name>Alex Stivala</name>
    </author>
    <author>
      <name>Antonietta Mira</name>
    </author>
    <author>
      <name>Garry Robins</name>
    </author>
    <author>
      <name>Alessandro Lomi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41598-018-29725-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41598-018-29725-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports | (2018) 8:11509
  https://www.nature.com/articles/s41598-018-29725-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.10311v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10311v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00634v4</id>
    <updated>2018-08-01T01:36:09Z</updated>
    <published>2015-11-02T19:12:05Z</published>
    <title>A Simple and Adaptive Dispersion Regression Model for Count Data</title>
    <summary>  Regression for count data is widely performed by models such as Poisson,
negative binomial (NB) and zero-inflated regression. A challenge often faced by
practitioners is the selection of the right model to take into account
dispersion, which typically occurs in count datasets. It is highly desirable to
have a unified model that can automatically adapt to the underlying dispersion
and that can be easily implemented in practice. In this paper, a discrete
Weibull regression model is shown to be able to adapt in a simple way to
different types of dispersions relative to Poisson regression: overdispersion,
underdispersion and covariate-specific dispersion. Maximum likelihood can be
used for efficient parameter estimation. The description of the model,
parameter inference and model diagnostics is accompanied by simulated and real
data analyses.
</summary>
    <author>
      <name>Hadeel S. Klakattawi</name>
    </author>
    <author>
      <name>Veronica Vinciotti</name>
    </author>
    <author>
      <name>Keming Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e20020142</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e20020142" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Entropy 20, no. 2 (2018): 142</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.00634v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00634v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11907v1</id>
    <updated>2018-07-31T16:33:08Z</updated>
    <published>2018-07-31T16:33:08Z</published>
    <title>Integrated Continuous-time Hidden Markov Models</title>
    <summary>  Motivated by applications in movement ecology, in this paper I propose a new
class of integrated continuous-time hidden Markov models in which each
observation depends on the underlying state of the process over the whole
interval since the previous observation, not only on its current state. I show
that under appropriate conditioning, such a model can be regarded as a
conventional hidden Markov model, enabling efficient evaluation of its
likelihood without sampling of its state sequence. This leads to an algorithm
for inference which is more efficient, and scales better with the number of
data, than existing methods. An application to animal movement data is given.
</summary>
    <author>
      <name>Paul G Blackwell</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09048v2</id>
    <updated>2018-07-31T15:10:28Z</updated>
    <published>2018-06-23T22:03:10Z</published>
    <title>A classification point-of-view about conditional Kendall's tau</title>
    <summary>  We show how the problem of estimating conditional Kendall's tau can be
rewritten as a classification task. Conditional Kendall's tau is a conditional
dependence parameter that is a characteristic of a given pair of random
variables. The goal is to predict whether the pair is concordant (value of $1$)
or discordant (value of $-1$) conditionally on some covariates. We prove the
consistency and the asymptotic normality of a family of penalized approximate
maximum likelihood estimators, including the equivalent of the logit and probit
regressions in our framework. Then, we detail specific algorithms adapting
usual machine learning techniques, including nearest neighbors, decision trees,
random forests and neural networks, to the setting of the estimation of
conditional Kendall's tau. A small simulation study compares their finite
sample properties. Finally, we apply all these estimators to a dataset of
European stock indices.
</summary>
    <author>
      <name>Alexis Derumigny</name>
    </author>
    <author>
      <name>Jean-David Fermanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09048v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09048v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10364v3</id>
    <updated>2018-07-31T14:00:02Z</updated>
    <published>2017-03-30T08:54:34Z</published>
    <title>Quantifying Uncertainty in Transdimensional Markov Chain Monte Carlo
  Using Discrete Markov Models</title>
    <summary>  Bayesian analysis often concerns an evaluation of models with different
dimensionality as is necessary in, for example, model selection or mixture
models. To facilitate this evaluation, transdimensional Markov chain Monte
Carlo (MCMC) relies on sampling a discrete indexing variable to estimate the
posterior model probabilities. However, little attention has been paid to the
precision of these estimates. If only few switches occur between the models in
the transdimensional MCMC output, precision may be low and assessment based on
the assumption of independent samples misleading. Here, we propose a new method
to estimate the precision based on the observed transition matrix of the
model-indexing variable. Assuming a first order Markov model, the method
samples from the posterior of the stationary distribution. This allows
assessment of the uncertainty in the estimated posterior model probabilities,
model ranks, and Bayes factors. Moreover, the method provides an estimate for
the effective sample size of the MCMC output. In two model-selection examples,
we show that the proposed approach provides a good assessment of the
uncertainty associated with the estimated posterior model probabilities.
</summary>
    <author>
      <name>Daniel W. Heck</name>
    </author>
    <author>
      <name>Antony M. Overstall</name>
    </author>
    <author>
      <name>Quentin F. Gronau</name>
    </author>
    <author>
      <name>Eric-Jan Wagenmakers</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11222-018-9828-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11222-018-9828-0" rel="related"/>
    <link href="http://arxiv.org/abs/1703.10364v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10364v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11796v1</id>
    <updated>2018-07-31T13:01:25Z</updated>
    <published>2018-07-31T13:01:25Z</published>
    <title>Bayesian Uncertainty Estimation Under Complex Sampling</title>
    <summary>  Multistage sampling designs utilized by federal statistical agencies are
typically constructed to maximize the efficiency of the target domain level
estimator (e.g., indexed by geographic area) within cost constraints to
administer survey instruments. Sampling designs are usually constructed to be
informative, whereby inclusion probabilities are correlated with the response
variable of interest to minimize the variance of the resulting estimator.
Multistage sampling designs may induce dependence between the sampled units;
for example, employment of a sampling step that selects geographically-indexed
clusters of units in order to efficiently manage the cost of collection. A data
analyst may use a sampling-weighted pseudo-posterior distribution to estimate
the population model on the observed sample. The dependence induced between
co-clustered units inflates the scale of the resulting pseudo-posterior
covariance matrix that has been shown to induce under coverage of the
credibility sets. While the pseudo-posterior distribution contracts on the true
population model parameters, we demonstrate that the scale and shape of the
asymptotic distributions are different between each of the MLE, the
pseudo-posterior and the MLE under simple random sampling. Motivated by the
different forms of the asymptotic covariance matrices and the within cluster
dependence, we devise a correction applied as a simple and fast post-processing
step to our MCMC draws from the pseudo-posterior distribution. Our updating
step projects the pseudo-posterior covariance matrix such that the nominal
coverage is approximately achieved with credibility sets that account for both
the distributions for population generation, $P_{\theta_0}$, and the
multistage, informative sampling, $P_{\nu}$. We demonstrate the efficacy of our
procedure on synthetic data and make an application to the National Survey on
Drug Use and Health.
</summary>
    <author>
      <name>Matthew R. Williams</name>
    </author>
    <author>
      <name>Terrance D. Savitsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62D05, 62F15, 62F12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07715v3</id>
    <updated>2018-07-30T21:43:04Z</updated>
    <published>2017-11-21T11:06:06Z</published>
    <title>Partially Observed Functional Data: The Case of Systematically Missing
  Parts</title>
    <summary>  New estimators for the mean and the covariance function for partially
observed functional data are proposed using a detour via the fundamental
theorem of calculus. The new estimators allow for a consistent estimation of
the mean and covariance function under specific violations of the
missing-completely-at-random assumption. The requirements of the estimation
procedure can be tested using a sequential multiple hypothesis test procedure.
An extensive simulation study compares the new estimators with the classical
estimators from the literature in different missing data scenarios. The
proposed methodology is motivated by the practical problem of estimating the
mean price curve in the German Control Reserve Market. In this auction market,
price curves are only partially observable and the underlying missing data
mechanism depends on systematic trading strategies which clearly violate the
missing-completely-at-random assumption. In contrast to the classical
estimators, the new estimators lead to useful estimates of the mean and
covariance functions. Supplementary materials are provided online.
</summary>
    <author>
      <name>Dominik Liebl</name>
    </author>
    <author>
      <name>Stefan Rameseder</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07715v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07715v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01635v2</id>
    <updated>2018-07-30T21:33:03Z</updated>
    <published>2018-07-04T15:19:19Z</published>
    <title>Randomization Inference for Peer Effects</title>
    <summary>  Many previous causal inference studies require no interference, that is, the
potential outcomes of a unit do not depend on the treatments of other units.
However, this no-interference assumption becomes unreasonable when a unit
interacts with other units in the same group or cluster. In a motivating
application, Peking University admits students through two channels: the
college entrance exam (also known as Gaokao) and recommendation (often based on
Olympiads in various subjects). The university randomly assigns students to
dorms, each of which hosts four students. Students within the same dorm live
together and have extensive interactions. Therefore, it is likely that peer
effects exist and the no-interference assumption does not hold. It is important
to understand peer effects, because they give useful guidance for future
roommate assignment to improve the performance of students. We define peer
effects using potential outcomes. We then propose a randomization-based
inference framework to study peer effects with arbitrary numbers of peers and
peer types. Our inferential procedure does not assume any parametric model on
the outcome distribution. Our analysis gives useful practical guidance for
policy makers of Peking University.
</summary>
    <author>
      <name>Xinran Li</name>
    </author>
    <author>
      <name>Peng Ding</name>
    </author>
    <author>
      <name>Qian Lin</name>
    </author>
    <author>
      <name>Dawei Yang</name>
    </author>
    <author>
      <name>Jun S. Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07518v3</id>
    <updated>2018-07-30T20:55:54Z</updated>
    <published>2017-11-20T19:53:23Z</published>
    <title>Treatment Effect Quantification for Time-to-event Endpoints --
  Estimands, Analysis Strategies, and beyond</title>
    <summary>  A draft addendum to ICH E9 has been released for public consultation in
August 2017. The addendum focuses on two topics particularly relevant for
randomized confirmatory clinical trials: estimands and sensitivity analyses.
The need to amend ICH E9 grew out of the realization of a lack of alignment
between the objectives of a clinical trial stated in the protocol and the
accompanying quantification of the "treatment effect" reported in a regulatory
submission. We embed time-to-event endpoints in the estimand framework, and
discuss how the four estimand attributes described in the addendum apply to
time-to-event endpoints. We point out that if the proportional hazards
assumption is not met, the estimand targeted by the most prevalent methods used
to analyze time-to-event endpoints, logrank test and Cox regression, depends on
the censoring distribution. We discuss for a large randomized clinical trial
how the analyses for the primary and secondary endpoints as well as the
sensitivity analyses actually performed in the trial can be seen in the context
of the addendum. To the best of our knowledge, this is the first attempt to do
so for a trial with a time-to-event endpoint. Questions that remain open with
the addendum for time-to-event endpoints and beyond are formulated, and
recommendations for planning of future trials are given. We hope that this will
provide a contribution to developing a common framework based on the final
version of the addendum that can be applied to design, protocols, statistical
analysis plans, and clinical study reports in the future.
</summary>
    <author>
      <name>Kaspar Rufibach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.07518v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07518v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11554v1</id>
    <updated>2018-07-30T20:14:07Z</updated>
    <published>2018-07-30T20:14:07Z</published>
    <title>Time-frequency transforms of white noises and Gaussian analytic
  functions</title>
    <summary>  A family of Gaussian analytic functions (GAFs) has recently been linked to
the Gabor transform of white Gaussian noise [Bardenet et al., 2017]. This
answered pioneering work by Flandrin [2015], who observed that the zeros of the
Gabor transform of white noise had a very regular distribution and proposed
filtering algorithms based on the zeros of a spectrogram. The mathematical link
with GAFs provides a wealth of probabilistic results to inform the design of
such signal processing procedures. In this paper, we study in a systematic way
the link between GAFs and a class of time-frequency transforms of Gaussian
white noises on Hilbert spaces of signals. Our main observation is a conceptual
correspondence between pairs (transform, GAF) and generating functions for
classical orthogonal polynomials. This correspondence covers some classical
time-frequency transforms, such as the Gabor transform and the Daubechies-Paul
analytic wavelet transform. It also unveils new windowed discrete Fourier
transforms, which map white noises to fundamental GAFs. All these transforms
may thus be of interest to the research program `filtering with zeros'. We also
identify the GAF whose zeros are the extrema of the Gabor transform of the
white noise and derive their first intensity. Moreover, we discuss important
subtleties in defining a white noise and its transform on infinite dimensional
Hilbert spaces. Finally, we provide quantitative estimates concerning the
finite-dimensional approximations of these white noises, which is of practical
interest when it comes to implementing signal processing algorithms based on
GAFs.
</summary>
    <author>
      <name>Rémi Bardenet</name>
    </author>
    <author>
      <name>Adrien Hardy</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11221v1</id>
    <updated>2018-07-30T08:17:54Z</updated>
    <published>2018-07-30T08:17:54Z</published>
    <title>Joint Estimation of Model and Observation Error Covariance Matrices in
  Data Assimilation: a Review</title>
    <summary>  This paper is a review of a crucial topic in data assimilation: the joint
estimation of model Q and observation R matrices. These covariances define the
observational and model errors via additive Gaussian white noises in
state-space models, the most common way of formulating data assimilation
problems. They are crucial because they control the relative weights of the
model forecasts and observations in reconstructing the state, and several
methods have been proposed since the 90's for their estimation. Some of them
are based on the moments of various innovations, including those in the
observation space or lag-innovations. Alternatively, other methods use
likelihood functions and maximum likelihood estimators or Bayesian approaches.
This review aims at providing a comprehensive summary of the proposed
methodologies and factually describing them as they appear in the literature.
We also discuss (i) remaining challenges for the different estimation methods,
(ii) some suggestions for possible improvements and combinations of the
approaches and (iii) perspectives for future works, in particular numerical
comparisons using toy-experiments and practical implementations in data
assimilation systems.
</summary>
    <author>
      <name>Pierre Tandeo</name>
    </author>
    <author>
      <name>Pierre Ailliot</name>
    </author>
    <author>
      <name>Marc Bocquet</name>
    </author>
    <author>
      <name>Alberto Carrassi</name>
    </author>
    <author>
      <name>Takemasa Miyoshi</name>
    </author>
    <author>
      <name>Manuel Pulido</name>
    </author>
    <author>
      <name>Yicun Zhen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The manuscript is being considered for publication at Monthly Weather
  Review</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09871v2</id>
    <updated>2018-07-30T08:04:17Z</updated>
    <published>2018-05-24T19:53:58Z</published>
    <title>Confidence region of singular vectors for high-dimensional and low-rank
  matrix regression</title>
    <summary>  Let ${\bf M}\in\mathbb{R}^{m_1\times m_2}$ be an unknown matrix with $r={\rm
rank}({\bf M})\ll \min(m_1,m_2)$ whose thin singular value decomposition is
denoted by ${\bf M}={\bf U}{\bf \Lambda}{\bf V}^{\top}$ where ${\bf
\Lambda}={\rm diag}(\lambda_1,\cdots,\lambda_r)$ contains its non-increasing
singular values. Low rank matrix regression refers to instances of estimating
${\bf M}$ from $n$ i.i.d. copies of random pair $\{({\bf X}, y)\}$ where ${\bf
X}\in\mathbb{R}^{m_1\times m_2}$ is a random measurement matrix and
$y\in\mathbb{R}$ is a noisy output satisfying $y={\rm tr}({\bf M}^{\top}{\bf
X})+\xi$ with $\xi$ being stochastic error independent of ${\bf X}$. The goal
of this paper is to construct efficient estimator (denoted by $\hat{\bf U}$ and
$\hat{\bf V}$) and confidence region of ${\bf U}$ and ${\bf V}$. In particular,
we characterize the distribution of $$ {\rm dist}^2\big[(\hat{\bf U},\hat{\bf
V}), ({\bf U},{\bf V})\big]=\|\hat{\bf U}\hat{\bf U}^{\top}-{\bf U}{\bf
U}^{\top}\|_{\rm F}^2+\|\hat{\bf V}\hat{\bf V}^{\top}-{\bf V}{\bf
V}^{\top}\|_{\rm F}^2. $$ We prove the asymptotic normality of properly
centered and normalized ${\rm dist}^2\big[(\hat{\bf U},\hat{\bf V}), ({\bf
U},{\bf V})\big]$ with data-dependent centering and normalization when
$r^{5/2}(m_1+m_2)^{3/2}=o(n/\log n)$, based on which confidence region of ${\bf
U}$ and ${\bf V}$ is constructed achieving any pre-determined confidence level
asymptotically.
</summary>
    <author>
      <name>Dong Xia</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09871v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09871v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11143v1</id>
    <updated>2018-07-30T02:21:07Z</updated>
    <published>2018-07-30T02:21:07Z</published>
    <title>ARM: Augment-REINFORCE-Merge Gradient for Discrete Latent Variable
  Models</title>
    <summary>  To backpropagate the gradients through discrete stochastic layers, we encode
the true gradients into a multiplication between random noises and the
difference of the same function of two different sets of discrete latent
variables, which are correlated with these random noises. The expectations of
that multiplication over iterations are zeros combined with spikes from time to
time. To modulate the frequencies, amplitudes, and signs of the spikes to
capture the temporal evolution of the true gradients, we propose the
augment-REINFORCE-merge (ARM) estimator that combines data augmentation, the
score-function estimator, permutation of the indices of latent variables, and
variance reduction for Monte Carlo integration using common random numbers. The
ARM estimator provides low-variance and unbiased gradient estimates for the
parameters of discrete distributions, leading to state-of-the-art performance
in both auto-encoding variational Bayes and maximum likelihood inference, for
discrete latent variable models with one or multiple discrete stochastic
layers.
</summary>
    <author>
      <name>Mingzhang Yin</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03356v2</id>
    <updated>2018-07-29T22:51:47Z</updated>
    <published>2018-03-09T02:26:49Z</published>
    <title>Exceedance probability for parameter estimates</title>
    <summary>  Many researchers and statisticians are conflicted over the practice of
hypothesis testing and statistical significance thresholds. There are several
alternatives, and in this paper we propose one that focuses on estimation. In
particular, we focus on the probability that a future parameter estimate will
exceed a specified amount. After briefly reviewing background on p-values,
significance thresholds, and a few alternatives, we describe the exceedance
probability for parameter estimates and provide examples of how the exceedance
probability, along with corresponding confidence intervals, can provide useful
information for the purposes of drawing inference and making decisions. We
focus on applications in one-sample tests and linear regression with potential
extensions to generalized linear models and Cox regression. We also analyze the
relationship between confidence intervals for the exceedance probability and
confidence intervals for parameter estimates, which leads to an interpretation
of confidence intervals that might be useful for teaching purposes.
</summary>
    <author>
      <name>Brian D. Segal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03356v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03356v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F25, 62F03" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09305v2</id>
    <updated>2018-07-29T15:31:01Z</updated>
    <published>2018-04-25T01:04:04Z</published>
    <title>Cross-Entropy Based Importance Sampling for Stochastic Simulation Models</title>
    <summary>  To efficiently evaluate system reliability based on Monte Carlo simulation,
importance sampling is used widely. The optimal importance sampling density was
derived in 1950s for the deterministic simulation model, which maps an input to
an output deterministically, and is approximated in practice using various
methods. For the stochastic simulation model whose output is random given an
input, the optimal importance sampling density was derived only recently. In
the existing literature, metamodel-based approaches have been used to
approximate this optimal density. However, building a satisfactory metamodel is
often difficult or time-consuming in practice. This paper proposes a
cross-entropy based method, which is automatic and does not require specific
domain knowledge. The proposed method uses an expectation--maximization
algorithm to guide the choice of a mixture distribution model for approximating
the optimal density. The method iteratively updates the approximated density to
minimize its estimated discrepancy, measured by estimated cross-entropy, from
the optimal density. The mixture model's complexity is controlled using the
cross-entropy information criterion. The method is empirically validated using
a numerical study and applied to a case study of evaluating the reliability of
wind turbine using a stochastic simulation model.
</summary>
    <author>
      <name>Quoc Dung Cao</name>
    </author>
    <author>
      <name>Youngjun Choe</name>
    </author>
    <link href="http://arxiv.org/abs/1804.09305v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09305v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11038v1</id>
    <updated>2018-07-29T10:58:59Z</updated>
    <published>2018-07-29T10:58:59Z</published>
    <title>Estimating Causal Effects Under Interference Using Bayesian Generalized
  Propensity Scores</title>
    <summary>  In most real-world systems units are interconnected and can be represented as
networks consisting of nodes and edges. For instance, in social systems
individuals can have social ties, family or financial relationships. In
settings where some units are exposed to a treatment and its effect spills over
connected units, estimating both the direct effect of the treatment and
spillover effects presents several challenges. First, assumptions on the way
and the extent to which spillover effects occur along the observed network are
required. Second, in observational studies, where the treatment assignment is
not under the control of the investigator, confounding and homophily are
potential threats to the identification and estimation of causal effects on
networks. Here, we make two structural assumptions: i) neighborhood
interference, which assumes interference operates only through a function of
the immediate neighbors' treatments ii) unconfoundedness of the individual and
neighborhood treatment, which rules out the presence of unmeasured confounding
variables, including those driving homophily. Under these assumptions we
develop a new covariate-adjustment estimator for treatment and spillover
effects in observational studies on networks. Estimation is based on a
generalized propensity score that balances individual and neighborhood
covariates across units under different levels of individual treatment and of
exposure to neighbors' treatment. Adjustment for propensity score is performed
using a penalized spline regression. Inference capitalizes on a three-step
Bayesian procedure which allows to take into account the uncertainty in the
propensity score estimation and avoiding model feedback. Finally, correlation
of interacting units is taken into account using a community detection
algorithm and incorporating random effects in the outcome model.
</summary>
    <author>
      <name>Laura Forastiere</name>
    </author>
    <author>
      <name>Fabrizia Mealli</name>
    </author>
    <author>
      <name>Albert Wu</name>
    </author>
    <author>
      <name>Edoardo Airoldi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1610.06511 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11027v1</id>
    <updated>2018-07-29T09:25:37Z</updated>
    <published>2018-07-29T09:25:37Z</published>
    <title>Consistent polynomial-time unseeded graph matching for Lipschitz
  graphons</title>
    <summary>  We propose a consistent polynomial-time method for the unseeded node matching
problem for networks with smooth underlying structures. Despite widely
conjectured by the research community that the structured graph matching
problem to be significantly easier than its worst case counterpart, well-known
to be NP-hard, the statistical version of the problem has stood a challenge
that resisted any solution both provable and polynomial-time. The closest
existing work requires quasi-polynomial time. Our method is based on the latest
advances in graphon estimation techniques and analysis on the concentration of
empirical Wasserstein distances. Its core is a simple yet unconventional
sampling-and-matching scheme that reduces the problem from unseeded to seeded.
Our method allows flexible efficiencies, is convenient to analyze and
potentially can be extended to more general settings. Our work enables a rich
variety of subsequent estimations and inferences.
</summary>
    <author>
      <name>Yuan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10987v1</id>
    <updated>2018-07-29T01:09:26Z</updated>
    <published>2018-07-29T01:09:26Z</published>
    <title>A new mixture-based fixed-effect model for a biometrical case-study
  related to immunogenecity with highly censored data</title>
    <summary>  We propose a new continuous-discrete mixture regression model which is useful
for describing highly censored data. We motivate our investigation based on a
case-study in biometry related to measles vaccines in Haiti. In this
case-study, the neutralization antibody level is explained by the type of
vaccine used, level of the dosage and gender of the patient. This mixture model
allows us to account for excess of censored observations and consists of the
Birnbaum-Saunders and Bernoulli distributions. These distributions describe the
antibody level and the point mass of the censoring observations. We estimate
the model parameters with the maximum likelihood method. Numerical evaluation
of the model is performed by Monte Carlo simulations and by an illustration
with biometrical data, both of which show its good performance and its
potential applications.
</summary>
    <author>
      <name>Mário F. Desousa</name>
    </author>
    <author>
      <name>Helton Saulo</name>
    </author>
    <author>
      <name>Manoel Santos-Neto</name>
    </author>
    <author>
      <name>Víctor Leiva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10981v1</id>
    <updated>2018-07-28T22:41:20Z</updated>
    <published>2018-07-28T22:41:20Z</published>
    <title>Prior-Proposal Recursive Bayesian Inference</title>
    <summary>  Bayesian models are naturally equipped to provide recursive inference because
they can formally reconcile new data and existing scientific information.
However, popular use of Bayesian methods often avoids priors that are based on
exact posterior distributions resulting from former studies. Recursive Bayesian
methods include two main approaches that we refer to as Prior- and
Proposal-Recursive Bayes. Prior-Recursive Bayes uses Bayesian updating, fitting
models to partitions of data sequentially, and provides a convenient way to
accommodate new data as they become available. Prior-Recursive Bayes uses the
posterior from the previous stage as the prior in the new stage based on the
latest data. By contrast, Proposal-Recursive Bayes is intended for use with
hierarchical Bayesian models and uses a set of transient priors in first stage
independent analyses of the data partitions. The second stage of
Proposal-Recursive Bayes uses the posterior distributions from the first stage
as proposals in an MCMC algorithm to fit the full model. The second-stage
recursive proposals simplify the Metropolis-Hastings ratio substantially and
can lead to computational advantages for the Proposal-Recursive Bayes method.
We combine Prior- and Proposal-Recursive concepts in a framework that can be
used to fit any Bayesian model exactly, and often with computational
improvements. We demonstrate our new method by fitting a geostatistical model
to spatially-explicit data in a sequence of stages, leading to computational
improvements by a factor of three in our example. While the method we propose
provides exact inference, it can also be coupled with modern approximation
methods leading to additional computational efficiency. Overall, our new
approach has implications for big data, streaming data, and optimal adaptive
design situations and can be modified to fit a broad class of Bayesian models
to data.
</summary>
    <author>
      <name>Mevin B. Hooten</name>
    </author>
    <author>
      <name>Devin S. Johnson</name>
    </author>
    <author>
      <name>Brian M. Brost</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03479v3</id>
    <updated>2018-07-28T05:28:25Z</updated>
    <published>2018-02-09T23:50:10Z</published>
    <title>Gaussian Process Landmarking on Manifolds</title>
    <summary>  As a means of improving analysis of biological shapes, we propose an
algorithm for sampling a Riemannian manifold by sequentially selecting points
with maximum uncertainty under a Gaussian process model. This greedy strategy
is known to be near-optimal in the experimental design literature, and appears
to outperform the use of user-placed landmarks in representing the geometry of
biological objects in our application. In the noiseless regime, we establish an
upper bound for the mean squared prediction error (MSPE) in terms of the number
of samples and geometric quantities of the manifold, demonstrating that the
MSPE for our proposed sequential design decays at a rate comparable to the
oracle rate achievable by any sequential or non-sequential optimal design; to
our knowledge this is the first result of this type for sequential experimental
design. The key is to link the greedy algorithm to reduced basis methods in the
context of model reduction for partial differential equations. We expect this
approach will find additional applications in other fields of research.
</summary>
    <author>
      <name>Tingran Gao</name>
    </author>
    <author>
      <name>Shahar Z. Kovalsky</name>
    </author>
    <author>
      <name>Ingrid Daubechies</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03479v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03479v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G15, 62K05, 65D18" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07184v2</id>
    <updated>2018-07-28T04:21:29Z</updated>
    <published>2018-03-19T22:32:05Z</published>
    <title>Adaptive Smoothing for Trajectory Reconstruction</title>
    <summary>  Trajectory reconstruction is the process of inferring the path of a moving
object between successive observations. In this paper, we propose a smoothing
spline -- which we name the V-spline -- that incorporates position and velocity
information and a penalty term that controls acceleration. We introduce a
particular adaptive V-spline designed to control the impact of irregularly
sampled observations and noisy velocity measurements. A cross-validation scheme
for estimating the V-spline parameters is given and we detail the performance
of the V-spline on four particularly challenging test datasets. Finally, an
application of the V-spline to vehicle trajectory reconstruction in two
dimensions is given, in which the penalty term is allowed to further depend on
known operational characteristics of the vehicle.
</summary>
    <author>
      <name>Zhanglong Cao</name>
    </author>
    <author>
      <name>David Bryant</name>
    </author>
    <author>
      <name>Tim Molteno</name>
    </author>
    <author>
      <name>Colin Fox</name>
    </author>
    <author>
      <name>Matthew Parry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00370v3</id>
    <updated>2018-07-28T02:20:48Z</updated>
    <published>2015-11-02T03:45:39Z</published>
    <title>A Two-Stage Penalized Least Squares Method for Constructing Large
  Systems of Structural Equations</title>
    <summary>  We propose a two-stage penalized least squares method to build large systems
of structural equations based on the instrumental variables view of the
classical two-stage least squares method. We show that, with large numbers of
endogenous and exogenous variables, the system can be constructed via
consistent estimation of a set of conditional expectations at the first stage,
and consistent selection of regulatory effects at the second stage. While the
consistent estimation at the first stage can be obtained via the ridge
regression, the adaptive lasso is employed at the second stage to achieve the
consistent selection. The resultant estimates of regulatory effects enjoy the
oracle properties. This method is computationally fast and allows for parallel
implementation. We demonstrate its effectiveness via simulation studies and
real data analysis.
</summary>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Min Ren</name>
    </author>
    <author>
      <name>Min Zhang</name>
    </author>
    <author>
      <name>Dabao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.00370v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00370v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10873v1</id>
    <updated>2018-07-28T02:04:59Z</updated>
    <published>2018-07-28T02:04:59Z</published>
    <title>Bayesian Sparse Propensity Score Estimation for Unit Nonresponse</title>
    <summary>  Nonresponse weighting adjustment using propensity score is a popular method
for handling unit nonresponse. However, including all available auxiliary
variables into the propensity model can lead to inefficient and inconsistent
estimation, especially with high-dimensional covariates. In this paper, a new
Bayesian method using the Spike-and-Slab prior is proposed for sparse
propensity score estimation. The proposed method is not based on any model
assumption on the outcome variable and is computationally efficient. Instead of
doing model selection and parameter estimation separately as in many
frequentist methods, the proposed method simultaneously selects the sparse
response probability model and provides consistent parameter estimation. Some
asymptotic properties of the proposed method are presented. The efficiency of
this sparse propensity score estimator is further improved by incorporating
related auxiliary variables from the full sample. The finite-sample performance
of the proposed method is investigated in two limited simulation studies,
including a partially simulated real data example from the Korean Labor and
Income Panel Survey.
</summary>
    <author>
      <name>Hejian Sang</name>
    </author>
    <author>
      <name>Gyuhyeong Goh</name>
    </author>
    <author>
      <name>Jae Kwang Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08925v2</id>
    <updated>2018-07-27T22:37:38Z</updated>
    <published>2018-07-24T06:34:16Z</published>
    <title>Anomaly detection in static networks using egonets</title>
    <summary>  Network data has rapidly emerged as an important and active area of
statistical methodology. In this paper we consider the problem of anomaly
detection in networks. Given a large background network, we seek to detect
whether there is a small anomalous subgraph present in the network, and if such
a subgraph is present, which nodes constitute the subgraph. We propose an
inferential tool based on egonets to answer this question. The proposed method
is computationally efficient and naturally amenable to parallel computing, and
easily extends to a wide variety of network models. We demonstrate through
simulation studies that the egonet method works well under a wide variety of
network models. We obtain some fascinating empirical results by applying the
egonet method on several well-studied benchmark datasets.
</summary>
    <author>
      <name>Srijan Sengupta</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08925v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08925v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10797v1</id>
    <updated>2018-07-27T18:56:10Z</updated>
    <published>2018-07-27T18:56:10Z</published>
    <title>Estimating a change point in a sequence of very high-dimensional
  covariance matrices</title>
    <summary>  This paper considers the problem of estimating a change point in the
covariance matrix in a sequence of high-dimensional vectors, where the
dimension is substantially larger than the sample size. A two-stage approach is
proposed to efficiently estimate the location of the change point. The first
step consists of a reduction of the dimension to identify elements of the
covariance matrices corresponding to significant changes. In a second step we
use the components after dimension reduction to determine the position of the
change point. Theoretical properties are developed for both steps and numerical
studies are conducted to support the new methodology.
</summary>
    <author>
      <name>H. Dette</name>
    </author>
    <author>
      <name>G. M. Pan</name>
    </author>
    <author>
      <name>Q. Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10678v1</id>
    <updated>2018-07-27T15:19:03Z</updated>
    <published>2018-07-27T15:19:03Z</published>
    <title>Survival of the Fittest Group: Factorial Analyses of Treatment Effects
  under Independent Right-Censoring</title>
    <summary>  This paper introduces new effect parameters for factorial survival designs
with possibly right-censored time-to-event data. In the special case of a
two-sample design it coincides with the concordance or Wilcoxon parameter in
survival analysis. More generally, the new parameters describe treatment or
interaction effects and we develop estimates and tests to infer their presence.
We rigorously study the asymptotic properties by means of empirical process
techniques and additionally suggest wild bootstrapping for a consistent and
distribution-free application of the inference procedures. The small sample
performance is discussed based on simulation results. The practical usefulness
of the developed methodology is exemplified on a data example about patients
with colon cancer by conducting one- and two-factorial analyses.
</summary>
    <author>
      <name>Dennis Dobler</name>
    </author>
    <author>
      <name>Markus Pauly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G09, 62G10, 62G20, 62H15, 62N02, 62N03" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10618v1</id>
    <updated>2018-07-27T13:39:28Z</updated>
    <published>2018-07-27T13:39:28Z</published>
    <title>Simple Bayesian testing of scientific expectations in linear regression
  models</title>
    <summary>  Scientific theories can often be formulated using equality and order
constraints on the relative effects in a linear regression model. For example,
it may be expected that the effect of the first predictor is larger than the
effect of the second predictor, and the second predictor is expected to be
larger than the third predictor. The goal is then to test such expectations
against competing scientific expectations or theories. In this paper a simple
default Bayes factor test is proposed for testing multiple hypotheses with
equality and order constraints on the effects of interest. The proposed testing
criterion can be computed without requiring external prior information about
the expected effects before observing the data. The method is implemented in
R-package called `{\tt lmhyp}' which is freely downloadable and ready to use.
The usability of the method and software is illustrated using empirical
applications from the social and behavioral sciences.
</summary>
    <author>
      <name>Joris Mulder</name>
    </author>
    <author>
      <name>Anton Olsson-Collentine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 1 figure, 1 table, 1 appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10467v1</id>
    <updated>2018-07-27T07:33:10Z</updated>
    <published>2018-07-27T07:33:10Z</published>
    <title>VIMCO: Variational Inference for Multiple Correlated Outcomes in
  Genome-wide Association Studies</title>
    <summary>  In Genome-Wide Association Studies (GWAS) where multiple correlated traits
have been measured on participants, a joint analysis strategy, whereby the
traits are analyzed jointly, can improve statistical power over a single-trait
analysis strategy. There are two questions of interest to be addressed when
conducting a joint GWAS analysis with multiple traits. The first question
examines whether a genetic loci is significantly associated with any of the
traits being tested. The second question focuses on identifying the specific
trait(s) that is associated with the genetic loci. Since existing methods
primarily focus on the first question, this paper seeks to provide a
complementary method that addresses the second question. We propose a novel
method, Variational Inference for Multiple Correlated Outcomes (VIMCO), that
focuses on identifying the specific trait that is associated with the genetic
loci, when performing a joint GWAS analysis of multiple traits, while
accounting for correlation among the multiple traits. We performed extensive
numerical studies and also applied VIMCO to analyze two datasets. The numerical
studies and real data analysis demonstrate that VIMCO improves statistical
power over single-trait analysis strategies when the multiple traits are
correlated and has comparable performance when the traits are not correlated.
</summary>
    <author>
      <name>Xingjie Shi</name>
    </author>
    <author>
      <name>Yuling Jiao</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <author>
      <name>Ching-Yu Cheng</name>
    </author>
    <author>
      <name>Can Yang</name>
    </author>
    <author>
      <name>Xinyi Lin</name>
    </author>
    <author>
      <name>Jin Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures, 1 R package on GitHub. Supplementary available
  upon request</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08852v3</id>
    <updated>2018-07-27T07:18:26Z</updated>
    <published>2018-01-26T15:42:11Z</published>
    <title>Calibration for Weak Variance-Alpha-Gamma Processes</title>
    <summary>  The weak variance-alpha-gamma process is a multivariate L\'evy process
constructed by weakly subordinating Brownian motion, possibly with correlated
components with an alpha-gamma subordinator. It generalises the
variance-alpha-gamma process of Semeraro constructed by traditional
subordination. We compare three calibration methods for the weak
variance-alpha-gamma process, method of moments, maximum likelihood estimation
(MLE) and digital moment estimation (DME). We derive a condition for Fourier
invertibility needed to apply MLE and show in our simulations that MLE produces
a better fit when this condition holds, while DME produces a better fit when it
is violated. We also find that the weak variance-alpha-gamma process exhibits a
wider range of dependence and produces a significantly better fit than the
variance-alpha-gamma process on an S&amp;P500-FTSE100 data set, and that DME
produces the best fit in this situation.
</summary>
    <author>
      <name>Boris Buchmann</name>
    </author>
    <author>
      <name>Kevin W. Lu</name>
    </author>
    <author>
      <name>Dilip B. Madan</name>
    </author>
    <link href="http://arxiv.org/abs/1801.08852v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08852v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G51, 62F10, 60E10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10451v1</id>
    <updated>2018-07-27T06:34:15Z</updated>
    <published>2018-07-27T06:34:15Z</published>
    <title>How to capitalize on a priori contrasts in linear (mixed) models: A
  tutorial</title>
    <summary>  Factorial experiments in research on memory, language, and in other areas are
often analyzed using analysis of variance (ANOVA). However, for experimental
factors with more than two levels, the ANOVA omnibus F-test is not informative
about the source of a main effect or interaction. This is unfortunate as
researchers typically have specific hypotheses about which condition means
differ from each other. A priori contrasts (i.e., comparisons planned before
the sample means are known) between specific conditions or combinations of
conditions are the appropriate way to represent such hypotheses in the
statistical model. Many researchers have pointed out that contrasts should be
"tested instead of, rather than as a supplement to, the ordinary `omnibus' F
test" (Hayes, 1973, p. 601). In this tutorial, we explain the mathematics
underlying different kinds of contrasts (i.e., treatment, sum, repeated,
Helmert, and polynomial contrasts), discuss their properties, and demonstrate
how they are applied in the R System for Statistical Computing (R Core Team,
2018). In this context, we explain the generalized inverse which is needed to
compute the weight coefficients for contrasts that test hypotheses that are not
covered by the default set of contrasts. A detailed understanding of contrast
coding is crucial for successful and correct specification in linear models
(including linear mixed models). Contrasts defined a priori yield far more
precise confirmatory tests of experimental hypotheses than standard omnibus
F-test.
</summary>
    <author>
      <name>Daniel J. Schad</name>
    </author>
    <author>
      <name>Sven Hohenstein</name>
    </author>
    <author>
      <name>Shravan Vasishth</name>
    </author>
    <author>
      <name>Reinhold Kliegl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages, 4 figures in main text, 1 figure in appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02802v2</id>
    <updated>2018-07-27T03:55:21Z</updated>
    <published>2016-11-09T02:34:24Z</published>
    <title>Pairwise Sequential Randomization and Its Properties</title>
    <summary>  In comparative studies, such as in causal inference and clinical trials,
balancing important covariates is often one of the most important concerns for
both efficient and credible comparison. However, chance imbalance still exists
in many randomized experiments. This phenomenon of covariate imbalance becomes
much more serious as the number of covariates $p$ increases. To address this
issue, we introduce a new randomization procedure, called pairwise sequential
randomization (PSR). The proposed method allocates the units sequentially and
adaptively, using information on the current level of imbalance and the
incoming unit's covariate. With a large number of covariates or a large number
of units, the proposed method shows substantial advantages over the traditional
methods in terms of the covariate balance, estimation accuracy, and
computational time, making it an ideal technique in the era of big data. The
proposed method attains the optimal covariate balance, in the sense that the
estimated treatment effect under the proposed method attains its minimum
variance asymptotically. Also the proposed method is widely applicable in both
causal inference and clinical trials. Numerical studies and real data analysis
provide further evidence of the advantages of the proposed method.
</summary>
    <author>
      <name>Yichen Qin</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Wei Ma</name>
    </author>
    <author>
      <name>Feifang Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02802v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02802v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07801v3</id>
    <updated>2018-07-27T03:22:00Z</updated>
    <published>2017-12-21T05:56:19Z</published>
    <title>Density Estimation with Contaminated Data: Minimax Rates and Theory of
  Adaptation</title>
    <summary>  This paper studies density estimation under pointwise loss in the setting of
contamination model. The goal is to estimate $f(x_0)$ at some
$x_0\in\mathbb{R}$ with i.i.d. observations, $$ X_1,\dots,X_n\sim
(1-\epsilon)f+\epsilon g, $$ where $g$ stands for a contamination distribution.
In the context of multiple testing, this can be interpreted as estimating the
null density at a point. We carefully study the effect of contamination on
estimation through the following model indices: contamination proportion
$\epsilon$, smoothness of target density $\beta_0$, smoothness of contamination
density $\beta_1$, and level of contamination $m$ at the point to be estimated,
i.e. $g(x_0)\leq m$. It is shown that the minimax rate with respect to the
squared error loss is of order $$
[n^{-\frac{2\beta_0}{2\beta_0+1}}]\vee[\epsilon^2(1\wedge
m)^2]\vee[n^{-\frac{2\beta_1}{2\beta_1+1}}\epsilon^{\frac{2}{2\beta_1+1}}], $$
which characterizes the exact influence of contamination on the difficulty of
the problem. We then establish the minimal cost of adaptation to contamination
proportion, to smoothness and to both of the numbers. It is shown that some
small price needs to be paid for adaptation in any of the three cases.
Variations of Lepski's method are considered to achieve optimal adaptation.
  The problem is also studied when there is no smoothness assumption on the
contamination distribution. This setting that allows for an arbitrary
contamination distribution is recognized as Huber's $\epsilon$-contamination
model. The minimax rate is shown to be $$
[n^{-\frac{2\beta_0}{2\beta_0+1}}]\vee [\epsilon^{\frac{2\beta_0}{\beta_0+1}}].
$$ The adaptation theory is also different from the smooth contamination case.
While adaptation to either contamination proportion or smoothness only costs a
logarithmic factor, adaptation to both numbers is proved to be impossible.
</summary>
    <author>
      <name>Haoyang Liu</name>
    </author>
    <author>
      <name>Chao Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07801v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07801v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04606v2</id>
    <updated>2018-07-27T02:38:39Z</updated>
    <published>2017-09-14T03:50:26Z</published>
    <title>Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials</title>
    <summary>  We consider goodness-of-fit tests with i.i.d. samples generated from a
categorical distribution $(p_1,...,p_k)$. For a given $(q_1,...,q_k)$, we test
the null hypothesis whether $p_j=q_{\pi(j)}$ for some label permutation $\pi$.
The uncertainty of label permutation implies that the null hypothesis is
composite instead of being singular. In this paper, we construct a testing
procedure using statistics that are defined as indefinite integrals of some
symmetric polynomials. This method is aimed directly at the invariance of the
problem, and avoids the need of matching the unknown labels. The asymptotic
distribution of the testing statistic is shown to be chi-squared, and its power
is proved to be nearly optimal under a local alternative hypothesis. Various
degenerate structures of the null hypothesis are carefully analyzed in the
paper. A two-sample version of the test is also studied.
</summary>
    <author>
      <name>Chao Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04606v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04606v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10375v1</id>
    <updated>2018-07-26T21:42:17Z</updated>
    <published>2018-07-26T21:42:17Z</published>
    <title>Integrative Multi-View Reduced-Rank Regression: Bridging Group-Sparse
  and Low-Rank Models</title>
    <summary>  Multi-view data have been routinely collected in various fields of science
and engineering. A general problem is to study the predictive association
between multivariate responses and multi-view predictor sets, all of which can
be of high dimensionality. It is likely that only a few views are relevant to
prediction, and the predictors within each relevant view contribute to the
prediction collectively rather than sparsely. We cast this new problem under
the familiar multivariate regression framework and propose an integrative
reduced-rank regression (iRRR), where each view has its own low-rank
coefficient matrix. As such, latent features are extracted from each view in a
supervised fashion. For model estimation, we develop a convex composite nuclear
norm penalization approach, which admits an efficient algorithm via alternating
direction method of multipliers. Extensions to non-Gaussian and incomplete data
are discussed. Theoretically, we derive non-asymptotic oracle bounds of iRRR
under a restricted eigenvalue condition. Our results recover oracle bounds of
several special cases of iRRR including Lasso, group Lasso and nuclear norm
penalized regression. Therefore, iRRR seamlessly bridges group-sparse and
low-rank methods and can achieve substantially faster convergence rate under
realistic settings of multi-view learning. Simulation studies and an
application in the Longitudinal Studies of Aging further showcase the efficacy
of the proposed methods.
</summary>
    <author>
      <name>Gen Li</name>
    </author>
    <author>
      <name>Xiaokang Liu</name>
    </author>
    <author>
      <name>Kun Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25, 62J02, 62J12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10338v1</id>
    <updated>2018-07-26T19:42:08Z</updated>
    <published>2018-07-26T19:42:08Z</published>
    <title>Beta Autoregressive Fractionally Integrated Moving Average Models</title>
    <summary>  In this work we introduce the class of beta autoregressive fractionally
integrated moving average models for continuous random variables taking values
in the continuous unit interval $(0,1)$. The proposed model accommodates a set
of regressors and a long-range dependent time series structure. We derive the
partial likelihood estimator for the parameters of the proposed model, obtain
the associated score vector and Fisher information matrix. We also prove the
consistency and asymptotic normality of the estimator under mild conditions.
Hypotheses testing, diagnostic tools and forecasting are also proposed. A Monte
Carlo simulation is considered to evaluate the finite sample performance of the
partial likelihood estimators and to study some of the proposed tests. An
empirical application is also presented and discussed.
</summary>
    <author>
      <name>Guilherme Pumi</name>
    </author>
    <author>
      <name>Marcio Valk</name>
    </author>
    <author>
      <name>Cleber Bisognin</name>
    </author>
    <author>
      <name>Fábio Mariano Bayer</name>
    </author>
    <author>
      <name>Taiane Schaedler Prass</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10, 62F12, 62J12, 62J99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10320v1</id>
    <updated>2018-07-26T18:51:42Z</updated>
    <published>2018-07-26T18:51:42Z</published>
    <title>High Dimensional Model Representation as a Glass Box in Supervised
  Machine Learning</title>
    <summary>  Prediction and explanation are key objects in supervised machine learning,
where predictive models are known as black boxes and explanatory models are
known as glass boxes. Explanation provides the necessary and sufficient
information to interpret the model output in terms of the model input. It
includes assessments of model output dependence on important input variables
and measures of input variable importance to model output. High dimensional
model representation (HDMR), also known as the generalized functional ANOVA
expansion, provides useful insight into the input-output behavior of supervised
machine learning models. This article gives applications of HDMR in supervised
machine learning. The first application is characterizing information leakage
in ``big-data'' settings. The second application is reduced-order
representation of elementary symmetric polynomials. The third application is
analysis of variance with correlated variables. The last application is
estimation of HDMR from kernel machine and decision tree black box
representations. These results suggest HDMR to have broad utility within
machine learning as a glass box representation.
</summary>
    <author>
      <name>Caleb Deen Bastian</name>
    </author>
    <author>
      <name>Herschel Rabitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages, 23 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10342v3</id>
    <updated>2018-07-26T17:58:43Z</updated>
    <published>2017-10-27T21:45:29Z</published>
    <title>Insights on Variance Estimation for Blocked and Matched Pairs Designs</title>
    <summary>  The current literature on blocking has two main branches. The first focuses
on larger blocks, with multiple treatment and control units in each block. The
second focuses on matched pairs, with a single treatment and control unit in
each block. In the former case, variance estimation is straightforward as one
can estimate the variance of units within each group and treatment arm. In the
latter case, this is not possible so researchers have proposed alternative
estimators that look at variance across the blocks. These alternative
estimators have been evaluated under different assumptions than found in the
large block literature. Neither literature handles cases with blocks of varying
size that contain singleton treatment or control units. Differences in the two
literatures has also created some confusion regarding the benefits of blocking
in general. In this paper, we reconcile the literatures by carefully examining
the performance of different estimators of treatment effect and of associated
variance estimators under several different frameworks. We provide variance
estimators for experiments containing blocks of different sizes, allowing for
singleton units in a treatment arm. We finally discuss in which situations
blocking is or is not guaranteed to reduce the variance of our estimator.
</summary>
    <author>
      <name>Nicole E. Pashley</name>
    </author>
    <author>
      <name>Luke W. Miratrix</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">66 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10342v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10342v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10259v1</id>
    <updated>2018-07-26T17:40:18Z</updated>
    <published>2018-07-26T17:40:18Z</published>
    <title>Unbiased inference for discretely observed hidden Markov model
  diffusions</title>
    <summary>  We develop an importance sampling (IS) type estimator for Bayesian joint
inference on the model parameters and latent states of a class of hidden Markov
models (HMMs). We are interested in the class of HMMs for which the hidden
state dynamics is a diffusion process and noisy observations are obtained at
discrete times. We suppose that the diffusion dynamics can not be simulated
exactly and hence one must time-discretise the diffusion. Our approach is based
on particle marginal Metropolis--Hastings (PMMH), particle filters (PFs), and
(randomised) multilevel Monte Carlo (rMLMC). The estimator is built upon a
single run of PMMH using a coarse discretisation of the model. The consequent
IS type correction is based on a single-term rMLMC estimator using output from
a PF developed for level difference integral estimation. The resulting IS type
estimator leads to inference without a bias from the time-discretisation. We
give convergence results, such as a central limit theorem, and recommend
allocations for algorithm inputs. The generality of our method sets it apart
from extant unbiased methods based on exact simulation, which require strong
conditions on the HMM diffusion. Moreover, our method is highly parallelisable.
We illustrate our method on two examples from the literature.
</summary>
    <author>
      <name>Jordan Franks</name>
    </author>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <author>
      <name>Kody Law</name>
    </author>
    <author>
      <name>Matti Vihola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05 (primary), 65C30, 65C35, 65C40 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04035v2</id>
    <updated>2018-07-26T16:25:46Z</updated>
    <published>2018-05-10T16:02:53Z</published>
    <title>Scaling limit of the Stein variational gradient descent part I: the mean
  field regime</title>
    <summary>  We study an interacting particle system in $\mathbf{R}^d$ motivated by Stein
variational gradient descent [Q. Liu and D. Wang, NIPS 2016], a deterministic
algorithm for sampling from a given probability density with unknown
normalization. We prove that in the large particle limit the empirical measure
converges to a solution of a non-local and nonlinear PDE. We also prove global
well-posedness and uniqueness of the solution to the limiting PDE. Finally, we
prove that the solution to the PDE converges to the unique invariant solution
in large time limit.
</summary>
    <author>
      <name>Jianfeng Lu</name>
    </author>
    <author>
      <name>Yulong Lu</name>
    </author>
    <author>
      <name>James Nolen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10138v1</id>
    <updated>2018-07-26T13:51:12Z</updated>
    <published>2018-07-26T13:51:12Z</published>
    <title>Block models for multipartite networks.Applications in ecology and
  ethnobiology</title>
    <summary>  Modeling relations between individuals is a classical question in social
sciences, ecology, etc. In order to uncover a latent structure in the data, a
popular approach consists in clustering individuals according to the observed
patterns of interactions. To do so, Stochastic block models (SBM) and Latent
Block models (LBM) are standard tools for clustering the individuals with
respect to their comportment in a unique network. However, when adopting an
integrative point of view, individuals are not involved in a unique network but
are part of several networks, resulting into a potentially complex multipartite
network. In this contribution, we propose a stochastic block model able to
handle multipartite networks, thus supplying a clustering of the individuals
based on their connection behavior in more than one network. Our model is an
extension of the latent block models (LBM) and stochastic block model (SBM).
The parameters -such as the marginal probabilities of assignment to blocks and
the matrix of probabilities of connections between blocks- are estimated
through a variational Expectation-Maximization procedure. The numbers of blocks
are chosen with the Integrated Completed Likelihood criterion, a penalized
likelihood criterion. The pertinence of our methodology is illustrated on two
datasets issued from ecology and ethnobiology.
</summary>
    <author>
      <name>Avner Bar-Hen</name>
    </author>
    <author>
      <name>Pierre Barbillon</name>
    </author>
    <author>
      <name>Sophie Donnet</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10100v1</id>
    <updated>2018-07-26T12:53:58Z</updated>
    <published>2018-07-26T12:53:58Z</published>
    <title>Two-Step Estimation and Inference with Possibly Many Included Covariates</title>
    <summary>  We study the implications of including many covariates in a first-step
estimate entering a two-step estimation procedure. We find that a first order
bias emerges when the number of \textit{included} covariates is "large"
relative to the square-root of sample size, rendering standard inference
procedures invalid. We show that the jackknife is able to estimate this "many
covariates" bias consistently, thereby delivering a new automatic
bias-corrected two-step point estimator. The jackknife also consistently
estimates the standard error of the original two-step point estimator. For
inference, we develop a valid post-bias-correction bootstrap approximation that
accounts for the additional variability introduced by the jackknife
bias-correction. We find that the jackknife bias-corrected point estimator and
the bootstrap post-bias-correction inference perform excellent in simulations,
offering important improvements over conventional two-step point estimators and
inference procedures, which are not robust to including many covariates. We
apply our results to an array of distinct treatment effect, policy evaluation,
and other applied microeconomics settings. In particular, we discuss production
function and marginal treatment effect estimation in detail.
</summary>
    <author>
      <name>Matias D. Cattaneo</name>
    </author>
    <author>
      <name>Michael Jansson</name>
    </author>
    <author>
      <name>Xinwei Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09509v3</id>
    <updated>2018-07-26T11:27:08Z</updated>
    <published>2016-11-29T06:50:12Z</published>
    <title>Model Confidence Bounds for Variable Selection</title>
    <summary>  In this article, we introduce the concept of model confidence bounds (MCB)
for variable selection in the context of nested models. Similarly to the
endpoints in the familiar confidence interval for parameter estimation, the MCB
identifies two nested models (upper and lower confidence bound models)
containing the true model at a given level of confidence. Instead of trusting a
single selected model obtained from a given model selection method, the MCB
proposes a group of nested models as candidates and the MCB's width and
composition enable the practitioner to assess the overall model selection
uncertainty. A new graphical tool --- the model uncertainty curve (MUC) --- is
introduced to visualize the variability of model selection and to compare
different model selection procedures. The MCB methodology is implemented by a
fast bootstrap algorithm that is shown to yield the correct asymptotic coverage
under rather general conditions. Our Monte Carlo simulations and real data
examples confirm the validity and illustrate the advantages of the proposed
method.
</summary>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Yuetian Luo</name>
    </author>
    <author>
      <name>Davide Ferrari</name>
    </author>
    <author>
      <name>Xiaonan Hu</name>
    </author>
    <author>
      <name>Yichen Qin</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09509v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09509v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10628v1</id>
    <updated>2018-07-26T11:21:14Z</updated>
    <published>2018-07-26T11:21:14Z</published>
    <title>A conditional independence framework for coherent modularized inference</title>
    <summary>  Inference in current domains of application are often complex and require us
to integrate the expertise of a variety of disparate panels of experts and
models coherently. In this paper we develop a formal statistical methodology to
guide the networking together of a diverse collection of probabilistic models.
In particular, we derive sufficient conditions that ensure inference remains
coherent across the composite before and after accommodating relevant evidence.
</summary>
    <author>
      <name>Manuele Leonelli</name>
    </author>
    <author>
      <name>Martine J. Barons</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1507.07394</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09969v1</id>
    <updated>2018-07-26T06:22:39Z</updated>
    <published>2018-07-26T06:22:39Z</published>
    <title>A Flexible Joint Longitudinal-Survival Modeling Framework for
  Incorporating Multiple Longitudinal Biomarkers</title>
    <summary>  We are interested in survival analysis of hemodialysis patients for whom
several biomarkers are recorded over time. Motivated by this challenging
problem, we propose a general framework for multivariate joint
longitudinal-survival modeling that can be used to examine the association
between several longitudinally recorded covariates and a time-to-event
endpoint. Our method allows for simultaneous modeling of longitudinal
covariates by taking their correlation into account. This leads to a more
efficient method for modeling their trajectories over time, and hence, it can
better capture their relationship to the survival outcomes.
</summary>
    <author>
      <name>Sepehr Akhavan-Masouleh</name>
    </author>
    <author>
      <name>Alexander Vandenberg-Rodes</name>
    </author>
    <author>
      <name>Babak Shahbaba</name>
    </author>
    <author>
      <name>Daniel L. Gillen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04532v5</id>
    <updated>2018-07-26T05:38:46Z</updated>
    <published>2016-07-15T14:42:32Z</published>
    <title>Should I stay or should I go? A latent threshold approach to large-scale
  mixture innovation models</title>
    <summary>  This paper proposes a straightforward algorithm to carry out inference in
large time-varying parameter vector autoregressions (TVP-VARs) with mixture
innovation components for each coefficient in the system. We significantly
decrease the computational burden by approximating the latent indicators that
drive the time-variation in the coefficients with a latent threshold process
that depends on the absolute size of the shocks. The merits of our approach are
illustrated with two applications. First, we forecast the US term structure of
interest rates and demonstrate forecast gains of the proposed mixture
innovation model relative to other benchmark models. Second, we apply our
approach to US macroeconomic data and find significant evidence for
time-varying effects of a monetary policy tightening.
</summary>
    <author>
      <name>Florian Huber</name>
    </author>
    <author>
      <name>Gregor Kastner</name>
    </author>
    <author>
      <name>Martin Feldkircher</name>
    </author>
    <link href="http://arxiv.org/abs/1607.04532v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04532v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00561v2</id>
    <updated>2018-07-25T23:13:22Z</updated>
    <published>2017-12-02T07:11:41Z</published>
    <title>Scalable Sparse Cox's Regression for Large-Scale Survival Data via
  Broken Adaptive Ridge</title>
    <summary>  This paper develops a new scalable sparse Cox regression tool for sparse
high-dimensional massive sample size (sHDMSS) survival data. The method is a
local $L_0$-penalized Cox regression via repeatedly performing reweighted
$L_2$-penalized Cox regression. We show that the resulting estimator enjoys the
best of $L_0$- and $L_2$-penalized Cox regressions while overcoming their
limitations. Specifically, the estimator is selection consistent, oracle for
parameter estimation, and possesses a grouping property for highly correlated
covariates. Simulation results suggest that when the sample size is large, the
proposed method with pre-specified tuning parameters has a comparable or better
performance than some popular penalized regression methods. More importantly,
because the method naturally enables adaptation of efficient algorithms for
massive $L_2$-penalized optimization and does not require costly data driven
tuning parameter selection, it has a significant computational advantage for
sHDMSS data, offering an average of 5-fold speedup over its closest competitor
in empirical studies.
</summary>
    <author>
      <name>Eric S. Kawaguchi</name>
    </author>
    <author>
      <name>Marc A. Suchard</name>
    </author>
    <author>
      <name>Zhenqiu Liu</name>
    </author>
    <author>
      <name>Gang Li</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07216v2</id>
    <updated>2018-07-25T22:13:31Z</updated>
    <published>2016-12-21T16:14:11Z</published>
    <title>The Essential Histogram</title>
    <summary>  The histogram is widely used as a simple, exploratory display of data, but it
is usually not clear how to choose the number and size of bins for this
purpose. We construct a confidence set of distribution functions that optimally
address the two main tasks of the histogram: estimating probabilities and
detecting features such as increases and (anti)modes in the distribution. We
define the essential histogram as the histogram in the confidence set with the
fewest bins. Thus the essential histogram is the simplest visualization of the
data that optimally achieves the main tasks of the histogram. We provide a fast
algorithm for computing the essential histogram, and we illustrate our
methodology with examples. An R-package is available on CRAN.
</summary>
    <author>
      <name>Housen Li</name>
    </author>
    <author>
      <name>Axel Munk</name>
    </author>
    <author>
      <name>Hannes Sieling</name>
    </author>
    <author>
      <name>Guenther Walther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A R-package "essHist" is available from
  https://CRAN.R-project.org/package=essHist</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07216v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07216v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G10, 62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03310v4</id>
    <updated>2018-07-25T19:30:16Z</updated>
    <published>2017-12-08T23:08:12Z</published>
    <title>Maximum entropy low-rank matrix recovery</title>
    <summary>  We propose in this paper a novel, information-theoretic method, called
MaxEnt, for efficient data acquisition for low-rank matrix recovery. This
proposed method has important applications to a wide range of problems,
including image processing and text document indexing. Fundamental to our
design approach is the so-called maximum entropy principle, which states that
the measurement masks which maximize the entropy of observations, also maximize
the information gain on the unknown matrix $\mathbf{X}$. Coupled with a
low-rank stochastic model for $\mathbf{X}$, such a principle (i) reveals novel
connections between information-theoretic sampling and subspace packings, and
(ii) yields efficient mask construction algorithms for matrix recovery, which
significantly outperforms random measurements. We illustrate the effectiveness
of MaxEnt in simulation experiments, and demonstrate its usefulness in two
real-world applications on image recovery and text document indexing.
</summary>
    <author>
      <name>Simon Mak</name>
    </author>
    <author>
      <name>Yao Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1712.03310v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03310v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06035v4</id>
    <updated>2018-07-25T02:48:09Z</updated>
    <published>2016-09-20T06:49:04Z</published>
    <title>AdaPT: An interactive procedure for multiple testing with side
  information</title>
    <summary>  We consider the problem of multiple hypothesis testing with generic side
information: for each hypothesis $H_i$ we observe both a p-value $p_i$ and some
predictor $x_i$ encoding contextual information about the hypothesis. For
large-scale problems, adaptively focusing power on the more promising
hypotheses (those more likely to yield discoveries) can lead to much more
powerful multiple testing procedures. We propose a general iterative framework
for this problem, called the Adaptive p-value Thresholding (AdaPT) procedure,
which adaptively estimates a Bayes-optimal p-value rejection threshold and
controls the false discovery rate (FDR) in finite samples. At each iteration of
the procedure, the analyst proposes a rejection threshold and observes
partially censored p-values, estimates the false discovery proportion (FDP)
below the threshold, and either stops to reject or proposes another threshold,
until the estimated FDP is below $\alpha$. Our procedure is adaptive in an
unusually strong sense, permitting the analyst to use any statistical or
machine learning method she chooses to estimate the optimal threshold, and to
switch between different models at each iteration as information accrues. We
demonstrate the favorable performance of AdaPT by comparing it to
state-of-the-art methods in five real applications and two simulation studies.
</summary>
    <author>
      <name>Lihua Lei</name>
    </author>
    <author>
      <name>William Fithian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by JRSS-B; Develop an R package adaptMT
  (https://github.com/lihualei71/adaptMT)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.06035v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06035v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06993v3</id>
    <updated>2018-07-24T20:55:45Z</updated>
    <published>2016-06-22T15:59:36Z</published>
    <title>Exact mean integrated squared error and bandwidth selection for kernel
  distribution function estimators</title>
    <summary>  An exact, closed form, and easy to compute expression for the mean integrated
squared error (MISE) of a kernel estimator of a normal mixture cumulative
distribution function is derived for the class of arbitrary order
Gaussian-based kernels. Comparisons are made with MISE of the empirical
distribution function, the infeasible minimum MISE of kernel estimators, and
the asymptotically optimal second order uniform kernel. The results afford
straightforward extensions to other classes of kernel functions and
distributions. The analysis also offers a guide on when to use higher order
kernels in distribution function estimation.
  A simple plug-in method of simultaneously selecting the optimal bandwidth and
kernel order is proposed based on a non-asymptotic approximation of the unknown
distribution by a normal mixture. A simulation study shows that the method
works well in finite samples, thus providing a viable alternative to existing
bandwidth selection procedures.
</summary>
    <author>
      <name>Vitaliy Oryshchenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06993v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06993v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09621v1</id>
    <updated>2018-07-24T15:14:22Z</updated>
    <published>2018-07-24T15:14:22Z</published>
    <title>Model uncertainty estimation in data assimilation for multi-scale
  systems with partially observed resolved variables</title>
    <summary>  Model uncertainty quantification is an essential component of effective Data
Assimilation (DA), although it can be a challenging task for complex partially
observed non-linear systems with highly non-Gaussian uncertainties. Model
errors associated with sub-grid scale processes are of particular interest in
meteorological DA studies; these are often represented through Stochastic
Parameterizations of the unresolved process. Many existing Stochastic
Parameterization schemes are only applicable when knowledge of the true
sub-grid scale process or full observations of the coarse scale process are
available, which is typically not the case in real applications. We present a
methodology for estimating the statistics of sub-grid scale processes for the
more realistic case that only partial observations of the coarse scale process
are available. The aim is to first estimate the conditional probability density
of additive model errors given the state of the system, from which samples can
be generated to simulate model error within any ensemble-based assimilation
framework. Model error realizations are estimated over a training period by
minimizing their conditional variance, constrained by available observations.
Special is that these errors are binned conditioned on the previous model state
during the minimization process, allowing for the recovery of complex
non-Gaussian error structures. We demonstrate the efficacy of the approach
through numerical experiments with the multi-scale Lorenz 96 system. Various
parameterizations of the Lorenz 96' model are considered with both small and
large time scale separations between slow (coarse scale) and fast (fine scale)
variables. Results indicate that both error estimates and forecasts are
improved with the proposed method compared to two existing methods for
accounting for model uncertainty in DA.
</summary>
    <author>
      <name>Sahani Pathiraja</name>
    </author>
    <author>
      <name>Peter Jan van Leeuwen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09148v1</id>
    <updated>2018-07-24T14:32:43Z</updated>
    <published>2018-07-24T14:32:43Z</published>
    <title>Collaborative double robustness using the $e$-score</title>
    <summary>  Estimation of causal parameters from observational data requires complete
confounder adjustment, as well as positivity of the propensity score for each
treatment arm. There is often a trade-off between these two assumptions:
confounding bias may be reduced through adjustment for a high-dimensional
pre-treatment covariate, but positivity is less likely in analyses with more
predictors of treatment. Under empirical positivity violations, propensity
score weights are highly variable, and doubly robust estimators suffer from
high variance and large finite sample bias. To solve this problem, we introduce
the $e$-score, which is defined through a dimension reduction for the
propensity score. This dimension reduction is based on a recent result known as
collaborative double robustness, which roughly states that a propensity score
conditioning only on the bias of the outcome regression estimator is sufficient
to attain double robustness. We propose methods to construct doubly robust
estimators based on the $e$-score, and discuss their properties such as
consistency, efficiency, and asymptotic distribution. This allows the
construction of asymptotically valid Wald-type confidence intervals and
hypothesis tests. We present an illustrative application on estimating the
effect of smoking on bone mineral content in adolescent girls well as a
synthetic data simulation illustrating the bias and variance reduction and
asymptotic normality achieved by our proposed estimators.
</summary>
    <author>
      <name>Iván Díaz</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09616v1</id>
    <updated>2018-07-24T13:05:08Z</updated>
    <published>2018-07-24T13:05:08Z</published>
    <title>Reliability analysis of general phased mission systems with a new
  survival signature</title>
    <summary>  It is often difficult for a phased mission system (PMS) to be highly
reliable, because this entails achieving high reliability in every phase of
operation. Consequently, reliability analysis of such systems is of critical
importance. However, efficient and interpretable analysis of PMSs enabling
general component lifetime distributions, arbitrary structures, and the
possibility that components skip phases has been an open problem.
  In this paper, we show that the survival signature can be used for
reliability analysis of PMSs with similar types of component in each phase,
providing an alternative to the existing limited approaches in the literature.
We then develop new methodology addressing the full range of challenges above.
The new method retains the attractive survival signature property of separating
the system structure from the component lifetime distributions, simplifying
computation, insight into, and inference for system reliability.
</summary>
    <author>
      <name>Xianzhen Huang</name>
    </author>
    <author>
      <name>Louis J. M. Aslett</name>
    </author>
    <author>
      <name>Frank P. A. Coolen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08911v2</id>
    <updated>2018-07-24T12:34:53Z</updated>
    <published>2017-08-29T17:56:42Z</published>
    <title>Simultaneous Variable and Covariance Selection with the Multivariate
  Spike-and-Slab Lasso</title>
    <summary>  We propose a Bayesian procedure for simultaneous variable and covariance
selection using continuous spike-and-slab priors in multivariate linear
regression models where q possibly correlated responses are regressed onto p
predictors. Rather than relying on a stochastic search through the
high-dimensional model space, we develop an ECM algorithm similar to the EMVS
procedure of Rockova &amp; George (2014) targeting modal estimates of the matrix of
regression coefficients and residual precision matrix. Varying the scale of the
continuous spike densities facilitates dynamic posterior exploration and allows
us to filter out negligible regression coefficients and partial covariances
gradually. Our method is seen to substantially outperform regularization
competitors on simulated data. We demonstrate our method with a re-examination
of data from a recent observational study of the effect of playing high school
football on several later-life cognition, psychological, and socio-economic
outcomes.
</summary>
    <author>
      <name>Sameer K. Deshpande</name>
    </author>
    <author>
      <name>Veronika Rockova</name>
    </author>
    <author>
      <name>Edward I. George</name>
    </author>
    <link href="http://arxiv.org/abs/1708.08911v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08911v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09037v1</id>
    <updated>2018-07-24T11:17:28Z</updated>
    <published>2018-07-24T11:17:28Z</published>
    <title>Likelihood-based meta-analysis with few studies: Empirical and
  simulation studies</title>
    <summary>  Standard random-effects meta-analysis methods perform poorly when applied to
few studies only. Such settings however are commonly encountered in practice.
It is unclear, whether or to what extent small-sample-size behaviour can be
improved by more sophisticated modeling.
  We consider several likelihood-based inference methods. Confidence intervals
are based on normal or Student-t approximations. We extract an empirical data
set of 40 meta-analyses from recent reviews published by the German Institute
for Quality and Efficiency in Health Care (IQWiG). Methods are then compared
empirically as well as in a simulation study, considering odds-ratio and risk
ratio effect sizes.
  Empirically, a majority of the identified meta-analyses include only 2
studies. In the simulation study, coverage probability is, in the presence of
heterogeneity and few studies, below the nominal level for all frequentist
methods based on normal approximation, in particular when sizes in
meta-analyses are not balanced, but improve when confidence intervals are
adjusted. Bayesian methods result in better coverage than the frequentist
methods with normal approximation in all scenarios. Credible intervals are
empirically and in the simulation study wider than unadjusted confidence
intervals, but considerably narrower than adjusted ones. Confidence intervals
based on the generalized linear mixed models are in general, slightly narrower
than those from other frequentist methods. Certain methods turned out
impractical due to frequent numerical problems.
  In the presence of between-study heterogeneity, especially with unbalanced
study sizes, caution is needed in applying meta-analytical methods to few
studies, as either coverage probabilities might be compromised, or intervals
are inconclusively wide. Bayesian estimation with a sensibly chosen prior for
between-trial heterogeneity may offer a promising compromise.
</summary>
    <author>
      <name>Svenja E. Seide</name>
    </author>
    <author>
      <name>Christian Röver</name>
    </author>
    <author>
      <name>Tim Friede</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05630v3</id>
    <updated>2018-07-24T10:12:09Z</updated>
    <published>2017-12-15T11:55:39Z</published>
    <title>Sparse principal component analysis via random projections</title>
    <summary>  We introduce a new method for sparse principal component analysis, based on
the aggregation of eigenvector information from carefully-selected random
projections of the sample covariance matrix. Unlike most alternative
approaches, our algorithm is non-iterative, so is not vulnerable to a bad
choice of initialisation. Our theory provides great detail on the statistical
and computational trade-off in our procedure, revealing a subtle interplay
between the effective sample size and the number of random projections that are
required to achieve the minimax optimal rate. Numerical studies provide further
insight into the procedure and confirm its highly competitive finite-sample
performance.
</summary>
    <author>
      <name>Milana Gataric</name>
    </author>
    <author>
      <name>Tengyao Wang</name>
    </author>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05630v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05630v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02368v2</id>
    <updated>2018-07-24T09:25:54Z</updated>
    <published>2018-02-07T09:37:20Z</published>
    <title>Group kernels for Gaussian process metamodels with categorical inputs</title>
    <summary>  Gaussian processes (GP) are widely used as a metamodel for emulating
time-consuming computer codes. We focus on problems involving categorical
inputs, with a potentially large number L of levels (typically several tens),
partitioned in G &lt;&lt; L groups of various sizes. Parsimonious covariance
functions, or kernels, can then be defined by block covariance matrices T with
constant covariances between pairs of blocks and within blocks. We study the
positive definiteness of such matrices to encourage their practical use. The
hierarchical group/level structure, equivalent to a nested Bayesian linear
model, provides a parameterization of valid block matrices T. The same model
can then be used when the assumption within blocks is relaxed, giving a
flexible parametric family of valid covariance matrices with constant
covariances between pairs of blocks. The positive definiteness of T is
equivalent to the positive definiteness of a smaller matrix of size G, obtained
by averaging each block. The model is applied to a problem in nuclear waste
analysis, where one of the categorical inputs is atomic number, which has more
than 90 levels.
</summary>
    <author>
      <name>Olivier Roustant</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GdR MASCOT-NUM, LIMOS, FAYOL-ENSMSE, UCA</arxiv:affiliation>
    </author>
    <author>
      <name>Esperan Padonou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEA/DAM</arxiv:affiliation>
    </author>
    <author>
      <name>Yves Deville</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEA/DAM</arxiv:affiliation>
    </author>
    <author>
      <name>Aloïs Clément</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEA/DAM</arxiv:affiliation>
    </author>
    <author>
      <name>Guillaume Perrin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DAM/DIF</arxiv:affiliation>
    </author>
    <author>
      <name>Jean Giorla</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DAM/DIF</arxiv:affiliation>
    </author>
    <author>
      <name>Henry Wynn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSE</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1802.02368v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02368v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08928v1</id>
    <updated>2018-07-24T06:58:36Z</updated>
    <published>2018-07-24T06:58:36Z</published>
    <title>Bivariate network meta-analysis for surrogate endpoint evaluation</title>
    <summary>  Surrogate endpoints are very important in regulatory decision-making in
healthcare, in particular if they can be measured early compared to the
long-term final clinical outcome and act as good predictors of clinical
benefit. Bivariate meta-analysis methods can be used to evaluate surrogate
endpoints and to predict the treatment effect on the final outcome from the
treatment effect measured on a surrogate endpoint. However, candidate surrogate
endpoints are often imperfect, and the level of association between the
treatment effects on the surrogate and final outcomes may vary between
treatments. This imposes a limitation on the pairwise methods which do not
differentiate between the treatments. We develop bivariate network
meta-analysis (bvNMA) methods which combine data on treatment effects on the
surrogate and final outcomes, from trials investigating heterogeneous treatment
contrasts. The bvNMA methods estimate the effects on both outcomes for all
treatment contrasts individually in a single analysis. At the same time, they
allow us to model the surrogacy patterns across multiple trials (different
populations) within a treatment contrast and across treatment contrasts, thus
enabling predictions of the treatment effect on the final outcome for a new
study in a new population or investigating a new treatment. Modelling
assumptions about the between-studies heterogeneity and the network
consistency, and their impact on predictions, are investigated using simulated
data and an illustrative example in advanced colorectal cancer. When the
strength of the surrogate relationships varies across treatment contrasts,
bvNMA has the advantage of identifying treatments for which surrogacy holds,
thus leading to better predictions.
</summary>
    <author>
      <name>Sylwia Bujkiewicz</name>
    </author>
    <author>
      <name>Dan Jackson</name>
    </author>
    <author>
      <name>John R Thompson</name>
    </author>
    <author>
      <name>Rebecca Turner</name>
    </author>
    <author>
      <name>Keith R Abrams</name>
    </author>
    <author>
      <name>Ian R White</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09267v1</id>
    <updated>2018-07-24T06:43:11Z</updated>
    <published>2018-07-24T06:43:11Z</published>
    <title>Asymptotic of Approximate Least Squares Estimators of Parameters
  Two-Dimensional Chirp Signal</title>
    <summary>  In this paper, we address the problem of parameter estimation of a 2-D chirp
model under the assumption that the errors are stationary. We extend the 2-D
periodogram method for the sinusoidal model, to find initial values to use in
any iterative procedure to compute the least squares estimators (LSEs) of the
unknown parameters, to the 2-D chirp model. Next we propose an estimator, known
as the approximate least squares estimator (ALSE), that is obtained by
maximising a periodogram-type function and is observed to be asymptotically
equivalent to the LSE. Moreover the asymptotic properties of these estimators
are obtained under slightly mild conditions than those required for the LSEs.
For the multiple component 2-D chirp model, we propose a sequential method of
estimation of the ALSEs, that significantly reduces the computational
difficulty involved in reckoning the LSEs and the ALSEs. We perform some
simulation studies to see how the proposed method works and a data set has been
analysed for illustrative purposes.
</summary>
    <author>
      <name>Rhythm Grover</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <author>
      <name>Amit Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08409v2</id>
    <updated>2018-07-24T01:59:17Z</updated>
    <published>2018-07-23T02:49:41Z</published>
    <title>Subsampling MCMC - A review for the survey statistician</title>
    <summary>  The rapid development of computing power and efficient Markov Chain Monte
Carlo (MCMC) simulation algorithms have revolutionized Bayesian statistics,
making it a highly practical inference method in applied work. However, MCMC
algorithms tend to be computationally demanding, and are particularly slow for
large datasets. Data subsampling has recently been suggested as a way to make
MCMC methods scalable on massively large data, utilizing efficient sampling
schemes and estimators from the survey sampling literature. These developments
tend to be unknown by many survey statisticians who traditionally work with
non-Bayesian methods, and rarely use MCMC. Our article reviews Subsampling
MCMC, a so called pseudo-marginal MCMC approach to speeding up MCMC through
data subsampling. The review is written for a survey statistician without
previous knowledge of MCMC methods since our aim is to motivate survey sampling
experts to contribute to the growing Subsampling MCMC literature.
</summary>
    <author>
      <name>Matias Quiroz</name>
    </author>
    <author>
      <name>Mattias Villani</name>
    </author>
    <author>
      <name>Robert Kohn</name>
    </author>
    <author>
      <name>Minh-Ngoc Tran</name>
    </author>
    <author>
      <name>Khue-Dung Dang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed Figure 3.6, which was corrupted in version 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08409v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08409v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08834v1</id>
    <updated>2018-07-23T21:20:06Z</updated>
    <published>2018-07-23T21:20:06Z</published>
    <title>Stable Multiple Time Step Simulation/Prediction from Lagged Dynamic
  Network Regression Models</title>
    <summary>  Recent developments in computers and automated data collection strategies
have greatly increased the interest in statistical modeling of dynamic
networks. Many of the statistical models employed for inference on large-scale
dynamic networks suffer from limited forward simulation/prediction ability. A
major problem with many of the forward simulation procedures is the tendency
for the model to become degenerate in only a few time steps, i.e., the
simulation/prediction procedure results in either null graphs or complete
graphs. Here, we describe an algorithm for simulating a sequence of networks
generated from lagged dynamic network regression models DNR(V), a sub-family of
TERGMs. We introduce a smoothed estimator for forward prediction based on
smoothing of the change statistics obtained for a dynamic network regression
model. We focus on the implementation of the algorithm, providing a series of
motivating examples with comparisons to dynamic network models from the
literature. We find that our algorithm significantly improves multi-step
prediction/simulation over standard DNR(V) forecasting. Furthermore, we show
that our method performs comparably to existing more complex dynamic network
analysis frameworks (SAOM and STERGMs) for small networks over short time
periods, and significantly outperforms these approaches over long time time
intervals and/or large networks.
</summary>
    <author>
      <name>Abhirup Mallik</name>
    </author>
    <author>
      <name>Zack W. Almquist</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08702v1</id>
    <updated>2018-07-23T16:25:27Z</updated>
    <published>2018-07-23T16:25:27Z</published>
    <title>A model specification test for the variance function in nonparametric
  regression</title>
    <summary>  The problem of testing for the parametric form of the conditional variance is
considered in a fully nonparametric regression model. A test statistic based on
a weighted $L_2$-distance between the empirical characteristic functions of
residuals constructed under the null hypothesis and under the alternative is
proposed and studied theoretically. The null asymptotic distribution of the
test statistic is obtained and employed to approximate the critical values.
Finite sample properties of the proposed test are numerically investigated in
several Monte Carlo experiments. The developed results assume independent data.
Their extension to dependent observations is also discussed.
</summary>
    <author>
      <name>Juan Carlos Pardo-Fernandez</name>
    </author>
    <author>
      <name>M. Dolores Jimenez-Gamero</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03494v2</id>
    <updated>2018-07-23T16:22:55Z</updated>
    <published>2017-07-11T23:28:52Z</published>
    <title>Unsupervised robust nonparametric learning of hidden community
  properties</title>
    <summary>  We consider learning of fundamental properties of communities in large noisy
networks, in the prototypical situation where the nodes or users are split into
two classes according to a binary property, e.g., according to their opinions
or preferences on a topic. For learning these properties, we propose a
nonparametric, unsupervised, and scalable graph scan procedure that is, in
addition, robust against a class of powerful adversaries. In our setup, one of
the communities can fall under the influence of a knowledgeable adversarial
leader, who knows the full network structure, has unlimited computational
resources and can completely foresee our planned actions on the network. We
prove strong consistency of our results in this setup with minimal assumptions.
In particular, the learning procedure estimates the baseline activity of normal
users asymptotically correctly with probability 1; the only assumption being
the existence of a single implicit community of asymptotically negligible
logarithmic size. We provide experiments on real and synthetic data to
illustrate the performance of our method, including examples with adversaries.
</summary>
    <author>
      <name>Mikhail A. Langovoy</name>
    </author>
    <author>
      <name>Akhilesh Gotmare</name>
    </author>
    <author>
      <name>Martin Jaggi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Experiments with new types of adversaries added</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.03494v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03494v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07705v3</id>
    <updated>2018-07-23T15:24:45Z</updated>
    <published>2017-06-23T13:51:52Z</published>
    <title>Spatially filtered unconditional quantile regression</title>
    <summary>  Conditional quantile regression (CQR) is a popular approach used in disaster
risk analysis, hedonic analysis, among others. However, the traditional CQR has
difficulty with the interpretation of the coefficient estimates and ignorance
of spatial dependence, which is a basic property of spatial data. To overcome
these limitations, this study develops a spatial unconditional quantile
regression (UQR) approach, which we term the spatially filtered UQR (SF-UQR).
Unlike CQR, SF-UQR estimates unconditional effects that are easier to interpret
considering spatial dependence. We then develop a computationally efficient
approach for SF-UQR estimation. The performance of the SF-UQR is tested by a
hedonic flood risk analysis in the Tokyo metropolitan area. SF-UQR is
implemented in an R package "spmoran."
</summary>
    <author>
      <name>Daisuke Murakami</name>
    </author>
    <author>
      <name>Hajime Seya</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07705v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07705v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08673v1</id>
    <updated>2018-07-23T15:22:21Z</updated>
    <published>2018-07-23T15:22:21Z</published>
    <title>Approximate Bayesian inference with queueing networks and coupled jump
  processes</title>
    <summary>  Queueing networks are systems of theoretical interest that give rise to
complex families of stochastic processes, and find widespread use in the
performance evaluation of interconnected resources. Yet, despite their
importance within applications, and in comparison to their counterpart
stochastic models in genetics or mathematical biology, there exist few relevant
approaches for transient inference and uncertainty quantification tasks in
these systems. This is a consequence of strong computational impediments and
distinctive properties of the Markov jump processes induced by queueing
networks. In this paper, we offer a comprehensive overview of the inferential
challenge and its comparison to analogue tasks within related mathematical
domains. We then discuss a model augmentation over an approximating network
system, and present a flexible and scalable variational Bayesian framework,
which is targeted at general-form open and closed queueing systems, with varied
service disciplines and priorities. The inferential procedure is finally
validated in a couple of uncertainty quantification tasks for network service
rates.
</summary>
    <author>
      <name>Iker Perez</name>
    </author>
    <author>
      <name>Giuliano Casale</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08660v1</id>
    <updated>2018-07-23T15:03:23Z</updated>
    <published>2018-07-23T15:03:23Z</published>
    <title>Bipartite Causal Inference with Interference</title>
    <summary>  Statistical methods to evaluate the effectiveness of interventions are
increasingly challenged by the inherent interconnectedness of units.
Specifically, a recent flurry of methods research has addressed the problem of
interference between observations, which arises when one observational unit's
outcome depends not only on its treatment but also the treatment assigned to
other units. We introduce the setting of bipartite causal inference with
interference, which arises when 1) treatments are defined on observational
units that are distinct from those at which outcomes are measured and 2) there
is interference between units in the sense that outcomes for some units depend
on the treatments assigned to many other units. Basic definitions and
formulations are provided for this setting, highlighting similarities and
differences with more commonly considered settings of causal inference with
interference. Several types of causal estimands are discussed, and a simple
inverse probability of treatment weighted estimator is developed for a subset
of simplified estimands. The estimators are deployed to evaluate how
interventions to reduce air pollution from 473 power plants in the U.S.
causally affect cardiovascular hospitalization among Medicare beneficiaries
residing at 23,458 zip code locations.
</summary>
    <author>
      <name>Corwin M. Zigler</name>
    </author>
    <author>
      <name>Georgia Papadogeorgou</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08448v1</id>
    <updated>2018-07-23T06:51:40Z</updated>
    <published>2018-07-23T06:51:40Z</published>
    <title>Modeling event cascades using networks of additive count sequences</title>
    <summary>  We propose a statistical model for networks of event count sequences built on
a cascade structure. We assume that each event triggers successor events, whose
counts follow additive probability distributions; the ensemble of counts is
given by their superposition. These assumptions allow the marginal distribution
of the count sequences and the conditional distribution of the event cascades
to have analytic forms. We present our model framework using Poisson and
negative binomial distributions as the building blocks. Based on this, we
describe a statistical method for estimating the model parameters and the event
cascades from the observed count sequences.
</summary>
    <author>
      <name>Shinsuke Koyama</name>
    </author>
    <author>
      <name>Yoshi Fujiwara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08440v1</id>
    <updated>2018-07-23T06:17:12Z</updated>
    <published>2018-07-23T06:17:12Z</published>
    <title>Network Global Testing by Counting Graphlets</title>
    <summary>  Consider a large social network with possibly severe degree heterogeneity and
mixed-memberships. We are interested in testing whether the network has only
one community or there are more than one communities. The problem is known to
be non-trivial, partially due to the presence of severe degree heterogeneity.
We construct a class of test statistics using the numbers of short paths and
short cycles, and the key to our approach is a general framework for canceling
the effects of degree heterogeneity. The tests compare favorably with existing
methods. We support our methods with careful analysis and numerical study with
simulated data and a real data example.
</summary>
    <author>
      <name>Jiashun Jin</name>
    </author>
    <author>
      <name>Zheng Tracy Ke</name>
    </author>
    <author>
      <name>Shengming Luo</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 35th International Conference on Machine
  Learning (ICML 2018), Stockholm, Sweden, PMLR Vol. 80, Pages 2338-2346, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.08440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H20, 62H15, 62P25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04359v2</id>
    <updated>2018-07-23T06:02:54Z</updated>
    <published>2018-04-12T07:32:18Z</published>
    <title>Efficiently Combining Pseudo Marginal and Particle Gibbs Sampling</title>
    <summary>  Particle Markov Chain Monte Carlo (PMCMC) is a general approach to carry out
Bayesian inference in non-linear and non-Gaussian state space models. Our
article shows how to scale up PMCMC in terms of the number of parameters and
number of time points by generating parameters that are highly correlated with
the states with the states integrated out using a pseudo marginal step while
the rest of the parameters are generated conditional on the states using
particle Gibbs. We make the PMCMC scalable in the number of observations by
using the same random numbers in the Metropolis-Hastings ratio of the pseudo
marginal step. We do so by expressing the target density of the PMCMC in terms
of the basic uniform or standard normal random numbers rather than in terms of
the particles, as has been done till now, and develop a constrained version of
conditional sequential Monte Carlo algorithm. We illustrate the methods using a
high dimensional factor stochastic volatility having both a large number of
parameters and a large number of latent states and show that our proposed
method makes the computation much more efficient.
</summary>
    <author>
      <name>David Gunawan</name>
    </author>
    <author>
      <name>Chris Carter</name>
    </author>
    <author>
      <name>Robert Kohn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages and 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08429v1</id>
    <updated>2018-07-23T05:10:30Z</updated>
    <published>2018-07-23T05:10:30Z</published>
    <title>Prediction based on conditional distributions of vine copulas</title>
    <summary>  Vine copula models are a flexible tool in multivariate non-Gaussian
distributions. For data from an observational study where the explanatory
variables and response variables are measured over sampling units, we propose a
vine copula regression method that uses regular vines and handles mixed
continuous and discrete variables. This method can efficiently compute the
conditional distribution of the response variable given the explanatory
variables. Furthermore, we provide a theoretical analysis of the asymptotic
conditional cumulative distribution function and quantile functions arising
from vine copulas. Assuming all variables have been transformed to standard
normal, the conditional quantile function could be asymptotically linear,
sublinear, or constant with respect to the explanatory variables in different
extreme directions, depending on the dependence properties of bivariate copulas
in joint tails. The performance of the proposed method is evaluated on
simulated data sets and a real data set. The experiments demonstrate that the
vine copula regression method is superior to linear regression in making
inferences with conditional heteroscedasticity.
</summary>
    <author>
      <name>Bo Chang</name>
    </author>
    <author>
      <name>Harry Joe</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08393v1</id>
    <updated>2018-07-23T01:13:37Z</updated>
    <published>2018-07-23T01:13:37Z</published>
    <title>Time-to-Event Model-Assisted Designs to Accelerate Phase I Clinical
  Trials</title>
    <summary>  Two useful strategies to speed up drug development are to increase the
patient accrual rate and use novel adaptive designs. Unfortunately, these two
strategies often conflict when the evaluation of the outcome cannot keep pace
with the patient accrual rate and thus the interim data cannot be observed in
time to make adaptive decisions. A similar logistic difficulty arises when the
outcome is of late onset. Based on a novel formulation and approximation of the
likelihood of the observed data, we propose a general methodology for
model-assisted designs to handle toxicity data that are pending due to fast
accrual or late-onset toxicity, and facilitate seamless decision making in
phase I dose-finding trials. The dose escalation/de-escalation rules of the
proposed time-to-event model-assisted designs can be tabulated before the trial
begins, which greatly simplifies trial conduct in practice compared to that
under existing methods. We show that the proposed designs have desirable finite
and large-sample properties and yield performance that is superior to that of
more complicated model-based designs. We provide user-friendly software for
implementing the designs.
</summary>
    <author>
      <name>Ruitao Lin</name>
    </author>
    <author>
      <name>Ying Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01789v2</id>
    <updated>2018-07-23T00:29:20Z</updated>
    <published>2018-05-04T13:59:59Z</published>
    <title>The conformable fractional grey system model</title>
    <summary>  The fractional order grey models (FGM) have appealed considerable interest of
research in recent years due to its higher effectiveness and flexibility than
the conventional grey models and other prediction models. However, the
definitions of the fractional order accumulation (FOA) and difference (FOD) is
computationally complex, which leads to difficulties for the theoretical
analysis and applications. In this paper, the new definition of the FOA are
proposed based on the definitions of Conformable Fractional Derivative, which
is called the Conformable Fractional Accumulation (CFA), along with its inverse
operation, the Conformable Fractional Difference (CFD). Then the new
Conformable Fractional Grey Model (CFGM) based on CFA and CFD is introduced
with detailed modelling procedures. The feasibility and simplicity and the CFGM
are shown in the numerical example. And the at last the comprehensive
real-world case studies of natural gas production forecasting in 11 countries
are presented, and results show that the CFGM is much more effective than the
existing FGM model in the 165 subcases.
</summary>
    <author>
      <name>Xin Ma</name>
    </author>
    <author>
      <name>Wenqing Wu</name>
    </author>
    <author>
      <name>Bo Zeng</name>
    </author>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Xinxing Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01789v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01789v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08390v1</id>
    <updated>2018-07-23T00:02:36Z</updated>
    <published>2018-07-23T00:02:36Z</published>
    <title>Score Permutation Based Finite Sample Inference for Generalized
  AutoRegressive Conditional Heteroskedasticity (GARCH) Models</title>
    <summary>  A standard model of (conditional) heteroscedasticity, i.e., the phenomenon
that the variance of a process changes over time, is the Generalized
AutoRegressive Conditional Heteroskedasticity (GARCH) model, which is
especially important for economics and finance. GARCH models are typically
estimated by the Quasi-Maximum Likelihood (QML) method, which works under mild
statistical assumptions. Here, we suggest a finite sample approach, called
ScoPe, to construct distribution-free confidence regions around the QML
estimate, which have exact coverage probabilities, despite no additional
assumptions about moments are made. ScoPe is inspired by the recently developed
Sign-Perturbed Sums (SPS) method, which however cannot be applied in the GARCH
case. ScoPe works by perturbing the score function using randomly permuted
residuals. This produces alternative samples which lead to exact confidence
regions. Experiments on simulated and stock market data are also presented, and
ScoPe is compared with the asymptotic theory and bootstrap approaches.
</summary>
    <author>
      <name>Balázs Csanád Csáji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Conference on Artificial Intelligence and
  Statistics (AISTATS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Machine Learning Research, Volume 51, 2016, pp.
  296-304</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.08390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08383v1</id>
    <updated>2018-07-22T23:29:28Z</updated>
    <published>2018-07-22T23:29:28Z</published>
    <title>PaloBoost: An Overfitting-robust TreeBoost with Out-of-Bag Sample
  Regularization Techniques</title>
    <summary>  Stochastic Gradient TreeBoost is often found in many winning solutions in
public data science challenges. Unfortunately, the best performance requires
extensive parameter tuning and can be prone to overfitting. We propose
PaloBoost, a Stochastic Gradient TreeBoost model that uses novel regularization
techniques to guard against overfitting and is robust to parameter settings.
PaloBoost uses the under-utilized out-of-bag samples to perform gradient-aware
pruning and estimate adaptive learning rates. Unlike other Stochastic Gradient
TreeBoost models that use the out-of-bag samples to estimate test errors,
PaloBoost treats the samples as a second batch of training samples to prune the
trees and adjust the learning rates. As a result, PaloBoost can dynamically
adjust tree depths and learning rates to achieve faster learning at the start
and slower learning as the algorithm converges. We illustrate how these
regularization techniques can be efficiently implemented and propose a new
formula for calculating feature importance to reflect the node coverages and
learning rates. Extensive experimental results on seven datasets demonstrate
that PaloBoost is robust to overfitting, is less sensitivity to the parameters,
and can also effectively identify meaningful features.
</summary>
    <author>
      <name>Yubin Park</name>
    </author>
    <author>
      <name>Joyce C. Ho</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08380v1</id>
    <updated>2018-07-22T23:05:46Z</updated>
    <published>2018-07-22T23:05:46Z</published>
    <title>Finite mixtures of matrix-variate Poisson-log normal distributions for
  three-way count data</title>
    <summary>  Three-way data structures, characterized by three entities, the units, the
variables and the occasions, are frequent in biological studies. In RNA
sequencing, three-way data structures are obtained when high-throughput
transcriptome sequencing data are collected for n genes across p conditions at
r occasions. Matrix-variate distributions offer a natural way to model
three-way data and mixtures of matrix-variate distributions can be used to
cluster three-way data. Clustering of gene expression data is carried out as
means to discovering gene co-expression networks. In this work, a mixture of
matrix-variate Poisson-log normal distributions is proposed for clustering read
counts from RNA sequencing. By considering the matrix-variate structure, full
information on the conditions and occasions of the RNA sequencing dataset is
simultaneously considered, and the number of covariance parameters to be
estimated is reduced. A Markov chain Monte Carlo expectation-maximization
algorithm is used for parameter estimation and information criteria are used
for model selection. The models are applied to both real and simulated data,
giving favourable clustering results.
</summary>
    <author>
      <name>Anjali Silva</name>
    </author>
    <author>
      <name>Steven J. Rothstein</name>
    </author>
    <author>
      <name>Paul D. McNicholas</name>
    </author>
    <author>
      <name>Sanjeena Subedi</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10488v3</id>
    <updated>2018-07-22T22:35:50Z</updated>
    <published>2017-05-30T07:40:52Z</published>
    <title>Bayesian model averaging over tree-based dependence structures for
  multivariate extremes</title>
    <summary>  Describing the complex dependence structure of extreme phenomena is
particularly challenging. To tackle this issue we develop a novel statistical
algorithm that describes extremal dependence taking advantage of the inherent
hierarchical dependence structure of the max-stable nested logistic
distribution and that identifies possible clusters of extreme variables using
reversible jump Markov chain Monte Carlo techniques. Parsimonious
representations are achieved when clusters of extreme variables are found to be
completely independent. Moreover, we significantly decrease the computational
complexity of full likelihood inference by deriving a recursive formula for the
nested logistic model likelihood. The algorithm performance is verified through
extensive simulation experiments which also compare different likelihood
procedures. The new methodology is used to investigate the dependence
relationships between extreme concentration of multiple pollutants in
California and how these pollutants are related to extreme weather conditions.
Overall, we show that our approach allows for the representation of complex
extremal dependence structures and has valid applications in multivariate data
analysis, such as air pollution monitoring, where it can guide policymaking.
</summary>
    <author>
      <name>Sabrina Vettori</name>
    </author>
    <author>
      <name>Raphaël Huser</name>
    </author>
    <author>
      <name>Johan Segers</name>
    </author>
    <author>
      <name>Marc G. Genton</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10488v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10488v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08933v2</id>
    <updated>2018-07-22T21:55:47Z</updated>
    <published>2017-07-27T16:55:22Z</published>
    <title>P-splines with an l1 penalty for repeated measures</title>
    <summary>  P-splines are penalized B-splines, in which finite order differences in
coefficients are typically penalized with an $\ell_2$ norm. P-splines can be
used for semiparametric regression and can include random effects to account
for within-subject variability. In addition to $\ell_2$ penalties,
$\ell_1$-type penalties have been used in nonparametric and semiparametric
regression to achieve greater flexibility, such as in locally adaptive
regression splines, $\ell_1$ trend filtering, and the fused lasso additive
model. However, there has been less focus on using $\ell_1$ penalties in
P-splines, particularly for estimating conditional means.
  In this paper, we demonstrate the potential benefits of using an $\ell_1$
penalty in P-splines with an emphasis on fitting non-smooth functions. We
propose an estimation procedure using the alternating direction method of
multipliers and cross validation, and provide degrees of freedom and
approximate confidence bands based on a ridge approximation to the $\ell_1$
penalized fit. We also demonstrate potential uses through simulations and an
application to electrodermal activity data collected as part of a stress study.
</summary>
    <author>
      <name>Brian D. Segal</name>
    </author>
    <author>
      <name>Michael R. Elliott</name>
    </author>
    <author>
      <name>Thomas Braun</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages, 26 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08933v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08933v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G08 (Primary), 62P10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02099v4</id>
    <updated>2018-07-22T13:47:22Z</updated>
    <published>2018-07-05T17:40:31Z</published>
    <title>The Role of the Propensity Score in Fixed Effect Models</title>
    <summary>  We develop a new approach for estimating average treatment effects in the
observational studies with unobserved cluster-level heterogeneity. The previous
approach relied heavily on linear fixed effect specifications that severely
limit the heterogeneity between clusters. These methods imply that linearly
adjusting for differences between clusters in average covariate values
addresses all concerns with cross-cluster comparisons. Instead, we consider an
exponential family structure on the within-cluster distribution of covariates
and treatments that implies that a low-dimensional sufficient statistic can
summarize the empirical distribution, where this sufficient statistic may
include functions of the data beyond average covariate values. Then we use
modern causal inference methods to construct flexible and robust estimators.
</summary>
    <author>
      <name>Dmitry Arkhangelsky</name>
    </author>
    <author>
      <name>Guido Imbens</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02099v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02099v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08256v1</id>
    <updated>2018-07-22T08:17:40Z</updated>
    <published>2018-07-22T08:17:40Z</published>
    <title>On the influence function for the Theil-like class of inequality
  measures</title>
    <summary>  On one hand, a large class of inequality measures, which includes the
generalized entropy, the Atkinson, the Gini, etc., for example, has been
introduced in Mergane and Lo (2013). On the other hand, the influence function
of statistics is an important tool in the asymptotics of a nonparametric
statistic. This function has been and is being determined and analysed in
various aspects for a large number of statistics. We proceed to a unifying
study of the IF of all the members of the so-called Theil-like family and
regroup those IF's in one formula. Comparative studies become easier.
</summary>
    <author>
      <name>Tchilabalo Abozou Kpanzou</name>
    </author>
    <author>
      <name>Diam Ba</name>
    </author>
    <author>
      <name>Pape Djiby Mergane</name>
    </author>
    <author>
      <name>Gane Samb Lo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08225v1</id>
    <updated>2018-07-22T02:06:28Z</updated>
    <published>2018-07-22T02:06:28Z</published>
    <title>The Hyperedge Event Model</title>
    <summary>  We introduce the hyperedge event model (HEM)---a generative model for events
that can be represented as directed edges with one sender and one or more
receivers or one receiver and one or more senders. We integrate a dynamic
version of the exponential random graph model (ERGM) of edge structure with a
survival model for event timing to jointly understand who interacts with whom,
and when. The HEM offers three innovations with respect to the
literature---first, it extends a growing class of dynamic network models to
model hyperedges. The current state-of-the-art approach to dealing with
hyperedges is to inappropriately break them into separate edges/events. Second,
our model involves a novel receiver selection distribution that is based on
established edge formation models, but assures non-empty receiver lists. Third,
the HEM integrates separate, but interacting, equations governing edge
formation and event timing. We use the HEM to analyze emails sent among
department managers in Montgomery County government in North Carolina. Our
application demonstrates that the model is effective at predicting and
explaining time-stamped network data involving edges with multiple receivers.
We present an out-of-sample prediction experiment to illustrate how researchers
can select between different specifications of the model.
</summary>
    <author>
      <name>Bomin Kim</name>
    </author>
    <author>
      <name>Aaron Schein</name>
    </author>
    <author>
      <name>Bruce A. Desmarais</name>
    </author>
    <author>
      <name>Hanna Wallach</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08216v1</id>
    <updated>2018-07-22T00:43:35Z</updated>
    <published>2018-07-22T00:43:35Z</published>
    <title>Sign-Perturbed Sums: A New System Identification Approach for
  Constructing Exact Non-Asymptotic Confidence Regions in Linear Regression
  Models</title>
    <summary>  We propose a new system identification method, called Sign-Perturbed Sums
(SPS), for constructing non-asymptotic confidence regions under mild
statistical assumptions. SPS is introduced for linear regression models,
including but not limited to FIR systems, and we show that the SPS confidence
regions have exact confidence probabilities, i.e., they contain the true
parameter with a user-chosen exact probability for any finite data set.
Moreover, we also prove that the SPS regions are star convex with the
Least-Squares (LS) estimate as a star center. The main assumptions of SPS are
that the noise terms are independent and symmetrically distributed about zero,
but they can be nonstationary, and their distributions need not be known. The
paper also proposes a computationally efficient ellipsoidal outer approximation
algorithm for SPS. Finally, SPS is demonstrated through a number of simulation
experiments.
</summary>
    <author>
      <name>Balázs Cs. Csáji</name>
    </author>
    <author>
      <name>Marco C. Campi</name>
    </author>
    <author>
      <name>Erik Weyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2014.2369000</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2014.2369000" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, 8 tables, 32 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Signal Processing, Volume 63, Issue 1, 2015,
  pp. 169-181</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.08216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08213v1</id>
    <updated>2018-07-21T23:51:07Z</updated>
    <published>2018-07-21T23:51:07Z</published>
    <title>T-optimal design for multivariate polynomial regression using
  semidefinite programming</title>
    <summary>  We consider T-optimal experiment design problems for discriminating
multivariate polynomial regression models where the design space is defined by
polynomial inequalities and the regression parameters are constrained to given
convex sets. The original optimality criterion is reformulated as a convex
optimization problem with a moment cone constraint. In the case of univariate
regression models, an exact semidefinite representation of the moment cone
constraint can be applied to obtain an equivalent semidefinite program. For
general multivariate cases, we apply a moment relaxation technique and
approximate the moment cone constraint by a hierarchy of
semidefinite-representable outer approximations. When the relaxation hierarchy
converges, the optimal discrimination design can be recovered from the optimal
moment matrix, and its optimality confirmed by an equivalence theorem. The
methodology is illustrated with several examples.
</summary>
    <author>
      <name>Yuguang Yue</name>
    </author>
    <author>
      <name>Lieven Vandenberghe</name>
    </author>
    <author>
      <name>Weng Kee Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02946v2</id>
    <updated>2018-07-21T04:28:28Z</updated>
    <published>2018-01-09T14:29:56Z</published>
    <title>Max-infinitely divisible models and inference for spatial extremes</title>
    <summary>  We propose a new flexible modelling framework for spatial extremes, based on
the class of max-infinitely divisible processes, extending the class of
max-stable processes while retaining dependence properties that are natural for
maxima: max-infinitely divisible models are positively associated, and they
yield a self-consistent family of models for block maxima defined over any time
unit. We propose two parametric construction principles for max-infinitely
divisible models, emphasising a spectral representation that allows for
asymptotic independence while keeping the max-stable extremal-t model as a
special case. Parameter estimation is performed by pairwise likelihood, and we
illustrate the benefits of our new modelling framework with an application to
Dutch wind gust maxima calculated over different time units.
</summary>
    <author>
      <name>Raphael Huser</name>
    </author>
    <author>
      <name>Thomas Opitz</name>
    </author>
    <author>
      <name>Emeric Thibaud</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02946v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02946v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08038v1</id>
    <updated>2018-07-20T21:32:33Z</updated>
    <published>2018-07-20T21:32:33Z</published>
    <title>Additive and multiplicative effects network models</title>
    <summary>  Network datasets typically exhibit certain types of statistical dependencies,
such as within-dyad correlation, row and column heterogeneity, and third-order
dependence patterns such as transitivity and clustering. The first two of these
can be well-represented statistically with a social relations model, a type of
additive random effects model originally developed for continuous dyadic data.
Third-order patterns can be represented with multiplicative random effects
models, which are related to matrix decompositions commonly used for
matrix-variate data analysis. Additionally, these multiplicative random effects
models generalize other popular latent variable network models, such as the
stochastic blockmodel and the latent space model. In this article we review a
general regression framework for the analysis of network data that combines
these two types of random effects and accommodates a variety of network data
types, including continuous, binary and ordinal network relations.
</summary>
    <author>
      <name>Peter D. Hoff</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25, 62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09326v2</id>
    <updated>2018-07-20T20:37:10Z</updated>
    <published>2017-10-25T16:32:00Z</published>
    <title>A Robust and Unified Framework for Estimating Heritability in Twin
  Studies using Generalized Estimating Equations</title>
    <summary>  The development of a complex disease is an intricate interplay of genetic and
environmental factors. "Heritability" is defined as the proportion of total
trait variance due to genetic factors within a given population. Studies with
monozygotic (MZ) and dizygotic (DZ) twins allow us to estimate heritability by
fitting an "ACE" model which estimates the proportion of trait variance
explained by additive genetic (A), common shared environment (C), and unique
non-shared environmental (E) latent effects, thus helping us better understand
disease risk and etiology. In this paper, we develop a flexible generalized
estimating equations framework ("GEE2") for fitting twin ACE models that
requires minimal distributional assumptions, rather only the first two moments
need to be correctly specified. We prove that two commonly used methods for
estimating heritability, the normal ACE model ("NACE") and Falconer's method,
can both be fit within this unified GEE2 framework, which additionally provides
robust standard errors. Although the traditional Falconer's method cannot
directly adjust for covariates, we show that the corresponding GEE2 version
("GEE2-Falconer") can incorporate covariate effects for both mean and
variance-level parameters (e.g. let heritability vary by sex or age). Given
non-normal data, we show that the GEE2 models attain significantly better
coverage of the true heritability compared to the traditional NACE and
Falconer's methods. Finally, we demonstrate that Falconer's method can
consistently estimate heritability even when the total variance differs between
MZ and DZ twins, whereas the NACE will produce biased estimates in such
settings.
</summary>
    <author>
      <name>Jaron Arbet</name>
    </author>
    <author>
      <name>Saonli Basu</name>
    </author>
    <author>
      <name>Matt McGue</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08018v1</id>
    <updated>2018-07-20T20:17:59Z</updated>
    <published>2018-07-20T20:17:59Z</published>
    <title>Information Estimation Using Non-Parametric Copulas</title>
    <summary>  Estimation of mutual information between random variables has become crucial
in a range of fields, from physics to neuroscience to finance. Estimating
information accurately over a wide range of conditions relies on the
development of flexible methods to describe statistical dependencies among
variables, without imposing potentially invalid assumptions on the data. Such
methods are needed in cases that lack prior knowledge of their statistical
properties and that have limited sample numbers. Here we propose a powerful and
generally applicable information estimator based on non-parametric copulas.
This estimator, called the non-parametric copula-based estimator (NPC), is
tailored to take into account detailed stochastic relationships in the data
independently of the data's marginal distributions. The NPC estimator can be
used both for continuous and discrete numerical variables and thus provides a
single framework for the mutual information estimation of both continuous and
discrete data. By extensive validation on artificial samples drawn from various
statistical distributions, we found that the NPC estimator compares well
against commonly used alternatives. Unlike methods not based on copulas, it
allows an estimation of information that is robust to changes of the details of
the marginal distributions. Unlike parametric copula methods, it remains
accurate regardless of the precise form of the interactions between the
variables. In addition, the NPC estimator had accurate information estimates
even at low sample numbers, in comparison to alternative estimators. The NPC
estimator therefore provides a good balance between general applicability to
arbitrarily shaped statistical dependencies in the data and shows accurate and
robust performance when working with small sample sizes.
</summary>
    <author>
      <name>Houman Safaai</name>
    </author>
    <author>
      <name>Arno Onken</name>
    </author>
    <author>
      <name>Christopher D. Harvey</name>
    </author>
    <author>
      <name>Stefano Panzeri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08976v2</id>
    <updated>2018-07-20T17:45:42Z</updated>
    <published>2017-10-24T20:17:45Z</published>
    <title>A class of multi-resolution approximations for large spatial datasets</title>
    <summary>  Gaussian processes are popular and flexible models for spatial, temporal, and
functional data, but they are computationally infeasible for large datasets. We
discuss Gaussian-process approximations that use basis functions at multiple
resolutions to achieve fast inference and that can (approximately) represent
any spatial covariance structure. We consider two special cases of this
multi-resolution-approximation framework, a taper version and a
domain-partitioning (block) version. We describe theoretical properties and
inference procedures, and study the computational complexity of the methods.
Numerical comparisons and an application to satellite data are also provided.
</summary>
    <author>
      <name>Matthias Katzfuss</name>
    </author>
    <author>
      <name>Wenlong Gong</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02641v5</id>
    <updated>2018-07-20T16:18:04Z</updated>
    <published>2017-07-09T21:24:25Z</published>
    <title>Automated versus do-it-yourself methods for causal inference: Lessons
  learned from a data analysis competition</title>
    <summary>  Statisticians have made great progress in creating methods that reduce our
reliance on parametric assumptions. However this explosion in research has
resulted in a breadth of inferential strategies that both create opportunities
for more reliable inference as well as complicate the choices that an applied
researcher has to make and defend. Relatedly, researchers advocating for new
methods typically compare their method to at best 2 or 3 other causal inference
strategies and test using simulations that may or may not be designed to
equally tease out flaws in all the competing methods. The causal inference data
analysis challenge, "Is Your SATT Where It's At?", launched as part of the 2016
Atlantic Causal Inference Conference, sought to make progress with respect to
both of these issues. The researchers creating the data testing grounds were
distinct from the researchers submitting methods whose efficacy would be
evaluated. Results from 30 competitors across the two versions of the
competition (black box algorithms and do-it-yourself analyses) are presented
along with post-hoc analyses that reveal information about the characteristics
of causal inference strategies and settings that affect performance. The most
consistent conclusion was that methods that flexibly model the response surface
perform better overall than methods that fail to do so. Finally new methods are
proposed that combine features of several of the top-performing submitted
methods.
</summary>
    <author>
      <name>Vincent Dorie</name>
    </author>
    <author>
      <name>Jennifer Hill</name>
    </author>
    <author>
      <name>Uri Shalit</name>
    </author>
    <author>
      <name>Marc Scott</name>
    </author>
    <author>
      <name>Dan Cervone</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02641v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02641v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07812v1</id>
    <updated>2018-07-20T12:38:06Z</updated>
    <published>2018-07-20T12:38:06Z</published>
    <title>A Theil-like Class of Inequality Measures, its Asymptotic Normality
  Theory and Applications</title>
    <summary>  In this paper, we consider a coherent theory about the asymptotic
representations for a family of inequality indices called Theil-Like Inequality
Measures (TLIM), within a Gaussian field. The theory uses the functional
empirical process approach. We provide the finite-distribution and uniform
asymptotic normality of the elements of the TLIM class in a unified approach
rather than in a case by case one. The results are then applied to some UEMOA
countries databases.
</summary>
    <author>
      <name>Pape Djiby Mergane</name>
    </author>
    <author>
      <name>Tchilabalo Abozou Kpanzou</name>
    </author>
    <author>
      <name>Diam Ba</name>
    </author>
    <author>
      <name>Gane Samb Lo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05, 62G20, 62G07, 91B82, 62P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07797v1</id>
    <updated>2018-07-20T11:35:35Z</updated>
    <published>2018-07-20T11:35:35Z</published>
    <title>The Sliding Window Discrete Fourier Transform</title>
    <summary>  This paper introduces a new tool for time-series analysis: the Sliding Window
Discrete Fourier Transform (SWDFT). The SWDFT is especially useful for
time-series with local- in-time periodic components. We define a 5-parameter
model for noiseless local periodic signals, then study the SWDFT of this model.
Our study illustrates several key concepts crucial to analyzing time-series
with the SWDFT, in particular Aliasing, Leakage, and Ringing. We also show how
these ideas extend to R &gt; 1 local periodic components, using the linearity
property of the Fourier transform. Next, we propose a simple procedure for
estimating the 5 parameters of our local periodic signal model using the SWDFT.
Our estimation procedure speeds up computation by using a trigonometric
identity that linearizes estimation of 2 of the 5 parameters. We conclude with
a very small Monte Carlo simulation study of our estimation procedure under
different levels of noise.
</summary>
    <author>
      <name>Lee F. Richardson</name>
    </author>
    <author>
      <name>William F. Eddy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09150v2</id>
    <updated>2018-07-20T05:18:15Z</updated>
    <published>2017-12-26T00:38:39Z</published>
    <title>Variational Bayes Estimation of Discrete-Margined Copula Models with
  Application to Time Series</title>
    <summary>  We propose a new variational Bayes estimator for high-dimensional copulas
with discrete, or a combination of discrete and continuous, margins. The method
is based on a variational approximation to a tractable augmented posterior, and
is faster than previous likelihood-based approaches. We use it to estimate
drawable vine copulas for univariate and multivariate Markov ordinal and mixed
time series. These have dimension $rT$, where $T$ is the number of observations
and $r$ is the number of series, and are difficult to estimate using previous
methods. The vine pair-copulas are carefully selected to allow for
heteroskedasticity, which is a feature of most ordinal time series data. When
combined with flexible margins, the resulting time series models also allow for
other common features of ordinal data, such as zero inflation, multiple modes
and under- or over-dispersion. Using six example series, we illustrate both the
flexibility of the time series copula models, and the efficacy of the
variational Bayes estimator for copulas of up to 792 dimensions and 60
parameters. This far exceeds the size and complexity of copula models for
discrete data that can be estimated using previous methods.
</summary>
    <author>
      <name>Ruben Loaiza-Maya</name>
    </author>
    <author>
      <name>Michael Stanley Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09150v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09150v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06221v2</id>
    <updated>2018-07-20T04:47:05Z</updated>
    <published>2017-02-21T00:28:39Z</published>
    <title>Determination of hysteresis in finite-state random walks using Bayesian
  cross validation</title>
    <summary>  Consider the problem of modeling hysteresis for finite-state random walks
using higher-order Markov chains. This Letter introduces a Bayesian framework
to determine, from data, the number of prior states of recent history upon
which a trajectory is statistically dependent. The general recommendation is to
use leave-one-out cross validation, using an easily-computable formula that is
provided in closed form. Importantly, Bayes factors using flat model priors are
biased in favor of too-complex a model (more hysteresis) when a large amount of
data is present and the Akaike information criterion (AIC) is biased in favor
of too-sparse a model (less hysteresis) when few data are present.
</summary>
    <author>
      <name>Joshua C. Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Reworked as totally different paper in arXiv:1706.08881</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07697v1</id>
    <updated>2018-07-20T02:23:49Z</updated>
    <published>2018-07-20T02:23:49Z</published>
    <title>Wild Residual Bootstrap Inference for Penalized Quantile Regression with
  Heteroscedastic Errors</title>
    <summary>  We consider a heteroscedastic regression model in which some of the
regression coefficients are zero but it is not known which ones. Penalized
quantile regression is a useful approach for analyzing such data. By allowing
different covariates to be relevant for modeling conditional quantile functions
at different quantile levels, it provides a more complete picture of the
conditional distribution of a response variable than mean regression. Existing
work on penalized quantile regression has been mostly focused on point
estimation. Although bootstrap procedures have recently been shown to be
effective for inference for penalized mean regression, they are not directly
applicable to penalized quantile regression with heteroscedastic errors. We
prove that a wild residual bootstrap procedure for unpenalized quantile
regression is asymptotically valid for approximating the distribution of a
penalized quantile regression estimator with an adaptive $L_1$ penalty and that
a modified version can be used to approximate the distribution of
$L_1$-penalized quantile regression estimator. The new methods do not need to
estimate the unknown error density function. We establish consistency,
demonstrate finite sample performance, and illustrate the applications on a
real data example.
</summary>
    <author>
      <name>Lan Wang</name>
    </author>
    <author>
      <name>Ingrid Van Keilegrom</name>
    </author>
    <author>
      <name>Adam Maidman</name>
    </author>
    <link href="http://arxiv.org/abs/1807.07697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08278v3</id>
    <updated>2018-07-19T15:39:35Z</updated>
    <published>2017-08-28T11:46:00Z</published>
    <title>Why optional stopping is a problem for Bayesians</title>
    <summary>  Recently, optional stopping has been a subject of debate in the Bayesian
psychology community. Rouder (2014) argues that optional stopping is no problem
for Bayesians, and even recommends the use of optional stopping in practice, as
do Wagenmakers et al. (2012). This article addresses the question whether
optional stopping is problematic for Bayesian methods, and specifies under
which circumstances and in which sense it is and is not. By slightly varying
and extending Rouder's (2014) experiment, we illustrate that, as soon as the
parameters of interest are equipped with default or pragmatic priors - which
means, in most practical applications of Bayes Factor hypothesis testing -
resilience to optional stopping can break down. We distinguish between four
types of default priors, each having their own specific issues with optional
stopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type
II and III priors).
</summary>
    <author>
      <name>Rianne de Heide</name>
    </author>
    <author>
      <name>Peter D. Grünwald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In this version an error in the section about regression is removed,
  and a table is added providing researchers with a simplified overview of four
  common default Bayes factors indicating which forms of optional stopping they
  can handle</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.08278v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08278v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03616v2</id>
    <updated>2018-07-19T13:05:00Z</updated>
    <published>2018-04-10T16:20:30Z</published>
    <title>Fast and scalable non-parametric Bayesian inference for Poisson point
  processes</title>
    <summary>  We study the problem of non-parametric Bayesian estimation of the intensity
function of a Poisson point process. The observations are assumed to be $n$
independent realisations of a Poisson point process on the interval $[0,T]$. We
propose two related approaches. In both approaches we model the intensity
function as piecewise constant on $N$ bins forming a partition of the interval
$[0,T]$. In the first approach the coefficients of the intensity function are
assigned independent Gamma priors. This leads to a closed form posterior
distribution, for which posterior inference is straightforward to perform in
practice, without need to recourse to approximate inference methods. The method
scales extremely well with the amount of data. On the theoretical side, we
prove that the approach is consistent: as $n\to \infty$, the posterior
distribution asymptotically concentrates around the "true", data-generating
intensity function at the rate that is optimal for estimating $h$-H\"older
regular intensity functions ($0 &lt; h\leq 1$), provided the number of
coefficients $N$ of the intensity function grows at a suitable rate depending
on the sample size $n$.
  In the second approach it is assumed that the prior distribution on the
coefficients of the intensity function forms a Gamma Markov chain. The
posterior distribution is no longer available in closed form, but inference can
be performed using a straightforward version of the Gibbs sampler. We show that
also this second approach scales well.
  Practical performance of our methods is first demonstrated via synthetic data
examples. It it shown that the second approach depends in a less sensitive way
on the choice of the number of bins $N$ and outperforms the first approach in
practice. Finally, we analyse three real datasets using our methodology: the UK
coal mining disasters data, the US mass shootings data and Donald Trump's
Twitter data.
</summary>
    <author>
      <name>Shota Gugushvili</name>
    </author>
    <author>
      <name>Frank van der Meulen</name>
    </author>
    <author>
      <name>Moritz Schauer</name>
    </author>
    <author>
      <name>Peter Spreij</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03616v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03616v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20 (Primary) 62M30 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07449v4</id>
    <updated>2018-07-19T12:24:55Z</updated>
    <published>2017-06-22T18:16:50Z</published>
    <title>Nonparametric Bayesian estimation of a Hölder continuous diffusion
  coefficient</title>
    <summary>  We consider a nonparametric Bayesian approach to estimate the diffusion
coefficient of a stochastic differential equation given discrete time
observations over a fixed time interval. As a prior on the diffusion
coefficient, we employ a histogram-type prior with piecewise constant
realisations on bins forming a partition of the time interval. Specifically,
these constants are realizations of independent inverse Gamma distributed
randoma variables. We justify our approach by deriving the rate at which the
corresponding posterior distribution asymptotically concentrates around the
data-generating diffusion coefficient. This posterior contraction rate turns
out to be optimal for estimation of a H\"older-continuous diffusion coefficient
with smoothness parameter $0&lt;\lambda\leq 1.$ Our approach is straightforward to
implement, as the posterior distributions turn out to be inverse Gamma again,
and leads to good practical results in a wide range of simulation examples.
Finally, we apply our method on exchange rate data sets.
</summary>
    <author>
      <name>Shota Gugushvili</name>
    </author>
    <author>
      <name>Frank van der Meulen</name>
    </author>
    <author>
      <name>Moritz Schauer</name>
    </author>
    <author>
      <name>Peter Spreij</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07449v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07449v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20 (Primary), 62M05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06993v1</id>
    <updated>2018-07-18T15:27:00Z</updated>
    <published>2018-07-18T15:27:00Z</published>
    <title>Cross Validation Based Model Selection via Generalized Method of Moments</title>
    <summary>  Structural estimation is an important methodology in empirical economics, and
a large class of structural models are estimated through the generalized method
of moments (GMM). Traditionally, selection of structural models has been
performed based on model fit upon estimation, which take the entire observed
samples. In this paper, we propose a model selection procedure based on
cross-validation (CV), which utilizes sample-splitting technique to avoid
issues such as over-fitting. While CV is widely used in machine learning
communities, we are the first to prove its consistency in model selection in
GMM framework. Its empirical property is compared to existing methods by
simulations of IV regressions and oligopoly market model. In addition, we
propose the way to apply our method to Mathematical Programming of Equilibrium
Constraint (MPEC) approach. Finally, we perform our method to online-retail
sales data to compare dynamic market model to static model.
</summary>
    <author>
      <name>Junpei Komiyama</name>
    </author>
    <author>
      <name>Hajime Shimao</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06776v1</id>
    <updated>2018-07-18T05:21:47Z</updated>
    <published>2018-07-18T05:21:47Z</published>
    <title>Detecting strong signals in gene perturbation experiments: An adaptive
  approach with power guarantee and FDR control</title>
    <summary>  The perturbation of a transcription factor should affect the expression
levels of its direct targets. However, not all genes showing changes in
expression are direct targets. To increase the chance of detecting direct
targets, we propose a modified two-group model where the null group corresponds
to genes which are not direct targets, but can have small non-zero effects. We
model the behaviour of genes from the null set by a Gaussian distribution with
unknown variance $\tau^2$, and we discuss and compare three methods which
adaptively estimate $\tau^2$ from the data: the iterated empirical Bayes
estimator, the truncated MLE and the central moment matching estimator. We
conduct a detailed analysis of the properties of the iterated EB estimate which
has the best performance in the simulations. In particular, we provide
theoretical guarantee of its good performance under mild conditions.
  We provide simulations comparing the new modeling approach with existing
methods, and the new approach shows more stable and better performance under
different situations. We also apply it to a real data set from gene knock-down
experiments and obtained better results compared with the original two-group
model testing for non-zero effects.
</summary>
    <author>
      <name>Leying Guan</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Wing Hung Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05163v3</id>
    <updated>2018-07-18T01:11:31Z</updated>
    <published>2018-04-14T04:25:53Z</published>
    <title>Development of a Common Patient Assessment Scale across the Continuum of
  Care: A Nested Multiple Imputation Approach</title>
    <summary>  Evaluating and tracking patients' functional status through the post-acute
care continuum requires a common instrument. However, different post-acute
service providers such as nursing homes, inpatient rehabilitation facilities
and home health agencies rely on different instruments to evaluate patients'
functional status. These instruments assess similar functional status domains,
but they comprise different activities, rating scales and scoring instructions.
These differences hinder the comparison of patients' assessments across health
care settings. We propose a two-step procedure that combines nested multiple
imputation with the multivariate ordinal probit (MVOP) model to obtain a common
patient assessment scale across the post-acute care continuum. Our procedure
imputes the unmeasured assessments at multiple assessment dates and enables
evaluation and comparison of the rates of functional improvement experienced by
patients treated in different health care settings using a common measure. To
generate multiple imputations of the unmeasured assessments using the MVOP
model, a likelihood-based approach that combines the EM algorithm and the
bootstrap method as well as a fully Bayesian approach using the data
augmentation algorithm are developed. Using a dataset on patients who suffered
a stroke, we simulate missing assessments and compare the MVOP model to
existing methods for imputing incomplete multivariate ordinal variables. We
show that, for all of the estimands considered, and in most of the experimental
conditions that were examined, the MVOP model appears to be superior. The
proposed procedure is then applied to patients who suffered a stroke and were
released from rehabilitation facilities either to skilled nursing facilities or
to their homes.
</summary>
    <author>
      <name>Chenyang Gu</name>
    </author>
    <author>
      <name>Roee Gutman</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05163v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05163v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08795v2</id>
    <updated>2018-07-17T22:48:08Z</updated>
    <published>2017-09-26T03:04:11Z</published>
    <title>On Stein's Identity and Near-Optimal Estimation in High-dimensional
  Index Models</title>
    <summary>  We consider estimating the parametric components of semi-parametric multiple
index models in a high-dimensional and non-Gaussian setting. Such models form a
rich class of non-linear models with applications to signal processing, machine
learning and statistics. Our estimators leverage the score function based first
and second-order Stein's identities and do not require the covariates to
satisfy Gaussian or elliptical symmetry assumptions common in the literature.
Moreover, to handle score functions and responses that are heavy-tailed, our
estimators are constructed via carefully thresholding their empirical
counterparts. We show that our estimator achieves near-optimal statistical rate
of convergence in several settings. We supplement our theoretical results via
simulation experiments that confirm the theory.
</summary>
    <author>
      <name>Zhuoran Yang</name>
    </author>
    <author>
      <name>Krishnakumar Balasubramanian</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08795v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08795v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06693v1</id>
    <updated>2018-07-17T22:28:43Z</updated>
    <published>2018-07-17T22:28:43Z</published>
    <title>Tensor Methods for Additive Index Models under Discordance and
  Heterogeneity</title>
    <summary>  Motivated by the sampling problems and heterogeneity issues common in high-
dimensional big datasets, we consider a class of discordant additive index
models. We propose method of moments based procedures for estimating the
indices of such discordant additive index models in both low and
high-dimensional settings. Our estimators are based on factorizing certain
moment tensors and are also applicable in the overcomplete setting, where the
number of indices is more than the dimensionality of the datasets. Furthermore,
we provide rates of convergence of our estimator in both high and
low-dimensional setting. Establishing such results requires deriving tensor
operator norm concentration inequalities that might be of independent interest.
Finally, we provide simulation results supporting our theory. Our contributions
extend the applicability of tensor methods for novel models in addition to
making progress on understanding theoretical properties of such tensor methods.
</summary>
    <author>
      <name>Krishnakumar Balasubramanian</name>
    </author>
    <author>
      <name>Jianqing Fan</name>
    </author>
    <author>
      <name>Zhuoran Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06690v1</id>
    <updated>2018-07-17T22:18:25Z</updated>
    <published>2018-07-17T22:18:25Z</published>
    <title>A transformation-based approach to Gaussian mixture density estimation
  for bounded data</title>
    <summary>  Finite mixture of Gaussian distributions provide a flexible semi-parametric
methodology for density estimation when the variables under investigation have
no boundaries. However, in practical applications variables may be partially
bounded (e.g. taking non-negative values) or completely bounded (e.g. taking
values in the unit interval). In this case the standard Gaussian finite mixture
model assigns non-zero densities to any possible values, even to those outside
the ranges where the variables are defined, hence resulting in severe bias. In
this paper we propose a transformation-based approach for Gaussian mixture
modelling in case of bounded variables. The basic idea is to carry out density
estimation not on the original data but on appropriately transformed data.
Then, the density for the original data can be obtained by a change of
variables. Both the transformation parameters and the parameters of the
Gaussian mixture are jointly estimated by the Expectation-Maximisation (EM)
algorithm. The methodology for partially and completely bounded data is
illustrated using both simulated data and real data applications.
</summary>
    <author>
      <name>Luca Scrucca</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06661v1</id>
    <updated>2018-07-17T20:33:34Z</updated>
    <published>2018-07-17T20:33:34Z</published>
    <title>Model selection for sequential designs in discrete finite systems using
  Bernstein kernels</title>
    <summary>  We view sequential design as a model selection problem to determine which new
observation is expected to be the most informative, given the existing set of
observations. For estimating a probability distribution on a bounded interval,
we use bounds constructed from kernel density estimators along with the
estimated density itself to estimate the information gain expected from each
observation. We choose Bernstein polynomials for the kernel functions because
they provide a complete set of basis functions for polynomials of finite degree
and thus have useful convergence properties. We illustrate the method with
applications to estimating network reliability polynomials, which give the
probability of certain sets of configurations in finite, discrete stochastic
systems.
</summary>
    <author>
      <name>Madhurima Nath</name>
    </author>
    <author>
      <name>Stephen Eubank</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09681v1</id>
    <updated>2018-07-17T16:20:01Z</updated>
    <published>2018-07-17T16:20:01Z</published>
    <title>Spatially varying coefficient modeling for large datasets: Eliminating N
  from spatial regressions</title>
    <summary>  While spatially varying coefficient (SVC) modeling is popular in applied
science, its computational burden is substantial. This is especially true if a
multiscale property of SVC is considered. Given this background, this study
develops a Moran's eigenvector-based spatially varying coefficients (M-SVC)
modeling approach that estimates multiscale SVCs computationally efficiently.
This estimation is accelerated through a (i) rank reduction, (ii)
pre-compression, and (iii) sequential likelihood maximization. Steps (i) and
(ii) eliminate the sample size N from the likelihood function; after these
steps, the likelihood maximization cost is independent of N. Step (iii) further
accelerates the likelihood maximization so that multiscale SVCs can be
estimated even if the number of SVCs, K, is large. The M-SVC approach is
compared with geographically weighted regression (GWR) through Monte Carlo
simulation experiments. These simulation results show that our approach is far
faster than GWR when N is large, despite numerically estimating 2K parameters
while GWR numerically estimates only 1 parameter. Then, the proposed approach
is applied to a land price analysis as an illustration. The developed SVC
estimation approach is implemented in the R package "spmoran."
</summary>
    <author>
      <name>Daisuke Murakami</name>
    </author>
    <author>
      <name>Daniel A. Griffith</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06480v1</id>
    <updated>2018-07-17T14:50:43Z</updated>
    <published>2018-07-17T14:50:43Z</published>
    <title>Roos' Matrix Permanent Approximation Bounds for Data Association
  Probabilities</title>
    <summary>  Matrix permanent plays a key role in data association probability
calculations. Exact algorithms (such as Ryser's) scale exponentially with
matrix size. Fully polynomial time randomized approximation schemes exist but
are quite complex. This letter introduces to the tracking community a simple
approximation algorithm with error bounds, recently developed by Bero Roos, and
illustrates its potential use for estimating probabilities of data association
hypotheses.
</summary>
    <author>
      <name>Lingji Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05660v2</id>
    <updated>2018-07-17T13:03:10Z</updated>
    <published>2015-05-21T09:48:06Z</published>
    <title>A factor model approach for the joint segmentation with between-series
  correlation</title>
    <summary>  We consider the segmentation of set of correlated time-series, the
correlation being allowed to take an arbitrary form but being the same at each
time-position. We show that encoding the dependency in a factor model enables
us to use the dynamic programming algorithm for the inference of the
breakpoints, which remains one the most efficient algorithm. We propose a model
selection procedure to determine both the number of breakpoints and the number
of factors. This proposed method is implemented in the FASeg R package, which
is available on the CRAN. We demonstrate the performances of our procedure
through simulation experiments and an application to geodesic data is
presented.
</summary>
    <author>
      <name>Xavier Collilieux</name>
    </author>
    <author>
      <name>Emilie Lebarbier</name>
    </author>
    <author>
      <name>Stéphane Robin</name>
    </author>
    <link href="http://arxiv.org/abs/1505.05660v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05660v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06367v1</id>
    <updated>2018-07-17T11:56:28Z</updated>
    <published>2018-07-17T11:56:28Z</published>
    <title>More good news on the (only) affine invariant test for multivariate
  reflected symmetry about an unknown center</title>
    <summary>  We revisit the problem of testing for multivariate reflected symmetry about
an unspecified point. Although this testing problem is invariant with respect
to full-rank affine transformations, among the hitherto few proposed tests only
the test studied in [12] respects this property. We identify a measure of
deviation $\Delta$ (say) from symmetry associated with the test statistic $T_n$
(say), and we obtain the limit normal distribution of $T_n$ as $n \to \infty$
under a fixed alternative to symmetry. Since a consistent estimator of the
variance of this limit normal distribution is available, we obtain an
asymptotic confidence interval for $\Delta$. The test, when applied to a
classical data set, strongly rejects the hypothesis of reflected symmetry,
although other tests even do not object against the much stronger hypothesis of
elliptical symmetry.
</summary>
    <author>
      <name>Norbert Henze</name>
    </author>
    <author>
      <name>Celeste Mayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 1 figure, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H15, 62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06217v1</id>
    <updated>2018-07-17T04:27:40Z</updated>
    <published>2018-07-17T04:27:40Z</published>
    <title>An exposition of the false confidence theorem</title>
    <summary>  A recent paper presents the "false confidence theorem" (FCT) which has
potentially broad implications for statistical inference using Bayesian
posterior uncertainty. This theorem says that with arbitrarily large
(sampling/frequentist) probability, there exists a set which does \textit{not}
contain the true parameter value, but which has arbitrarily large posterior
probability. Since the use of Bayesian methods has become increasingly popular
in applications of science, engineering, and business, it is critically
important to understand when Bayesian procedures lead to problematic
statistical inferences or interpretations. In this paper, we consider a number
of examples demonstrating the paradoxical nature of false confidence to begin
to understand the contexts in which the FCT does (and does not) play a
meaningful role in statistical inference. Our examples illustrate that models
involving marginalization to non-linear, not one-to-one functions of multiple
parameters play a key role in more extreme manifestations of false confidence.
</summary>
    <author>
      <name>Iain Carmichael</name>
    </author>
    <author>
      <name>Jonathan P Williams</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06111v1</id>
    <updated>2018-07-16T21:04:55Z</updated>
    <published>2018-07-16T21:04:55Z</published>
    <title>Markov chain random fields, spatial Bayesian networks, and optimal
  neighborhoods for simulation of categorical fields</title>
    <summary>  The Markov chain random field (MCRF) model/theory provides a non-linear
spatial Bayesian updating solution at the neighborhood nearest data level for
simulating categorical spatial variables. In the MCRF solution, the spatial
dependencies among nearest data and the central random variable is a
probabilistic directed acyclic graph that conforms to a Bayesian network on
spatial data. By selecting different neighborhood sizes and structures,
applying the conditional independence assumption to nearest neighbors, or
incorporating ancillary information, one may construct specific MCRF models
based on the MCRF general solution for various application purposes. Simplified
MCRF models based on assuming the conditional independence of nearest data
involve only spatial transition probabilities, and one can implement them
easily in sequential simulations. In this article, we prove the spatial
Bayesian network characteristic of MCRFs, and test the optimal neighborhoods
under the spatial conditional independence assumption. The testing results
indicate that the quadrantal (i.e., one nearest datum per quadrant)
neighborhood is generally the best choice for the simplified MCRF solution,
performing better than other sectored neighborhoods and non-sectored
neighborhoods with regard to simulation accuracy and pattern rationality.
</summary>
    <author>
      <name>Weidong Li</name>
    </author>
    <author>
      <name>Chuanrong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05819v1</id>
    <updated>2018-07-16T12:33:58Z</updated>
    <published>2018-07-16T12:33:58Z</published>
    <title>Bayes factor testing of equality and order constraints on measures of
  association in social research</title>
    <summary>  Measures of association play a central role in the social sciences to
quantify the degree of association between the variables of interest. In many
applications researchers can translate scientific expectations to hypotheses
with equality and/or order constraints on these measures of association. In
this paper a Bayes factor test is proposed for testing multiple hypotheses with
constraints on the measures of association between ordinal and/or continuous
variables, possibly after correcting for certain covariates. This test can be
used to obtain a direct answer to the research question how much evidence there
is in the data for a social science theory relative to competing theories. The
accompanying software package 'BCT' allows users to apply the methodology in an
easy manner. An empirical application from leisure studies about the
associations between life, leisure and relationship satisfaction is used to
illustrate the methodology.
</summary>
    <author>
      <name>Joris Mulder</name>
    </author>
    <author>
      <name>John P. T. M. Gelissen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 3 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05781v1</id>
    <updated>2018-07-16T10:43:30Z</updated>
    <published>2018-07-16T10:43:30Z</published>
    <title>Improving Safety of the Continual Reassessment Method via a Modified
  Allocation Rule</title>
    <summary>  This paper proposes a novel criterion for the allocation of patients in
Phase~I dose-escalation clinical trials aiming to find the maximum tolerated
dose (MTD). Conventionally, using a model-based approach the next patient is
allocated to the dose with the toxicity estimate closest (in terms of the
absolute or squared distance) to the maximum acceptable toxicity. This
approach, however, ignores the uncertainty in point estimates and ethical
concerns of assigning a lot of patients to overly toxic doses. Motivated by
recent discussions in the theory of estimation in restricted parameter spaces,
we propose a criterion which accounts for both of these issues. The criterion
requires a specification of one additional parameter only which has a simple
and intuitive interpretation. We incorporate the proposed criterion into the
one-parameter Bayesian continual reassessment method (CRM) and show, using
simulations, that it results in the same proportion of correct selections on
average as the original design, but in fewer mean number of toxic responses. A
comparison to other model-based dose-escalation designs demonstrates that the
proposed design can result in either the same mean accuracy as alternatives but
fewer number of toxic responses, or in a higher mean accuracy but the same
number of toxic responses. We conclude that the new criterion makes the
existing model-based designs more ethical without losing efficiency in the
context of Phase I clinical trials.
</summary>
    <author>
      <name>Pavel Mozgunov</name>
    </author>
    <author>
      <name>Thomas Jaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 7 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04394v2</id>
    <updated>2018-07-16T07:03:31Z</updated>
    <published>2018-05-11T13:40:59Z</published>
    <title>False discovery rate control under reduced precision computation for
  analysis of neuroimaging data</title>
    <summary>  The mitigation of false positives is an important issue when conducting
multiple hypothesis testing. The most popular paradigm for false positives
mitigation in high-dimensional applications is via the control of the false
discovery rate (FDR). Multiple testing data from neuroimaging experiments can
be very large, and reduced precision storage of such data is often required.
Reduced precision computation is often a problem in the analysis of legacy data
and data arising from legacy pipelines. We present a method for FDR control
that is applicable in cases where only p\text{-values} or test statistics (with
common and known null distribution) are available, and when those
p\text{-values} or test statistics are encoded in a reduced precision format.
Our method is based on an empirical-Bayes paradigm where the probit
transformation of the p\text{-values} (called the z\text{-scores}) are modeled
as a two-component mixture of normal distributions. Due to the reduced
precision of the p\text{-values} or test statistics, the usual approach for
fitting mixture models may not be feasible. We instead use a binned-data
technique, which can be proved to consistently estimate the z\text{-score}
distribution parameters under mild correlation assumptions, as is often the
case in neuroimaging data. A simulation study shows that our methodology is
competitive when compared with popular alternatives, especially with data in
the presence of misspecification. We demonstrate the applicability of our
methodology in practice via a brain imaging study of mice.
</summary>
    <author>
      <name>Hien D. Nguyen</name>
    </author>
    <author>
      <name>Yohan Yee</name>
    </author>
    <author>
      <name>Geoffrey J. McLachlan</name>
    </author>
    <author>
      <name>Jason P. Lerch</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04394v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04394v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05675v1</id>
    <updated>2018-07-16T04:30:58Z</updated>
    <published>2018-07-16T04:30:58Z</published>
    <title>A latent factor approach for prediction from multiple assays</title>
    <summary>  In many domains such as healthcare or finance, data often come in different
assays or measurement modalities, with features in each assay having a common
theme. Simply concatenating these assays together and performing prediction can
be effective but ignores this structure. In this setting, we propose a model
which contains latent factors specific to each assay, as well as a common
latent factor across assays. We frame our model-fitting procedure, which we
call the "Sparse Factor Method" (SFM), as an optimization problem and present
an iterative algorithm to solve it.
</summary>
    <author>
      <name>J. Kenneth Tay</name>
    </author>
    <author>
      <name>Robert Tibshirani</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05662v1</id>
    <updated>2018-07-16T02:48:16Z</updated>
    <published>2018-07-16T02:48:16Z</published>
    <title>Data Reduction in Markov model using EM algorithm</title>
    <summary>  This paper describes a data reduction technique in case of a markov chain of
specified order. Instead of observing all the transitions in a markov chain we
record only a few of them and treat the remaining part as missing. The decision
about which transitions to be filtered is taken before the observation process
starts. Based on the filtered chain we try to estimate the parameters of the
markov model using EM algorithm. In the first half of the paper we characterize
a class of filtering mechanism for which all the parameters remain
identifiable. In the later half we explain methods of estimation and testing
about the transition probabilities of the markov chain based on the filtered
data. The methods are first developed assuming a simple markov model with each
probability of transition positive, but then generalized for models with
structural zeroes in the transition probability matrix. Further extension is
also done for multiple markov chains. The performance of the developed method
of estimation is studied using simulated data along with a real life data.
</summary>
    <author>
      <name>Atanu Kumar Ghosh</name>
    </author>
    <author>
      <name>Arnab Chakraborty</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02434v2</id>
    <updated>2018-07-16T02:46:10Z</updated>
    <published>2017-01-10T04:26:06Z</published>
    <title>A Conceptual Introduction to Hamiltonian Monte Carlo</title>
    <summary>  Hamiltonian Monte Carlo has proven a remarkable empirical success, but only
recently have we begun to develop a rigorous understanding of why it performs
so well on difficult problems and how it is best applied in practice.
Unfortunately, that understanding is confined within the mathematics of
differential geometry which has limited its dissemination, especially to the
applied communities for which it is particularly important. In this review I
provide a comprehensive conceptual account of these theoretical foundations,
focusing on developing a principled intuition behind the method and its optimal
implementations rather of any exhaustive rigor. Whether a practitioner or a
statistician, the dedicated reader will acquire a solid grasp of how
Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly,
when it fails.
</summary>
    <author>
      <name>Michael Betancourt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">60 pages, 42 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02434v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02434v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04967v4</id>
    <updated>2018-07-15T19:07:28Z</updated>
    <published>2016-09-16T09:33:32Z</published>
    <title>Semiparametric estimation for isotropic max-stable space-time processes</title>
    <summary>  Regularly varying space-time processes have proved useful to study extremal
dependence in space-time data. We propose a semiparametric estimation procedure
based on a closed form expression of the extremogram to estimate parametric
models of extremal dependence functions. We establish the asymptotic properties
of the resulting parameter estimates and propose subsampling procedures to
obtain asymptotically correct confidence intervals. A simulation study shows
that the proposed procedure works well for moderate sample sizes and is robust
to small departures from the underlying model. Finally, we apply this
estimation procedure to fitting a max-stable process to radar rainfall
measurements in a region in Florida. Complementary results and some proofs of
key results are presented together with the simulation study in the supplement.
</summary>
    <author>
      <name>Sven Buhl</name>
    </author>
    <author>
      <name>Richard A. Davis</name>
    </author>
    <author>
      <name>Claudia Klüppelberg</name>
    </author>
    <author>
      <name>Christina Steinkohl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04967v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04967v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G70, 62F12, 62G32, 62M30, 62P12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01665v2</id>
    <updated>2018-07-15T15:51:07Z</updated>
    <published>2017-03-05T21:14:28Z</published>
    <title>Anisotropic functional Laplace deconvolution</title>
    <summary>  In the present paper we consider the problem of estimating a
three-dimensional function $f$ based on observations from its noisy Laplace
convolution. Our study is motivated by the analysis of Dynamic Contrast
Enhanced (DCE) imaging data. We construct an adaptive wavelet-Laguerre
estimator of $f$, derive minimax lower bounds for the $L^2$-risk when $f$
belongs to a three-dimensional Laguerre-Sobolev ball and demonstrate that the
wavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a
wide range of Laguerre-Sobolev spaces. We carry out a limited simulations study
and show that the estimator performs well in a finite sample setting. Finally,
we use the technique for the solution of the Laplace deconvolution problem on
the basis of DCE Computerized Tomography data.
</summary>
    <author>
      <name>Rida Benhaddou</name>
    </author>
    <author>
      <name>Marianna Pensky</name>
    </author>
    <author>
      <name>Rasika Rajapakshage</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01665v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01665v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62G05, , secondary 62G08, 62P35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01559v4</id>
    <updated>2018-07-15T09:47:23Z</updated>
    <published>2016-05-05T11:42:35Z</published>
    <title>High-dimensional Bayesian inference via the Unadjusted Langevin
  Algorithm</title>
    <summary>  We consider in this paper the problem of sampling a high-dimensional
probability distribution $\pi$ having a density with respect to the Lebesgue
measure on $\mathbb{R}^d$, known up to a normalization constant $x \mapsto
\pi(x)= \mathrm{e}^{-U(x)}/\int_{\mathbb{R}^d} \mathrm{e}^{-U(y)} \mathrm{d}
y$. Such problem naturally occurs for example in Bayesian inference and machine
learning. Under the assumption that $U$ is continuously differentiable, $\nabla
U$ is globally Lipschitz and $U$ is strongly convex, we obtain non-asymptotic
bounds for the convergence to stationarity in Wasserstein distance of order $2$
and total variation distance of the sampling method based on the Euler
discretization of the Langevin stochastic differential equation, for both
constant and decreasing step sizes. The dependence on the dimension of the
state space of these bounds is explicit. The convergence of an appropriately
weighted empirical measure is also investigated and bounds for the mean square
error and exponential deviation inequality are reported for functions which are
measurable and bounded. An illustration to Bayesian inference for binary
regression is presented to support our claims.
</summary>
    <author>
      <name>Alain Durmus</name>
    </author>
    <author>
      <name>Eric Moulines</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary material available at
  https://hal.inria.fr/hal-01176084/. arXiv admin note: substantial text
  overlap with arXiv:1507.05021</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01559v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01559v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05570v2</id>
    <updated>2018-07-15T03:23:55Z</updated>
    <published>2017-11-15T13:53:18Z</published>
    <title>Extended Sensitivity Analysis for Heterogeneous Unmeasured Confounding
  with An Application to Sibling Studies of Returns to Education</title>
    <summary>  The conventional model for assessing insensitivity to hidden bias in paired
observational studies constructs a worst-case distribution for treatment
assignments subject to bounds on the maximal bias to which any given pair is
subjected. In studies where rare cases of extreme hidden bias are suspected,
the maximal bias may be substantially larger than the typical bias across
pairs, such that a correctly specified bound on the maximal bias would yield an
unduly pessimistic perception of the study's robustness to hidden bias. We
present an extended sensitivity analysis which allows researchers to
simultaneously bound the maximal and typical bias perturbing the pairs under
investigation while maintaining the desired Type I error rate. We motivate and
illustrate our method with two sibling studies on the impact of schooling on
earnings, one containing information of cognitive ability of siblings and the
other not. Cognitive ability, clearly influential of both earnings and degree
of schooling, is likely similar between members of most sibling pairs yet
could, conceivably, vary drastically for some siblings. The method is
straightforward to implement, simply requiring the solution to a quadratic
program. $\texttt{R}$ code is provided in the supplementary materials.
</summary>
    <author>
      <name>Colin B. Fogarty</name>
    </author>
    <author>
      <name>Raiden B. Hasegawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Both authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08045v3</id>
    <updated>2018-07-15T00:56:22Z</updated>
    <published>2017-08-27T02:55:55Z</published>
    <title>Backward Simulation of Stochastic Process using a Time Reverse Monte
  Carlo method</title>
    <summary>  The "backward simulation" of a stochastic process is defined as the
stochastic dynamics that trace a time-reversed path from the target region to
the initial configuration. If the probabilities calculated by the original
simulation are easily restored from those obtained by backward dynamics, we can
use it as a computational tool. It is shown that the naive approach to backward
simulation does not work as expected. As a remedy, the Time Reverse Monte Carlo
method (TRMC) based on the ideas of Sequential Importance Sampling (SIS) and
Sequential Monte Carlo (SMC) is proposed and successfully tested with a
stochastic typhoon model and the Lorenz 96 model. TRMC with SMC, which contains
resampling steps, is shown to be more efficient for simulations with a larger
number of time steps. A limitation of TRMC and its relation to the Bayes
formula are also discussed.
</summary>
    <author>
      <name>Shinichi Takayanagi</name>
    </author>
    <author>
      <name>Yukito Iba</name>
    </author>
    <link href="http://arxiv.org/abs/1708.08045v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08045v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01532v3</id>
    <updated>2018-07-14T17:33:57Z</updated>
    <published>2016-08-04T13:44:45Z</published>
    <title>Fixed-Effect Regressions on Network Data</title>
    <summary>  This paper considers inference on fixed effects in a linear regression model
estimated from network data. An important special case of our setup is the
two-way regression model. This is a workhorse technique in the analysis of
matched data sets, such as employer-employee or student-teacher panel data. We
formalize how the structure of the network affects the accuracy with which the
fixed effects can be estimated. This allows us to derive sufficient conditions
on the network for consistent estimation and asymptotically-valid inference to
be possible. Estimation of moments is also considered. We allow for general
networks and our setup covers both the dense and sparse case. We provide
numerical results for the estimation of teacher value-added models and
regressions with occupational dummies.
</summary>
    <author>
      <name>Koen Jochmans</name>
    </author>
    <author>
      <name>Martin Weidner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages main text (including bibliography), 15 pages supplementary
  material</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01532v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01532v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05405v1</id>
    <updated>2018-07-14T15:03:24Z</updated>
    <published>2018-07-14T15:03:24Z</published>
    <title>The conditional permutation test</title>
    <summary>  We propose a general new method, the \emph{conditional permutation test}, for
testing the conditional independence of variables $X$ and $Y$ given a
potentially high-dimensional random vector $Z$ that may contain confounding
factors. The proposed test permutes entries of $X$ non-uniformly, so as to
respect the existing dependence between $X$ and $Z$ and thus account for the
presence of these confounders. Like the conditional randomization test of
\citet{candes2018panning}, our test relies on the availability of an
approximation to the distribution of $X \mid Z$---while
\citet{candes2018panning}'s test uses this estimate to draw new $X$ values, for
our test we use this approximation to design an appropriate non-uniform
distribution on permutations of the $X$ values already seen in the true data.
We provide an efficient Markov Chain Monte Carlo sampler for the implementation
of our method, and establish bounds on the Type~I error in terms of the error
in the approximation of the conditional distribution of $X\mid Z$, finding
that, for the worst case test statistic, the inflation in Type I error of the
conditional permutation test is no larger than that of the conditional
randomization test. We validate these theoretical results with experiments on
simulated data and on the Capital Bikeshare data set.
</summary>
    <author>
      <name>Thomas B. Berrett</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <author>
      <name>Rina Foygel Barber</name>
    </author>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04003v2</id>
    <updated>2018-07-14T04:20:11Z</updated>
    <published>2018-07-11T08:49:55Z</published>
    <title>A Multidimensional Hierarchical Framework for Modeling Speed and Ability
  in Computer-based Multidimensional Tests</title>
    <summary>  In psychological and educational computer-based multidimensional tests,
latent speed, a rate of the amount of labor performed on the items with respect
to time, may also be multidimensional. To capture the multidimensionality of
latent speed, this study firstly proposed a multidimensional log-normal
response time (RT) model to consider the potential multidimensional latent
speed. Further, to simultaneously take into account the response accuracy (RA)
and RTs in multidimensional tests, a multidimensional hierarchical modeling
framework was proposed. The framework is an extension of the van der Linden
(2007; doi:10.1007/s11336-006-1478-z) and allows a "plug-and-play approach"
with alternative choices of multidimensional models for RA and RT. The model
parameters within the framework were estimated using the Bayesian Markov chain
Monte Carlo method. The 2012 Program for International Student Assessment
computer-based mathematics data were analyzed first to illustrate the
implications and applications of the proposed models. The results indicated
that it is appropriate to simultaneously consider the multidimensionality of
latent speed and latent ability for multidimensional tests. A brief simulation
study was conducted to evaluate the parameter recovery of the proposed model
and the consequences of ignoring the multidimensionality of latent speed.
</summary>
    <author>
      <name>Peida Zhan</name>
    </author>
    <author>
      <name>Hong Jiao</name>
    </author>
    <author>
      <name>Wen-Chung Wang</name>
    </author>
    <author>
      <name>Kaiwen Man</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 3 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05313v1</id>
    <updated>2018-07-14T00:20:37Z</updated>
    <published>2018-07-14T00:20:37Z</published>
    <title>Learning Causal Hazard Ratio with Endogeneity</title>
    <summary>  Cox's proportional hazards model is one of the most popular statistical
models to evaluate associations of a binary exposure with a censored failure
time outcome. When confounding factors are not fully observed, the exposure
hazard ratio estimated using a Cox model is not causally interpretable. To
address this, we propose novel approaches for identification and estimation of
the causal hazard ratio in the presence of unmeasured confounding factors. Our
approaches are based on a binary instrumental variable and an additional
no-interaction assumption. We derive, to the best of our knowledge, the first
consistent estimator of the population marginal causal hazard ratio within an
instrumental variable framework. Our estimator admits a closed-form
representation, and hence avoids the drawbacks of estimating equation based
estimators. Our approach is illustrated via simulation studies and a data
analysis.
</summary>
    <author>
      <name>Linbo Wang</name>
    </author>
    <author>
      <name>Eric Tchetgen Tchetgen</name>
    </author>
    <author>
      <name>Torben Martinussen</name>
    </author>
    <author>
      <name>Stijn Vansteelandt</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05358v3</id>
    <updated>2018-07-13T22:51:33Z</updated>
    <published>2017-12-14T17:37:52Z</published>
    <title>Model comparison for Gibbs random fields using noisy reversible jump
  Markov chain Monte Carlo</title>
    <summary>  The reversible jump Markov chain Monte Carlo (RJMCMC) method offers an
across-model simulation approach for Bayesian estimation and model comparison,
by exploring the sampling space that consists of several models of possibly
varying dimensions. A naive implementation of RJMCMC to models like Gibbs
random fields suffers from computational difficulties: the posterior
distribution for each model is termed doubly-intractable since computation of
the likelihood function is rarely available. Consequently, it is simply
impossible to simulate a transition of the Markov chain in the presence of
likelihood intractability. A variant of RJMCMC is presented, called noisy
RJMCMC, where the underlying transition kernel is replaced with an
approximation based on unbiased estimators. Based on previous theoretical
developments, convergence guarantees for the noisy RJMCMC algorithm are
provided. The experiments show that the noisy RJMCMC algorithm can be much more
efficient than other exact methods, provided that an estimator with controlled
Monte Carlo variance is used, a fact which is in agreement with the theoretical
analysis.
</summary>
    <author>
      <name>Lampros Bouranis</name>
    </author>
    <author>
      <name>Nial Friel</name>
    </author>
    <author>
      <name>Florian Maire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Computational Statistics and Data
  Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05358v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05358v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05274v1</id>
    <updated>2018-07-13T20:18:32Z</updated>
    <published>2018-07-13T20:18:32Z</published>
    <title>Sparse semiparametric canonical correlation analysis for data of mixed
  types</title>
    <summary>  Canonical correlation analysis investigates linear relationships between two
sets of variables, but often works poorly on modern data sets due to
high-dimensionality and mixed data types (continuous/binary/zero-inflated). We
propose a new approach for sparse canonical correlation analysis of mixed data
types that does not require explicit parametric assumptions. Our main
contribution is the use of truncated latent Gaussian copula to model the data
with excess zeroes, which allows us to derive a rank-based estimator of latent
correlation matrix without the estimation of marginal transformation functions.
The resulting semiparametric sparse canonical correlation analysis method works
well in high-dimensional settings as demonstrated via numerical studies, and
application to the analysis of association between gene expression and micro
RNA data of breast cancer patients.
</summary>
    <author>
      <name>Grace Yoon</name>
    </author>
    <author>
      <name>Raymond J. Carroll</name>
    </author>
    <author>
      <name>Irina Gaynanova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 11 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08665v2</id>
    <updated>2018-07-13T19:02:06Z</updated>
    <published>2017-03-25T09:11:28Z</published>
    <title>Full likelihood inference for max-stable data</title>
    <summary>  We show how to perform full likelihood inference for max-stable multivariate
distributions or processes based on a stochastic Expectation-Maximisation
algorithm, which combines statistical and computational efficiency in
high-dimensions. The good performance of this methodology is demonstrated by
simulation based on the popular logistic and Brown--Resnick models, and it is
shown to provide dramatic computational time improvements with respect to a
direct computation of the likelihood. Strategies to further reduce the
computational burden are also discussed.
</summary>
    <author>
      <name>Raphaël Huser</name>
    </author>
    <author>
      <name>Clément Dombry</name>
    </author>
    <author>
      <name>Mathieu Ribatet</name>
    </author>
    <author>
      <name>Marc G. Genton</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08665v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08665v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05234v1</id>
    <updated>2018-07-13T18:00:17Z</updated>
    <published>2018-07-13T18:00:17Z</published>
    <title>Optimal designs for frequentist model averaging</title>
    <summary>  We consider the problem of designing experiments for the estimation of a
target in regression analysis if there is uncertainty about the parametric form
of the regression function. A new optimality criterion is proposed, which
minimizes the asymptotic mean squared error of the frequentist model averaging
estimate by the choice of an experimental design. Necessary conditions for the
optimal solution of a locally and Bayesian optimal design problem are
established. The results are illustrated in several examples and it is
demonstrated that Bayesian optimal designs can yield a reduction of the mean
squared error of the model averaging estimator up to $45\%$.
</summary>
    <author>
      <name>Kira Alhorn</name>
    </author>
    <author>
      <name>Kirsten Schorning</name>
    </author>
    <author>
      <name>Holger Dette</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05048v1</id>
    <updated>2018-07-13T13:09:28Z</updated>
    <published>2018-07-13T13:09:28Z</published>
    <title>Improved Methods for Making Inferences About Multiple Skipped
  Correlations</title>
    <summary>  A skipped correlation has the advantage of dealing with outliers in a manner
that takes into account the overall structure of the data cloud. For p-variate
data, $p \ge 2$, there is an extant method for testing the hypothesis of a zero
correlation for each pair of variables that is designed to control the
probability of one or more Type I errors. And there are methods for the related
situation where the focus is on the association between a dependent variable
and $p$ explanatory variables. However, there are limitations and several
concerns with extant techniques. The paper describes alternative approaches
that deal with these issues.
</summary>
    <author>
      <name>Rand Wilcox</name>
    </author>
    <author>
      <name>Guillaume Rousselet</name>
    </author>
    <author>
      <name>Cyril Pernet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05035v1</id>
    <updated>2018-07-13T12:29:06Z</updated>
    <published>2018-07-13T12:29:06Z</published>
    <title>Conditional Masking to Numerical Data</title>
    <summary>  Protecting the privacy of data-sets has become hugely important these days.
Many real-life data-sets like income data, medical data need to be secured
before making it public. However, security comes at the cost of losing some
useful statistical information about the data-set. Data obfuscation deals with
this problem of masking a data-set in such a way that the utility of the data
is maximized while minimizing the risk of the disclosure of sensitive
information. Two popular approaches to data obfuscation for numerical data
involves (i) data swapping and (ii) adding noise to data. While the former
masks well sacrificing the whole of correlation information, the latter gives
estimates for most of the popular statistics like mean, variance, quantiles,
correlation but fails to give an unbiased estimate of the distribution curve of
the original data. In this paper, we propose a mixed method of obfuscation
combining the above two approaches and discuss how the proposed method succeeds
in giving an unbiased estimation of the distribution curve while giving
reliable estimates of the other well-known statistics like moments,
correlation.
</summary>
    <author>
      <name>Debolina Ghatak</name>
    </author>
    <author>
      <name>Bimak K Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09656v1</id>
    <updated>2018-07-13T12:20:47Z</updated>
    <published>2018-07-13T12:20:47Z</published>
    <title>An Improved Bound for Security in an Identity Disclosure Problem</title>
    <summary>  Identity disclosure of an individual from a released data is a matter of
concern especially if it belongs to a category with low frequency in the
data-set. Nayak et al. (2016) discussed this problem vividly in a census report
and suggested a method of obfuscation, which would ensure that the probability
of correctly identifying a unit from released data, would not exceed t for some
1/3 &lt; t &lt; 1. However, we observe that for the above method the level of
security could be extended under certain conditions. In this paper, we discuss
some conditions under which one can achieve a security for any 0 &lt; t &lt; 1.
</summary>
    <author>
      <name>Debolina Ghatak</name>
    </author>
    <author>
      <name>Bimal K Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14pages, 2tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02536v2</id>
    <updated>2018-07-13T01:15:32Z</updated>
    <published>2017-09-08T04:45:30Z</published>
    <title>Covariances, Robustness, and Variational Bayes</title>
    <summary>  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale datasets. However, even when MFVB provides accurate posterior means
for certain parameters, it often mis-estimates variances and covariances.
Furthermore, prior robustness measures have remained undeveloped for MFVB. By
deriving a simple formula for the effect of infinitesimal model perturbations
on MFVB posterior means, we provide both improved covariance estimates and
local robustness measures for MFVB, thus greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
relating derivatives of posterior expectations to posterior covariances and
include the Laplace approximation as a special case. Our key condition is that
the MFVB approximation provides good estimates of a select subset of posterior
means---an assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC.
</summary>
    <author>
      <name>Ryan Giordano</name>
    </author>
    <author>
      <name>Tamara Broderick</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for review to JMLR</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02536v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02536v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01500v2</id>
    <updated>2018-07-13T00:08:47Z</updated>
    <published>2018-05-03T18:34:52Z</published>
    <title>Noisin: Unbiased Regularization for Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks (RNNs) are powerful models of sequential data. They
have been successfully used in domains such as text and speech. However, RNNs
are susceptible to overfitting; regularization is important. In this paper we
develop Noisin, a new method for regularizing RNNs. Noisin injects random noise
into the hidden states of the RNN and then maximizes the corresponding marginal
likelihood of the data. We show how Noisin applies to any RNN and we study many
different types of noise. Noisin is unbiased--it preserves the underlying RNN
on average. We characterize how Noisin regularizes its RNN both theoretically
and empirically. On language modeling benchmarks, Noisin improves over dropout
by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We
also compared the state-of-the-art language model of Yang et al. 2017, both
with and without Noisin. On the Penn Treebank, the method with Noisin more
quickly reaches state-of-the-art performance.
</summary>
    <author>
      <name>Adji B. Dieng</name>
    </author>
    <author>
      <name>Rajesh Ranganath</name>
    </author>
    <author>
      <name>Jaan Altosaar</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the International Conference on Machine Learning,
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01500v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01500v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03490v2</id>
    <updated>2018-07-12T23:09:14Z</updated>
    <published>2018-02-10T01:08:07Z</published>
    <title>A whitening approach to probabilistic canonical correlation analysis for
  omics data integration</title>
    <summary>  Background: Canonical correlation analysis (CCA) is a classic statistical
tool for investigating complex multivariate data. Correspondingly, it has found
many diverse applications, ranging from molecular biology and medicine to
social science and finance. Intriguingly, despite the importance and
pervasiveness of CCA, only recently a probabilistic understanding of CCA is
developing, moving from an algorithmic to a model-based perspective and
enabling its application to large-scale settings.
  Results: Here, we revisit CCA from the perspective of statistical whitening
of random variables and propose a simple yet flexible probabilistic model for
CCA in the form of a two-layer latent variable generative model. The advantages
of this variant of probabilistic CCA include non-ambiguity of the latent
variables, provisions for negative canonical correlations, possibility of
non-normal generative variables, as well as ease of interpretation on all
levels of the model. In addition, we show that it lends itself to
computationally efficient estimation in high-dimensional settings using
regularized inference. We test our approach to CCA analysis in simulations and
apply it to two omics data sets illustrating the integration of gene expression
data, lipid concentrations and methylation levels.
  Conclusions: Our whitening approach to CCA provides a unifying perspective on
CCA, linking together sphering procedures, multivariate regression and
corresponding probabilistic generative models. Furthermore, we offer an
efficient computer implementation in the "whitening" R package available at
https://CRAN.R-project.org/package=whitening .
</summary>
    <author>
      <name>Takoua Jendoubi</name>
    </author>
    <author>
      <name>Korbinian Strimmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03490v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03490v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09834v3</id>
    <updated>2018-07-12T21:12:26Z</updated>
    <published>2018-01-30T03:02:12Z</published>
    <title>A Flexible Procedure for Mixture Proportion Estimation in
  Positive--Unlabeled Learning</title>
    <summary>  Positive--unlabeled (PU) learning considers two samples, a positive set P
with observations from only one class and an unlabeled set U with observations
from two classes. The goal is to classify observations in U. Class mixture
proportion estimation (MPE) in U is a key step in PU learning. Blanchard et al.
[2010] showed that MPE in PU learning is a generalization of the problem of
estimating the proportion of true null hypotheses in multiple testing problems.
Motivated by this idea, we propose reducing the problem to one dimension via
construction of a probabilistic classifier trained on the P and U data sets
followed by application of a one--dimensional mixture proportion method from
the multiple testing literature to the observation class probabilities. The
flexibility of this framework lies in the freedom to choose the classifier and
the one--dimensional MPE method. We prove consistency of two mixture proportion
estimators using bounds from empirical process theory, develop tuning parameter
free implementations, and demonstrate that they have competitive performance on
simulated waveform data and a protein signaling problem.
</summary>
    <author>
      <name>Zhenfeng Lin</name>
    </author>
    <author>
      <name>James P. Long</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages (including 7 pages of Appendix), 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09834v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09834v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05920v1</id>
    <updated>2018-07-12T19:00:31Z</updated>
    <published>2018-07-12T19:00:31Z</published>
    <title>Sequential Sampling for Optimal Bayesian Classification of Sequencing
  Count Data</title>
    <summary>  High throughput technologies have become the practice of choice for
comparative studies in biomedical applications. Limited number of sample points
due to sequencing cost or access to organisms of interest necessitates the
development of efficient sample collections to maximize the power of downstream
statistical analyses. We propose a method for sequentially choosing training
samples under the Optimal Bayesian Classification framework. Specifically
designed for RNA sequencing count data, the proposed method takes advantage of
efficient Gibbs sampling procedure with closed-form updates. Our results shows
enhanced classification accuracy, when compared to random sampling.
</summary>
    <author>
      <name>Ariana Broumand</name>
    </author>
    <author>
      <name>Siamak Zamani Dadaneh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, accepted in Asilomar Conference on Signals,
  Systems, and Computers 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01146v2</id>
    <updated>2018-07-12T16:03:25Z</updated>
    <published>2017-10-03T13:38:47Z</published>
    <title>An Updated Literature Review of Distance Correlation and its
  Applications to Time Series</title>
    <summary>  The concept of distance covariance/correlation was introduced recently to
characterize dependence among vectors of random variables. We review some
statistical aspects of distance covariance/correlation function and we
demonstrate its applicability to time series analysis. We will see that the
auto-distance covariance/correlation function is able to identify nonlinear
relationships and can be employed for testing the i.i.d.\ hypothesis.
Comparisons with other measures of dependence are included.
</summary>
    <author>
      <name>Dominic Edelmann</name>
    </author>
    <author>
      <name>Konstantinos Fokianos</name>
    </author>
    <author>
      <name>Maria Pitsillou</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01146v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01146v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05066v1</id>
    <updated>2018-07-12T15:39:21Z</updated>
    <published>2018-07-12T15:39:21Z</published>
    <title>Bayesian Estimation Under Informative Sampling with Unattenuated
  Dependence</title>
    <summary>  An informative sampling design leads to unit inclusion probabilities that are
correlated with the response variable of interest. However, multistage sampling
designs may also induce higher order dependencies, which are typically ignored
in the literature when establishing consistency of estimators for survey data
under a condition requiring asymptotic independence among the unit inclusion
probabilities. We refine and relax this condition of asymptotic independence or
asymptotic factorization and demonstrate that consistency is still achieved in
the presence of residual sampling dependence. A popular approach for conducting
inference on a population based on a survey sample is the use of a
pseudo-posterior, which uses sampling weights based on first order inclusion
probabilities to exponentiate the likelihood. We show that the pseudo-posterior
is consistent not only for survey designs which have asymptotic factorization,
but also for designs with residual or unattenuated dependence. Using the
complex sampling design of the National Survey on Drug Use and Health, we
explore the impact of multistage designs and order based sampling. The use of
the survey-weighted pseudo-posterior together with our relaxed requirements for
the survey design establish a broad class of analysis models that can be
applied to a wide variety of survey data sets.
</summary>
    <author>
      <name>Matthew R. Williams</name>
    </author>
    <author>
      <name>Terrance D. Savitsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1710.10102</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04680v1</id>
    <updated>2018-07-12T15:37:32Z</updated>
    <published>2018-07-12T15:37:32Z</published>
    <title>Unseeded low-rank graph matching by transform-based unsupervised point
  registration</title>
    <summary>  The problem of learning a correspondence relationship between nodes of two
networks has drawn much attention of the computer science community and
recently that of statisticians. The unseeded version of this problem, in which
we do not know any part of the true correspondence, is a long-standing
challenge. For low-rank networks, the problem can be translated into an
unsupervised point registration problem, in which two point sets generated from
the same distribution are matchable by an unknown orthonormal transformation.
Conventional methods generally lack consistency guarantee and are usually
computationally costly.
  In this paper, we propose a novel approach to this problem. Instead of
simultaneously estimating the unknown correspondence and orthonormal
transformation to match up the two point sets, we match their distributions via
minimizing our designed loss function capturing the discrepancy between their
Laplace transforms, thus avoiding the optimization over all possible
correspondences. This dramatically reduces the dimension of the optimization
problem from $\Omega(n^2)$ parameters to $O(d^2)$ parameters, where $d$ is the
fixed rank, and enables convenient theoretical analysis. In this paper, we
provide arguably the first consistency guarantee and explicit error rate for
general low-rank models. Our method provides control over the computational
complexity ranging from $\omega(n)$ (any growth rate faster than $n$) to
$O(n^2)$ while pertaining consistency. We demonstrate the effectiveness of our
method through several numerical examples.
</summary>
    <author>
      <name>Yuan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.5019v2</id>
    <updated>2018-07-12T13:51:22Z</updated>
    <published>2011-02-24T15:37:09Z</published>
    <title>Unsupervised nonparametric detection of unknown objects in noisy images
  based on percolation theory</title>
    <summary>  We develop an unsupervised, nonparametric, and scalable statistical learning
method for detection of unknown objects in noisy images. The method uses
results from percolation theory and random graph theory. We present an
algorithm that allows to detect objects of unknown shapes and sizes in the
presence of nonparametric noise of unknown level. The noise density is assumed
to be unknown and can be very irregular. The algorithm has linear complexity
and exponential accuracy and is appropriate for real-time systems. We prove
strong consistency and scalability of our method in this setup with minimal
assumptions.
</summary>
    <author>
      <name>Mikhail A. Langovoy</name>
    </author>
    <author>
      <name>Olaf Wittich</name>
    </author>
    <author>
      <name>Patrick Laurie Davies</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added references, updated terminology, changed the order of the
  authors and the title. arXiv admin note: substantial text overlap with
  arXiv:1102.4803, arXiv:1102.5014</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.5019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.5019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00629v2</id>
    <updated>2018-07-12T09:21:34Z</updated>
    <published>2017-09-02T20:40:43Z</published>
    <title>Nonparametric density estimation from observations with multiplicative
  measurement errors</title>
    <summary>  In this paper we study the problem of pointwise density estimation from
observations with multiplicative measurement errors. We elucidate the main
feature of this problem: the influence of the estimation point on the
estimation accuracy. In particular, we show that, depending on whether this
point is separated away from zero or not, there are two different regimes in
terms of the rates of convergence of the minimax risk. In both regimes we
develop kernel--type density estimators and prove upper bounds on their maximal
risk over suitable nonparametric classes of densities. We show that the
proposed estimators are rate--optimal by establishing matching lower bounds on
the minimax risk. Finally we test our estimation procedures on simulated data.
</summary>
    <author>
      <name>Denis Belomestny</name>
    </author>
    <author>
      <name>Alexander Goldenshluger</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00629v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00629v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G05, 60G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04450v1</id>
    <updated>2018-07-12T07:43:18Z</updated>
    <published>2018-07-12T07:43:18Z</published>
    <title>Jackknife empirical likelihood based inference for Probability weighted
  moments</title>
    <summary>  In the present article, we discuss jackknife empirical likelihood (JEL) and
adjusted jackknife empirical likelihood (AJEL) based inference for finding
confidence intervals for probability weighted moment (PWM). We obtain the
asymptotic distribution of the JEL ratio and AJEL ratio statistics. We compare
the performance of the proposed confidence intervals with recently developed
methods in terms of coverage probability and average length. We also develop
JEL and AJEL based test for PWM and study it properties. Finally we illustrate
our method using rainfall data of Indian states.
</summary>
    <author>
      <name>Deepesh Bhati</name>
    </author>
    <author>
      <name>Sudheesh K Kattumannil</name>
    </author>
    <author>
      <name>N Sreelakshmi</name>
    </author>
    <link href="http://arxiv.org/abs/1807.04450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04431v1</id>
    <updated>2018-07-12T06:08:33Z</updated>
    <published>2018-07-12T06:08:33Z</published>
    <title>Statistical Inference with Local Optima</title>
    <summary>  We study the statistical properties of an estimator derived by applying a
gradient ascent method with multiple initializations to a multi-modal
likelihood function. We derive the population quantity that is the target of
this estimator and study the properties of confidence intervals (CIs)
constructed from asymptotic normality and the bootstrap approach. In
particular, we analyze the coverage deficiency due to finite number of random
initializations. We also investigate the CIs by inverting the likelihood ratio
test, the score test, and the Wald test, and we show that the resulting CIs may
be very different. We provide a summary of the uncertainties that we need to
consider while making inference about the population. Note that we do not
provide a solution to the problem of multiple local maxima; instead, our goal
is to investigate the effect from local maxima on the behavior of our
estimator. In addition, we analyze the performance of the EM algorithm under
random initializations and derive the coverage of a CI with a finite number of
initializations. Finally, we extend our analysis to a nonparametric mode
hunting problem.
</summary>
    <author>
      <name>Yen-Chi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">66 page, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04429v1</id>
    <updated>2018-07-12T05:25:41Z</updated>
    <published>2018-07-12T05:25:41Z</published>
    <title>Bootstrapping Max Statistics in High Dimensions: Near-Parametric Rates
  Under Weak Variance Decay and Application to Functional Data Analysis</title>
    <summary>  In recent years, bootstrap methods have drawn attention for their ability to
approximate the laws of "max statistics" in high-dimensional problems. A
leading example of such a statistic is the coordinate-wise maximum of a sample
average of $n$ random vectors in $\mathbb{R}^p$. Existing results for this
statistic show that the bootstrap can work when $n\ll p$, and rates of
approximation (in Kolmogorov distance) have been obtained with only logarithmic
dependence in $p$. Nevertheless, one of the challenging aspects of this setting
is that established rates tend to scale like $n^{-1/6}$ as a function of $n$.
  The main purpose of this paper is to demonstrate that improvement in rate is
possible when extra model structure is available. Specifically, we show that if
the coordinate-wise variances of the observations exhibit decay, then a nearly
$n^{-1/2}$ rate can be achieved, independent of $p$. Furthermore, a surprising
aspect of this dimension-free rate is that it holds even when the decay is very
weak. As a numerical illustration, we show how these ideas can be used in the
context of functional data analysis to construct simultaneous confidence
intervals for the Fourier coefficients of a mean function.
</summary>
    <author>
      <name>Miles E. Lopes</name>
    </author>
    <author>
      <name>Zhenhua Lin</name>
    </author>
    <author>
      <name>Hans-Georg Mueller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04426v1</id>
    <updated>2018-07-12T05:05:09Z</updated>
    <published>2018-07-12T05:05:09Z</published>
    <title>A likelihood-ratio type test for stochastic block models with bounded
  degrees</title>
    <summary>  A fundamental problem in network data analysis is to test Erd\"{o}s-R\'{e}nyi
model $\mathcal{G}\left(n,\frac{a+b}{2n}\right)$ versus a bisection stochastic
block model $\mathcal{G}\left(n,\frac{a}{n},\frac{b}{n}\right)$, where $a,b&gt;0$
are constants that represent the expected degrees of the graphs and $n$ denotes
the number of nodes. This problem serves as the foundation of many other
problems such as testing-based methods for determining the number of
communities (\cite{BS16,L16}) and community detection (\cite{MS16}). Existing
work has been focusing on growing-degree regime $a,b\to\infty$
(\cite{BS16,L16,MS16,BM17,B18,GL17a,GL17b}) while leaving the bounded-degree
regime untreated. In this paper, we propose a likelihood-ratio (LR) type
procedure based on regularization to test stochastic block models with bounded
degrees. We derive the limit distributions as power Poisson laws under both
null and alternative hypotheses, based on which the limit power of the test is
carefully analyzed. We also examine a Monte-Carlo method that partly resolves
the computational cost issue. The proposed procedures are examined by both
simulated and real-world data. The proof depends on a contiguity theory
developed by Janson \cite{J95}.
</summary>
    <author>
      <name>Mingao Yuan</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <author>
      <name>Zuofeng Shang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.04426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09523v2</id>
    <updated>2018-07-12T04:50:27Z</updated>
    <published>2017-06-29T00:20:37Z</published>
    <title>Bayesian regression tree models for causal inference: regularization,
  confounding, and heterogeneous effects</title>
    <summary>  This paper develops a semi-parametric Bayesian regression model for
estimating heterogeneous treatment effects from observational data. Standard
nonlinear regression models, which may work quite well for prediction, can
yield badly biased estimates of treatment effects when fit to data with strong
confounding. Our Bayesian causal forests model avoids this problem by directly
incorporating an estimate of the propensity function in the specification of
the response model, implicitly inducing a covariate-dependent prior on the
regression function. This new parametrization also allows treatment
heterogeneity to be regularized separately from the prognostic effect of
control variables, making it possible to informatively "shrink to homogeneity",
in contrast to existing Bayesian non- and semi-parametric approaches.
</summary>
    <author>
      <name>P. Richard Hahn</name>
    </author>
    <author>
      <name>Jared S. Murray</name>
    </author>
    <author>
      <name>Carlos Carvalho</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09523v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09523v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05021v3</id>
    <updated>2018-07-12T02:45:53Z</updated>
    <published>2017-10-13T17:51:46Z</published>
    <title>Simultaneous Detection of Signal Regions With Applications in
  Genome-Wide Association Studies</title>
    <summary>  We consider in this paper detection of signal regions associated with disease
outcomes in Genome-Wide Association Studies (GWAS). Gene- or region-based
methods have become increasingly popular in GWAS as a complementary approach to
traditional individual variant analysis. However, these methods test for the
association between an outcome and the genetic variants in a pre-specified
region, e.g., a gene. In view of massive intergenic regions in GWAS and
substantial interests in identifying signal regions for subsequent fine
mapping, we propose a computationally efficient quadratic scan (Q-SCAN)
statistic based method to detect the existence and the locations of signal
regions by scanning the genome continuously. The proposed method accounts for
the correlation (linkage disequilibrium) among genetic variants, and allows for
signal regions to have both signal and neutral variants, and signal variants
whose effects can be in different directions. We study the asymptotic
properties of the proposed Q-SCAN statistics. We derive an asymptotic threshold
that controls for the family-wise error rate, and show that under regularity
conditions the proposed method consistently selects the true signal regions. We
perform simulation studies to evaluate the finite sample performance of the
proposed method. Our simulation results show that the proposed procedure
outperforms the existing methods, especially when signal regions have signal
variants whose effects are in different directions, or are contaminated with
neutral variants, or have correlated variants. We apply the proposed method to
analyze a lung cancer genome-wide association study to identify the genetic
regions that are associated with lung cancer risk.
</summary>
    <author>
      <name>Zilin Li</name>
    </author>
    <author>
      <name>Xihong Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1710.05021v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05021v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00019v3</id>
    <updated>2018-07-12T00:59:58Z</updated>
    <published>2017-09-29T18:26:03Z</published>
    <title>Fully Bayesian Estimation Under Informative Sampling</title>
    <summary>  Bayesian estimation is increasingly popular for performing model based
inference to support policymaking. These data are often collected from surveys
under informative sampling designs where subject inclusion probabilities are
designed to be correlated with the response variable of interest. Sampling
weights constructed from marginal inclusion probabilities are typically used to
form an exponentiated pseudo likelihood that adjusts the population likelihood
for estimation on the sample due to ease-of-estimation. We propose an
alternative adjustment based on a Bayes rule construction that simultaneously
performs weight smoothing and estimates the population model parameters in a
fully Bayesian construction. We formulate conditions on known marginal and
pairwise inclusion probabilities that define a class of sampling designs where
$L_{1}$ consistency of the joint posterior is guaranteed. We compare
performances between the two approaches on synthetic data, which reveals that
our fully Bayesian approach better estimates posterior uncertainty without a
requirement to calibrate the normalization of the sampling weights. We
demonstrate our method on an application concerning the National Health and
Nutrition Examination Survey exploring the relationship between caffeine
consumption and systolic blood pressure.
</summary>
    <author>
      <name>Luis G. Leon-Novelo</name>
    </author>
    <author>
      <name>Terrance D. Savitsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages 1-29 conform the main paper and they include seven figures and
  three tables. Pages 30-36 contain Supplementary Material and pages 36-37
  contain references</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00019v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00019v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06310v2</id>
    <updated>2018-07-11T23:27:53Z</updated>
    <published>2018-02-17T23:42:24Z</published>
    <title>Characterizing and Learning Equivalence Classes of Causal DAGs under
  Interventions</title>
    <summary>  We consider the problem of learning causal DAGs in the setting where both
observational and interventional data is available. This setting is common in
biology, where gene regulatory networks can be intervened on using chemical
reagents or gene deletions. Hauser and B\"uhlmann (2012) previously
characterized the identifiability of causal DAGs under perfect interventions,
which eliminate dependencies between targeted variables and their direct
causes. In this paper, we extend these identifiability results to general
interventions, which may modify the dependencies between targeted variables and
their causes without eliminating them. We define and characterize the
interventional Markov equivalence class that can be identified from general
(not necessarily perfect) intervention experiments. We also propose the first
provably consistent algorithm for learning DAGs in this setting and evaluate
our algorithm on simulated and biological datasets.
</summary>
    <author>
      <name>Karren D. Yang</name>
    </author>
    <author>
      <name>Abigail Katcoff</name>
    </author>
    <author>
      <name>Caroline Uhler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06310v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06310v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02421v2</id>
    <updated>2018-07-11T23:08:22Z</updated>
    <published>2018-07-04T23:23:05Z</published>
    <title>Large-Scale Multiple Hypothesis Testing with the Normal-Beta Prime Prior</title>
    <summary>  We revisit the problem of simultaneously testing the means of $n$ independent
normal observations under sparsity. We take a Bayesian approach to this problem
by introducing a scale-mixture prior known as the normal-beta prime (NBP)
prior. We first derive new concentration properties when the beta prime density
is employed for a scale parameter in Bayesian hierarchical models. To detect
signals in our data, we then propose a hypothesis test based on thresholding
the posterior shrinkage weight under the NBP prior. Taking the loss function to
be the expected number of misclassified tests, we show that our test procedure
asymptotically attains the optimal Bayes risk when the signal proportion $p$ is
known. When $p$ is unknown, we introduce an empirical Bayes variant of our test
which also asymptotically attains the Bayes Oracle risk in the entire range of
sparsity parameters $p \propto n^{-\epsilon}, \epsilon \in (0, 1)$. We further
consider a restricted marginal maximum likelihood (REML) approach for
estimating a key hyperparameter in the NBP prior and examine multiple testing
under this framework. Numerical experiments strongly suggest that adaptive test
procedures based on REML also have the oracle property for multiple testing. We
illustrate our methodology through simulations and analysis of a prostate
cancer data set.
</summary>
    <author>
      <name>Ray Bai</name>
    </author>
    <author>
      <name>Malay Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 4 figures, 1 table. Table updated with new results on
  2018-07-11. arXiv admin note: text overlap with arXiv:1710.04369</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02421v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02421v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02147v2</id>
    <updated>2018-07-11T19:42:25Z</updated>
    <published>2017-07-07T12:37:50Z</published>
    <title>Classification of geometrical objects by integrating currents and
  functional data analysis. An application to a 3D database of Spanish child
  population</title>
    <summary>  This paper focuses on the application of Discriminant Analysis to a set of
geometrical objects (bodies) characterized by currents. A current is a relevant
mathematical object to model geometrical data, like hypersurfaces, through
integration of vector fields along them. As a consequence of the choice of a
vector-valued Reproducing Kernel Hilbert Space (RKHS) as a test space to
integrate hypersurfaces, it is possible to consider that hypersurfaces are
embedded in this Hilbert space. This embedding enables us to consider
classification algorithms of geometrical objects. A method to apply Functional
Discriminant Analysis in the obtained vector-valued RKHS is given. This method
is based on the eigenfunction decomposition of the kernel. So, the novelty of
this paper is the reformulation of a size and shape classification problem in
Functional Data Analysis terms using the theory of currents and vector-valued
RKHS. This approach is applied to a 3D database obtained from an anthropometric
survey of the Spanish child population with a potential application to online
sales of children's wear.
</summary>
    <author>
      <name>Sonia Barahona</name>
    </author>
    <author>
      <name>Pablo Centella</name>
    </author>
    <author>
      <name>Ximo Gual-Arnau</name>
    </author>
    <author>
      <name>Maria Victoria Ibáñez</name>
    </author>
    <author>
      <name>Amelia Simó</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02147v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02147v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05179v3</id>
    <updated>2018-07-11T17:52:28Z</updated>
    <published>2017-01-18T18:42:26Z</published>
    <title>Covariate powered cross-weighted multiple testing</title>
    <summary>  A fundamental task in the analysis of datasets with many variables is
screening for associations. This can be cast as a multiple testing task, where
the major challenge is achieving high detection power while controlling type I
error. We consider $m$ hypothesis tests represented by pairs $((P_i,
X_i))_{1\leq i \leq m}$ of p-values $P_i$ and covariates $X_i$, such that $P_i
\perp X_i$ under the null hypothesis. Here, we show how to use information
potentially available in the covariates about heterogeneities among hypotheses
to increase power compared to conventional procedures that only use the $P_i$.
To this end, we upgrade existing weighted multiple testing procedures through
the Independent Hypothesis Weighting (IHW) framework to use data-driven weights
which are a function of the covariate $X_i$. Finite sample guarantees, e.g.
false discovery rate (FDR) control, are derived from cross-weighting, a novel
data-splitting approach that enables learning the weight-covariate function
without overfitting as long as the hypotheses can be partitioned into
independent folds, with arbitrary within-fold dependence. We show how the
increased power of IHW can be understood in terms of the conditional two-groups
model. A key implication of IHW is that hypothesis rejection in many common
multiple testing setups should not proceed according to the ranking of the
p-values, but by an alternative ranking implied by the covariate-weighted
p-values.
</summary>
    <author>
      <name>Nikolaos Ignatiadis</name>
    </author>
    <author>
      <name>Wolfgang Huber</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05179v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05179v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06468v2</id>
    <updated>2018-07-11T15:09:36Z</updated>
    <published>2016-12-20T00:57:02Z</published>
    <title>Sequential Monte Carlo with transformations</title>
    <summary>  This paper introduces methodology for performing Bayesian inference
sequentially on a sequence of posteriors on spaces of different dimensions. We
show how this may be achieved through the use of sequential Monte Carlo (SMC)
samplers (Del Moral et al., 2006, 2007), making use of the full flexibility of
this framework in order that the method is computationally efficient. In
particular, we introduce the innovation of using deterministic transformations
to move particles effectively between target distributions with different
dimensions. This approach, combined with adaptive methods, yields an extremely
flexible and general algorithm for Bayesian model comparison that is suitable
for use in applications where the acceptance rate in reversible jump Markov
chain Monte Carlo (RJMCMC) is low. We demonstrate this approach on the
well-studied problem of model comparison for mixture models, and for the novel
application of inferring coalescent trees sequentially, as data arrives.
</summary>
    <author>
      <name>Richard G Everitt</name>
    </author>
    <author>
      <name>Richard Culliford</name>
    </author>
    <author>
      <name>Felipe Medina-Aguayo</name>
    </author>
    <author>
      <name>Daniel J Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06468v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06468v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04164v1</id>
    <updated>2018-07-11T14:40:31Z</updated>
    <published>2018-07-11T14:40:31Z</published>
    <title>Using Recursive Partitioning to Find and Estimate Heterogenous Treatment
  Effects In Randomized Clinical Trials</title>
    <summary>  Heterogeneous treatment effects can be very important in the analysis of
randomized clinical trials. Heightened risks or enhanced benefits may exist for
particular subsets of study subjects. When the heterogeneous treatment effects
are specified as the research is being designed, there are proper and readily
available analysis techniques. When the heterogeneous treatment effects are
inductively obtained as an experiment's data are analyzed, significant
complications are introduced. There can be a need for special loss functions
designed to find local average treatment effects and for techniques that
properly address post selection statistical inference. In this paper, we tackle
both while undertaking a recursive partitioning analysis of a randomized
clinical trial testing whether individuals on probation, who are low risk, can
be minimally supervised with no increase in recidivism.
</summary>
    <author>
      <name>Richard Berk</name>
    </author>
    <author>
      <name>Matthew Olson</name>
    </author>
    <author>
      <name>Andreas Buja</name>
    </author>
    <author>
      <name>Aurelie Ouss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 1 figure, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04133v1</id>
    <updated>2018-07-11T13:49:49Z</updated>
    <published>2018-07-11T13:49:49Z</published>
    <title>Robust relative error estimation</title>
    <summary>  Relative error estimation has been recently used in regression analysis. A
crucial issue of the existing relative error estimation procedures is that they
are sensitive to outliers. To address this issue, we employ the
$\gamma$-likelihood function, which is constructed through $\gamma$-cross
entropy with keeping the original statistical model in use. The estimating
equation has a redescending property, a desirable property in robust
statistics, for a broad class of noise distributions. To find a minimizer of
the negative $\gamma$-likelihood function, a majorize-minimization (MM)
algorithm is constructed. The proposed algorithm is guaranteed to decrease the
negative $\gamma$-likelihood function at each iteration. We also derive
asymptotic normality of the corresponding estimator together with a simple
consistent estimator of the asymptotic covariance matrix, so that we can
readily construct approximate confidence sets. Monte Carlo simulation is
conducted to investigate the effectiveness of the proposed procedure. Real data
analysis illustrates the usefulness of our proposed procedure.
</summary>
    <author>
      <name>Kei Hirose</name>
    </author>
    <author>
      <name>Hiroki Masuda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04272v1</id>
    <updated>2018-07-11T13:26:02Z</updated>
    <published>2018-07-11T13:26:02Z</published>
    <title>Towards a Complete Picture of Covariance Functions on Spheres Cross Time</title>
    <summary>  With the advent of wide-spread global and continental-scale spatiotemporal
datasets, increased attention has been given to covariance functions on spheres
over time. This paper provides a characterization theorem for covariance
functions of random fields defined over $d$-dimensional spheres cross time. The
result characterizes the relationship between covariance functions for spheres
over time and space-time covariance functions defined over Euclidean spaces. We
then show that the Gneiting class of space-time covariance functions
\citep{gneiting2002} can be extended to spheres cross time by replacing the
squared Euclidean distance with the great circle distance. Additionally, we
provide a new class of covariance functions using our characterization theorem,
giving an example of a class correspondence between covariance functions over
Euclidean spaces with compact support and covariance functions over spheres
cross time.
  We discuss modeling details using nearest-neighbor Gaussian processes in a
Bayesian framework for our extension of the Gneiting class. In this context, we
illustrate the value of our proposed classes by comparing them to currently
established nonseparable covariance classes using out-of-sample predictive
criteria. These comparisons are carried out on a simulated dataset and two
climate reanalysis datasets from the National Centers for Environmental
Prediction and National Center for Atmospheric Research. In our simulation
study, we establish that covariance parameters from a generative model from our
class can be identified in model fitting and that predictive performance is as
good or better than competing covariance models. In our real data examples, we
show that our covariance class has better predictive performance than competing
models, and we discuss results in the context of the climate processes that we
model.
</summary>
    <author>
      <name>Philip White</name>
    </author>
    <author>
      <name>Emilio Porcu</name>
    </author>
    <link href="http://arxiv.org/abs/1807.04272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02588v2</id>
    <updated>2018-07-11T12:09:30Z</updated>
    <published>2018-06-07T09:51:23Z</published>
    <title>Designing Experiments to Measure Incrementality on Facebook</title>
    <summary>  The importance of Facebook advertising has risen dramatically in recent
years, with the platform accounting for almost 20% of the global online ad
spend in 2017. An important consideration in advertising is incrementality: how
much of the change in an experimental metric is an advertising campaign
responsible for. To measure incrementality, Facebook provide lift studies. As
Facebook lift studies differ from standard A/B tests, the online
experimentation literature does not describe how to calculate parameters such
as power and minimum sample size. Facebook also offer multi-cell lift tests,
which can be used to compare campaigns that don't have statistically identical
audiences. In this case, there is no literature describing how to measure the
significance of the difference in incrementality between cells, or how to
estimate the power or minimum sample size. We fill these gaps in the literature
by providing the statistical power and required sample size calculation for
Facebook lift studies. We then generalise the statistical significance, power,
and required sample size calculation to multi-cell lift studies. We represent
our results theoretically in terms of the distributions of test metrics and in
practical terms relating to the metrics used by practitioners, making all of
our code publicly available.
</summary>
    <author>
      <name>C. H. Bryan Liu</name>
    </author>
    <author>
      <name>Elaine M. Bettaney</name>
    </author>
    <author>
      <name>Benjamin Paul Chamberlain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted into 2018 AdKDD &amp; TargetAd Workshop in conjunction with KDD
  2018; 6 pages, 4 figures, and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03712v2</id>
    <updated>2018-07-11T08:38:57Z</updated>
    <published>2018-07-02T07:49:11Z</published>
    <title>Certified dimension reduction in nonlinear Bayesian inverse problems</title>
    <summary>  We propose a dimension reduction technique for Bayesian inverse problems with
nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation
noise. The likelihood function is approximated by a ridge function, i.e., a map
which depends non-trivially only on a few linear combinations of the
parameters. We build this ridge approximation by minimizing an upper bound on
the Kullback--Leibler divergence between the posterior distribution and its
approximation. This bound, obtained via logarithmic Sobolev inequalities,
allows one to certify the error of the posterior approximation. Computing the
bound requires computing the second moment matrix of the gradient of the
log-likelihood function. In practice, a sample-based approximation of the upper
bound is then required. We provide an analysis that enables control of the
posterior approximation error due to this sampling. Numerical and theoretical
comparisons with existing methods illustrate the benefits of the proposed
methodology.
</summary>
    <author>
      <name>Olivier Zahm</name>
    </author>
    <author>
      <name>Tiangang Cui</name>
    </author>
    <author>
      <name>Kody Law</name>
    </author>
    <author>
      <name>Alessio Spantini</name>
    </author>
    <author>
      <name>Youssef Marzouk</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03712v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03712v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03889v1</id>
    <updated>2018-07-10T22:19:10Z</updated>
    <published>2018-07-10T22:19:10Z</published>
    <title>Estimators of the proportion of false null hypotheses: I "universal
  construction via Lebesgue-Stieltjes integral equations and uniform
  consistency under independence"</title>
    <summary>  The proportion of false null hypotheses is a very important quantity in
statistical modelling and inference based on the two-component mixture model
and its extensions, and in control and estimation of the false discovery rate
and false non-discovery rate. Most existing estimators of this proportion
threshold p-values, deconvolve the mixture model under constraints on the
components, or depend heavily on the location-shift property of distributions.
Hence, they usually are not consistent, applicable to non-location-shift
distributions, or applicable to discrete statistics or p-values. To eliminate
these shortcomings, we construct uniformly consistent estimators of the
proportion as solutions to Lebesgue-Stieltjes integral equations. In
particular, we provide such estimators respectively for random variables whose
distributions have separable characteristic functions, form discrete natural
exponential families with infinite supports, and form natural exponential
families with separable moment sequences. We provide the speed of convergence
and uniform consistency class for each such estimator. The constructions use
Fourier transform, Mellin transform or probability generating functions, and
have connections with Bessel functions. In addition, we provide example
distribution families for which a consistent estimator of the proportion cannot
be constructed using our techniques.
</summary>
    <author>
      <name>Xiongzhi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages in one-and-half line spacing, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35C05, 60E05, 60F10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03874v1</id>
    <updated>2018-07-10T21:39:11Z</updated>
    <published>2018-07-10T21:39:11Z</published>
    <title>Node-specific effects in latent space modelling of multidimensional
  networks</title>
    <summary>  Observed multidimensional network data can have different levels of
complexity, as nodes may be characterized by heterogeneous individual-specific
features. Also, such characteristics may vary across the networks. This article
discusses a novel class of models for multidimensional networks, able to deal
with different levels of heterogeneity within and between networks. The
proposed framework is developed within the family of latent space models, in
order to distinguish recurrent symmetrical relations between the nodes from
node-specific features in the different views. Models parameters are estimated
via a Markov Chain Monte Carlo algorithm. Simulated data and also FAO fruits
import/export data are analysed to illustrate the performances of the proposed
models.
</summary>
    <author>
      <name>Silvia D'Angelo</name>
    </author>
    <author>
      <name>Marco Alfò</name>
    </author>
    <author>
      <name>Thomas Brendan Murphy</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02762v3</id>
    <updated>2018-07-10T18:11:18Z</updated>
    <published>2016-11-08T23:08:02Z</published>
    <title>Generalized Cluster Trees and Singular Measures</title>
    <summary>  In this paper, we study the $\alpha$-cluster tree ($\alpha$-tree) under both
singular and nonsingular measures. The $\alpha$-tree uses probability contents
within a level set to construct a cluster tree so that it is well-defined for
singular measures. We first derive the convergence rate for a density level set
around critical points, which leads to the convergence rate for estimating an
$\alpha$-tree under nonsingular measures. For singular measures, we study how
the kernel density estimator (KDE) behaves and prove that the KDE is not
uniformly consistent but pointwisely consistent after rescaling. We further
prove that the estimated $\alpha$-tree fails to converge in the $L_\infty$
metric but is still consistent under the integrated distance. We also observe a
new type of critical points--the dimensional critical points (DCPs)--of a
singular measure. DCPs occur only at singular measures, and similar to the
usual critical points, DCPs contribute to cluster tree topology as well.
Building on the analysis of the KDE and DCPs, we prove the topological
consistency of an estimated $\alpha$-tree.
</summary>
    <author>
      <name>Yen-Chi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">51 pages, 6 figures; accepted to the Annals of Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02762v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02762v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20 (Primary), 62G05, 62G07 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03693v1</id>
    <updated>2018-07-10T15:05:01Z</updated>
    <published>2018-07-10T15:05:01Z</published>
    <title>Customised Structural Elicitation</title>
    <summary>  Established methods for structural elicitation typically rely on code
modelling standard graphical models classes, most often Bayesian networks.
However, more appropriate models may arise from asking the expert questions in
common language about what might relate to what and exploring the logical
implications of the statements. Only after identifying the best matching
structure should this be embellished into a fully quantified probability model.
Examples of the efficacy and potential of this more flexible approach are shown
below for four classes of graphical models: Bayesian networks, Chain Event
Graphs, Multi-regression Dynamic Models, and Flow Graphs. We argue that to be
fully effective any structural elicitation phase must first be customised to an
application and if necessary new types of structure with their own bespoke
semantics elicited.
</summary>
    <author>
      <name>Rachel L. Wilkerson</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03582v1</id>
    <updated>2018-07-10T12:03:16Z</updated>
    <published>2018-07-10T12:03:16Z</published>
    <title>Construction of Confidence Intervals</title>
    <summary>  Introductory texts on statistics typically only cover the classical "two
sigma" confidence interval for the mean value and do not describe methods to
obtain confidence intervals for other estimators. The present technical report
fills this gap by first defining different methods for the construction of
confidence intervals, and then by their application to a binomial proportion,
the mean value, and to arbitrary estimators. Beside the frequentist approach,
the likelihood ratio and the highest posterior density approach are explained.
Two methods to estimate the variance of general maximum likelihood estimators
are described (Hessian, Jackknife), and for arbitrary estimators the bootstrap
is suggested. For three examples, the different methods are evaluated by means
of Monte Carlo simulations with respect to their coverage probability and
interval length. R code is given for all methods, and the practitioner obtains
a guideline which method should be used in which cases.
</summary>
    <author>
      <name>Christoph Dalitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02597v2</id>
    <updated>2018-07-10T08:29:24Z</updated>
    <published>2018-01-08T18:20:53Z</published>
    <title>Monte Carlo modified profile likelihood in models for clustered data</title>
    <summary>  The main focus of the analysts who deal with clustered data is usually not on
the clustering variables, and hence the group-specific parameters are treated
as nuisance. If a fixed effects formulation is preferred and the total number
of clusters is large relative to the single-group sizes, classical frequentist
techniques relying on the profile likelihood are often misleading. The use of
alternative tools, such as modifications to the profile likelihood or
integrated likelihoods, for making accurate inference on a parameter of
interest can be complicated by the presence of nonstandard modelling and/or
sampling assumptions. We show here how to employ Monte Carlo simulation in
order to approximate the modified profile likelihood in some of these
unconventional frameworks. The proposed solution is widely applicable and is
shown to retain the usual properties of the modified profile likelihood. The
approach is examined in two istances particularly relevant in applications,
i.e. missing-data models and survival models with unspecified censoring
distribution. The effectiveness of the proposed solution is validated via
simulation studies and two clinical trial applications.
</summary>
    <author>
      <name>Claudia Di Caterina</name>
    </author>
    <author>
      <name>Giuliana Cortese</name>
    </author>
    <author>
      <name>Nicola Sartori</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02597v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02597v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03469v1</id>
    <updated>2018-07-10T03:37:55Z</updated>
    <published>2018-07-10T03:37:55Z</published>
    <title>Pairwise Covariates-adjusted Block Model for Community Detection</title>
    <summary>  One of the most fundamental problems in network study is community detection.
The stochastic block model (SBM) is one widely used model for network data with
different estimation methods developed with their community detection
consistency results unveiled. However, the SBM is restricted by the strong
assumption that all nodes in the same community are stochastically equivalent,
which may not be suitable for practical applications. We introduce pairwise
covariates-adjusted stochastic block model (PCABM), a generalization of SBM
that incorporates pairwise covariate information. We study the maximum
likelihood estimates of the coefficients for the covariates as well as the
community assignments. It is shown that both the coefficient estimates of the
covariates and the community assignments are consistent under suitable sparsity
conditions. Spectral clustering with adjustment (SCWA) is introduced to
efficiently solve PCABM. Under certain conditions, we derive the error bound of
community estimation under SCWA and show that it is community detection
consistent. PCABM compares favorably with the SBM or degree-corrected
stochastic block model (DCBM) under a wide range of simulated and real networks
when covariate information is accessible.
</summary>
    <author>
      <name>Sihan Huang</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07747v5</id>
    <updated>2018-07-10T03:29:17Z</updated>
    <published>2017-10-21T03:26:53Z</published>
    <title>Localization for MCMC: sampling high-dimensional posterior distributions
  with local structure</title>
    <summary>  We investigate how ideas from covariance localization in numerical weather
prediction can be used in Markov chain Monte Carlo (MCMC) sampling of
high-dimensional posterior distributions arising in Bayesian inverse problems.
To localize an inverse problem is to enforce an anticipated "local" structure
by (i) neglecting small off-diagonal elements of the prior precision and
covariance matrices; and (ii) restricting the influence of observations to
their neighborhood. For linear problems we can specify the conditions under
which posterior moments of the localized problem are close to those of the
original problem. We explain physical interpretations of our assumptions about
local structure and discuss the notion of high dimensionality in local
problems, which is different from the usual notion of high dimensionality in
function space MCMC. The Gibbs sampler is a natural choice of MCMC algorithm
for localized inverse problems and we demonstrate that its convergence rate is
independent of dimension for localized linear problems. Nonlinear problems can
also be tackled efficiently by localization and, as a simple illustration of
these ideas, we present a localized Metropolis-within-Gibbs sampler. Several
linear and nonlinear numerical examples illustrate localization in the context
of MCMC samplers for inverse problems.
</summary>
    <author>
      <name>Matthias Morzfeld</name>
    </author>
    <author>
      <name>Xin T. Tong</name>
    </author>
    <author>
      <name>Youssef M. Marzouk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.07747v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07747v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05, 80M31, 62C10, 74G75" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03462v1</id>
    <updated>2018-07-10T03:23:06Z</updated>
    <published>2018-07-10T03:23:06Z</published>
    <title>A Curious Result on Breaking Ties among Sample Medians</title>
    <summary>  It is well known that any sample median value (not necessarily unique)
minimizes the empirical $L^1$ loss. Interestingly, we show that the minimizer
of the $L^{1+\epsilon}$ loss exhibits a singular phenomenon that provides a
unique definition for the sample median as $\epsilon \rightarrow 0$. This
definition is the unique point among all candidate median values that balances
the $logarithmic$ moment of the empirical distribution. The result generalizes
directly to breaking ties among sample quantiles.
</summary>
    <author>
      <name>Peter M. Aronow</name>
    </author>
    <author>
      <name>Donald K. K. Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, no tables or figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00789v3</id>
    <updated>2018-07-10T02:05:25Z</updated>
    <published>2017-11-02T15:51:16Z</published>
    <title>WARP: Wavelets with adaptive recursive partitioning for
  multi-dimensional data</title>
    <summary>  Traditional statistical wavelet analysis carries out modeling and inference
under a given, predetermined wavelet transform. This approach can quickly lose
efficiency for multi-dimensional data (e.g., observations measured on a
multi-dimensional grid), because a predetermined wavelet transform does not
adaptively exploit the information or energy distribution in a problem-specific
manner. This work aims to overcome this challenge within the multivariate
wavelet analysis framework by incorporating adaptivity into the wavelet
transform itself in a principled manner. By exploiting a connection between
wavelet transforms and permutations on the index space of multi-dimensional
functions, we show that the desired adaptive wavelet transform can be achieved
by adopting a layer of Bayesian hierarchical modeling on the space of such
permutations. In particular, when combined with the Haar basis, exact Bayesian
inference under the model can be achieved analytically through a recursive
message passing algorithm with an efficient computational complexity that is
linear with sample size. We also provide recipe for incorporating block
shrinkage and general wavelet bases into the framework, all while maintaining
such adaptivity. We demonstrate via extensive numerical experiments that with
our framework even simple 1D Haar wavelets can achieve excellent performance in
the context of 2D and 3D image reconstruction, outperforming state-of-the-art
wavelet and non-wavelet methods especially in noisy, low signal-to-noise ratio
settings at a fraction of the computational cost. Furthermore, we investigate
the source of the gain by quantitatively comparing the efficacy of energy
concentration under our adaptive wavelet transform with that of classical fixed
wavelet transforms.
</summary>
    <author>
      <name>Meng Li</name>
    </author>
    <author>
      <name>Li Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1711.00789v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00789v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03419v1</id>
    <updated>2018-07-09T23:11:17Z</updated>
    <published>2018-07-09T23:11:17Z</published>
    <title>On Causal Discovery with Equal Variance Assumption</title>
    <summary>  Prior work has shown that causal structure can be uniquely identified from
observational data when these follow a structural equation model whose error
terms have equal variances. We show that this fact is implied by an ordering
among (conditional) variances. We demonstrate that ordering estimates of these
variances yields a simple yet state-of-the-art method for causal structure
learning that is readily extendable to high-dimensional problems.
</summary>
    <author>
      <name>Mathias Drton</name>
    </author>
    <author>
      <name>Wenyu Chen</name>
    </author>
    <author>
      <name>Y. Samuel Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03413v1</id>
    <updated>2018-07-09T22:45:50Z</updated>
    <published>2018-07-09T22:45:50Z</published>
    <title>What to make of non-inferiority and equivalence testing with a
  post-specified margin?</title>
    <summary>  In order to determine whether or not an effect is absent based on a
statistical test, the recommended frequentist tool is the equivalence test.
Typically, it is expected that an appropriate equivalence margin has been
specified before any data is observed. Unfortunately, this can be a difficult
task. If the margin is too small, then the test's power will be substantially
reduced. If the margin is too large, any claims of equivalence or
non-inferiority will be meaningless. Moreover, it remains unclear how defining
the margin afterwards will bias one's results. In this short article, we
consider a series of hypothetical scenarios in which the margin is defined
post-hoc or is otherwise considered controversial. We also review a number of
relevant, potentially problematic actual studies from health research, with the
aim of motivating a critical discussion as to what is acceptable and desirable
in the reporting and interpretation of equivalence tests.
</summary>
    <author>
      <name>Harlan Campbell</name>
    </author>
    <author>
      <name>Paul Gustafson</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03375v1</id>
    <updated>2018-07-09T20:30:12Z</updated>
    <published>2018-07-09T20:30:12Z</published>
    <title>Predictive Directions for Individualized Treatment Selection in Clinical
  Trials</title>
    <summary>  In many clinical trials, individuals in different subgroups have experience
differential treatment effects. This leads to individualized differences in
treatment benefit. In this article, we introduce the general concept of
predictive directions, which are risk scores motivated by potential outcomes
considerations. These techniques borrow heavily from sufficient dimension
reduction (SDR) and causal inference methodology. Under some conditions, one
can use existing methods from the SDR literature to estimate the directions
assuming an idealized complete data structure, which subsequently yields an
obvious extension to clinical trial datasets. In addition, we generalize the
direction idea to a nonlinear setting that exploits support vector machines.
The methodology is illustrated with application to a series of colorectal
cancer clinical trials.
</summary>
    <author>
      <name>Debashis Ghosh</name>
    </author>
    <author>
      <name>Youngjoo Cho</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07542v2</id>
    <updated>2018-07-09T19:03:00Z</updated>
    <published>2017-09-21T23:29:30Z</published>
    <title>Heteroscedastic BART Using Multiplicative Regression Trees</title>
    <summary>  BART (Bayesian Additive Regression Trees) has become increasingly popular as
a flexible and scalable nonparametric regression approach for modern applied
statistics problems. For the practitioner dealing with large and complex
nonlinear response surfaces, its advantages include a matrix-free formulation
and the lack of a requirement to prespecify a confining regression basis.
Although flexible in fitting the mean, BART has been limited by its reliance on
a constant variance error model. This homoscedastic assumption is unrealistic
in many applications. Alleviating this limitation, we propose HBART, a
nonparametric heteroscedastic elaboration of BART. In BART, the mean function
is modeled with a sum of trees, each of which determines an additive
contribution to the mean. In HBART, the variance function is further modeled
with a product of trees, each of which determines a multiplicative contribution
to the variance. Like the mean model, this flexible, multidimensional variance
model is entirely nonparametric with no need for the prespecification of a
confining basis. Moreover, with this enhancement, HBART can provide insights
into the potential relationships of the predictors with both the mean and the
variance. Practical implementations of HBART with revealing new diagnostic
plots are demonstrated with simulated and real data on used car prices, fishing
catch production and alcohol consumption.
</summary>
    <author>
      <name>Matthew Pratola</name>
    </author>
    <author>
      <name>Hugh Chipman</name>
    </author>
    <author>
      <name>Edward George</name>
    </author>
    <author>
      <name>Robert McCulloch</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07542v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07542v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03265v1</id>
    <updated>2018-07-09T16:34:39Z</updated>
    <published>2018-07-09T16:34:39Z</published>
    <title>Efficient convergence through adaptive learning in sequential Monte
  Carlo Expectation Maximization</title>
    <summary>  Expectation maximization (EM) is a technique for estimating
maximum-likelihood parameters of a latent variable model given observed data by
alternating between taking expectations of sufficient statistics, and
maximizing the expected log likelihood. For situations where sufficient
statistics are intractable, stochastic approximation EM (SAEM) is often used,
which uses Monte Carlo techniques to approximate the expected log likelihood.
Two common implementations of SAEM, Batch EM (BEM) and online EM (OEM), are
parameterized by a "learning rate", and their efficiency depend strongly on
this parameter. We propose an extension to the OEM algorithm, termed
Introspective Online Expectation Maximization (IOEM), which removes the need
for specifying this parameter by adapting the learning rate according to trends
in the parameter updates. We show that our algorithm matches the efficiency of
the optimal BEM and OEM algorithms in multiple models, and that the efficiency
of IOEM can exceed that of BEM/OEM methods with optimal learning rates when the
model has many parameters. A Python implementation is available at
https://github.com/luntergroup/IOEM.git.
</summary>
    <author>
      <name>Donna Henderson</name>
    </author>
    <author>
      <name>Gerton Lunter</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C60 65B99 62F10 62L12 62M05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.5966v5</id>
    <updated>2018-07-09T16:10:04Z</updated>
    <published>2013-10-22T15:48:35Z</published>
    <title>Non-marginal Decisions: A Novel Bayesian Multiple Testing Procedure</title>
    <summary>  In this paper we consider the problem of multiple testing when the hypotheses
are dependent. In most of the existing literature, either Bayesian or
non-Bayesian, the decision rules mainly focus on the validity of the test
procedure rather than actually utilizing the dependency to increase efficiency.
Moreover, the decisions regarding different hypotheses are marginal in the
sense that they do not depend upon each other directly. However, in realistic
situations, the hypotheses are usually dependent, and hence it is desirable
that the decisions regarding the dependent hypotheses are taken jointly.
  In this article we develop a novel Bayesian multiple testing procedure that
coherently takes this requirement into consideration. Our method, which is
based on new notions of error and non-error terms, substantially enhances
efficiency by judicious exploitation of the dependence structure among the
hypotheses. We prove that our method minimizes the posterior expected loss
associated with a an additive "0-1" loss function, we also prove theoretical
results on the relevant error probabilities, establishing the coherence and
usefulness of our method. The optimal decision configuration is not available
in closed form and we propose a novel and efficient simulated annealing
algorithm for the purpose of optimization, which is also generically applicable
to binary optimization problems.
  Numerical studies demonstrate that in dependent situations, our method
performs significantly better than some existing popular conventional multiple
testing methods, in terms of accuracy and power control. Moreover, application
of our ideas to a real, spatial data set associated with radionuclide
concentration in Rongelap islands yielded insightful results.
</summary>
    <author>
      <name>Noirrit K. Chandra</name>
    </author>
    <author>
      <name>Sourabh Bhattacharya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A significantly updated version</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.5966v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.5966v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03184v1</id>
    <updated>2018-07-09T14:15:11Z</updated>
    <published>2018-07-09T14:15:11Z</published>
    <title>Prediction regions through Inverse Regression</title>
    <summary>  Predict a new response from a covariate is a challenging task in regression,
which raises new question since the era of high-dimensional data. In this
paper, we are interested in the inverse regression method from a theoretical
viewpoint. Theoretical results have already been derived for the well-known
linear model, but recently, the curse of dimensionality has increased the
interest of practitioners and theoreticians into generalization of those
results for various estimators, calibrated for the high-dimension context. To
deal with high-dimensional data, inverse regression is used in this paper. It
is known to be a reliable and efficient approach when the number of features
exceeds the number of observations. Indeed, under some conditions, dealing with
the inverse regression problem associated to a forward regression problem
drastically reduces the number of parameters to estimate and make the problem
tractable. When both the responses and the covariates are multivariate,
estimators constructed by the inverse regression are studied in this paper, the
main result being explicit asymptotic prediction regions for the response. The
performances of the proposed estimators and prediction regions are also
analyzed through a simulation study and compared with usual estimators.
</summary>
    <author>
      <name>Emilie Devijver</name>
    </author>
    <author>
      <name>Emeline Perthame</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F12, 62F25, 62J05, 62E20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03113v1</id>
    <updated>2018-07-09T13:28:15Z</updated>
    <published>2018-07-09T13:28:15Z</published>
    <title>Sampling and Inference for Beta Neutral-to-the-Left Models of Sparse
  Networks</title>
    <summary>  Empirical evidence suggests that heavy-tailed degree distributions occurring
in many real networks are well-approximated by power laws with exponents $\eta$
that may take values either less than and greater than two. Models based on
various forms of exchangeability are able to capture power laws with $\eta &lt;
2$, and admit tractable inference algorithms; we draw on previous results to
show that $\eta &gt; 2$ cannot be generated by the forms of exchangeability used
in existing random graph models. Preferential attachment models generate power
law exponents greater than two, but have been of limited use as statistical
models due to the inherent difficulty of performing inference in
non-exchangeable models. Motivated by this gap, we design and implement
inference algorithms for a recently proposed class of models that generates
$\eta$ of all possible values. We show that although they are not exchangeable,
these models have probabilistic structure amenable to inference. Our methods
make a large class of previously intractable models useful for statistical
inference.
</summary>
    <author>
      <name>Benjamin Bloem-Reddy</name>
    </author>
    <author>
      <name>Adam Foster</name>
    </author>
    <author>
      <name>Emile Mathieu</name>
    </author>
    <author>
      <name>Yee Whye Teh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the proceedings of Conference on
  Uncertainty in Artificial Intelligence (UAI) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03090v1</id>
    <updated>2018-07-09T13:06:00Z</updated>
    <published>2018-07-09T13:06:00Z</published>
    <title>A partial orthogonalization method for simulating covariance and
  concentration graph matrices</title>
    <summary>  Structure learning methods for covariance and concentration graphs are often
validated on synthetic models, usually obtained by randomly generating: (i) an
undirected graph, and (ii) a compatible symmetric positive definite (SPD)
matrix. In order to ensure positive definiteness in (ii), a dominant diagonal
is usually imposed. However, the link strengths in the resulting graphical
model, determined by off-diagonal entries in the SPD matrix, are in many
scenarios extremely weak. Recovering the structure of the undirected graph thus
becomes a challenge, and algorithm validation is notably affected. In this
paper, we propose an alternative method which overcomes such problem yet
yielding a compatible SPD matrix. We generate a partially row-wise-orthogonal
matrix factor, where pairwise orthogonal rows correspond to missing edges in
the undirected graph. In numerical experiments ranging from moderately dense to
sparse scenarios, we obtain that, as the dimension increases, the link strength
we simulate is stable with respect to the structure sparsity. Importantly, we
show in a real validation setting how structure recovery is greatly improved
for all learning algorithms when using our proposed method, thereby producing a
more realistic comparison framework.
</summary>
    <author>
      <name>Irene Córdoba</name>
    </author>
    <author>
      <name>Gherardo Varando</name>
    </author>
    <author>
      <name>Concha Bielza</name>
    </author>
    <author>
      <name>Pedro Larrañaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08027v2</id>
    <updated>2018-07-09T11:13:50Z</updated>
    <published>2018-03-21T17:26:41Z</published>
    <title>Efficient Bandwidth Estimation in Two-dimensional Filtered
  Backprojection PET Reconstruction</title>
    <summary>  A method to efficiently estimate the bandwidth of the reconstruction filter
in two-dimensional Positron Emission Tomography is presented. The approach uses
matrix notation to recast the convolution Filtered Backprojection equation into
its equivalent Backprojected Filtering form. Results on eigendecomposition of
symmetric two-dimensional circulant matrices are derived and their
computational implications in making reconstruction bandwidth estimation a
simple extension of standard backprojected filtering investigated to provide a
computationally efficient generalized cross-validation estimator. Performance
evaluations on a wide range of simulation experiments are promising. The
superior performance holds at both low and high total expected emissions,
pointing to the method's applicability in a wide range of radio-tracer dosage
situations. The approach is easily applied to the more general class of
elliptically symmetric filters, with reconstruction performance often better
than even that obtained with the true optimal radially symmetric filter.
</summary>
    <author>
      <name>Ranjan Maitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.08027v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08027v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03299v1</id>
    <updated>2018-07-09T08:47:19Z</updated>
    <published>2018-07-09T08:47:19Z</published>
    <title>Optimization of a SSP's Header Bidding Strategy using Thompson Sampling</title>
    <summary>  Over the last decade, digital media (web or app publishers) generalized the
use of real time ad auctions to sell their ad spaces. Multiple auction
platforms, also called Supply-Side Platforms (SSP), were created. Because of
this multiplicity, publishers started to create competition between SSPs. In
this setting, there are two successive auctions: a second price auction in each
SSP and a secondary, first price auction, called header bidding auction,
between SSPs.In this paper, we consider an SSP competing with other SSPs for ad
spaces. The SSP acts as an intermediary between an advertiser wanting to buy ad
spaces and a web publisher wanting to sell its ad spaces, and needs to define a
bidding strategy to be able to deliver to the advertisers as many ads as
possible while spending as little as possible. The revenue optimization of this
SSP can be written as a contextual bandit problem, where the context consists
of the information available about the ad opportunity, such as properties of
the internet user or of the ad placement.Using classical multi-armed bandit
strategies (such as the original versions of UCB and EXP3) is inefficient in
this setting and yields a low convergence speed, as the arms are very
correlated. In this paper we design and experiment a version of the Thompson
Sampling algorithm that easily takes this correlation into account. We combine
this bayesian algorithm with a particle filter, which permits to handle
non-stationarity by sequentially estimating the distribution of the highest bid
to beat in order to win an auction. We apply this methodology on two real
auction datasets, and show that it significantly outperforms more classical
approaches.The strategy defined in this paper is being developed to be deployed
on thousands of publishers worldwide.
</summary>
    <author>
      <name>Grégoire Jauvion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Grislain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Sielenou Dkengne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <author>
      <name>Aurélien Garivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Gerchinovitz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219917</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219917" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 24th ACM SIGKDD International Conference on Knowledge
  Discovery &amp; Data Mining, Aug 2018, London, United Kingdom</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.03299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03705v1</id>
    <updated>2018-07-09T07:40:58Z</updated>
    <published>2018-07-09T07:40:58Z</published>
    <title>Decision making under uncertainty using imprecise probabilities</title>
    <summary>  Various ways for decision making with imprecise probabilities (admissibility,
maximal expected utility, maximality, E-admissibility, $\Gamma$-maximax,
$\Gamma$-maximin, all of which are well-known from the literature) are
discussed and compared. We generalize a well-known sufficient condition for
existence of optimal decisions. A simple numerical example shows how these
criteria can work in practice, and demonstrates their differences. Finally, we
suggest an efficient approach to calculate optimal decisions under these
decision criteria.
</summary>
    <author>
      <name>Matthias C. M. Troffaes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ijar.2006.06.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ijar.2006.06.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Approximate Reasoning 45 (2007) 17-29</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.03705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02930v1</id>
    <updated>2018-07-09T03:38:52Z</updated>
    <published>2018-07-09T03:38:52Z</published>
    <title>Computing the statistical significance of optimized communities in
  networks</title>
    <summary>  It is often of interest to find communities in network data as a form of
unsupervised learning, either for feature discovery or scientific study. The
vast majority of community detection methods proceed via optimization of a
quality function, which is possible even on random networks without
communities. Therefore there is usually not an easy way to tell if an optimized
community is significant, in this context meaning more internally connected
than would be expected under a random graph model without true communities.
This paper introduces FOCS (Fast Optimized Community Significance), a new
approach for computing a significance score for individual communities.
</summary>
    <author>
      <name>John Palowitch</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02877v1</id>
    <updated>2018-07-08T20:22:39Z</updated>
    <published>2018-07-08T20:22:39Z</published>
    <title>Moderated Network Models</title>
    <summary>  Pairwise network models such as the Gaussian Graphical Model (GGM) are a
powerful and intuitive way to analyze dependencies in multivariate data. A key
assumption of the GGM is that each pairwise interaction is independent of the
values of all other variables. However, in psychological research this is often
implausible. In this paper, we extend the GGM by allowing each pairwise
interaction between two variables to be moderated by (a subset of) all other
variables in the model, and thereby introduce a Moderated Network Model (MNM).
We show how to construct the MNW and propose an L1-regularized nodewise
regression approach to estimate it. We provide performance results in a
simulation study and show that MNMs outperform the split-sample based methods
Network Comparison Test (NCT) and Fused Graphical Lasso (FGL) in detecting
moderation effects. Finally, we provide a fully reproducible tutorial on how to
estimate MNMs with the R-package mgm and discuss possible issues with model
misspecification.
</summary>
    <author>
      <name>Jonas Haslbeck</name>
    </author>
    <author>
      <name>Denny Borsboom</name>
    </author>
    <author>
      <name>Lourens Waldorp</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08876v2</id>
    <updated>2018-07-08T18:19:35Z</updated>
    <published>2017-11-24T06:16:22Z</published>
    <title>Is it even rainier in North Vancouver? A non-parametric rank-based test
  for semicontinuous longitudinal data</title>
    <summary>  When the outcome of interest is semicontinuous and collected longitudinally,
efficient testing can be difficult. Daily rainfall data is an excellent example
which we use to illustrate the various challenges. Even under the simplest
scenario, the popular 'two-part model', which uses correlated random-effects to
account for both the semicontinuous and longitudinal characteristics of the
data, often requires prohibitively intensive numerical integration and
difficult interpretation. Reducing data to binary (truncating continuous
positive values to equal one), while relatively straightforward, leads to a
potentially substantial loss in power. We propose an alternative: using a
non-parametric rank test recently proposed for joint longitudinal survival
data. We investigate the benefits of such a test for the analysis of
semicontinuous longitudinal data with regards to power and computational
feasibility.
</summary>
    <author>
      <name>Harlan Campbell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages with SAS and R code</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.08876v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08876v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02817v1</id>
    <updated>2018-07-08T13:40:05Z</updated>
    <published>2018-07-08T13:40:05Z</published>
    <title>Integration of survey data and big observational data for finite
  population inference using mass imputation</title>
    <summary>  Multiple data sources are becoming increasingly available for statistical
analyses in the era of big data. As an important example in finite-population
inference, we consider an imputation approach to combining a probability sample
with big observational data. Unlike the usual imputation for missing data
analysis, we create imputed values for the whole elements in the probability
sample. Such mass imputation is attractive in the context of survey data
integration (Kim and Rao, 2012). We extend mass imputation as a tool for data
integration of survey data and big non-survey data. The mass imputation methods
and their statistical properties are presented. The matching estimator of
Rivers (2007) is also covered as a special case. Variance estimation with
mass-imputed data is discussed. The simulation results demonstrate the proposed
estimators outperform existing competitors in terms of robustness and
efficiency.
</summary>
    <author>
      <name>Shu Yang</name>
    </author>
    <author>
      <name>Jae Kwang Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02814v1</id>
    <updated>2018-07-08T13:25:45Z</updated>
    <published>2018-07-08T13:25:45Z</published>
    <title>Measurement Errors as Bad Leverage Points</title>
    <summary>  Errors-in-variables is a long-standing, difficult issue in linear regression;
and progress depends in part on new identifying assumptions. I characterize
measurement error as bad-leverage points and assume that fewer than half the
sample observations are heavily contaminated, in which case a high-breakdown
robust estimator may be able to isolate and down weight or discard the
problematic data. In simulations of simple and multiple regression where eiv
affects 25% of the data and R-squared is mediocre, certain high-breakdown
estimators have small bias and reliable confidence intervals.
</summary>
    <author>
      <name>Eric Blankmeyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02789v1</id>
    <updated>2018-07-08T09:26:54Z</updated>
    <published>2018-07-08T09:26:54Z</published>
    <title>The modal age of Statistics</title>
    <summary>  Recently, a number of statistical problems have found an unexpected solution
by inspecting them through a "modal point of view". These include classical
tasks such as clustering or regression. This has led to a renewed interest in
estimation and inference for the mode. This paper offers an extensive survey of
the traditional approaches to mode estimation and explores the consequences of
applying this modern modal methodology to other, seemingly unrelated, fields.
</summary>
    <author>
      <name>José E. Chacón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02737v1</id>
    <updated>2018-07-08T01:49:32Z</updated>
    <published>2018-07-08T01:49:32Z</published>
    <title>A Causal Bootstrap</title>
    <summary>  The bootstrap, introduced by Efron (1982), has become a very popular method
for estimating variances and constructing confidence intervals. A key insight
is that one can approximate the properties of estimators by using the empirical
distribution function of the sample as an approximation for the true
distribution function. This approach views the uncertainty in the estimator as
coming exclusively from sampling uncertainty. We argue that for causal
estimands the uncertainty arises entirely, or partially, from a different
source, corresponding to the stochastic nature of the treatment received. We
develop a bootstrap procedure that accounts for this uncertainty, and compare
its properties to that of the classical bootstrap.
</summary>
    <author>
      <name>Guido Imbens</name>
    </author>
    <author>
      <name>Konrad Menzel</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09735v2</id>
    <updated>2018-07-07T22:06:45Z</updated>
    <published>2018-03-26T17:46:39Z</published>
    <title>A Scalable Empirical Bayes Approach to Variable Selection in Generalized
  Linear Models</title>
    <summary>  A new empirical Bayes approach to variable selection in the context of
generalized linear models is developed. The proposed algorithm scales to
situations in which the number of putative explanatory variables is very large,
possibly much larger than the number of responses. The coefficients in the
linear predictor are modeled as a three-component mixture allowing the
explanatory variables to have a random positive effect on the response, a
random negative effect, or no effect. A key assumption is that only a small
(but unknown) fraction of the candidate variables have a non-zero effect. This
assumption, in addition to treating the coefficients as random effects
facilitates an approach that is computationally efficient. In particular, the
number of parameters that have to be estimated is small, and remains constant
regardless of the number of explanatory variables. The model parameters are
estimated using a Generalized Alternating Maximization algorithm which is
scalable, and leads to significantly faster convergence compared with
simulation-based fully Bayesian methods.
</summary>
    <author>
      <name>Haim Bar</name>
    </author>
    <author>
      <name>James Booth</name>
    </author>
    <author>
      <name>Martin T. Wells</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1510.03781</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09735v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09735v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06404v2</id>
    <updated>2018-07-07T20:23:57Z</updated>
    <published>2016-12-19T21:01:36Z</published>
    <title>Random Walk Models of Network Formation and Sequential Monte Carlo
  Methods for Graphs</title>
    <summary>  We introduce a class of generative network models that insert edges by
connecting the starting and terminal vertices of a random walk on the network
graph. Within the taxonomy of statistical network models, this class is
distinguished by permitting the location of a new edge to explicitly depend on
the structure of the graph, but being nonetheless statistically and
computationally tractable. In the limit of infinite walk length, the model
converges to an extension of the preferential attachment model---in this sense,
it can be motivated alternatively by asking what preferential attachment is an
approximation to. Theoretical properties, including the limiting degree
sequence, are studied analytically. If the entire history of the graph is
observed, parameters can be estimated by maximum likelihood. If only the final
graph is available, its history can be imputed using MCMC. We develop a class
of sequential Monte Carlo algorithms that are more generally applicable to
sequential network models, and may be of interest in their own right. The model
parameters can be recovered from a single graph generated by the model.
Applications to data clarify the role of the random walk length as a length
scale of interactions within the graph.
</summary>
    <author>
      <name>Benjamin Bloem-Reddy</name>
    </author>
    <author>
      <name>Peter Orbanz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/rssb.12289</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/rssb.12289" rel="related"/>
    <link href="http://arxiv.org/abs/1612.06404v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06404v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06031v3</id>
    <updated>2018-07-07T18:15:20Z</updated>
    <published>2017-10-16T23:27:15Z</published>
    <title>Large Scale Replication Projects in Contemporary Psychological Research</title>
    <summary>  Replication is complicated in psychological research because studies of a
given psychological phenomenon can never be direct or exact replications of one
another, and thus effect sizes vary from one study of the phenomenon to the
next--an issue of clear importance for replication. Current large scale
replication projects represent an important step forward for assessing
replicability, but provide only limited information because they have thus far
been designed in a manner such that heterogeneity either cannot be assessed or
is intended to be eliminated. Consequently, the non-trivial degree of
heterogeneity found in these projects represents a lower bound on
heterogeneity. We recommend enriching large scale replication projects going
forward by em- bracing heterogeneity. We argue this is key for assessing
replicability: if effect sizes are sufficiently heterogeneous--even if the sign
of the effect is consistent--the phenomenon in question does not seem
particularly replicable and the theory underlying it seems poorly constructed
and in need of enrichment. Uncovering why and revising theory in light of it
will lead to improved theory that explains heterogeneity and in- creases
replicability. Given this, large scale replication projects can play an
important role not only in assessing replicability but also in advancing
theory.
</summary>
    <author>
      <name>Blakeley B. McShane</name>
    </author>
    <author>
      <name>Jennifer L. Tackett</name>
    </author>
    <author>
      <name>Ulf Bockenholt</name>
    </author>
    <author>
      <name>Andrew Gelman</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06031v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06031v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.02596v2</id>
    <updated>2018-07-07T10:56:30Z</updated>
    <published>2018-03-07T11:03:36Z</published>
    <title>Revisiting differentially private linear regression: optimal and
  adaptive prediction &amp; estimation in unbounded domain</title>
    <summary>  We revisit the problem of linear regression under a differential privacy
constraint. By consolidating existing pieces in the literature, we clarify the
correct dependence of the feature, label and coefficient domains in the
optimization error and estimation error, hence revealing the delicate price of
differential privacy in statistical estimation and statistical learning.
Moreover, we propose simple modifications of two existing DP algorithms: (a)
posterior sampling, (b) sufficient statistics perturbation, and show that they
can be upgraded into **adaptive** algorithms that are able to exploit
data-dependent quantities and behave nearly optimally **for every instance**.
Extensive experiments are conducted on both simulated data and real data, which
conclude that both AdaOPS and AdaSSP outperform the existing techniques on
nearly all 36 data sets that we test on.
</summary>
    <author>
      <name>Yu-Xiang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Uncertainty in Artificial Intelligence (UAI-2018), Monterey, CA</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02603v1</id>
    <updated>2018-07-07T02:13:18Z</updated>
    <published>2018-07-07T02:13:18Z</published>
    <title>A Note on the Shannon Entropy of Short Sequences</title>
    <summary>  For source sequences of length L symbols we proposed to use a more realistic
value to the usual benchmark of number of code letters by source letters. Our
idea is based on a quantifier of information fluctuation of a source, F(U),
which corresponds to the second central moment of the random variable that
measures the information content of a source symbol. An alternative
interpretation of typical sequences is additionally provided through this
approach.
</summary>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>Raydonal Ospina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94Axx, 68P30, 94A15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02602v1</id>
    <updated>2018-07-07T02:05:30Z</updated>
    <published>2018-07-07T02:05:30Z</published>
    <title>Robust Estimation for Two-Dimensional Autoregressive Processes Based on
  Bounded Innovation Propagation Representations</title>
    <summary>  Robust methods have been a successful approach to deal with contaminations
and noises in image processing. In this paper, we introduce a new robust method
for two-dimensional autoregressive models. Our method, called BMM-2D, relies on
representing a two-dimensional autoregressive process with an auxiliary model
to attenuate the effect of contamination (outliers). We compare the performance
of our method with existing robust estimators and the least squares estimator
via a comprehensive Monte Carlo simulation study which considers different
levels of replacement contamination and window sizes. The results show that the
new estimator is superior to the other estimators, both in accuracy and
precision. An application to image filtering highlights the findings and
illustrates how the estimator works in practical applications.
</summary>
    <author>
      <name>Grisel Maribel Britos</name>
    </author>
    <author>
      <name>Silvia María Ojeda</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02590v1</id>
    <updated>2018-07-06T23:49:26Z</updated>
    <published>2018-07-06T23:49:26Z</published>
    <title>Resample-smoothing of Voronoi intensity estimators</title>
    <summary>  Voronoi intensity estimators, which are non-parametric estimators for
intensity functions of point processes, are both parameter-free and adaptive;
the intensity estimate at a given location is given by the reciprocal size of
the Voronoi/Dirichlet cell containing that location. Their major drawback,
however, is that they tend to under-smooth the data in regions where the point
density of the observed point pattern is high and over-smooth in regions where
the point density is low. To remedy this problem, i.e. to find some
middle-ground between over- and under-smoothing, we propose an additional
smoothing technique for Voronoi intensity estimators for point processes in
arbitrary metric spaces, which is based on repeated independent thinnings of
the point process/pattern. Through a simulation study we show that our
resample-smoothing technique improves the estimation significantly. In
addition, we study statistical properties such as unbiasedness and variance,
and propose a rule-of-thumb and a data-driven cross-validation approach to
choose the amount of thinning/smoothing to apply. We finally apply our proposed
intensity estimation scheme to two datasets: locations of pine saplings (planar
point pattern) and motor vehicle traffic accidents (linear network point
pattern).
</summary>
    <author>
      <name>M. Mehdi Moradi</name>
    </author>
    <author>
      <name>Ottmar Cronie</name>
    </author>
    <author>
      <name>Ege Rubak</name>
    </author>
    <author>
      <name>Raphael Lachieze-Rey</name>
    </author>
    <author>
      <name>Jorge Mateu</name>
    </author>
    <author>
      <name>Adrian Baddeley</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G55, 62H11, 62M30, 60D05, 62G07, 62G09" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02535v1</id>
    <updated>2018-07-06T18:08:46Z</updated>
    <published>2018-07-06T18:08:46Z</published>
    <title>Invertible Particle Flow-based Sequential MCMC with extension to
  Gaussian Mixture noise models</title>
    <summary>  Sequential state estimation in non-linear and non-Gaussian state spaces has a
wide range of applications in statistics and signal processing. One of the most
effective non-linear filtering approaches, particle filtering, suffers from
weight degeneracy in high-dimensional filtering scenarios. Several avenues have
been pursued to address high-dimensionality. Among these, particle flow
particle filters construct effective proposal distributions by using invertible
flow to migrate particles continuously from the prior distribution to the
posterior, and sequential Markov chain Monte Carlo (SMCMC) methods use
rejection sampling to improve filtering performance. In this paper, we propose
to combine the strengths of invertible particle flow and SMCMC by constructing
a composite Metropolis-Hastings (MH) kernel within the SMCMC framework using
invertible particle flow. In addition, we propose a Gaussian mixture model
(GMM)-based particle flow algorithm to construct effective MH kernels for
multi-modal distributions. Simulation results show that for high-dimensional
state estimation example problems the proposed kernels significantly increase
the acceptance rate with minimal computational overhead and improve estimation
accuracy compared with state-of-the-art filtering algorithms.
</summary>
    <author>
      <name>Yunpeng Li</name>
    </author>
    <author>
      <name>Soumyasundar Pal</name>
    </author>
    <author>
      <name>Mark Coates</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01242v3</id>
    <updated>2018-07-06T11:23:00Z</updated>
    <published>2018-01-04T04:30:22Z</published>
    <title>Sparse Bayesian ARX models with flexible noise distributions</title>
    <summary>  This paper considers the problem of estimating linear dynamic system models
when the observations are corrupted by random disturbances with nonstandard
distributions. The paper is particularly motivated by applications where sensor
imperfections involve significant contribution of outliers or wrap-around
issues resulting in multi-modal distributions such as commonly encountered in
robotics applications. As will be illustrated, these nonstandard measurement
errors can dramatically compromise the effectiveness of standard estimation
methods, while a computational Bayesian approach developed here is demonstrated
to be equally effective as standard methods in standard measurement noise
scenarios, but dramatically more effective in nonstandard measurement noise
distribution scenarios.
</summary>
    <author>
      <name>Johan Dahlin</name>
    </author>
    <author>
      <name>Adrian Wills</name>
    </author>
    <author>
      <name>Brett Ninness</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures. Accepted for publication in the Proceedings of
  the 18th IFAC Symposium on System Identification (SYSID). Typos corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01242v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01242v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02357v1</id>
    <updated>2018-07-06T11:19:19Z</updated>
    <published>2018-07-06T11:19:19Z</published>
    <title>Autoregressive Wild Bootstrap Inference for Nonparametric Trends</title>
    <summary>  In this paper we propose an autoregressive wild bootstrap method to construct
confidence bands around a smooth deterministic trend. The bootstrap method is
easy to implement and does not require any adjustments in the presence of
missing data, which makes it particularly suitable for climatological
applications. We establish the asymptotic validity of the bootstrap method for
both pointwise and simultaneous confidence bands under general conditions,
allowing for general patterns of missing data, serial dependence and
heteroskedasticity. The finite sample properties of the method are studied in a
simulation study. We use the method to study the evolution of trends in daily
measurements of atmospheric ethane obtained from a weather station in the Swiss
Alps, where the method can easily deal with the many missing observations due
to adverse weather conditions.
</summary>
    <author>
      <name>Marina Friedrich</name>
    </author>
    <author>
      <name>Stephan Smeekes</name>
    </author>
    <author>
      <name>Jean-Pierre Urbain</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02289v1</id>
    <updated>2018-07-06T07:25:32Z</updated>
    <published>2018-07-06T07:25:32Z</published>
    <title>Interleaved lattice-based maximin distance designs</title>
    <summary>  We propose a new method to construct maximin distance designs with arbitrary
number of dimensions and points. The proposed designs hold interleaved-layer
structures and are by far the best maximin distance designs in four or more
dimensions. Applicable to distance measures with equal or unequal weights, our
method is useful for emulating computer experiments when a relatively accurate
priori guess on the variable importance is available.
</summary>
    <author>
      <name>Xu He</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62K99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02244v1</id>
    <updated>2018-07-06T03:41:56Z</updated>
    <published>2018-07-06T03:41:56Z</published>
    <title>A Bayesian Framework for Non-Collapsible Models</title>
    <summary>  In this paper, we discuss the non-collapsibility concept and propose a new
approach based on Dirichlet process mixtures to estimate the conditional effect
of covariates in non-collapsible models. Using synthetic data, we evaluate the
performance of our proposed method and examine its sensitivity under different
settings. We also apply our method to real data on access failure among
hemodialysis patients.
</summary>
    <author>
      <name>Sepehr Akhavan Masouleh</name>
    </author>
    <author>
      <name>Babak Shahbaba</name>
    </author>
    <author>
      <name>Daniel L. Gillen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06053v2</id>
    <updated>2018-07-06T00:07:12Z</updated>
    <published>2018-03-16T01:43:58Z</published>
    <title>The world of research has gone berserk: modeling the consequences of
  requiring "greater statistical stringency" for scientific publication</title>
    <summary>  In response to growing concern about the reliability and reproducibility of
published science, researchers have proposed adopting measures of greater
statistical stringency, including suggestions to require larger sample sizes
and to lower the highly criticized p&lt;0.05 significance threshold. While pros
and cons are vigorously debated, there has been little to no modeling of how
adopting these measures might affect what type of science is published. In this
paper, we develop a novel optimality model that, given current incentives to
publish, predicts a researcher's most rational use of resources in terms of the
number of studies to undertake, the statistical power to devote to each study,
and the desirable pre-study odds to pursue. We then develop a methodology that
allows one to estimate the reliability of published research by considering a
distribution of preferred research strategies. Using this approach, we
investigate the merits of adopting measures of `greater statistical stringency'
with the goal of informing the ongoing debate.
</summary>
    <author>
      <name>Harlan Campbell</name>
    </author>
    <author>
      <name>Paul Gustafson</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04251v2</id>
    <updated>2018-07-05T23:03:07Z</updated>
    <published>2018-06-11T21:36:24Z</published>
    <title>Put the odds on your side: a new measure for epidemiological
  associations</title>
    <summary>  The odds ratio (OR) is a measure of effect size commonly used in
observational research. OR reflects statistical association between a binary
outcome, such as the presence of a health condition, and a binary predictor,
such as an exposure to a pollutant. Statistical inference and interval
estimation for OR are often performed on the logarithmic scale, due to
asymptotic convergence of log(OR) to a normal distribution. Here, we propose a
new normalized measure of effect size, $\gamma'$, and derive its asymptotic
distribution. We show that the new statistic, based on the $\gamma'$
distribution, is more powerful than the traditional one for testing the
hypothesis $H_0$: log(OR)=0. The new normalized effect size is termed `gamma
prime' in the spirit of $D'$, a normalized measure of genetic linkage
disequilibrium, which ranges from -1 to 1 for a pair of genetic loci. The
normalization constant for $\gamma'$ is based on the maximum range of the
standardized effect size, for which we establish a peculiar connection to the
Laplace Limit Constant. Furthermore, while standardized effects are of little
value on their own, we propose a powerful application, in which standardized
effects are employed as an intermediate step in an approximate, yet accurate
posterior inference for raw effect size measures, such as log(OR) and
$\gamma'$.
</summary>
    <author>
      <name>Olga A Vsevolozhskaya</name>
    </author>
    <author>
      <name>Dmitri V Zaykin</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02191v1</id>
    <updated>2018-07-05T22:14:59Z</updated>
    <published>2018-07-05T22:14:59Z</published>
    <title>An MCMC Approach to Empirical Bayes Inference and Bayesian Sensitivity
  Analysis via Empirical Processes</title>
    <summary>  Consider a Bayesian situation in which we observe $Y \sim p_{\theta}$, where
$\theta \in \Theta$, and we have a family $\{ \nu_h, \, h \in \mathcal{H} \}$
of potential prior distributions on $\Theta$. Let $g$ be a real-valued function
of $\theta$, and let $I_g(h)$ be the posterior expectation of $g(\theta)$ when
the prior is $\nu_h$. We are interested in two problems: (i) selecting a
particular value of $h$, and (ii) estimating the family of posterior
expectations $\{ I_g(h), \, h \in \mathcal{H} \}$. Let $m_y(h)$ be the marginal
likelihood of the hyperparameter $h$: $m_y(h) = \int p_{\theta}(y) \,
\nu_h(d\theta)$. The empirical Bayes estimate of $h$ is, by definition, the
value of $h$ that maximizes $m_y(h)$. It turns out that it is typically
possible to use Markov chain Monte Carlo to form point estimates for $m_y(h)$
and $I_g(h)$ for each individual $h$ in a continuum, and also confidence
intervals for $m_y(h)$ and $I_g(h)$ that are valid pointwise. However, we are
interested in forming estimates, with confidence statements, of the entire
families of integrals $\{ m_y(h), \, h \in \mathcal{H} \}$ and $\{ I_g(h), \, h
\in \mathcal{H} \}$: we need estimates of the first family in order to carry
out empirical Bayes inference, and we need estimates of the second family in
order to do Bayesian sensitivity analysis. We establish strong consistency and
functional central limit theorems for estimates of these families by using
tools from empirical process theory. We give two applications, one to Latent
Dirichlet Allocation, which is used in topic modelling, and the other is to a
model for Bayesian variable selection in linear regression.
</summary>
    <author>
      <name>Hani Doss</name>
    </author>
    <author>
      <name>Yeonhee Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 3 figures. To appear in The Annals of Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15 (Primary), 62F12 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04503v2</id>
    <updated>2018-07-05T19:44:25Z</updated>
    <published>2018-06-08T06:36:34Z</published>
    <title>Tutorial: Maximum likelihood estimation in the context of an optical
  measurement</title>
    <summary>  The method of maximum likelihood estimation (MLE) is a widely used
statistical approach for estimating the values of one or more unknown
parameters of a probabilistic model based on observed data. In this tutorial, I
briefly review the mathematical foundations of MLE, then reformulate the
problem for the measurement of a spatially-varying optical intensity
distribution. In this context, the detection of each individual photon is
treated as a random event, the outcome being the photon's location. A typical
measurement consists of many detected photons, which accumulate to form a
spatial intensity profile. Here, I show a straightforward derivation for the
likelihood function and Fisher information matrix (FIM) associated with a
measurement of multiple photons incident on a detector comprised of a discrete
array of pixels. An estimate for the parameter(s) of interest may then be
obtained by maximizing the likelihood function, while the FIM determines the
uncertainty of the estimate. To illustrate these concepts, several simple
examples are presented for the one- and two-parameter cases, revealing many
interesting properties of the MLE formalism, as well as some practical
considerations for optical experiments. Throughout these examples, connections
are also drawn to optical applications of quantum weak measurements, including
off-null ellipsometry and scatterometry.
</summary>
    <author>
      <name>Anthony Vella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">82 pages, 52 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04503v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04503v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02161v1</id>
    <updated>2018-07-05T19:33:23Z</updated>
    <published>2018-07-05T19:33:23Z</published>
    <title>Minimizing Sensitivity to Model Misspecification</title>
    <summary>  We propose a framework to improve the predictions based on an economic model,
and the estimates of the model parameters, when the model may be misspecified.
We rely on a local asymptotic approach where the degree of misspecification is
indexed by the sample size. We derive formulas to construct estimators whose
mean squared error is minimax in a neighborhood of the reference model, based
on simple one-step adjustments. We construct confidence intervals that contain
the true parameter under both correct specification and local misspecification.
We calibrate the degree of misspecification using a model detection error
approach, which allows us to perform systematic sensitivity analysis in both
point-identified and partially-identified settings. To illustrate our approach
we study panel data models where the distribution of individual effects may be
misspecified and the number of time periods is small, and we revisit the
structural evaluation of a conditional cash transfer program in Mexico.
</summary>
    <author>
      <name>Stéphane Bonhomme</name>
    </author>
    <author>
      <name>Martin Weidner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages main text, 21 pages appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05589v3</id>
    <updated>2018-07-05T19:00:46Z</updated>
    <published>2016-04-19T14:26:29Z</published>
    <title>Coupling couples with copulas: analysis of assortative matching on risk
  attitude</title>
    <summary>  We investigate patterns of assortative matching on risk attitude, using
self-reported (ordinal) data on risk attitudes for males and females within
married couples, from the German Socio-Economic Panel over the period
2004-2012. We apply a novel copula-based bivariate panel ordinal model.
Estimation is in two steps: firstly, a copula-based Markov model is used to
relate the marginal distribution of the response in different time periods,
separately for males and females; secondly, another copula is used to couple
the males' and females' conditional (on the past) distributions. We find
positive dependence, both in the middle of the distribution, and in the joint
tails, and we interpret this as positive assortative matching (PAM). Hence we
reject standard assortative matching theories based on risk-sharing
assumptions, and favour models based on alternative assumptions such as the
ability of agents to control income risk. We also find evidence of
"assimilation"; that is, PAM appearing to increase with years of marriage.
</summary>
    <author>
      <name>Aristidis K. Nikoloulopoulos</name>
    </author>
    <author>
      <name>Peter G. Moffatt</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05589v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05589v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07618v5</id>
    <updated>2018-07-05T16:12:33Z</updated>
    <published>2016-08-26T22:14:37Z</published>
    <title>Multiresolution network models</title>
    <summary>  Many existing statistical and machine learning tools for social network
analysis focus on a single level of analysis. Methods designed for clustering
optimize a global partition of the graph, whereas projection based approaches
(e.g. the latent space model in the statistics literature) represent in rich
detail the roles of individuals. Many pertinent questions in sociology and
economics, however, span multiple scales of analysis. Further, many questions
involve comparisons across disconnected graphs that will, inevitably be of
different sizes, either due to missing data or the inherent heterogeneity in
real-world networks. We propose a class of network models that represent
network structure on multiple scales and facilitate comparison across graphs
with different numbers of individuals. These models differentially invest
modeling effort within subgraphs of high density, often termed communities,
while maintaining a parsimonious structure between said subgraphs. We show that
our model class is projective, highlighting an ongoing discussion in the social
network modeling literature on the dependence of inference paradigms on the
size of the observed graph. We illustrate the utility of our method using data
on household relations from Karnataka, India.
</summary>
    <author>
      <name>Bailey K. Fosdick</name>
    </author>
    <author>
      <name>Tyler H. McCormick</name>
    </author>
    <author>
      <name>Thomas Brendan Murphy</name>
    </author>
    <author>
      <name>Tin Lok James Ng</name>
    </author>
    <author>
      <name>Ted Westling</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07618v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07618v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01987v1</id>
    <updated>2018-07-05T13:26:28Z</updated>
    <published>2018-07-05T13:26:28Z</published>
    <title>Model-based Clustering</title>
    <summary>  Mixture models extend the toolbox of clustering methods available to the data
analyst. They allow for an explicit definition of the cluster shapes and
structure within a probabilistic framework and exploit estimation and inference
techniques available for statistical models in general. In this chapter an
introduction to cluster analysis is provided, model-based clustering is related
to standard heuristic clustering methods and an overview on different ways to
specify the cluster model is given. Post-processing methods to determine a
suitable clustering, infer cluster distribution characteristics and validate
the cluster solution are discussed. The versatility of the model-based
clustering approach is illustrated by giving an overview on the different areas
of applications.
</summary>
    <author>
      <name>Bettina Grün</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a preprint of a chapter forthcoming in Handbook of Mixture
  Analysis, edited by Gilles Celeux, Sylvia Fr\"uhwirth-Schnatter, and
  Christian P. Robert</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05042v3</id>
    <updated>2018-07-05T11:16:44Z</updated>
    <published>2016-07-18T12:29:20Z</published>
    <title>An empirical comparison of global and local functional depths</title>
    <summary>  A functional data depth provides a center-outward ordering criterion which
allows the definition of measures such as median, trimmed means, central
regions or ranks in a functional framework. A functional data depth can be
global or local. With global depths, the degree of centrality of a curve $x$
depends equally on the rest of the sample observations, while with local
depths, the contribution of each observation in defining the degree of
centrality of $x$ decreases as the distance from $x$ increases. We empirically
compare the global and the local approaches to the functional depth problem
focusing on three global and two local functional depths. First, we consider
two real data sets and show that global and local depths may provide different
insights. Second, we use simulated data to show when we should expect
differences between a global and a local approach to the functional depth
problem.
</summary>
    <author>
      <name>Carlo Sguera</name>
    </author>
    <author>
      <name>Rosa E. Lillo</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05042v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05042v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00038v3</id>
    <updated>2018-07-04T18:20:37Z</updated>
    <published>2017-11-30T19:21:47Z</published>
    <title>Augmented Minimax Linear Estimation</title>
    <summary>  Many statistical estimands can expressed as continuous linear functionals of
a conditional expectation function. This includes the average treatment effect
under unconfoundedness and generalizations for continuous-valued and
personalized treatments. In this paper, we discuss a general approach to
estimating such quantities: we begin with a simple plug-in estimator based on
an estimate of the conditional expectation function, and then correct the
plug-in estimator by subtracting a minimax linear estimate of its error. We
show that our method is semiparametrically efficient under weak conditions and
observe promising performance on both real and simulated data.
</summary>
    <author>
      <name>David A. Hirshberg</name>
    </author>
    <author>
      <name>Stefan Wager</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00038v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00038v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02566v3</id>
    <updated>2018-07-04T15:26:20Z</updated>
    <published>2016-07-09T04:04:07Z</published>
    <title>Robust causal inference with continuous instruments using the local
  instrumental variable curve</title>
    <summary>  Instrumental variables are commonly used to estimate effects of a treatment
afflicted by unmeasured confounding, and in practice instruments are often
continuous (e.g., measures of distance, or treatment preference). However,
available methods for continuous instruments have important limitations: they
either require restrictive parametric assumptions for identification, or else
rely on modeling both the outcome and treatment process well (and require
modeling effect modification by all adjustment covariates). In this work we
develop the first semiparametric doubly robust estimators of the local
instrumental variable effect curve, i.e., the effect among those who would take
treatment for instrument values above some threshold and not below. In addition
to being robust to misspecification of either the instrument or
treatment/outcome processes, our approach also incorporates information about
the instrument mechanism and allows for flexible data-adaptive estimation of
effect modification. We discuss asymptotic properties under weak conditions,
and use the methods to study infant mortality effects of neonatal intensive
care units with high versus low technical capacity, using travel time as an
instrument.
</summary>
    <author>
      <name>Edward H. Kennedy</name>
    </author>
    <author>
      <name>Scott A. Lorch</name>
    </author>
    <author>
      <name>Dylan S. Small</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02566v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02566v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05916v2</id>
    <updated>2018-07-04T15:11:07Z</updated>
    <published>2017-07-19T01:55:14Z</published>
    <title>Multiple Imputation of Missing Values in Household Data with Structural
  Zeros</title>
    <summary>  We present an approach for imputation of missing items in multivariate
categorical data nested within households. The approach relies on a latent
class model that (i) allows for household level and individual level variables,
(ii) ensures that impossible household configurations have zero probability in
the model, and (iii) can preserve multivariate distributions both within
households and across households. We present a Gibbs sampler for estimating the
model and generating imputations. We also describe strategies for improving the
computational efficiency of the model estimation. We illustrate the performance
of the approach with data that mimic the variables collected in typical
population censuses.
</summary>
    <author>
      <name>Olanrewaju Akande</name>
    </author>
    <author>
      <name>Jerome Reiter</name>
    </author>
    <author>
      <name>Andrés F. Barrientos</name>
    </author>
    <link href="http://arxiv.org/abs/1707.05916v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05916v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04715v5</id>
    <updated>2018-07-03T20:18:56Z</updated>
    <published>2018-06-12T18:55:19Z</published>
    <title>CID Models on Real-world Social Networks and Goodness of Fit
  Measurements</title>
    <summary>  Assessing the model fit quality of statistical models for network data is an
ongoing and under-examined topic in statistical network analysis. Traditional
metrics for evaluating model fit on tabular data such as the Bayesian
Information Criterion are not suitable for models specialized for network data.
We propose a novel self-developed goodness of fit (GOF) measure, the
`stratified-sampling cross-validation' (SCV) metric, that uses a procedure
similar to traditional cross-validation via stratified-sampling to select dyads
in the network's adjacency matrix to be removed. SCV is capable of intuitively
expressing different models' ability to predict on missing dyads. Using SCV on
real-world social networks, we identify the appropriate statistical models for
different network structures and generalize such patterns. In particular, we
focus on conditionally independent dyad (CID) models such as the Erdos Renyi
model, the stochastic block model, the sender-receiver model, and the latent
space model.
</summary>
    <author>
      <name>Jun Hee Kim</name>
    </author>
    <author>
      <name>Eun Kyung Kwon</name>
    </author>
    <author>
      <name>Qian Sha</name>
    </author>
    <author>
      <name>Brian Junker</name>
    </author>
    <author>
      <name>Tracy Sweet</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04715v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04715v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01346v1</id>
    <updated>2018-07-03T19:02:32Z</updated>
    <published>2018-07-03T19:02:32Z</published>
    <title>Finite Sample $L_2$ Bounds for Sequential Monte Carlo and Adaptive Path
  Selection</title>
    <summary>  We prove a bound on the finite sample error of sequential Monte Carlo (SMC)
on static spaces using the $L_2$ distance between interpolating distributions
and the mixing times of Markov kernels. This result is unique in that it is the
first finite sample convergence result for SMC that does not require an upper
bound on the importance weights. Using this bound we show that careful
selection of the interpolating distributions can lead to substantial
improvements in the computational complexity of the algorithm. This result also
justifies the adaptive selection of SMC distributions using the relative
effective sample size commonly used in the literature and we establish
conditions guaranteeing the approximation accuracy of the adaptive SMC
approach. We then demonstrate empirically that this procedure provides
nearly-optimal sequences of distributions in an automatic fashion for realistic
examples.
</summary>
    <author>
      <name>Joseph Marion</name>
    </author>
    <author>
      <name>Scott C. Schmidler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J22, 65C40, 65C60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03627v2</id>
    <updated>2018-07-03T18:21:27Z</updated>
    <published>2018-03-09T18:27:22Z</published>
    <title>A local depth measure for general data</title>
    <summary>  We herein introduce a general local depth measure for data in a Banach space,
based on the use of one-dimensional projections. Theoretical properties of the
local depth measure are studied, as well as, strong consistency results of the
local depth measure and also of the local depth regions. In addition, we
propose a clustering procedure based on local depths. Applications of the
clustering procedure are illustrated on some artificial and real data sets for
multivariate, functional and multifunctional data, obtaining very promising
results.
</summary>
    <author>
      <name>Lucas Fernandez-Piana</name>
    </author>
    <author>
      <name>Marcela Svarc</name>
    </author>
    <link href="http://arxiv.org/abs/1803.03627v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03627v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01239v1</id>
    <updated>2018-07-03T15:35:52Z</updated>
    <published>2018-07-03T15:35:52Z</published>
    <title>Bayesian Spatial Analysis of Hardwood Tree Counts in Forests via MCMC</title>
    <summary>  In this paper, we perform Bayesian Inference to analyze spatial tree count
data from the Timiskaming and Abitibi River forests in Ontario, Canada. We
consider a Bayesian Generalized Linear Geostatistical Model and implement a
Markov Chain Monte Carlo algorithm to sample from its posterior distribution.
How spatial predictions for new sites in the forests change as the amount of
training data is reduced is studied and compared with a Logistic Regression
model without a spatial effect. Finally, we discuss a stratified sampling
approach for selecting subsets of data that allows for potential better
predictions.
</summary>
    <author>
      <name>Reihaneh Entezari</name>
    </author>
    <author>
      <name>Patrick E. Brown</name>
    </author>
    <author>
      <name>Jeffrey S. Rosenthal</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01152v1</id>
    <updated>2018-07-03T13:21:05Z</updated>
    <published>2018-07-03T13:21:05Z</published>
    <title>Probability Based Independence Sampler for Bayesian Quantitative
  Learning in Graphical Log-Linear Marginal Models</title>
    <summary>  Bayesian methods for graphical log-linear marginal models have not been
developed in the same extent as traditional frequentist approaches. In this
work, we introduce a novel Bayesian approach for quantitative learning for such
models. These models belong to curved exponential families that are difficult
to handle from a Bayesian perspective. Furthermore, the likelihood cannot be
analytically expressed as a function of the marginal log-linear interactions,
but only in terms of cell counts or probabilities.
  Posterior distributions cannot be directly obtained, and MCMC methods are
needed. Finally, a well-defined model requires parameter values that lead to
compatible marginal probabilities. Hence, any MCMC should account for this
important restriction. We construct a fully automatic and efficient MCMC
strategy for quantitative learning for graphical log-linear marginal models
that handles these problems. While the prior is expressed in terms of the
marginal log-linear interactions, we build an MCMC algorithm that employs a
proposal on the probability parameter space. The corresponding proposal on the
marginal log-linear interactions is obtained via parameter transformation.
  By this strategy, we achieve to move within the desired target space. At each
step, we directly work with well-defined probability distributions.
  Moreover, we can exploit a conditional conjugate setup to build an efficient
proposal on probability parameters. The proposed methodology is illustrated by
a simulation study and a real dataset.
</summary>
    <author>
      <name>Ioannis Ntzoufras</name>
    </author>
    <author>
      <name>Claudia Tarantola</name>
    </author>
    <author>
      <name>Monia Lupparelli</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01133v1</id>
    <updated>2018-07-03T12:57:56Z</updated>
    <published>2018-07-03T12:57:56Z</published>
    <title>Time Series Modeling on Dynamic Networks</title>
    <summary>  We consider multivariate time series on dynamic networks with a fixed number
of vertices. Each component of the time series is assigned to a vertex of the
underlying network. The dependency of the various components of the time series
is modeled dynamically by means of the edges. We make use of a multivariate
doubly stochastic time series framework, that is we assume linear processes for
which the coefficient matrices are stochastic processes themselves. We
explicitly allow for dependence in the dynamics of the coefficient matrices,
including of course an i.i.d. structure as is typically assumed in random
coefficients models. Autoregressive moving average models are defined in this
framework and stationarity conditions are discussed for network autoregressive
models. Estimators of the parameters are discussed for various
parameterizations of such network autoregressive models and how this can be
used to forecast such a process. The finite sample behavior of the forecast
approach is investigated and a real data example is presented.
</summary>
    <author>
      <name>Jonas Krampe</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01111v1</id>
    <updated>2018-07-03T12:19:34Z</updated>
    <published>2018-07-03T12:19:34Z</published>
    <title>The inverse xgamma distribution: statistical properties and different
  methods of estimation</title>
    <summary>  This paper proposed a new probability distribution named as inverse xgamma
distribution (IXGD). Different mathematical and statistical properties,viz.,
reliability characteristics, moments, inverse moments, stochastic ordering and
order statistics of the proposed distribution have been derived and discussed.
The estimation of the parameter of IXGD has been approached by different
methods of estimation, namely, maximum likelihood method of estimation (MLE),
Least square method of estimation (LSE), Weighted least square method of
estimation (WLSE), Cram'er-von-Mises method of estimation (CME) and maximum
product spacing method of estimation (MPSE). Asymptotic confidence interval
(ACI) of the parameter is also obtained. A simulation study has been carried
out to compare the performance of the obtained estimators and corresponding ACI
in terms of average widths and corresponding coverage probabilities. Finally,
two real data sets have been used to demonstrate the applicability of IXGD in
real life situations.
</summary>
    <author>
      <name>Abhimanyu Singh Yadav</name>
    </author>
    <author>
      <name>Sudhansu S. Maiti</name>
    </author>
    <author>
      <name>Mahendra Saha</name>
    </author>
    <author>
      <name>Arvind Pandey</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62BF, 62C10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01585v1</id>
    <updated>2018-07-03T07:19:40Z</updated>
    <published>2018-07-03T07:19:40Z</published>
    <title>cvBMS and cvBMA: filling in the gaps</title>
    <summary>  With this technical report, we provide mathematical and implementational
details of cross-validated Bayesian model selection (cvBMS) and averaging
(cvBMA) that could not be communicated in the corresponding peer-reviewed
journal articles. This will allow statisticians and developers to comprehend
internal functionalities of cvBMS and cvBMA for further development of these
techniques.
</summary>
    <author>
      <name>Joram Soch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00816v3</id>
    <updated>2018-07-03T03:08:16Z</updated>
    <published>2018-01-02T20:13:35Z</published>
    <title>Relaxed covariate overlap and margin-based causal effect estimation</title>
    <summary>  In most nonrandomized observational studies, differences between treatment
groups may arise not only due to the treatment but also because of the effect
of confounders. Therefore, causal inference regarding the treatment effect is
not as straightforward as in a randomized trial. To adjust for confounding due
to measured covariates, a variety of methods based on the potential outcomes
framework are used to estimate average treatment effects. One of the key
assumptions is treatment positivity, which states that the probability of
treatment is bounded away from zero and one for any possible combination of the
confounders. Methods for performing causal inference when this assumption is
violated are relatively limited. In this article, we discuss a new
balance-related condition involving the convex hulls of treatment groups, which
I term relaxed covariate overlap. An advantage of this concept is that it can
be linked to a concept from machine learning, termed the margin. Introduction
of relaxed covariate overlap leads to an approach in which one can perform
causal inference in a three-step manner. The methodology is illustrated with
two examples.
</summary>
    <author>
      <name>Debashis Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00816v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00816v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00943v1</id>
    <updated>2018-07-03T01:16:42Z</updated>
    <published>2018-07-03T01:16:42Z</published>
    <title>Segmented correspondence curve regression model for quantifying
  reproducibility of high-throughput experiments</title>
    <summary>  The reliability of a high-throughput biological experiment relies highly on
the settings of the operational factors in its experimental and data-analytic
procedures. Understanding how operational factors influence the reproducibility
of the experimental outcome is critical for constructing robust workflows and
obtaining reliable results. One challenge in this area is that candidates at
different levels of significance may respond to the operational factors
differently. To model this heterogeneity, we develop a novel segmented
regression model, based on the rank concordance between candidates from
different replicate samples, to characterize the varying effects of operational
factors for candidates at different levels of significance. A grid search
method is developed to identify the change point in response to the operational
factors and estimate the covariate effects accounting for the change. A
sup-likelihood-ratio-type test is proposed to test the existence of a change
point. Simulation studies show that our method yields a well-calibrated type I
error, is powerful in detecting the difference in reproducibility, and achieves
a better model fitting than the existing method. An application on a ChIP-seq
dataset reveals interesting insights on how sequencing depth affects the
reproducibility of experimental results, demonstrating the usefulness of our
method in designing cost-effective and reliable high-throughput workflows.
</summary>
    <author>
      <name>Feipeng Zhang</name>
    </author>
    <author>
      <name>Frank Shen</name>
    </author>
    <author>
      <name>Tao Yang</name>
    </author>
    <author>
      <name>Qunhua Li</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00931v1</id>
    <updated>2018-07-03T00:07:03Z</updated>
    <published>2018-07-03T00:07:03Z</published>
    <title>Controlling the False Discovery Rate via Knockoff for High Dimensional
  Ising Model Variable Selection</title>
    <summary>  In high dimensional data analysis, it is important to effectively control the
fraction of false discoveries and ensure large enough power for variable
selection. In a lot of contemporary data applications, a large set of
covariates are discrete variables. In this paper we propose Ising knockoff
(IKF) for variable selection in high dimensional regression with discrete
covariates. Under mild conditions, we show that the false discovery rate (FDR)
is controlled under a target level in a finite sample if the underlying Ising
model is known, and the FDR is asymptotically controlled even when the
parameters of the Ising model is estimated. We also provide theoretical results
on the power for our proposed method. Using simulations and a genome science
data set, we show that IKF has higher power than existing knockoff procedures
mostly tailored for continuous covariate distributions.
</summary>
    <author>
      <name>Yuxiang Xie</name>
    </author>
    <author>
      <name>Kwun Chuen Gary Chan</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06303v4</id>
    <updated>2018-07-02T20:07:39Z</updated>
    <published>2016-12-19T18:39:21Z</published>
    <title>Remote effects spatial process models for modeling teleconnections</title>
    <summary>  While most spatial data can be modeled with the assumption that distant
points are uncorrelated, some problems require dependence at both far and short
distances. We introduce a model to directly incorporate dependence in phenomena
that influence a distant response. Spatial climate problems often have such
modeling needs as data are influenced by local factors in addition to remote
phenomena, known as teleconnections. Teleconnections arise from complex
interactions between the atmosphere and ocean, of which the El Nino--Southern
Oscillation teleconnection is a well-known example. Our model extends the
standard geostatistical modeling framework to account for effects of covariates
observed on a spatially remote domain. We frame our model as an extension of
spatially varying coefficient models. Connections to existing methods are
highlighted and further modeling needs are addressed by additionally drawing on
spatial basis functions and predictive processes. Notably, our approach allows
users to model teleconnected data without pre-specifying teleconnection
indices, which other methods often require. We adopt a hierarchical Bayesian
framework to conduct inference and make predictions. The method is demonstrated
by predicting precipitation in Colorado while accounting for local factors and
teleconnection effects with Pacific Ocean sea surface temperatures. We show how
the proposed model improves upon standard methods for estimating teleconnection
effects and discuss its utility for climate applications.
</summary>
    <author>
      <name>Joshua Hewitt</name>
    </author>
    <author>
      <name>Jennifer A. Hoeting</name>
    </author>
    <author>
      <name>James Done</name>
    </author>
    <author>
      <name>Erin Towler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06303v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06303v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06945v1</id>
    <updated>2018-07-02T14:56:37Z</updated>
    <published>2018-07-02T14:56:37Z</published>
    <title>Cyclostationary Statistical Models and Algorithms for Anomaly Detection
  Using Multi-Modal Data</title>
    <summary>  A framework is proposed to detect anomalies in multi-modal data. A deep
neural network-based object detector is employed to extract counts of objects
and sub-events from the data. A cyclostationary model is proposed to model
regular patterns of behavior in the count sequences. The anomaly detection
problem is formulated as a problem of detecting deviations from learned
cyclostationary behavior. Sequential algorithms are proposed to detect
anomalies using the proposed model. The proposed algorithms are shown to be
asymptotically efficient in a well-defined sense. The developed algorithms are
applied to a multi-modal data consisting of CCTV imagery and social media posts
to detect a 5K run in New York City.
</summary>
    <author>
      <name>Taposh Banerjee</name>
    </author>
    <author>
      <name>Gene Whipps</name>
    </author>
    <author>
      <name>Prudhvi Gurram</name>
    </author>
    <author>
      <name>Vahid Tarokh</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06660v4</id>
    <updated>2018-07-02T14:06:24Z</updated>
    <published>2017-10-18T10:13:51Z</published>
    <title>Variable selection for the prediction of C[0,1]-valued AR processes
  using RKHS</title>
    <summary>  A model for the prediction of functional time series is introduced, where
observations are assumed to be continuous random functions. We model the
dependence of the data with a nonstandard autoregressive structure, motivated
in terms of the Reproducing Kernel Hilbert Space (RKHS) generated by the
auto-covariance function of the data. The new approach helps to find relevant
points of the curves in terms of prediction accuracy. This dimension reduction
technique is particularly useful for applications, since the results are
usually directly interpretable in terms of the original curves. An empirical
study involving real and simulated data is included, which generates
competitive results. Supplementary material includes R-Code, tables and
mathematical comments.
</summary>
    <author>
      <name>Beatriz Bueno-Larraz</name>
    </author>
    <author>
      <name>Johannes Klepsch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.06660v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06660v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6950v5</id>
    <updated>2018-07-02T13:25:52Z</updated>
    <published>2013-04-25T16:06:00Z</published>
    <title>Inference on a Distribution Function from Ranked Set Samples</title>
    <summary>  Consider independent observations $(X_1,R_1)$, $(X_2,R_2)$, \ldots,
$(X_n,R_n)$ with random or fixed ranks $R_i \in \{1,2,\ldots,k\}$, while
conditional on $R_i = r$, the random variable $X_i$ has the same distribution
as the $r$-th order statistic within a random sample of size $k$ from an
unknown continuous distribution function $F$. Such observation schemes are
utilized in situations in which ranking observations is much easier than
obtaining their precise values. Two well-known special cases are ranked set
sampling (McIntyre 1952) and judgement post-stratification (MacEachern et al.
2004).
  Within a general setting including unbalanced ranked set sampling we derive
and compare the asymptotic distributions of three different estimators of the
distribution function $F$ as $n \to \infty$ with fixed $k$: The stratified
estimator of Stokes and Sager (1988), the nonparametric maximum-likelihood
estimator of Kvam and Samaniego (1994) and a moment-based estimator of Chen
(2001). Our functional central limit theorems generalize and refine previous
asymptotic analyses. In addition we discuss briefly pointwise and simultaneous
confidence intervals for the distribution function $F$ with guaranteed coverage
probability for finite sample sizes.
  The methods are illustrated with a real data example, and the potential
impact of imperfect rankings is investigated in a small simulation experiment.
All in all, the moment-based estimator seems to offer a good compromise between
efficiency and robustness versus imperfect ranking, in addition to
computational efficiency.
</summary>
    <author>
      <name>Lutz Duembgen</name>
    </author>
    <author>
      <name>Ehsan Zamanzade</name>
    </author>
    <link href="http://arxiv.org/abs/1304.6950v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6950v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05, 62G15, 62G20, 62G30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07716v2</id>
    <updated>2018-07-02T12:11:43Z</updated>
    <published>2017-09-22T12:36:43Z</published>
    <title>Testing first-order intensity model in non-homogeneous Poisson point
  processes with covariates</title>
    <summary>  Modelling the first-order intensity function is one of the main aims in point
process theory, and it has been approached so far from different perspectives.
One appealing model describes the intensity as a function of a spatial
covariate. In the recent literature, estimation theory and several applications
have been developed assuming this model, but without formally checking this
assumption. In this paper we address this problem for a non-homogeneous Poisson
point process, by proposing a new test based on an $L^2$-distance. We also
prove the asymptotic normality of the statistic and we suggest a bootstrap
procedure to accomplish the calibration. Two applications with real data are
presented and a simulation study to better understand the performance of our
proposals is accomplished. Finally some possible extensions of the present work
to non-Poisson processes and to a multi-dimensional covariate context are
detailed.
</summary>
    <author>
      <name>M. I. Borrajo</name>
    </author>
    <author>
      <name>W. González-Manteiga</name>
    </author>
    <author>
      <name>M. D. Martínez-Miranda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages (23 main doc + 7 appendix); 9 figures; 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07716v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07716v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G10, 62G20, 60G55, 60-02" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00615v1</id>
    <updated>2018-07-02T12:07:47Z</updated>
    <published>2018-07-02T12:07:47Z</published>
    <title>A new decision theoretic sampling plan for type-I and type-I hybrid
  censored samples from the exponential distribution</title>
    <summary>  The study proposes a new decision theoretic sampling plan (DSP) for Type-I
and Type-I hybrid censored samples when the lifetimes of individual items are
exponentially distributed with a scale parameter. The DSP is based on an
estimator of the scale parameter which always exists, unlike the MLE which may
not always exist. Using a quadratic loss function and a decision function based
on the proposed estimator, a DSP is derived. To obtain the optimum DSP, a
finite algorithm is used. Numerical results demonstrate that in terms of the
Bayes risk, the optimum DSP is as good as the Bayesian sampling plan (BSP)
proposed by \cite{lin2002bayesian} and \cite{liang2013optimal}. The proposed
DSP performs better than the sampling plan of \cite{Lam1994bayesian} and
\cite{lin2008-10exact} in terms of Bayes risks. The main advantage of the
proposed DSP is that for higher degree polynomial and non-polynomial loss
functions, it can be easily obtained as compared to the BSP.
</summary>
    <author>
      <name>Deepak Prajapati</name>
    </author>
    <author>
      <name>Sharmistha Mitra</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08805v2</id>
    <updated>2018-07-02T10:48:55Z</updated>
    <published>2016-10-27T14:38:47Z</published>
    <title>Estimation of the Volume Under the ROC Surface in Presence of
  Nonignorable Verification Bias</title>
    <summary>  The volume under the receiver operating characteristic surface (VUS) is
useful for measuring the overall accuracy of a diagnostic test when the
possible disease status belongs to one of three ordered categories. In medical
studies, the VUS of a new test is typically estimated through a sample of
measurements obtained by some suitable sample of patients. However, in many
cases, only a subset of such patients has the true disease status assessed by a
gold standard test. In this paper, for a continuous-scale diagnostic test, we
propose four estimators of the VUS which accommodate for nonignorable
missingness of the disease status. The estimators are based on a parametric
model which jointly describes both the disease and the verification process.
Identifiability of the model is discussed. Consistency and asymptotic normality
of the proposed estimators are shown, and variance estimation is discussed. The
finite-sample behavior is investigated by means of simulation experiments. An
illustration is provided.
</summary>
    <author>
      <name>Khanh To Duc</name>
    </author>
    <author>
      <name>Monica Chiogna</name>
    </author>
    <author>
      <name>Gianfranco Adimari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.08805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P10, 62C99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02009v3</id>
    <updated>2018-07-02T07:57:37Z</updated>
    <published>2017-02-07T13:45:35Z</published>
    <title>Quadratic regression for functional response models</title>
    <summary>  We consider the problem of constructing a regression model with a functional
predictor and a functional response. We extend the functional linear model to
the quadratic model, where the quadratic term also takes the interaction
between the argument of the functional data into consideration. We assume that
the predictor and the coefficient functions are expressed by basis expansions,
and then parameters included in the model are estimated by the penalized
likelihood method assuming that the error function follows a Gaussian process.
Monte Carlo simulations are conducted to illustrate the efficacy of the
proposed method. Finally, we apply the proposed method to the analysis of
meteorological data and explore the results.
</summary>
    <author>
      <name>Hidetoshi Matsui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02009v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02009v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J02" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06131v4</id>
    <updated>2018-07-01T23:28:16Z</updated>
    <published>2017-03-17T17:50:44Z</published>
    <title>Inference via low-dimensional couplings</title>
    <summary>  We investigate the low-dimensional structure of deterministic transformations
between random variables, i.e., transport maps between probability measures. In
the context of statistics and machine learning, these transformations can be
used to couple a tractable "reference" measure (e.g., a standard Gaussian) with
a target measure of interest. Direct simulation from the desired measure can
then be achieved by pushing forward reference samples through the map. Yet
characterizing such a map---e.g., representing and evaluating it---grows
challenging in high dimensions. The central contribution of this paper is to
establish a link between the Markov properties of the target measure and the
existence of low-dimensional couplings, induced by transport maps that are
sparse and/or decomposable. Our analysis not only facilitates the construction
of transformations in high-dimensional settings, but also suggests new
inference methodologies for continuous non-Gaussian graphical models. For
instance, in the context of nonlinear state-space models, we describe new
variational algorithms for filtering, smoothing, and sequential parameter
inference. These algorithms can be understood as the natural
generalization---to the non-Gaussian case---of the square-root
Rauch-Tung-Striebel Gaussian smoother.
</summary>
    <author>
      <name>Alessio Spantini</name>
    </author>
    <author>
      <name>Daniele Bigoni</name>
    </author>
    <author>
      <name>Youssef Marzouk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">78 pages, 25 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06131v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06131v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10210v3</id>
    <updated>2018-07-01T19:22:59Z</updated>
    <published>2017-03-29T19:36:07Z</published>
    <title>A test of weak separability for multi-way functional data, with
  application to brain connectivity studies</title>
    <summary>  This paper concerns the modeling of multi-way functional data where double or
multiple indices are involved. We introduce a concept of weak separability. The
weakly separable structure supports the use of factorization methods that
decompose the signal into its spatial and temporal components. The analysis
reveals interesting connections to the usual strongly separable covariance
structure, and provides insights into tensor methods for multi-way functional
data. We propose a formal test for the weak separability hypothesis, where the
asymptotic null distribution of the test statistic is a chi-square type
mixture. The method is applied to study brain functional connectivity derived
from source localized magnetoencephalography signals during motor tasks.
</summary>
    <author>
      <name>Brian Lynch</name>
    </author>
    <author>
      <name>Kehui Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10210v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10210v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00365v1</id>
    <updated>2018-07-01T18:19:42Z</updated>
    <published>2018-07-01T18:19:42Z</published>
    <title>Calculation of sample size guaranteeing the required width of the
  empirical confidence interval with predefined probability</title>
    <summary>  The goal of any estimation study is an interval estimation of a the
parameter(s) of interest. These estimations are mostly expressed using
empirical confidence intervals that are based on sample point estimates of the
corresponding parameter(s). In contrast, calculations of the necessary sample
size usually use expected confidence intervals that are based on the expected
value of the parameter(s). The approach that guarantees the required
probability of the required width of empirical confidence interval is known at
least since 1989. However, till now, this approach is not implemented for most
software and is not even described in many modern papers and textbooks. Here we
present the concise description of the approach to sample size calculation for
obtaining empirical confidence interval of the required width with the
predefined probability and give a framework of its general implementation. We
illustrate the approach in Normal, Poisson, and Binomial distributions. The
numeric results showed that the sample size necessary to obtain the required
width of empirical confidence interval with the standard probability of $0.8$
or $0.9$ may be more than 20\% larger than the sample size calculated for the
expected values of the parameters.
</summary>
    <author>
      <name>Ilya Novikov</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00359v1</id>
    <updated>2018-07-01T17:18:27Z</updated>
    <published>2018-07-01T17:18:27Z</published>
    <title>On null hypotheses in survival analysis</title>
    <summary>  The conventional nonparametric tests in survival analysis, such as the
log-rank test, assess the null hypothesis that the hazards are equal at all
times. However, hazards are hard to interpret causally, and other null
hypotheses are more relevant in many scenarios with survival outcomes. To allow
for a wider range of null hypotheses, we present a generic approach to define
test statistics. This approach utilizes the fact that a wide range of common
parameters in survival analysis can be expressed as solutions of differential
equations. Thereby, we can test hypotheses based on survival parameters that
solve differential equations driven by hazards, and it is easy to implement the
tests on a computer. We present simulations, suggesting that our generic
approach performs well for several hypotheses in a range of scenarios. Finally,
we extend the strategy to to allow for testing conditional on covariates.
</summary>
    <author>
      <name>Mats Julius Stensrud</name>
    </author>
    <author>
      <name>Kjetil Røysland</name>
    </author>
    <author>
      <name>Pål Christie Ryalen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00347v1</id>
    <updated>2018-07-01T15:55:25Z</updated>
    <published>2018-07-01T15:55:25Z</published>
    <title>Robust Inference Under Heteroskedasticity via the Hadamard Estimator</title>
    <summary>  Drawing statistical inferences from large datasets in a model-robust way is
an important problem in statistics and data science. In this paper, we propose
methods that are robust to large and unequal noise in different observational
units (i.e., heteroskedasticity) for statistical inference in linear
regression. We leverage the Hadamard estimator, which is unbiased for the
variances of ordinary least-squares regression. This is in contrast to the
popular White's sandwich estimator, which can be substantially biased in high
dimensions. We propose to estimate the signal strength, noise level,
signal-to-noise ratio, and mean squared error via the Hadamard estimator. We
develop a new degrees of freedom adjustment that gives more accurate confidence
intervals than variants of White's sandwich estimator. Moreover, we provide
conditions ensuring the estimator is well-defined, by studying a new random
matrix ensemble in which the entries of a random orthogonal projection matrix
are squared. We also show approximate normality, using the second-order
Poincare inequality. Our work provides improved statistical theory and methods
for linear regression in high dimensions.
</summary>
    <author>
      <name>Edgar Dobriban</name>
    </author>
    <author>
      <name>Weijie J. Su</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00305v1</id>
    <updated>2018-07-01T10:12:38Z</updated>
    <published>2018-07-01T10:12:38Z</published>
    <title>Bayesian Nonparametrics for Directional Statistics</title>
    <summary>  A density basis of the trigonometric polynomials, suitable for mixture
modelling, is introduced. Statistical and geometric properties are derived,
suggesting it as a circular analogue to the Bernstein polynomial densities.
Nonparametric priors are constructed and strong posterior consistency is
obtained for a wide class of densities. We conclude by comparing posterior mean
estimates to other circular density estimation methods, also based on
trigonometric polynomials, previously suggested in the literature.
</summary>
    <author>
      <name>Olivier Binette</name>
    </author>
    <author>
      <name>Simon Guillotte</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00282v1</id>
    <updated>2018-07-01T07:05:23Z</updated>
    <published>2018-07-01T07:05:23Z</published>
    <title>A horse racing between the block maxima method and the
  peak-over-threshold approach</title>
    <summary>  Classical extreme value statistics consists of two fundamental approaches:
the block maxima (BM) method and the peak-over-threshold (POT) approach. It
seems to be general consensus among researchers in the field that the POT
method makes use of extreme observations more efficiently than the BM method.
We shed light on this discussion from three different perspectives. First,
based on recent theoretical results for the BM approach, we provide a
theoretical comparison in i.i.d.\ scenarios. We argue that the data generating
process may favour either one or the other approach. Second, if the underlying
data possesses serial dependence, we argue that the choice of a method should
be primarily guided by the ultimate statistical interest: for instance, POT is
preferable for quantile estimation, while BM is preferable for return level
estimation. Finally, we discuss the two approaches for multivariate
observations and identify various open ends for future research.
</summary>
    <author>
      <name>Axel Bücher</name>
    </author>
    <author>
      <name>Chen Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00810v1</id>
    <updated>2018-07-01T04:53:59Z</updated>
    <published>2018-07-01T04:53:59Z</published>
    <title>The risk function of the goodness-of-fit tests for tail models</title>
    <summary>  This paper contributes to answering a question that is of crucial importance
in risk management and extreme value theory: How to select the threshold above
which one assumes that the tail of a distribution follows a generalized Pareto
distribution. This question has gained increasing attention, particularly in
finance institutions, as the recent regulative norms require the assessment of
risk at high quantiles. Recent methods answer this question by multiple uses of
the standard goodness-of-fit tests. These tests are based on a particular
choice of symmetric weighting of the mean square error between the empirical
and the fitted tail distributions. Assuming an asymmetric weighting, which
rates high quantiles more than small ones, we propose new goodness-of-fit tests
and automated threshold selection procedures. We consider a parameterized
family of asymmetric weight functions and calculate the corresponding mean
square error as a loss function. We then explicitly determine the risk function
as the finite sample expected value of the loss function. Finally, the risk
function can be used to discuss the question of which symmetric or asymmetric
weight function and, thus, which goodness-of-fit test should be used in a new
method for determining the threshold value.
</summary>
    <author>
      <name>Ingo Hoffmann</name>
    </author>
    <author>
      <name>Christoph J. Börner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 Figure. arXiv admin note: text overlap with arXiv:1805.10040</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C99, 62E17" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.4; I.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05230v3</id>
    <updated>2018-07-01T01:09:51Z</updated>
    <published>2017-01-18T20:51:27Z</published>
    <title>Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index
  Models for Binary Outcomes</title>
    <summary>  We consider the recovery of regression coefficients, denoted by
$\boldsymbol{\beta}_0$, for a single index model (SIM) relating a binary
outcome $Y$ to a set of possibly high dimensional covariates $\boldsymbol{X}$,
based on a large but 'unlabeled' dataset $\mathcal{U}$, with $Y$ never
observed. On $\mathcal{U}$, we fully observe $\boldsymbol{X}$ and additionally,
a surrogate $S$ which, while not being strongly predictive of $Y$ throughout
the entirety of its support, can forecast it with high accuracy when it assumes
extreme values. Such datasets arise naturally in modern studies involving large
databases such as electronic medical records (EMR) where $Y$, unlike
$(\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies,
an example of $Y$ and $S$ would be the true disease phenotype and the count of
the associated diagnostic codes respectively. Assuming another SIM for $S$
given $\boldsymbol{X}$, we show that under sparsity assumptions, we can recover
$\boldsymbol{\beta}_0$ proportionally by simply fitting a least squares LASSO
estimator to the subset of the observed data on $(\boldsymbol{X}, S)$
restricted to the extreme sets of $S$, with $Y$ imputed using the surrogacy of
$S$. We obtain sharp finite sample performance bounds for our estimator,
including deterministic deviation bounds and probabilistic guarantees. We
demonstrate the effectiveness of our approach through multiple simulation
studies, as well as by application to real data from an EMR study conducted at
the Partners HealthCare Systems.
</summary>
    <author>
      <name>Abhishek Chakrabortty</name>
    </author>
    <author>
      <name>Matey Neykov</name>
    </author>
    <author>
      <name>Raymond Carroll</name>
    </author>
    <author>
      <name>Tianxi Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 3 tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05230v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05230v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J12, 62J07, 62H30, 62G32, 62F10, 62F30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03840v3</id>
    <updated>2018-06-30T12:27:25Z</updated>
    <published>2018-02-11T23:22:36Z</published>
    <title>Uncharted Forest a Technique for Exploratory Data Analysis</title>
    <summary>  Exploratory data analysis is crucial for developing and understanding
classification models from high-dimensional datasets. We explore the utility of
a new unsupervised tree ensemble called uncharted forest for visualizing class
associations, sample-sample associations, class heterogeneity, and
uninformative classes for provenance studies. The uncharted forest algorithm
can be used to partition data using random selections of variables and metrics
based on statistical spread. After each tree is grown, a tally of the samples
that arrive at every terminal node is maintained. Those tallies are stored in
single sample association matrix and a likelihood measure for each sample being
partitioned with one another can be made. That matrix may be readily viewed as
a heat map, and the probabilities can be quantified via new metrics that
account for class or cluster membership. We display the advantages and
limitations of using this technique by applying it to two classification
datasets and three provenance study datasets. Two of the metrics presented in
this paper are also compared with widely used metrics from two algorithms that
have variance-based clustering mechanisms.
</summary>
    <author>
      <name>Casey Kneale</name>
    </author>
    <author>
      <name>Steven D. Brown</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.talanta.2018.06.061</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.talanta.2018.06.061" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Talanta. 189, (2018), 71-78</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.03840v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03840v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00447v3</id>
    <updated>2018-06-29T23:21:10Z</updated>
    <published>2016-04-02T01:04:55Z</published>
    <title>Ordering-Free Inference from Locally Dependent Data</title>
    <summary>  This paper focuses on a data-rich environment where the data set has a very
large cross-sectional dimension, is likely to exhibit local dependence, and yet
is hard to determine the dependence ordering. Such a situation arises, for
example, when the data set is collected from the Internet, through a method of
web crawling. This paper proposes an approach of randomized subsampling
inference, where one constructs a test statistic by aggregating many randomized
test statistics using random draws of subsamples, and uses for inference the
conditional distribution of the test statistic given data. This paper explores
two approaches of such inference: one based on an M-type statistic constructed
from randomized mean statistics and the other based on a U-type statistic
constructed from randomized U-statistics. This paper provides conditions for
local dependence, the number of the random draws, and the subsample size, under
which randomized subsampling inference is asymptotically valid. From the Monte
Carlo simulation studies, this paper finds that the randomized subsampling
inference based on the U-type statistics performs better than that based on the
M-type statistics.
</summary>
    <author>
      <name>Kyungchul Song</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00447v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00447v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E20, 62M30, 62M40, 62M86" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11544v1</id>
    <updated>2018-06-29T17:18:28Z</updated>
    <published>2018-06-29T17:18:28Z</published>
    <title>Nonparametric learning from Bayesian models with randomized objective
  functions</title>
    <summary>  Bayesian learning is built on an assumption that the model space contains a
true reflection of the data generating mechanism. This assumption is
problematic, particularly in complex data environments. Here we present a
Bayesian nonparametric approach to learning that makes use of statistical
models, but does not assume that the model is true. Our approach has provably
better properties than using a parametric model and admits a trivially
parallelizable Monte Carlo sampling scheme that affords massive scalability on
modern computer architectures. The model-based aspect of learning is
particularly attractive for regularizing nonparametric inference when the
sample size is small, and also for correcting approximate approaches such as
variational Bayes (VB). We demonstrate the approach on a number of examples
including VB classifiers and Bayesian random forests.
</summary>
    <author>
      <name>S. P. Lyddon</name>
    </author>
    <author>
      <name>S. G. Walker</name>
    </author>
    <author>
      <name>C. C. Holmes</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07423v2</id>
    <updated>2018-06-29T15:34:12Z</updated>
    <published>2018-05-17T16:03:41Z</published>
    <title>Efficient simulation of Gaussian Markov random fields by Chebyshev
  polynomial approximation</title>
    <summary>  This paper presents an algorithm to simulate Gaussian random vectors whose
precision matrix can be expressed as a polynomial of a sparse matrix. This
situation arises in particular when simulating Gaussian Markov random fields
obtained by the discretization by finite elements of the solutions of some
stochastic partial derivative equations. The proposed algorithm uses a
Chebyshev polynomial approximation to compute simulated vectors with a linear
complexity. This method is asymptotically exact as the approximation order
grows. Criteria based on tests of the statistical properties of the produced
vectors are derived to determine minimal orders of approximation.
</summary>
    <author>
      <name>Mike Pereira</name>
    </author>
    <author>
      <name>Nicolas Desassis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07423v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07423v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11392v1</id>
    <updated>2018-06-29T13:03:52Z</updated>
    <published>2018-06-29T13:03:52Z</published>
    <title>Revealing subgroup structure in ranked data using a Bayesian WAND</title>
    <summary>  Ranked data arise in many areas of application ranging from the ranking of
up-regulated genes for cancer to the ranking of academic statistics journals.
Complications can arise when rankers do not report a full ranking of all
entities; for example, they might only report their top--$M$ ranked entities
after seeing some or all entities. It can also be useful to know whether
rankers are equally informative, and whether some entities are effectively
judged to be exchangeable. Recent work has focused on determining an aggregate
(overall) ranking but such summaries can be misleading when there is important
subgroup structure in the data. In this paper we propose a flexible Bayesian
nonparametric model for dealing with heterogeneous structure and ranker
reliability in ranked data. The model is a Weighted Adapted Nested Dirichlet
(WAND) process mixture of Plackett-Luce models and inference proceeds through a
simple and efficient Gibbs sampling scheme for posterior sampling. The richness
of information in the posterior distribution allows us to infer many details of
the structure both between ranker groups and between entity groups (within
ranker groups), in contrast to many other (Bayesian) analyses. The methodology
is illustrated using several simulation studies and two real data examples.
</summary>
    <author>
      <name>Stephen R. Johnson</name>
    </author>
    <author>
      <name>Daniel A. Henderson</name>
    </author>
    <author>
      <name>Richard J. Boys</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11028v3</id>
    <updated>2018-06-29T12:40:08Z</updated>
    <published>2017-10-30T15:58:02Z</published>
    <title>Probabilistic Count Matrix Factorization for Single Cell Expression Data
  Analysis</title>
    <summary>  The development of high throughput single-cell sequencing technologies now
allows the investigation of the population level diversity of cellular
transcriptomes. This diversity has shown two faces. First, the expression
dynamics (gene to gene variability) can be quantified more accurately, thanks
to the measurement of lowly-expressed genes. Second, the cell-to-cell
variability is high, with a low proportion of cells expressing the same gene at
the same time/level. Those emerging patterns appear to be very challenging from
the statistical point of view, especially to represent and to provide a
summarized view of single-cell expression data. PCA is one of the most powerful
framework to provide a suitable representation of high dimensional datasets, by
searching for latent directions catching the most variability in the data.
Unfortunately, classical PCA is based on Euclidean distances and projections
that work poorly in presence of over-dispersed counts that show drop-out events
(zero-inflation) like single-cell expression data We propose a probabilistic
Count Matrix Factorization (pCMF) approach for single-cell expression data
analysis, that relies on a sparse Gamma-Poisson factor model. This hierarchical
model is inferred using a variational EM algorithm. It is able to jointly build
a low dimensional representation of cells and genes. We show how this
probabilistic framework induces a geometry that is suitable for single-cell
data visualization, and produces a compression of the data that is very
powerful for clustering purposes. Our method is competed against other standard
representation methods like t-SNE, and we illustrate its performance for the
representation of single-cell data. We especially focus on publicly available
single-cell RNA-seq datasets.
</summary>
    <author>
      <name>Ghislain Durif</name>
    </author>
    <author>
      <name>Laurent Modolo</name>
    </author>
    <author>
      <name>Jeff E. Mold</name>
    </author>
    <author>
      <name>Sophie Lambert-Lacroix</name>
    </author>
    <author>
      <name>Franck Picard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 2 figures, Appendix 10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11028v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11028v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05144v2</id>
    <updated>2018-06-29T10:54:07Z</updated>
    <published>2018-06-13T16:56:11Z</published>
    <title>Joint calibrated estimation of inverse probability of treatment and
  censoring weights for marginal structural models</title>
    <summary>  Marginal structural models (MSMs) with inverse probability weighting offer an
approach to estimating causal effects of treatment sequences on repeated
outcome measures in the presence of time-varying confounding and dependent
censoring. However, when weights are estimated by maximum likelihood, inverse
probability weighted estimators (IPWEs) can be inefficient and unstable in
practice. We propose a joint calibration approach for inverse probability of
treatment and censoring weights to improve the efficiency and robustness of the
IPWEs for MSMs with time-varying treatments of arbitrary (i.e., binary and
non-binary) distributions. Specifically, novel calibration restrictions are
derived by explicitly eliminating covariate associations with both the
treatment assignment process and the censoring process after weighting the
current sample (i.e., to optimise covariate balance in finite samples). A
convex minimization procedure is developed to implement the calibration.
Simulations show that IPWEs with calibrated weights perform better than IPWEs
with weights from maximum likelihood. We apply our method to a natural history
study of HIV for estimating the cumulative effect of highly active
antiretroviral therapy on CD4 cell counts over time.
</summary>
    <author>
      <name>Sean Yiu</name>
    </author>
    <author>
      <name>Li Su</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05144v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05144v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00644v5</id>
    <updated>2018-06-29T09:17:43Z</updated>
    <published>2018-01-02T13:47:43Z</published>
    <title>Matching with Text Data: An Experimental Evaluation of Methods for
  Matching Documents and of Measuring Match Quality</title>
    <summary>  Matching for causal inference is a well-studied problem, but standard methods
fail when the units to match are text documents: the high-dimensional and rich
nature of the data renders exact matching infeasible, causes propensity scores
to produce incomparable matches, and makes assessing match quality difficult.
In this paper, we characterize a framework for matching text documents that
decomposes existing methods into: (1) the choice of text representation, and
(2) the choice of distance metric. We investigate how different choices within
this framework affect both the quantity and quality of matches identified
through a systematic multifactor evaluation experiment using human subjects.
Altogether we evaluate 84 unique text matching methods along with 5 comparison
methods taken from the literature. Our experimental results identify methods
that generate matches with higher subjective match quality than current
state-of-the-art techniques. We enhance the precision of these results by
developing a predictive model to estimate the match quality of pairs of text
documents as a function of our 84 distance scores. This model, which we find
successfully mimics human judgment, also allows for approximate and
unsupervised evaluation of new procedures. We then employ the identified best
method to engage with a substantive debate in the study of media bias using a
data set of front-page news articles from thirteen news sources. Media bias is
composed of topic selection bias and presentation bias; using text matching to
control for topic selection, we find that both components contribute to media
bias, though some news sources rely on one component more than the other.
</summary>
    <author>
      <name>Reagan Mozer</name>
    </author>
    <author>
      <name>Luke Miratrix</name>
    </author>
    <author>
      <name>Aaron Russell Kaufman</name>
    </author>
    <author>
      <name>L. Jason Anastasopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00644v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00644v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08962v2</id>
    <updated>2018-06-29T09:17:15Z</updated>
    <published>2018-04-24T11:34:06Z</published>
    <title>Data-driven regularization of Wasserstein barycenters with an
  application to multivariate density registration</title>
    <summary>  We present a framework to simultaneously align and smooth data in the form of
multiple point clouds sampled from unknown densities with support in a
$d$-dimensional Euclidean space. This work is motivated by applications in
bioinformatics where researchers aim to automatically homogenize large datasets
to compare and analyze characteristics within a same cell population.
Inconveniently, the information acquired is most certainly noisy due to
mis-alignment caused by technical variations of the environment. To overcome
this problem, we propose to register multiple point clouds by using the notion
of regularized barycenters (or Fr\'{e}chet mean) of a set of probability
measures with respect to the Wasserstein metric. A first approach consists in
penalizing a Wasserstein barycenter with a convex functional as recently
proposed in Bigot and al. (2018). A second strategy is to transform the
Wasserstein metric itself into an entropy regularized transportation cost
between probability measures as introduced in Cuturi (2013). The main
contribution of this work is to propose data-driven choices for the
regularization parameters involved in each approach using the
Goldenshluger-Lepski's principle. Simulated data sampled from Gaussian mixtures
are used to illustrate each method, and an application to the analysis of flow
cytometry data is finally proposed.
</summary>
    <author>
      <name>Jérémie Bigot</name>
    </author>
    <author>
      <name>Elsa Cazelles</name>
    </author>
    <author>
      <name>Nicolas Papadakis</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08962v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08962v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G07, 62G20, 62H10, 62P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11294v1</id>
    <updated>2018-06-29T08:14:53Z</updated>
    <published>2018-06-29T08:14:53Z</published>
    <title>Properties of the weighted log-rank test under delayed effects
  assumption in the design of confirmatory studies with delayed effects</title>
    <summary>  Proportional hazards are a common assumption when designing confirmatory
clinical trials in oncology. This assumption not only affects the analysis part
but also the sample size calculation. The presence of delayed effects causes a
change in the hazard ratio while the trial is ongoing since at the beginning we
do not observe any difference between treatment arms and after some unknown
time point, the differences between treatment arms will start to appear. Hence,
the proportional hazards assumption no longer holds and both sample size
calculation and analysis methods to be used should be reconsidered. The
weighted log-rank test allows a weighting for early, middle and late
differences through the Fleming and Harrington class of weights, and is proven
to be more efficient when the proportional hazards assumption does not hold.
The Fleming and Harrington class of weights, along with the estimated delay,
can be incorporated into the sample size calculation in order to maintain the
desired power once the treatment arm differences start to appear. In this
article, we explore the impact of delayed effects in group sequential and
adaptive group sequential designs, and make an empirical evaluation in terms of
power and type-I error rate of the of the weighted log-rank in a simulated
scenario. We also give some practical recommendations regarding which
methodology should be used in the presence of delayed effects depending on
certain characteristics of the trial.
</summary>
    <author>
      <name>Jose L Jimenez</name>
    </author>
    <author>
      <name>Viktoriya Stalbovskaya</name>
    </author>
    <author>
      <name>Byron Jones</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02605v2</id>
    <updated>2018-06-29T01:40:10Z</updated>
    <published>2018-04-08T00:27:45Z</published>
    <title>Moving Beyond Sub-Gaussianity in High-Dimensional Statistics:
  Applications in Covariance Estimation and Linear Regression</title>
    <summary>  Concentration inequalities form an essential toolkit in the study of
high-dimensional statistical methods. Most of the relevant statistics
literature is based on the assumptions of sub-Gaussian/sub-exponential random
vectors. In this paper, we bring together various probability inequalities for
sums of independent random variables under much weaker exponential type
(sub-Weibull) tail assumptions. These results extract a part sub-Gaussian tail
behavior in finite samples, matching the asymptotics governed by the central
limit theorem, and are compactly represented in terms of a new Orlicz
quasi-norm - the Generalized Bernstein-Orlicz norm - that typifies such tail
behaviors.
  We illustrate the usefulness of these inequalities through the analysis of
four fundamental problems in high-dimensional statistics. In the first two
problems, we study the rate of convergence of the sample covariance matrix in
terms of the maximum elementwise norm and the maximum k-sub-matrix operator
norm which are key quantities of interest in bootstrap procedures and
high-dimensional structured covariance matrix estimation. The third example
concerns the restricted eigenvalue condition, required in high dimensional
linear regression, which we verify for all sub-Weibull random vectors under
only marginal (not joint) tail assumptions on the covariates. To our knowledge,
this is the first unified result obtained in such generality. In the final
example, we consider the Lasso estimator for linear regression and establish
its rate of convergence under much weaker tail assumptions (on the errors as
well as the covariates) than those in the existing literature. The common
feature in all our results is that the convergence rates under most exponential
tails match the usual ones under sub-Gaussian assumptions. Finally, we also
establish a high-dimensional CLT and tail bounds for empirical processes for
sub-Weibulls.
</summary>
    <author>
      <name>Arun Kumar Kuchibhotla</name>
    </author>
    <author>
      <name>Abhishek Chakrabortty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages (supplementary material - 39 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02605v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02605v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G50, 62J05, 60B20, 62J07, 62E17, 60F05, 60E15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11237v1</id>
    <updated>2018-06-29T00:49:03Z</updated>
    <published>2018-06-29T00:49:03Z</published>
    <title>Nonparametric competing risks analysis using Bayesian Additive
  Regression Trees (BART)</title>
    <summary>  Many time-to-event studies are complicated by the presence of competing
risks. Such data are often analyzed using Cox models for the cause specific
hazard function or Fine-Gray models for the subdistribution hazard. In practice
regression relationships in competing risks data with either strategy are often
complex and may include nonlinear functions of covariates, interactions,
high-dimensional parameter spaces and nonproportional cause specific or
subdistribution hazards. Model misspecification can lead to poor predictive
performance. To address these issues, we propose a novel approach to flexible
prediction modeling of competing risks data using Bayesian Additive Regression
Trees (BART). We study the simulation performance in two-sample scenarios as
well as a complex regression setting, and benchmark its performance against
standard regression techniques as well as random survival forests. We
illustrate the use of the proposed method on a recently published study of
patients undergoing hematopoietic stem cell transplantation.
</summary>
    <author>
      <name>Rodney Sparapani</name>
    </author>
    <author>
      <name>Brent R. Logan</name>
    </author>
    <author>
      <name>Robert E. McCulloch</name>
    </author>
    <author>
      <name>Purushottam W. Laud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.11237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62N99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11229v1</id>
    <updated>2018-06-28T23:20:18Z</updated>
    <published>2018-06-28T23:20:18Z</published>
    <title>Additivity Assessment in Nonparametric Models Using Ratio of Pseudo
  Marginal Likelihoods</title>
    <summary>  Nonparametric regression models such as Bayesian Additive Regression Trees
(BART) can be useful in fitting flexible functions of a set of covariates to a
response, while accounting for nonlinearities and interactions. However, they
are often cumbersome to interpret. Breaking down the function into additive
components, if appropriate, could simplify the interpretation and improve the
utility of the model. On the other hand, establishing nonadditivity can be
useful in determining the need for individualized predictions and treatment
selection. Testing additivity of single covariates in nonparametric regression
models has been extensively studied. However, additivity assessment of
nonparametric functions of disjoint sets of variables has not received as much
attention. We propose a method for detection of nonadditivity of two disjoint
sets of variables by fitting the sum of two BART models, each using its own set
of variables. We then compare the pseudo marginal likelihood (PsML) of this
sum-of- BARTs model vs. a single-BART model with all the variables together, in
a ratio known as Pseudo Bayes Factor (PsBF). A special case of our method
checks additivity between one variable of interest and another set of
variables, where the additive model allows for direct interpretation of the
variable of interest while adjusting for the remaining variables in a flexible,
nonparametric manner. We extended the above approaches to allow a binary
response using a logit link. We also propose a systematic way to design
simulations that are used in additivity assessment. In simulation studies, PsBF
showed better performance compared to out-of-sample prediction error in
choosing the correct model, while avoiding computationally expensive
cross-validation and providing an interpretable criterion for model selection.
We applied our approach to two different examples with a continuous and binary
outcomes.
</summary>
    <author>
      <name>Bonifride Tuyishimire</name>
    </author>
    <author>
      <name>Brent R Logan</name>
    </author>
    <author>
      <name>Purushottam W Laud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.11229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11220v1</id>
    <updated>2018-06-28T22:24:42Z</updated>
    <published>2018-06-28T22:24:42Z</published>
    <title>A Bootstrap Method for Goodness of Fit and Model Selection with a Single
  Observed Network</title>
    <summary>  Network models are applied in numerous domains where data can be represented
as a system of interactions among pairs of actors. While both statistical and
mechanistic network models are increasingly capable of capturing various
dependencies amongst these actors, these dependencies imply the lack of
independence. This poses statistical challenges for analyzing such data,
especially when there is only a single observed network, and often leads to
intractable likelihoods regardless of the modeling paradigm, which limit the
application of existing statistical methods for networks. We explore a
subsampling bootstrap procedure to serve as the basis for goodness of fit and
model selection with a single observed network that circumvents the
intractability of such likelihoods. Our approach is based on flexible
resampling distributions formed from the single observed network, allowing for
finer and higher dimensional comparisons than simply point estimates of
quantities of interest. We include worked examples for model selection, with
simulation, and assessment of goodness of fit, with duplication-divergence
model fits for yeast (S.cerevisiae) protein-protein interaction data from the
literature. The proposed procedure produces a flexible resampling distribution
that can be based on any statistics of one's choosing and can be employed
regardless of choice of model.
</summary>
    <author>
      <name>Sixing Chen</name>
    </author>
    <author>
      <name>Jukka-Pekka Onnela</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 6 figures, presented at Netsci 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.11220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11219v1</id>
    <updated>2018-06-28T22:24:10Z</updated>
    <published>2018-06-28T22:24:10Z</published>
    <title>Using Exposure Mappings as Side Information in Experiments with
  Interference</title>
    <summary>  Exposure mappings are widely used to model potential outcomes in the presence
of interference, where each unit's outcome may depend not only on its own
treatment, but also on the treatment of other units as well. However, in
practice these models may be only a crude proxy for social dynamics. In this
work, we give estimands and estimators that are robust to the misspecification
of an exposure model. In the first part, we require the treatment effect to be
nonnegative (or "monotone") in both direct effects and spillovers. In the
second part, we consider a weaker estimand ("contrasts attributable to
treatment") which makes no restrictions on the interference at all.
</summary>
    <author>
      <name>David Choi</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00642v2</id>
    <updated>2018-06-28T21:26:51Z</updated>
    <published>2017-12-02T17:25:32Z</published>
    <title>Causal inference in the context of an error prone exposure: air
  pollution and mortality</title>
    <summary>  We propose a new approach for estimating causal effects when the exposure is
measured with error and confounding adjustment is performed via a generalized
propensity score (GPS). Using validation data, we propose a regression
calibration (RC)-based adjustment for a continuous error-prone exposure
combined with GPS to adjust for confounding (RC-GPS). The outcome analysis is
conducted after transforming the corrected continuous exposure into a
categorical exposure. We consider confounding adjustment in the context of GPS
subclassification, inverse probability treatment weighting (IPTW) and matching.
In simulations with varying degrees of exposure error and confounding bias,
RC-GPS eliminates bias from exposure error and confounding compared to standard
approaches that rely on the error-prone exposure. We applied RC-GPS to a rich
data platform to estimate the causal effect of long-term exposure to fine
particles ($PM_{2.5}$) on mortality in New England for the period from 2000 to
2012. The main study consists of $2,202$ zip codes covered by $217,660$ 1km
$\times$ 1km grid cells with yearly mortality rates, yearly $PM_{2.5}$ averages
estimated from a spatio-temporal model (error-prone exposure) and several
potential confounders. The internal validation study includes a subset of 83
1km $\times$ 1km grid cells within 75 zip codes from the main study with
error-free yearly $PM_{2.5}$ exposures obtained from monitor stations. Under
assumptions of non-interference and weak unconfoundedness, using matching we
found that exposure to moderate levels of $PM_{2.5}$ ($8 &lt;$ $PM_{2.5}$ $\leq
10\ {\rm \mu g/m^3}$) causes a $2.8\%$ ($95\%$ CI: $0.6\%, 3.6\%$) increase in
all-cause mortality compared to low exposure ($PM_{2.5}$ $\leq 8\ {\rm \mu
g/m^3}$).
</summary>
    <author>
      <name>Xiao Wu</name>
    </author>
    <author>
      <name>Danielle Braun</name>
    </author>
    <author>
      <name>Marianthi-Anna Kioumourtzoglou</name>
    </author>
    <author>
      <name>Christine Choirat</name>
    </author>
    <author>
      <name>Qian Di</name>
    </author>
    <author>
      <name>Francesca Dominici</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.10040v4</id>
    <updated>2018-06-28T20:29:12Z</updated>
    <published>2016-10-31T18:05:47Z</published>
    <title>A Spatial Modeling Approach for Linguistic Object Data: Analysing
  dialect sound variations across Great Britain</title>
    <summary>  Dialect variation is of considerable interest in linguistics and other social
sciences. However, traditionally it has been studied using proxies
(transcriptions) rather than acoustic recordings directly. We introduce novel
statistical techniques to analyse geolocalised speech recordings and to explore
the spatial variation of pronunciations continuously over the region of
interest, as opposed to traditional isoglosses, which provide a discrete
partition of the region. Data of this type require an explicit modeling of the
variation in the mean and the covariance. Usual Euclidean metrics are not
appropriate, and we therefore introduce the concept of $d$-covariance, which
allows consistent estimation both in space and at individual locations. We then
propose spatial smoothing for these objects which accounts for the possibly non
convex geometry of the domain of interest. We apply the proposed method to data
from the spoken part of the British National Corpus, deposited at the British
Library, London, and we produce maps of the dialect variation over Great
Britain. In addition, the methods allow for acoustic reconstruction across the
domain of interest, allowing researchers to listen to the statistical analysis.
</summary>
    <author>
      <name>Shahin Tavakoli</name>
    </author>
    <author>
      <name>Davide Pigoli</name>
    </author>
    <author>
      <name>John A. D. Aston</name>
    </author>
    <author>
      <name>John S. Coleman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.10040v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.10040v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G08, 62M30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11153v1</id>
    <updated>2018-06-28T19:35:22Z</updated>
    <published>2018-06-28T19:35:22Z</published>
    <title>Evaluation of adaptive treatment strategies in an observational study
  where time-varying covariates are not monitored systematically</title>
    <summary>  In studies based on electronic health records (EHR), the frequency of
covariate monitoring can vary by covariate type, across patients, and over
time. This can lead to major challenges: first, the difference in monitoring
protocols may invalidate the extrapolation of study results obtained in one
population to the other, and second, monitoring can act as a time-varying
confounder of the causal effect of a time-varying treatment on the outcomes of
interest. This paper demonstrates how to account for non-systematic covariate
monitoring when evaluating dynamic treatment interventions, and how to evaluate
joint dynamic treatment-censoring and static monitoring interventions, in a
real world, EHR-based, comparative effectiveness research (CER) study of
patients with type II diabetes mellitus. First, we show that the effects of
dynamic treatment-censoring regimes can be identified by including indicators
of monitoring events in the adjustment set. Second, we demonstrate the poor
performance of the standard inverse probability weighting (IPW) estimator of
the effects of joint treatment-censoring-monitoring interventions, due to a
large decrease in data support resulting in a large increase in standard errors
and concerns over finite-sample bias from near-violations of the positivity
assumption for the monitoring process. Finally, we detail an alternate IPW
estimator of the effects of these interventions using the No Direct Effect
assumption. We demonstrate that this estimator can result in improved
efficiency but at the cost of increased bias concerns over structural
near-violations of the positivity assumption for the treatment process. To
conclude, this paper develops and illustrates new tools that researchers can
exploit to appropriately account for non-systematic covariate monitoring in
CER, and to ask new causal questions about the joint effects of treatment and
monitoring interventions.
</summary>
    <author>
      <name>Noémi Kreif</name>
    </author>
    <author>
      <name>Oleg Sofrygin</name>
    </author>
    <author>
      <name>Julie Schmittdiel</name>
    </author>
    <author>
      <name>Alyce Adams</name>
    </author>
    <author>
      <name>Richard Grant</name>
    </author>
    <author>
      <name>Zheng Zhu</name>
    </author>
    <author>
      <name>Mark van der Laan</name>
    </author>
    <author>
      <name>Romain Neugebauer</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08816v4</id>
    <updated>2018-06-28T17:33:30Z</updated>
    <published>2016-09-28T08:40:25Z</published>
    <title>Identifying Causal Effects With Proxy Variables of an Unmeasured
  Confounder</title>
    <summary>  We consider a causal effect that is confounded by an unobserved variable, but
with observed proxy variables of the confounder. We show that, with at least
two independent proxy variables satisfying a certain rank condition, the causal
effect is nonparametrically identified, even if the measurement error
mechanism, i.e., the conditional distribution of the proxies given the con-
founder, may not be identified. Our result generalizes the identification
strategy of Kuroki &amp; Pearl (2014) that rests on identification of the
measurement error mechanism. When only one proxy for the confounder is
available, or the required rank condition is not met, we develop a strategy to
test the null hypothesis of no causal effect.
</summary>
    <author>
      <name>Wang Miao</name>
    </author>
    <author>
      <name>Zhi Geng</name>
    </author>
    <author>
      <name>Eric Tchetgen Tchetgen</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08816v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08816v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11083v1</id>
    <updated>2018-06-28T16:57:55Z</updated>
    <published>2018-06-28T16:57:55Z</published>
    <title>Bootstrap Based Inference for Sparse High-Dimensional Time Series Models</title>
    <summary>  Fitting sparse models to high dimensional time series is an important area of
statistical inference. In this paper we consider sparse vector autoregressive
models and develop appropriate bootstrap methods to infer properties of such
processes, like the construction of confidence intervals and of tests for
individual or for groups of model parameters. Our bootstrap methodology
generates pseudo time series using a model-based bootstrap procedure which
involves an estimated, sparsified version of the underlying vector
autoregressive model. Inference is performed using so-called de-sparsified or
de-biased estimators of the autoregressive model parameters. We derive the
asymptotic distribution of such estimators in the time series context and
establish asymptotic validity of the bootstrap procedure proposed for
estimation and, appropriately modified, for testing purposes. In particular we
focus on testing that a group of autoregressive coefficients equals zero. Our
theoretical results are complemented by simulations which investigate the
finite sample performance of the bootstrap methodology proposed. A real-life
data application is also presented.
</summary>
    <author>
      <name>J. Krampe</name>
    </author>
    <author>
      <name>J-P. Kreiss</name>
    </author>
    <author>
      <name>E. Paparoditis</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09565v3</id>
    <updated>2018-06-28T15:41:21Z</updated>
    <published>2018-02-26T19:29:27Z</published>
    <title>Conjugate Bayes for probit regression via unified skew-normals</title>
    <summary>  Regression models for dichotomous data are ubiquitous in statistics. Besides
being useful for inference on binary responses, such methods are also
fundamental building-blocks in more complex formulations, covering density
regression, nonparametric classification, graphical models, and others. Within
the Bayesian setting, inference typically proceeds by updating the Gaussian
priors for the coefficients with the likelihood induced by probit or logit
regressions for the binary responses. In this updating, the apparent absence of
a tractable posterior has motivated a variety of computational methods,
including Markov Chain Monte Carlo routines and algorithms which approximate
the posterior. Despite being routinely implemented, current Markov Chain Monte
Carlo methodologies face mixing or time-efficiency issues in large p and small
n studies, whereas approximate routines fail to capture the skewness typically
observed in the posterior. This article shows that the posterior distribution
for the probit coefficients has a unified skew-normal kernel, under Gaussian
priors. This result allows fast and accurate Bayesian inference for a wide
class of applications, especially in large p and small-to-moderate n studies
where state-of-the-art computational methods face substantial issues. These
notable advances are quantitatively outlined in a genetic study, and further
motivate the development of a wider class of conjugate priors for probit
regression along with a novel independent sampler from the unified skew-normal
posterior.
</summary>
    <author>
      <name>Daniele Durante</name>
    </author>
    <link href="http://arxiv.org/abs/1802.09565v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09565v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11032v1</id>
    <updated>2018-06-28T15:22:43Z</updated>
    <published>2018-06-28T15:22:43Z</published>
    <title>A depth-based method for functional time series forecasting</title>
    <summary>  An approach is presented for making predictions about functional time series.
The method is applied to data coming from periodically correlated processes and
electricity demand, obtaining accurate point forecasts and narrow prediction
bands that cover high proportions of the forecasted functional datum, for a
given confidence level. The method is computationally efficient and
substantially different to other functional time series methods, offering a new
insight for the analysis of these data structures.
</summary>
    <author>
      <name>Antonio Elías</name>
    </author>
    <author>
      <name>Raúl Jiménez</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08142v5</id>
    <updated>2018-06-28T14:46:09Z</updated>
    <published>2017-01-27T18:30:09Z</published>
    <title>Modelling Preference Data with the Wallenius Distribution</title>
    <summary>  The Wallenius distribution is a generalisation of the Hypergeometric
distribution where weights are assigned to balls of different colours. This
naturally defines a model for ranking categories which can be used for
classification purposes. Since, in general, the resulting likelihood is not
analytically available, we adopt an approximate Bayesian computational (ABC)
approach for estimating the importance of the categories. We illustrate the
performance of the estimation procedure on simulated datasets. Finally, we use
the new model for analysing two datasets about movies ratings and Italian
academic statisticians' journal preferences. The latter is a novel dataset
collected by the authors.
</summary>
    <author>
      <name>Clara Grazian</name>
    </author>
    <author>
      <name>Fabrizio Leisen</name>
    </author>
    <author>
      <name>Brunero Liseo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08142v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08142v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03496v2</id>
    <updated>2018-06-28T14:32:09Z</updated>
    <published>2017-10-10T10:21:37Z</published>
    <title>Admissible multi-arm stepped-wedge cluster randomized trial designs</title>
    <summary>  Numerous publications have now addressed the principles of designing,
analyzing, and reporting the results of, stepped-wedge cluster randomized
trials. In contrast, there is little research available pertaining to the
design and analysis of multi-arm stepped-wedge cluster randomized trials,
utilized to evaluate the effectiveness of multiple experimental interventions.
In this paper, we address this by explaining how the required sample size in
these multi-arm trials can be ascertained when data are to be analyzed using a
linear mixed model. We then go on to describe how the design of such trials can
be optimized to balance between minimizing the cost of the trial, and
minimizing some function of the covariance matrix of the treatment effect
estimates. Using a recently commenced trial that will evaluate the
effectiveness of sensor monitoring in an occupational therapy rehabilitation
program for older persons after hip fracture as an example, we demonstrate that
our designs could reduce the number of observations required for a fixed power
level by up to 58%. Consequently, when logistical constraints permit the
utilization of any one of a range of possible multi-arm stepped-wedge cluster
randomized trial designs, researchers should consider employing our approach to
optimize their trials efficiency.
</summary>
    <author>
      <name>Michael Grayling</name>
    </author>
    <author>
      <name>Adrian Mander</name>
    </author>
    <author>
      <name>James Wason</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03496v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03496v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04826v4</id>
    <updated>2018-06-28T13:44:24Z</updated>
    <published>2018-02-13T19:27:38Z</published>
    <title>Leveraging the Exact Likelihood of Deep Latent Variable Models</title>
    <summary>  Deep latent variable models (DLVMs) combine the approximation abilities of
deep neural networks and the statistical foundations of generative models.
Variational methods are commonly used for inference; however, the exact
likelihood of these models has been largely overlooked. The purpose of this
work is to study the general properties of this quantity and to show how they
can be leveraged in practice. We focus on important inferential problems that
rely on the likelihood: estimation and missing data imputation. First, we
investigate maximum likelihood estimation for DLVMs: in particular, we show
that most unconstrained models used for continuous data have an unbounded
likelihood function. This problematic behaviour is demonstrated to be a source
of mode collapse. We also show how to ensure the existence of maximum
likelihood estimates, and draw useful connections with nonparametric mixture
models. Finally, we describe an algorithm for missing data imputation using the
exact conditional likelihood of a deep latent variable model. On several data
sets, our algorithm consistently and significantly outperforms the usual
imputation scheme used for DLVMs.
</summary>
    <author>
      <name>Pierre-Alexandre Mattei</name>
    </author>
    <author>
      <name>Jes Frellsen</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04826v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04826v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10947v1</id>
    <updated>2018-06-28T13:20:39Z</updated>
    <published>2018-06-28T13:20:39Z</published>
    <title>Extremely efficient permutation and bootstrap hypothesis tests using R</title>
    <summary>  Re-sampling based statistical tests are known to be computationally heavy,
but reliable when small sample sizes are available. Despite their nice
theoretical properties not much effort has been put to make them efficient. In
this paper we treat the case of Pearson correlation coefficient and two
independent samples t-test. We propose a highly computationally efficient
method for calculating permutation based p-values in these two cases. The
method is general and can be applied or be adopted to other similar two sample
mean or two mean vectors cases.
</summary>
    <author>
      <name>Christina Chatzipantsiou</name>
    </author>
    <author>
      <name>Marios Dimitriadis</name>
    </author>
    <author>
      <name>Manos Papadakis</name>
    </author>
    <author>
      <name>Michail Tsagris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Theis is a pre-print of the paper that was accepted in the Journal of
  Modern Applied Statistical Methods</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07007v2</id>
    <updated>2018-06-28T12:27:48Z</updated>
    <published>2017-02-22T21:09:19Z</published>
    <title>Detecting causal associations in large nonlinear time series datasets</title>
    <summary>  Identifying causal relationships from observational time series data is a key
problem in disciplines such as climate science or neuroscience, where
experiments are often not possible. Data-driven causal inference is challenging
since datasets are often high-dimensional and nonlinear with limited sample
sizes. Here we introduce a novel method that flexibly combines linear or
nonlinear conditional independence tests with a causal discovery algorithm that
allows to reconstruct causal networks from large-scale time series datasets. We
validate the method on a well-established climatic teleconnection connecting
the tropical Pacific with extra-tropical temperatures and using large-scale
synthetic datasets mimicking the typical properties of real data. The
experiments demonstrate that our method outperforms alternative techniques in
detection power from small to large-scale datasets and opens up entirely new
possibilities to discover causal networks from time series across a range of
research fields.
</summary>
    <author>
      <name>Jakob Runge</name>
    </author>
    <author>
      <name>Peer Nowack</name>
    </author>
    <author>
      <name>Marlene Kretschmer</name>
    </author>
    <author>
      <name>Seth Flaxman</name>
    </author>
    <author>
      <name>Dino Sejdinovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.07007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01139v3</id>
    <updated>2018-06-28T10:48:51Z</updated>
    <published>2018-06-04T14:16:43Z</published>
    <title>Text to brain: predicting the spatial distribution of neuroimaging
  observations from text reports</title>
    <summary>  Despite the digital nature of magnetic resonance imaging, the resulting
observations are most frequently reported and stored in text documents. There
is a trove of information untapped in medical health records, case reports, and
medical publications. In this paper, we propose to mine brain medical
publications to learn the spatial distribution associated with anatomical
terms. The problem is formulated in terms of minimization of a risk on
distributions which leads to a least-deviation cost function. An efficient
algorithm in the dual then learns the mapping from documents to brain
structures. Empirical results using coordinates extracted from the
brain-imaging literature show that i) models must adapt to semantic variation
in the terms used to describe a given anatomical structure, ii) voxel-wise
parameterization leads to higher likelihood of locations reported in unseen
documents, iii) least-deviation cost outperforms least-square. As a proof of
concept for our method, we use our model of spatial distributions to predict
the distribution of specific neurological conditions from text-only reports.
</summary>
    <author>
      <name>Jérôme Dockès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <author>
      <name>Demian Wassermann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <author>
      <name>Russell Poldrack</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <author>
      <name>Fabian Suchanek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <author>
      <name>Bertrand Thirion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <author>
      <name>Gaël Varoquaux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MICCAI 2018 - 21st International Conference on Medical Image
  Computing and Computer Assisted Intervention, Sep 2018, Granada, Spain.
  pp.1-18, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.01139v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01139v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10761v1</id>
    <updated>2018-06-28T04:06:17Z</updated>
    <published>2018-06-28T04:06:17Z</published>
    <title>Survey of multifidelity methods in uncertainty propagation, inference,
  and optimization</title>
    <summary>  In many situations across computational science and engineering, multiple
computational models are available that describe a system of interest. These
different models have varying evaluation costs and varying fidelities.
Typically, a computationally expensive high-fidelity model describes the system
with the accuracy required by the current application at hand, while
lower-fidelity models are less accurate but computationally cheaper than the
high-fidelity model. Outer-loop applications, such as optimization, inference,
and uncertainty quantification, require multiple model evaluations at many
different inputs, which often leads to computational demands that exceed
available resources if only the high-fidelity model is used. This work surveys
multifidelity methods that accelerate the solution of outer-loop applications
by combining high-fidelity and low-fidelity model evaluations, where the
low-fidelity evaluations arise from an explicit low-fidelity model (e.g., a
simplified physics approximation, a reduced model, a data-fit surrogate, etc.)
that approximates the same output quantity as the high-fidelity model. The
overall premise of these multifidelity methods is that low-fidelity models are
leveraged for speedup while the high-fidelity model is kept in the loop to
establish accuracy and/or convergence guarantees. We categorize multifidelity
methods according to three classes of strategies: adaptation, fusion, and
filtering. The paper reviews multifidelity methods in the outer-loop contexts
of uncertainty propagation, inference, and optimization.
</summary>
    <author>
      <name>Benjamin Peherstorfer</name>
    </author>
    <author>
      <name>Karen Willcox</name>
    </author>
    <author>
      <name>Max Gunzburger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">will appear in SIAM Review</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65-02, 62-02, 49-02" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10706v1</id>
    <updated>2018-06-27T22:39:10Z</updated>
    <published>2018-06-27T22:39:10Z</published>
    <title>Global jump filters and quasi likelihood analysis for volatility</title>
    <summary>  We propose a new estimation scheme for estimation of the volatility
parameters of a semimartingale with jumps based on a jump-detection filter. Our
filter uses all of data to analyze the relative size of increments and to
discriminate jumps more precisely. We construct quasi maximum likelihood
estimators and quasi Bayesian estimators, and show limit theorems for them
including $L^p$-estimates of the error and asymptotic mixed normality based on
the framework of quasi likelihood analysis. The global jump filters do not need
a restrictive condition for the distribution of the small jumps. By numerical
simulation we show that our "global" method obtains better estimates of the
volatility parameter than the previous "local" method.
</summary>
    <author>
      <name>Haruhiko Inatsugu</name>
    </author>
    <author>
      <name>Nakahiro Yoshida</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M09, 62M86" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05066v2</id>
    <updated>2018-06-27T19:17:29Z</updated>
    <published>2018-03-13T22:28:17Z</published>
    <title>Discussion on Bayesian Cluster Analysis: Point Estimation and Credible
  Balls by Sara Wade and Zoubin Ghahramani</title>
    <summary>  I begin my discussion by giving an overview of the main results. Then I
proceed to touch upon issues about whether the credible ball constructed can be
interpreted as a confidence ball, suggestions on reducing computational costs,
and posterior consistency or contraction rates.
</summary>
    <author>
      <name>William Weimin Yoo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/17-BA1073</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/17-BA1073" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bayesian Anal., Volume 13, Number 2 (2018), 599-600</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.05066v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05066v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62H30, secondary 62C10, 62G15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03170v2</id>
    <updated>2018-06-27T19:15:30Z</updated>
    <published>2017-11-08T21:17:39Z</published>
    <title>Penalized Orthogonal Iteration for Sparse Estimation of Generalized
  Eigenvalue Problem</title>
    <summary>  We propose a new algorithm for sparse estimation of eigenvectors in
generalized eigenvalue problems (GEP). The GEP arises in a number of modern
data-analytic situations and statistical methods, including principal component
analysis (PCA), multiclass linear discriminant analysis (LDA), canonical
correlation analysis (CCA), sufficient dimension reduction (SDR) and invariant
co-ordinate selection. We propose to modify the standard generalized orthogonal
iteration with a sparsity-inducing penalty for the eigenvectors. To achieve
this goal, we generalize the equation-solving step of orthogonal iteration to a
penalized convex optimization problem. The resulting algorithm, called
penalized orthogonal iteration, provides accurate estimation of the true
eigenspace, when it is sparse. Also proposed is a computationally more
efficient alternative, which works well for PCA and LDA problems. Numerical
studies reveal that the proposed algorithms are competitive, and that our
tuning procedure works well. We demonstrate applications of the proposed
algorithm to obtain sparse estimates for PCA, multiclass LDA, CCA and SDR.
Supplementary materials are available online.
</summary>
    <author>
      <name>Sungkyu Jung</name>
    </author>
    <author>
      <name>Jeongyoun Ahn</name>
    </author>
    <author>
      <name>Yongho Jeon</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03170v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03170v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10582v1</id>
    <updated>2018-06-27T17:27:02Z</updated>
    <published>2018-06-27T17:27:02Z</published>
    <title>A functional approach to estimation of the parameters of generalized
  negative binomial and gamma distributions</title>
    <summary>  The generalized negative binomial distribution (GNB) is a new flexible family
of discrete distributions that are mixed Poisson laws with the mixing
generalized gamma (GG) distributions. This family of discrete distributions is
very wide and embraces Poisson distributions, negative binomial distributions,
Sichel distributions, Weibull--Poisson distributions and many other types of
distributions supplying descriptive statistics with many flexible models. These
distributions seem to be very promising for the statistical description of many
real phenomena. GG distributions are widely applied in signal and image
processing and other practical problems. The statistical estimation of the
parameters of GNB and GG distributions is quite complicated. To find estimates,
the methods of moments or maximum likelihood can be used as well as two-stage
grid EM-algorithms. The paper presents a methodology based on the search for
the best distribution using the minimization of $\ell^p$-distances and
$L^p$-metrics for GNB and GG distributions, respectively. This approach, first,
allows to obtain parameter estimates without using grid methods and solving
systems of nonlinear equations and, second, yields not point estimates as the
methods of moments or maximum likelihood do, but the estimate for the density
function. In other words, within this approach the set of decisions is not a
Euclidean space, but a functional space.
</summary>
    <author>
      <name>Andrey K. Gorshenin</name>
    </author>
    <author>
      <name>Victor Yu. Korolev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, The XXI International Conference on Distributed
  Computer and Communication Networks: Control, Computation, Communications
  (DCCN 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03239v2</id>
    <updated>2018-06-27T16:04:35Z</updated>
    <published>2017-04-11T11:14:41Z</published>
    <title>Sparse Bayesian vector autoregressions in huge dimensions</title>
    <summary>  We develop a Bayesian vector autoregressive (VAR) model that is capable of
handling vast dimensional information sets. Three features are introduced to
permit reliable estimation of the model. First, we assume that the reduced-form
errors in the VAR feature a factor stochastic volatility structure, allowing
for conditional equation-by-equation estimation. Second, we apply a
Dirichlet-Laplace prior to the VAR coefficients to cure the curse of
dimensionality. Finally, since simulation-based methods are needed to simulate
from the joint posterior distribution, we utilize recent innovations to
efficiently sample from high-dimensional multivariate Gaussian distributions
that improve upon recent algorithms by large margins. In the empirical exercise
we apply the model to US data and evaluate its forecasting capabilities.
</summary>
    <author>
      <name>Gregor Kastner</name>
    </author>
    <author>
      <name>Florian Huber</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03239v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03239v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10495v1</id>
    <updated>2018-06-27T14:18:00Z</updated>
    <published>2018-06-27T14:18:00Z</published>
    <title>Impact of predictor measurement heterogeneity across settings on
  performance of prediction models: a measurement error perspective</title>
    <summary>  Clinical prediction models have an important role in contemporary medicine. A
vital aspect to consider is whether a prediction model is transportable to
individuals that were not part of the set in which the prediction model was
derived. Transportability of prediction models can be hampered when predictors
are measured differently at derivation and (external) validation of the model.
This may occur, for instance, when predictors are measured using different
protocols or when tests are produced by different manufacturers. Although such
heterogeneity in predictor measurement across derivation and validation samples
is very common, the impact on the performance of prediction models at external
validation is not well-studied. Using analytical and simulation approaches, we
examined the external performance of prediction models under different
scenarios of heterogeneous predictor measurement. These scenarios were defined
and clarified using an established taxonomy of measurement error models. The
results of our simulations indicate that predictor measurement heterogeneity
induces miscalibration of prediction models and affects discrimination and
accuracy at external validation, to extents that predictions in new
observations may no longer be clinically useful. The measurement error
perspective was found to be helpful in identifying and predicting effects of
heterogeneous predictor measurements across settings of derivation, validation
and application. Our work indicates that consideration of consistency of
measurement procedures across settings is of paramount importance in prediction
research.
</summary>
    <author>
      <name>Kim Luijken</name>
    </author>
    <author>
      <name>Rolf H. H. Groenwold</name>
    </author>
    <author>
      <name>Ben van Calster</name>
    </author>
    <author>
      <name>Ewout W. Steyerberg</name>
    </author>
    <author>
      <name>Maarten van Smeden</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97K80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10483v1</id>
    <updated>2018-06-27T13:46:13Z</updated>
    <published>2018-06-27T13:46:13Z</published>
    <title>A Robustified posterior for Bayesian inference on a large number of
  parallel effects</title>
    <summary>  Many modern experiments, such as microarray gene expression and genome-wide
association studies, present the problem of estimating a large number of
parallel effects. Bayesian inference is a popular approach for analyzing such
data by modeling the large number of unknown parameters as random effects from
a common prior distribution. However, misspecification of the prior
distribution, particularly in the tails of the distribution, can lead to
erroneous estimates of the random effects, especially for the largest and most
interesting effects. This paper proposes a robustified posterior distribution
that eliminates the impact of a misspecified prior on one component of the
standard posterior by replacing that component with an asymptotically correct
form. The proposed posterior can be combined with a flexible working prior to
achieve superior inference across different structures of the underlying
effects of interest.
</summary>
    <author>
      <name>Arthur Berg</name>
    </author>
    <author>
      <name>J G Liao</name>
    </author>
    <author>
      <name>Timothy L McMurry</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09949v2</id>
    <updated>2018-06-27T11:08:14Z</updated>
    <published>2018-06-26T12:49:52Z</published>
    <title>Conditional bias robust estimation of the total of curve data by
  sampling in a finite population: an illustration on electricity load curves</title>
    <summary>  For marketing or power grid management purposes, many studies based on the
analysis of the total electricity consumption curves of groups of customers are
now carried out by electricity companies. Aggregated total or mean load curves
are estimated using individual curves measured at fine time grid and collected
according to some sampling design. Due to the skewness of the distribution of
electricity consumptions, these samples often contain outlying curves which may
have an important impact on the usual estimation procedures. We introduce
several robust estimators of the total consumption curve which are not
sensitive to such outlying curves. These estimators are based on the
conditional bias approach and robust functional methods. We also derive mean
square error estimators of these robust estimators and finally, we evaluate and
compare the performance of the suggested estimators on Irish electricity data.
</summary>
    <author>
      <name>Hervé Cardot</name>
    </author>
    <author>
      <name>Anne De Moliner Anne</name>
    </author>
    <author>
      <name>Camelia Goga</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09949v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09949v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05717v4</id>
    <updated>2018-06-27T10:53:11Z</updated>
    <published>2017-12-15T15:39:26Z</published>
    <title>Random forward models and log-likelihoods in Bayesian inverse problems</title>
    <summary>  We consider the use of randomised forward models and log-likelihoods within
the Bayesian approach to inverse problems. Such random approximations to the
exact forward model or log-likelihood arise naturally when a computationally
expensive model is approximated using a cheaper stochastic surrogate, as in
Gaussian process emulation (kriging), or in the field of probabilistic
numerical methods. We show that the Hellinger distance between the exact and
approximate Bayesian posteriors is bounded by moments of the difference between
the true and approximate log-likelihoods. Example applications of these
stability results are given for randomised misfit models in large data
applications and the probabilistic solution of ordinary differential equations.
</summary>
    <author>
      <name>H. C. Lie</name>
    </author>
    <author>
      <name>T. J. Sullivan</name>
    </author>
    <author>
      <name>A. L. Teckentrup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05717v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05717v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15, 62G08, 65C99, 65D05, 65D30, 65J22, 68W20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10403v1</id>
    <updated>2018-06-27T10:46:29Z</updated>
    <published>2018-06-27T10:46:29Z</published>
    <title>Quantile-based clustering</title>
    <summary>  A new cluster analysis method, $K$-quantiles clustering, is introduced.
$K$-quantiles clustering can be computed by a simple greedy algorithm in the
style of the classical Lloyd's algorithm for $K$-means. It can be applied to
large and high-dimensional datasets. It allows for within-cluster skewness and
internal variable scaling based on within-cluster variation. Different versions
allow for different levels of parsimony and computational efficiency. Although
$K$-quantiles clustering is conceived as nonparametric, it can be connected to
a fixed partition model of generalized asymmetric Laplace-distributions. The
consistency of $K$-quantiles clustering is proved, and it is shown that
$K$-quantiles clusters correspond to well separated mixture components in a
nonparametric mixture. In a simulation, $K$-quantiles clustering is compared
with a number of popular clustering methods with good results. A
high-dimensional microarray dataset is clustered by $K$-quantiles.
</summary>
    <author>
      <name>Christian Hennig</name>
    </author>
    <author>
      <name>Cinzia Viroli</name>
    </author>
    <author>
      <name>Laura Anderlucci</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08417v2</id>
    <updated>2018-06-26T23:22:17Z</updated>
    <published>2018-02-23T07:34:50Z</published>
    <title>Geometric Lower Bounds for Distributed Parameter Estimation under
  Communication Constraints</title>
    <summary>  We consider parameter estimation in distributed networks, where each sensor
in the network observes an independent sample from an underlying distribution
and has $k$ bits to communicate its sample to a centralized processor which
computes an estimate of a desired parameter. We develop lower bounds for the
minimax risk of estimating the underlying parameter under squared $\ell_2$ loss
for a large class of distributions. Our results show that under mild regularity
conditions, the communication constraint reduces the effective sample size by a
factor of $d$ when $k$ is small, where $d$ is the dimension of the estimated
parameter. Furthermore, this penalty reduces at most exponentially with
increasing $k$, which is the case for some models, e.g., estimating
high-dimensional distributions. For other models however, we show that the
sample size reduction is re-mediated only linearly with increasing $k$, e.g.
when some sub-Gaussian structure is available. We apply our results to the
distributed setting with product Bernoulli model, multinomial model, and
dense/sparse Gaussian location models which recover or strengthen existing
results.
  Our approach significantly deviates from existing approaches for developing
information-theoretic lower bounds for communication-efficient estimation. We
circumvent the need for strong data processing inequalities used in prior work
and develop a geometric approach which builds on a new representation of the
communication constraint. This approach allows us to strengthen and generalize
existing results with simpler and more transparent proofs.
</summary>
    <author>
      <name>Yanjun Han</name>
    </author>
    <author>
      <name>Ayfer Özgür</name>
    </author>
    <author>
      <name>Tsachy Weissman</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08417v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08417v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08405v2</id>
    <updated>2018-06-26T23:19:20Z</updated>
    <published>2018-02-23T06:33:37Z</published>
    <title>Local moment matching: A unified methodology for symmetric functional
  estimation and distribution estimation under Wasserstein distance</title>
    <summary>  We present \emph{Local Moment Matching (LMM)}, a unified methodology for
symmetric functional estimation and distribution estimation under Wasserstein
distance. We construct an efficiently computable estimator that achieves the
minimax rates in estimating the distribution up to permutation, and show that
the plug-in approach of our unlabeled distribution estimator is "universal" in
estimating symmetric functionals of discrete distributions. Instead of doing
best polynomial approximation explicitly as in existing literature of
functional estimation, the plug-in approach conducts polynomial approximation
implicitly and attains the optimal sample complexity for the entropy, power sum
and support size functionals.
</summary>
    <author>
      <name>Yanjun Han</name>
    </author>
    <author>
      <name>Jiantao Jiao</name>
    </author>
    <author>
      <name>Tsachy Weissman</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08405v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08405v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10234v1</id>
    <updated>2018-06-26T22:42:15Z</updated>
    <published>2018-06-26T22:42:15Z</published>
    <title>Scalable Gaussian Process Inference with Finite-data Mean and Variance
  Guarantees</title>
    <summary>  Gaussian processes (GPs) offer a flexible class of priors for nonparametric
Bayesian regression, but popular GP posterior inference methods are typically
prohibitively slow or lack desirable finite-data guarantees on quality. We
develop an approach to scalable approximate GP regression with finite-data
guarantees on the accuracy of pointwise posterior mean and variance estimates.
Our main contribution is a novel objective for approximate inference in the
nonparametric setting: the "preconditioned Fisher (pF) divergence." We show
that unlike popular divergences such as Kullback--Leibler (used in variational
inference), the pF divergence bounds the 2-Wasserstein distance, which in turn
bounds the pointwise difference of the mean and variance functions. We
demonstrate that, for sparse GP likelihood approximations, we can minimize the
pF divergence efficiently. Our experiments show that optimizing the pF
divergence has the same computational requirements as variational sparse GPs
while providing comparable empirical performance---in addition to our novel
finite-data quality guarantees.
</summary>
    <author>
      <name>Jonathan H. Huggins</name>
    </author>
    <author>
      <name>Trevor Campbell</name>
    </author>
    <author>
      <name>Mikołaj Kasprzak</name>
    </author>
    <author>
      <name>Tamara Broderick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10163v1</id>
    <updated>2018-06-26T18:35:08Z</updated>
    <published>2018-06-26T18:35:08Z</published>
    <title>Flexible Multiple Testing with the FACT Algorithm</title>
    <summary>  Modern high-throughput science often leads to multiple testing problems:
researchers test many hypotheses, wishing to find the significant discoveries.
The development of flexible multiple testing methods is thus a central problem
in statistics. In this paper, we introduce the new Fast Closed Testing (FACT)
method for multiple testing, controlling the family-wise error rate. Our method
relies on symmetry and monotonicity to enable the classical closed testing
principle in the important setting of large datasets. As the closed testing
principle is more than 40 years old, we find it surprising that this simple and
fundamental algorithm has not been described before. Our FACT method is general
and flexible, and can be used to design powerful new architectures for multiple
testing. We showcase it by proposing the Simes-Higher Criticism fusion test,
which is powerful for detecting both a few strong signals, and also many
moderate signals. We illustrate the method in simulations and in a genome-wide
association study of coronary artery disease, and obtain more power than with
existing methods.
</summary>
    <author>
      <name>Edgar Dobriban</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10093v1</id>
    <updated>2018-06-26T16:18:24Z</updated>
    <published>2018-06-26T16:18:24Z</published>
    <title>Estimation of large block covariance matrices: Application to the
  analysis of gene expression data</title>
    <summary>  Motivated by an application in molecular biology, we propose a novel,
efficient and fully data-driven approach for estimating large block structured
sparse covariance matrices in the case where the number of variables is much
larger than the number of samples without limiting ourselves to block diagonal
matrices. Our approach consists in approximating such a covariance matrix by
the sum of a low-rank sparse matrix and a diagonal matrix. Our methodology can
also deal with matrices for which the block structure only appears if the
columns and rows are permuted according to an unknown permutation. Our
technique is implemented in the R package \texttt{BlockCov} which is available
from the Comprehensive R Archive Network and from GitHub. In order to
illustrate the statistical and numerical performance of our package some
numerical experiments are provided as well as a thorough comparison with
alternative methods. Finally, our approach is applied to gene expression data
in order to better understand the toxicity of acetaminophen on the liver of
rats.
</summary>
    <author>
      <name>Marie Perrot-Dockès</name>
    </author>
    <author>
      <name>Céline Lévy-Leduc</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10089v1</id>
    <updated>2018-06-26T16:07:37Z</updated>
    <published>2018-06-26T16:07:37Z</published>
    <title>New Estimation Approaches for the Linear Ballistic Accumulator Model</title>
    <summary>  The Linear Ballistic Accumulator (LBA) model of Brown (2008) is used as a
measurement tool to answer questions about applied psychology. These analyses
involve parameter estimation and model selection, and modern approaches use
hierarchical Bayesian methods and Markov chain Monte Carlo (MCMC) to estimate
the posterior distribution of the parameters. Although there are a range of
approaches used for model selection, they are all based on the posterior
samples produced via MCMC, which means that the model selection inferences
inherit properties of the MCMC sampler. We address these constraints by
proposing two new approaches to the Bayesian estimation of the hierarchical LBA
model. Both methods are qualitatively different from all existing approaches,
and are based on recent advances in particle-based Monte-Carlo methods. The
first approach is based on particle MCMC, using Metropolis-within-Gibbs steps
and the second approach uses a version of annealed importance sampling. Both
methods have important differences from all existing methods, including greatly
improved sampling efficiency and parallelisability for high-performance
computing. An important further advantage of our annealed importance sampling
algorithm is that an estimate of the marginal likelihood is obtained as a
byproduct of sampling. This makes it straightforward to then apply model
selection via Bayes factors. The new approaches we develop provide
opportunities to apply the LBA model with greater confidence than before, and
to extend its use to previously intractable cases. We illustrate the proposed
methods with pseudo-code, and by application to simulated and real datasets.
</summary>
    <author>
      <name>David Gunawan</name>
    </author>
    <author>
      <name>Scott Brown</name>
    </author>
    <author>
      <name>Robert Kohn</name>
    </author>
    <author>
      <name>Minh-Ngoc Tran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 11 figures and 11 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03838v2</id>
    <updated>2018-06-26T12:25:06Z</updated>
    <published>2017-11-10T14:51:27Z</published>
    <title>Modeling Asymmetric Relationships from Symmetric Networks</title>
    <summary>  Many relationships requiring mutual agreement between pairs of actors produce
observable networks that are symmetric and undirected. Nevertheless the
unobserved, asymmetric network is often of primary scientific interest. We
propose a method that probabilistically reconstructs the unobserved, asymmetric
network from the observed, symmetric graph using a regression-based framework
that allows for inference on predictors of actors' decisions. We apply this
model to the bilateral investment treaty network. Our approach extracts
politically relevant information about the network structure that is
inaccessible to alternative approaches and has superior predictive performance.
</summary>
    <author>
      <name>Arturas Rozenas</name>
    </author>
    <author>
      <name>Shahryar Minhas</name>
    </author>
    <author>
      <name>John Ahlquist</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03838v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03838v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07868v3</id>
    <updated>2018-06-26T08:50:54Z</updated>
    <published>2017-04-25T19:10:11Z</published>
    <title>New robust statistical procedures for polytomous logistic regression
  models</title>
    <summary>  This paper derives a new family of estimators, namely the minimum density
power divergence estimators, as a robust generalization of the maximum
likelihood estimator for the polytomous logistic regression model. Based on
these estimators, a family of Wald-type test statistics for linear hypotheses
is introduced. Robustness properties of both the proposed estimators and the
test statistics are theoretically studied through the classical influence
function analysis. Appropriate real life examples are presented to justify the
requirement of suitable robust statistical procedures in place of the
likelihood based inference for the polytomous logistic regression model. The
validity of the theoretical results established in the paper are further
confirmed empirically through suitable simulation studies. Finally, an approach
for the data-driven selection of the robustness tuning parameter is proposed
with empirical justifications.
</summary>
    <author>
      <name>E. Castilla</name>
    </author>
    <author>
      <name>A. Ghosh</name>
    </author>
    <author>
      <name>N. Martín</name>
    </author>
    <author>
      <name>L. Pardo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/biom.12890</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/biom.12890" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Some typos were corrected from the published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Biometrics, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.07868v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07868v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09014v2</id>
    <updated>2018-06-26T06:34:08Z</updated>
    <published>2018-06-23T18:01:18Z</published>
    <title>Assumption Lean Regression</title>
    <summary>  It is well known that models used in conventional regression analysis are
commonly misspecified. A standard response is little more than a shrug. Data
analysts invoke Box's maxim that all models are wrong and then proceed as if
the results are useful nevertheless. In this paper, we provide an alternative.
Regression models are treated explicitly as approximations of a true response
surface that can have a number of desirable statistical properties, including
estimates that are asymptotically unbiased. Valid statistical inference
follows. We generalize the formulation to include regression functionals, which
broadens substantially the range of potential applications. An empirical
application is provided to illustrate the paper's key concepts.
</summary>
    <author>
      <name>Richard Berk</name>
    </author>
    <author>
      <name>Andreas Buja</name>
    </author>
    <author>
      <name>Lawrence Brown</name>
    </author>
    <author>
      <name>Edward George</name>
    </author>
    <author>
      <name>Arun Kumar Kuchibhotla</name>
    </author>
    <author>
      <name>Weijie J. Su</name>
    </author>
    <author>
      <name>Linda Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for review, 21 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04602v2</id>
    <updated>2018-06-26T06:23:41Z</updated>
    <published>2018-05-11T21:54:21Z</published>
    <title>Stochastic Approximation EM for Logistic Regression with Missing Values</title>
    <summary>  Logistic regression is a common classification method in supervised learning.
Surprisingly, there are very few solutions for performing it and selecting
variables in the presence of missing values. We propose a stochastic
approximation version of the EM algorithm based on Metropolis-Hasting sampling,
to perform statistical inference for logistic regression with incomplete data.
We suggest a complete approach, including the estimation of parameters and
their variance, derivation of confidence intervals, a model selection
procedure, and a method for prediction on test sets with missing values. The
method is computationally efficient, and its good coverage and variable
selection properties are demonstrated in a simulation study where we contrast
its performances to other methods. We then illustrate the method on a dataset
of severely traumatized patients from Paris hospitals to predict the occurrence
of hemorrhagic shock, a leading cause of early preventable death in severe
trauma cases. The aim is to consolidate the current red flag procedure, a
binary alert identifying patients with a high risk of severe hemorrhage. The
methodology is implemented in the R package misaem.
</summary>
    <author>
      <name>Wei Jiang</name>
    </author>
    <author>
      <name>Julie Josse</name>
    </author>
    <author>
      <name>Marc Lavielle</name>
    </author>
    <author>
      <name>Tobias Gauss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 10 figure, R package misaem
  https://github.com/wjiang94/misaem, R implementations
  https://github.com/wjiang94/miSAEM_logReg</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09734v1</id>
    <updated>2018-06-26T00:09:16Z</updated>
    <published>2018-06-26T00:09:16Z</published>
    <title>Main effects and interactions in mixed and incomplete data frames</title>
    <summary>  A mixed data frame (MDF) is a table collecting categorical, numerical and
count observations. The use of MDF is widespread in statistics and the
applications are numerous from abundance data in ecology to recommender
systems. In many cases, an MDF exhibits simultaneously main effects, such as
row, column or group effects and interactions, for which a low-rank model has
often been suggested. Although the literature on low-rank approximations is
very substantial, with few exceptions, existing methods do not allow to
incorporate main effects and interactions while providing statistical
guarantees. The present work fills this gap. We propose an estimation method
which allows to recover simultaneously the main effects and the interactions.
We show that our method is near optimal under conditions which are met in our
targeted applications. Numerical experiments using both simulated and survey
data are provided to support our claims.
</summary>
    <author>
      <name>Geneviève Robin</name>
    </author>
    <author>
      <name>Olga Klopp</name>
    </author>
    <author>
      <name>Julie Josse</name>
    </author>
    <author>
      <name>Éric Moulines</name>
    </author>
    <author>
      <name>Robert Tibshirani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 1 figure, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01762v2</id>
    <updated>2018-06-25T19:58:12Z</updated>
    <published>2017-11-06T07:56:27Z</published>
    <title>A fast subsampling method for estimating the distribution of
  signal-to-noise ratio statistics in nonparametric time series regression
  models</title>
    <summary>  Signal-to-noise ratio (SNR) statistics play a central role in many
applications. A common situation where SNR is studied is when a continuous time
signal is sampled at a fixed frequency with some noise in the background. While
estimation methods exist, little is known about its distribution when the noise
is not weakly stationary. In this paper we develop a fast nonparametric method
to estimate the distribution of an SNR statistic when the noise belongs to a
fairly general class of stochastic processes that encompasses both short and
long-range dependence, as well as nonlinearities. The method is based on a
combination of smoothing and subsampling techniques. It is fast in the sense
that computations are only operated at the subsample level allowing to manage
the typical enormous sample size produced by modern data acquisition
technologies. We derive asymptotic guarantees for the proposed method, and we
show the finite sample performance based on numerical experiments.
</summary>
    <author>
      <name>Francesco Giordano</name>
    </author>
    <author>
      <name>Pietro Coretto</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01762v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01762v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G09 (primary), 62G08, 60G35, 62M86 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01833v3</id>
    <updated>2018-06-25T16:41:24Z</updated>
    <published>2016-07-06T22:33:43Z</published>
    <title>Numerical algorithms on the affine Grassmannian</title>
    <summary>  The affine Grassmannian is a noncompact smooth manifold that parameterizes
all affine subspaces of a fixed dimension. It is a natural generalization of
Euclidean space, points being zero-dimensional affine subspaces. We will
realize the affine Grassmannian as a matrix manifold and extend Riemannian
optimization algorithms including steepest descent, Newton method, and
conjugate gradient, to real-valued functions on the affine Grassmannian. Like
their counterparts for the Grassmannian, these algorithms are in the style of
Edelman--Arias--Smith --- they rely only on standard numerical linear algebra
and are readily computable.
</summary>
    <author>
      <name>Lek-Heng Lim</name>
    </author>
    <author>
      <name>Ken Sze-Wai Wong</name>
    </author>
    <author>
      <name>Ke Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01833v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01833v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="14M15, 90C30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09529v1</id>
    <updated>2018-06-25T15:35:10Z</updated>
    <published>2018-06-25T15:35:10Z</published>
    <title>Spiked covariances and principal components analysis in high-dimensional
  random effects models</title>
    <summary>  We study principal components analyses in multivariate random and mixed
effects linear models, assuming a spherical-plus-spikes structure for the
covariance matrix of each random effect. We characterize the behavior of
outlier sample eigenvalues and eigenvectors of MANOVA variance components
estimators in such models under a high-dimensional asymptotic regime. Our
results show that an aliasing phenomenon may occur in high dimensions, in which
eigenvalues and eigenvectors of the MANOVA estimate for one variance component
may be influenced by the other components. We propose an alternative procedure
for estimating the true principal eigenvalues and eigenvectors that
asymptotically corrects for this aliasing problem.
</summary>
    <author>
      <name>Zhou Fan</name>
    </author>
    <author>
      <name>Iain M. Johnstone</name>
    </author>
    <author>
      <name>Yi Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09388v2</id>
    <updated>2018-06-25T14:54:48Z</updated>
    <published>2017-11-26T13:49:14Z</published>
    <title>Model misspecification and bias for inverse probability weighting and
  doubly robust estimators</title>
    <summary>  In the causal inference literature an estimator belonging to a class of
semi-parametric estimators is called robust if it has desirable properties
under the assumption that at least one of the working models is correctly
specified. In this paper we propose a crude analytical approach to study the
large sample bias of semi-parameteric estimators of the average causal effect
when all working models are misspecified. We apply our approach to three
prototypical estimators, two inverse probability weighting (IPW) estimators,
using a misspecified propensity score model, and a doubly robust (DR)
estimator, using misspecified models for the outcome regression and the
propensity score. To analyze the question of when the use of two misspecified
models are better than one we derive necessary and sufficient conditions for
when the DR estimator has a smaller bias than a simple IPW estimator and when
it has a smaller bias than an IPW estimator with normalized weights. If the
misspecificiation of the outcome model is moderate the comparisons of the
biases of the IPW and DR estimators suggest that the DR estimator has a smaller
bias than the IPW estimators. However, all biases include the PS-model error
and we suggest that a researcher is careful when modeling the PS whenever such
a model is involved.
</summary>
    <author>
      <name>Ingeborg Waernbaum</name>
    </author>
    <author>
      <name>Laura Pazzagli</name>
    </author>
    <link href="http://arxiv.org/abs/1711.09388v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09388v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09473v1</id>
    <updated>2018-06-25T14:07:04Z</updated>
    <published>2018-06-25T14:07:04Z</published>
    <title>Accounting for phenology in the analysis of animal movement</title>
    <summary>  The analysis of animal tracking data provides an important source of
scientific understanding and discovery in ecology. Observations of animal
trajectories using telemetry devices provide researchers with information about
the way animals interact with their environment and each other. For many
species, specific geographical features in the landscape can have a strong
effect on behavior. Such features may correspond to a single point (e.g., dens
or kill sites), or to higher-dimensional subspaces (e.g., rivers or lakes).
Features may be relatively static in time (e.g., coastlines or home-range
centers), or may be dynamic (e.g., sea ice extent or areas of high-quality
forage for herbivores). We introduce a novel model for animal movement that
incorporates active selection for dynamic features in a landscape.
  Our approach is motivated by the study of polar bear (Ursus maritimus)
movement. During the sea ice melt season, polar bears spend much of their time
on sea ice above shallow, biologically productive water where they hunt seals.
The changing distribution and characteristics of sea ice throughout the late
spring through early fall means that the location of valuable habitat is
constantly shifting. We develop a model for the movement of polar bears that
accounts for the effect of this important landscape feature. We introduce a
two-stage procedure for approximate Bayesian inference that allows us to
analyze over 300,000 observed locations of 186 polar bears from 2012--2016. We
use our proposed model to answer a particular question posed by wildlife
managers who seek to cluster polar bears from the Beaufort and Chukchi seas
into sub-populations.
</summary>
    <author>
      <name>Henry R. Scharf</name>
    </author>
    <author>
      <name>Mevin B. Hooten</name>
    </author>
    <author>
      <name>Ryan R. Wilson</name>
    </author>
    <author>
      <name>George M. Durner</name>
    </author>
    <author>
      <name>Todd C. Atwood</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01192v4</id>
    <updated>2018-06-25T13:09:44Z</updated>
    <published>2016-02-03T05:28:13Z</published>
    <title>Prediction models for network-linked data</title>
    <summary>  Prediction algorithms typically assume the training data are independent
samples, but in many modern applications samples come from individuals
connected by a network. For example, in adolescent health studies of
risk-taking behaviors, information on the subjects' social network is often
available and plays an important role through network cohesion, the empirically
observed phenomenon of friends behaving similarly. Taking cohesion into account
in prediction models should allow us to improve their performance. Here we
propose a network-based penalty on individual node effects to encourage
similarity between predictions for linked nodes, and show that incorporating it
into prediction leads to improvement over traditional models both theoretically
and empirically when network cohesion is present. The penalty can be used with
many loss-based prediction methods, such as regression, generalized linear
models, and Cox's proportional hazard model. Applications to predicting levels
of recreational activity and marijuana usage among teenagers from the AddHealth
study based on both demographic covariates and friendship networks are
discussed in detail and show that our approach to taking friendships into
account can significantly improve predictions of behavior while providing
interpretable estimates of covariate effects.
</summary>
    <author>
      <name>Tianxi Li</name>
    </author>
    <author>
      <name>Elizaveta Levina</name>
    </author>
    <author>
      <name>Ji Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01192v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01192v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09362v1</id>
    <updated>2018-06-25T10:20:51Z</updated>
    <published>2018-06-25T10:20:51Z</published>
    <title>Approximate Bayesian inference for mixture cure models</title>
    <summary>  Cure models in survival analysis deal with populations in which a part of the
individuals cannot experience the event of interest. Mixture cure models
consider the target population as a mixture of susceptible and non-susceptible
individuals. The statistical analysis of these models focuses on examining the
probability of cure (incidence model) and inferring on the time-to-event in the
susceptible subpopulation (latency model).
  Bayesian inference on mixture cure models has typically relied upon Markov
chain Monte Carlo (MCMC) methods. The integrated nested Laplace approximation
(INLA) is a recent and attractive approach for doing Bayesian inference. INLA
in its natural definition cannot fit mixture models but recent research has new
proposals that combine INLA and MCMC methods to extend its applicability to
them (Bivand et al., 2014, G\'omez-Rubio et al., 2017, G\'omez-Rubio and Rue,
2018}.
  This paper focuses on the implementation of INLA in mixture cure models. A
general mixture cure survival model with covariate information for the latency
and the incidence model within a general scenario with censored and
non-censored information is discussed. The fact that non-censored individuals
undoubtedly belong to the uncured population is a valuable information that was
incorporated in the inferential process.
</summary>
    <author>
      <name>Elena Lázaro</name>
    </author>
    <author>
      <name>Carmen Armero</name>
    </author>
    <author>
      <name>Virgilio Gómez-Rubio</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01325v2</id>
    <updated>2018-06-25T07:41:54Z</updated>
    <published>2018-06-04T18:57:29Z</published>
    <title>Adaptive Critical Value for Constrained Likelihood Ratio Testing</title>
    <summary>  We present a new way of testing ordered hypotheses against all alternatives
which overpowers the classical approach both in simplicity and statistical
power. Our new method tests the constrained likelihood ratio statistic against
the quantile of one and only one chi-squared random variable with a
data-dependent degrees of freedom instead of a mixture of chi-squares. Our new
test is proved to have a valid finite-sample significance level $\alpha$ and
provides more power especially for sparse alternatives (those with a few or
moderate number of null constraints violations) in comparison to the classical
approach. Our method is also easier to use than the classical approach which
requires to calculate or simulate a set of complicated weights. Two special
cases are considered with more details, namely the case of testing orthants
$\mu_1&lt;0, \cdots, \mu_n&lt;0$ and the isotonic case of testing $\mu_1&lt;\mu_2&lt;\mu_3$
against all alternatives. Contours of the difference in power are shown for
these examples showing the interest of our new approach.
</summary>
    <author>
      <name>Diaa Al Mohamad</name>
    </author>
    <author>
      <name>Jelle J. Goeman</name>
    </author>
    <author>
      <name>Erik W. van Zwet</name>
    </author>
    <author>
      <name>Eric A. Cator</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We proved the conjecture from last version. We found out that some
  part of this works was already published in the literature and was made clear
  in the current version. The main text is the first 16 pages. The appendix
  includes other ideas and a part that was already discussed in the literature</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01325v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01325v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03386v1</id>
    <updated>2018-06-25T07:31:55Z</updated>
    <published>2018-06-25T07:31:55Z</published>
    <title>Data-driven pattern identification and outlier detection in time series</title>
    <summary>  We address the problem of data-driven pattern identification and outlier
detection in time series. To this end, we use singular value decomposition
(SVD) which is a well-known technique to compute a low-rank approximation for
an arbitrary matrix. By recasting the time series as a matrix it becomes
possible to use SVD to highlight the underlying patterns and periodicities.
This is done without the need for specifying user-defined parameters. From a
data mining perspective, this opens up new ways of analyzing time series in a
data-driven, bottom-up fashion. However, in order to get correct results, it is
important to understand how the SVD-spectrum of a time series is influenced by
various characteristics of the underlying signal and noise. In this paper, we
have extended the work in earlier papers by initiating a more systematic
analysis of these effects. We then illustrate our findings on some real-life
data.
</summary>
    <author>
      <name>Abdolrahman Khoshrou</name>
    </author>
    <author>
      <name>Eric J. Pauwels</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00636v4</id>
    <updated>2018-06-25T00:47:36Z</updated>
    <published>2018-04-02T17:27:52Z</published>
    <title>Recursive Optimization of Convex Risk Measures: Mean-Semideviation
  Models</title>
    <summary>  We develop and analyze recursive, data-driven, stochastic subgradient methods
for optimizing a new and versatile class of convex risk measures, termed here
as mean-semideviations. Their construction relies on on the concept of a risk
regularizer, a one-dimensional nonlinear map with certain properties,
essentially generalizing the positive part weighting function in the
mean-upper-semideviation risk measure. After we formally introduce
mean-semideviations, we study their basic properties, and we present a
fundamental constructive characterization result, demonstrating their
generality.
  We then introduce and rigorously analyze the MESSAGEp algorithm, an efficient
stochastic subgradient procedure for iteratively solving convex
mean-semideviation risk-averse problems to optimality. The MESSAGEp algorithm
may be derived as an application of the T-SCGD algorithm of (Yang et al.,
2018). However, the generic theoretical framework of (Yang et al., 2018) is
narrow and structurally restrictive, as far as optimization of
mean-semideviations is concerned, including the classical
mean-upper-semideviation risk measure. By exploiting problem structure, we
propose a substantially weaker set of assumptions, under which we establish
pathwise convergence of the MESSAGEp algorithm, under the same strong sense as
in (Yang et al., 2018). The new framework reveals a fundamental trade-off
between the expansiveness of the random position function and the smoothness of
the particular mean-semideviation risk measure under consideration. Further, we
explicitly show that the class of mean-semideviation problems supported under
our framework is strictly larger than the respective class of problems
supported in (Yang et al., 2018). Thus, applicability of compositional
stochastic optimization is established for a strictly wider spectrum of
mean-semideviation problems, justifying the purpose of this work.
</summary>
    <author>
      <name>Dionysios S. Kalogerias</name>
    </author>
    <author>
      <name>Warren B. Powell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">78 pages, 3 figures, working paper. Update: Fixed some nonsense</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00636v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00636v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07265v2</id>
    <updated>2018-06-25T00:02:26Z</updated>
    <published>2017-12-19T23:50:49Z</published>
    <title>Model-based curve registration via stochastic approximation EM algorithm</title>
    <summary>  Functional data often exhibit both amplitude and phase variation around a
common base shape, with phase variation represented by a so called warping
function. The process removing phase variation by curve alignment and inference
of the warping functions is referred to as curve registration. When functional
data are observed with substantial noise, model-based methods can be employed
for simultaneous smoothing and curve registration. However, the nonlinearity of
the model often renders the inference computationally challenging. In this
paper, we propose an alternative method for model-based curve registration
which is computationally more stable and efficient than existing approaches in
the literature. We apply our method to the analysis of elephant seal dive
profiles and show that more intuitive groupings can be obtained by clustering
on phase variations via the predicted warping functions.
</summary>
    <author>
      <name>Eric Fu</name>
    </author>
    <author>
      <name>Nancy Heckman</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07265v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07265v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05554v3</id>
    <updated>2018-06-24T15:52:36Z</updated>
    <published>2018-03-15T00:53:25Z</published>
    <title>Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models</title>
    <summary>  Learning a Bayesian network (BN) from data can be useful for decision-making
or discovering causal relationships. However, traditional methods often fail in
modern applications, which exhibit a larger number of observed variables than
data points. The resulting uncertainty about the underlying network as well as
the desire to incorporate prior information recommend a Bayesian approach to
learning the BN, but the highly combinatorial structure of BNs poses a striking
challenge for inference. The current state-of-the-art methods such as order
MCMC are faster than previous methods but prevent the use of many natural
structural priors and still have running time exponential in the maximum
indegree of the true directed acyclic graph (DAG) of the BN. We here propose an
alternative posterior approximation based on the observation that, if we
incorporate empirical conditional independence tests, we can focus on a
high-probability DAG associated with each order of the vertices. We show that
our method allows the desired flexibility in prior specification, removes
timing dependence on the maximum indegree and yields provably good posterior
approximations; in addition, we show that it achieves superior accuracy,
scalability, and sampler mixing on several datasets.
</summary>
    <author>
      <name>Raj Agrawal</name>
    </author>
    <author>
      <name>Tamara Broderick</name>
    </author>
    <author>
      <name>Caroline Uhler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 30th International Conference on Machine Learning.
  2018, to appear. 16 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05554v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05554v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09611v1</id>
    <updated>2018-06-24T13:47:57Z</updated>
    <published>2018-06-24T13:47:57Z</published>
    <title>Robustness of deepest projection regression functional</title>
    <summary>  Depth notions in regression have been systematically proposed and examined in
Zuo (2018). One of the prominent advantages of notion of depth is that it can
be directly utilized to introduce median-type deepest estimating functionals
(or estimators in empirical distribution case) for location or regression
parameters in a multi-dimensional setting.
  Regression depth shares the advantage. Depth induced deepest estimating
functionals are expected to inherit desirable and inherent robustness
properties ( e.g. bounded maximum bias and influence function and high
breakdown point) as their univariate location counterpart does. Investigating
and verifying the robustness of the deepest projection estimating functional
(in terms of maximum bias, asymptotic and finite sample breakdown point, and
influence function) is the major goal of this article.
  It turns out that the deepest projection estimating functional possesses a
bounded influence function and the best possible asymptotic breakdown point as
well as the best finite sample breakdown point with robust choice of its
univariate regression and scale component.
</summary>
    <author>
      <name>Yijun Zuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1805.02046</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62G05, Secondary 62G08, 62G35, 62G30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02046v2</id>
    <updated>2018-06-24T13:37:02Z</updated>
    <published>2018-05-05T11:55:23Z</published>
    <title>On general notions of depth for regression</title>
    <summary>  Depth notions in location have fascinated tremendous attention in the
literature. In fact data depth and its applications remain one of the most
active research topics in statistics in the last two decades. Most favored
notions of depth in location include Tukey (1975) halfspace depth (HD), Liu
(1990) simplicial depth, and projection depth (Stahel (1981) and Donoho (1982),
Liu (1992), Zuo and Serfling (2000) (ZS00) and Zuo (2003)), among others.
  Depth notions in regression have also been proposed, sporadically
nevertheless. Regression depth (RD) of Rousseeuw and Hubert (1999) (RH99) is
the most famous one which is a direct extension of Tukey HD to regression.
Others include Carrizosa (1996) and the ones induced from Marrona and Yohai
(1993) (MY93) proposed in this article. Is there any relationship between
Carrizosa depth and RD of RH99? Do these depth notions possess desirable
properties? What are the desirable properties? Can existing notions really
serve as depth functions in regression? These questions remain open.
  Revealing the equivalence between Carrizosa depth and RD of RH99; expanding
location depth evaluating criteria in ZS00 for regression depth notions;
examining the existing regression notions with respect to the gauges; and
proposing the regression counterpart of the eminent projection depth in
location are the four major objectives of the article.
</summary>
    <author>
      <name>Yijun Zuo</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02046v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02046v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G09 (Primary), 6262G05, 62G15, 62G20 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09043v1</id>
    <updated>2018-06-23T21:54:10Z</updated>
    <published>2018-06-23T21:54:10Z</published>
    <title>A breakpoint detection in the mean model with heterogeneous variance on
  fixed time-intervals</title>
    <summary>  This work is motivated by an application for the homogeneization of
GNSS-derived IWV (Integrated Water Vapour) series. Indeed, these GPS series are
affected by abrupt changes due to equipment changes or environemental effects.
The detection and correction of the series from these changes is a crucial step
before any use for climate studies. In addition to these abrupt changes, it has
been observed in the series a non-stationary of the variability. We propose in
this paper a new segmentation model that is a breakpoint detection in the mean
model of a Gaussian process with heterogeneous variance on known
time-intervals. In this segmentation case, the dynamic programming (DP)
algorithm used classically to infer the breakpoints can not be applied anymore.
We propose a procedure in two steps: we first estimate robustly the variances
and then apply the classical inference by plugging these estimators. The
performance of our proposed procedure is assessed through simulation
experiments. An application to real GNSS data is presented.
</summary>
    <author>
      <name>Olivier Bock</name>
    </author>
    <author>
      <name>Xavier Collilieux</name>
    </author>
    <author>
      <name>François Guillamon</name>
    </author>
    <author>
      <name>Emilie Lebarbier</name>
    </author>
    <author>
      <name>Claire Pascal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05, 62M10, 62P12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09000v1</id>
    <updated>2018-06-23T16:25:28Z</updated>
    <published>2018-06-23T16:25:28Z</published>
    <title>On Markov chain Monte Carlo for sparse and filamentary distributions</title>
    <summary>  A novel strategy that combines a given collection of reversible Markov
kernels is proposed. It consists in a Markov chain that moves, at each
iteration, according to one of the available Markov kernels selected via a
state-dependent probability distribution which is thus dubbed locally informed.
In contrast to random-scan approaches that assume a constant selection
probability distribution, the state-dependent distribution is typically
specified so as to privilege moving according to a kernel which is relevant for
the local topology of the target distribution.
  The second contribution is to characterize situations where a locally
informed strategy should be preferred to its random-scan counterpart. We find
that for a specific class of target distribution, referred to as sparse and
filamentary, that exhibits a strong correlation between some variables and/or
which concentrates its probability mass on some low dimensional linear
subspaces or on thinned curved manifolds, a locally informed strategy converges
substantially faster and yields smaller asymptotic variances than an equivalent
random-scan algorithm.
  The research is at this stage essentially speculative: this paper combines a
series of observations on this topic, both theoretical and empirical, that
could serve as a groundwork for further investigations.
</summary>
    <author>
      <name>Florian Maire</name>
    </author>
    <author>
      <name>Pierre Vandekerkhove</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J10, 60J20, 60J22, 65C40, 65C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08911v1</id>
    <updated>2018-06-23T05:39:11Z</updated>
    <published>2018-06-23T05:39:11Z</published>
    <title>Overlapping Sliced Inverse Regression for Dimension Reduction</title>
    <summary>  Sliced inverse regression (SIR) is a pioneer tool for supervised dimension
reduction. It identifies the effective dimension reduction space, the subspace
of significant factors with intrinsic lower dimensionality. In this paper, we
propose to refine the SIR algorithm through an overlapping slicing scheme. The
new algorithm, called overlapping sliced inverse regression (OSIR), is able to
estimate the effective dimension reduction space and determine the number of
effective factors more accurately. We show that such overlapping procedure has
the potential to identify the information contained in the derivatives of the
inverse regression curve, which helps to explain the superiority of OSIR. We
also prove that OSIR algorithm is $\sqrt n $-consistent and verify its
effectiveness by simulations and real applications.
</summary>
    <author>
      <name>Ning Zhang</name>
    </author>
    <author>
      <name>Zhou Yu</name>
    </author>
    <author>
      <name>Qiang Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1806.08911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04312v4</id>
    <updated>2018-06-23T04:45:44Z</updated>
    <published>2017-05-11T17:57:40Z</published>
    <title>FDR-Corrected Sparse Canonical Correlation Analysis with Applications to
  Imaging Genomics</title>
    <summary>  Reducing the number of false discoveries is presently one of the most
pressing issues in the life sciences. It is of especially great importance for
many applications in neuroimaging and genomics, where datasets are typically
high-dimensional, which means that the number of explanatory variables exceeds
the sample size. The false discovery rate (FDR) is a criterion that can be
employed to address that issue. Thus it has gained great popularity as a tool
for testing multiple hypotheses. Canonical correlation analysis (CCA) is a
statistical technique that is used to make sense of the cross-correlation of
two sets of measurements collected on the same set of samples (e.g., brain
imaging and genomic data for the same mental illness patients), and sparse CCA
extends the classical method to high-dimensional settings. Here we propose a
way of applying the FDR concept to sparse CCA, and a method to control the FDR.
The proposed FDR correction directly influences the sparsity of the solution,
adapting it to the unknown true sparsity level. Theoretical derivation as well
as simulation studies show that our procedure indeed keeps the FDR of the
canonical vectors below a user-specified target level. We apply the proposed
method to an imaging genomics dataset from the Philadelphia Neurodevelopmental
Cohort. Our results link the brain connectivity profiles derived from brain
activity during an emotion identification task, as measured by functional
magnetic resonance imaging (fMRI), to the corresponding subjects' genomic data.
</summary>
    <author>
      <name>Alexej Gossmann</name>
    </author>
    <author>
      <name>Pascal Zille</name>
    </author>
    <author>
      <name>Vince Calhoun</name>
    </author>
    <author>
      <name>Yu-Ping Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">- Clarification of the definition of FDR for CCA in Section III;
  results unchanged. - Corrected typos. - Added IEEE copyright notice for the
  accepted article</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04312v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04312v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03719v4</id>
    <updated>2018-06-23T02:14:00Z</updated>
    <published>2018-05-09T20:19:25Z</published>
    <title>Parameter estimation for high dimensional change point regression models
  without grid search</title>
    <summary>  We propose an L1 regularized estimator for the parameters of a high
dimensional change point regression model and provide the corresponding rates
of convergence for the regression as well change point estimates. Importantly,
the computational cost of our estimator is 2Lasso(n,p), where Lasso(n,p)
represents the computational burden of one Lasso optimization. In comparison,
existing grid search based approaches to this problem require a computational
cost of at least nLasso(n,p) optimizations. We work under a subgaussian random
design where the underlying assumptions in our study are milder than those
currently assumed in the high dimensional change point regression literature.
We allow the true change point parameter $\tau_{0n}$ to possibly move to the
boundaries of its parametric space, and the jump size $\|\beta_0-\gamma_0\|_2$
to possibly diverge as $n$ increases. We also characterize the corresponding
effects of these quantities on the rates of convergence of the regression and
change point estimates. Simulations are performed to empirically evaluate
performance of the proposed estimators. The methodology is applied to community
level socio-economic data of the U.S., collected from the 1990 U.S. census and
other sources.
</summary>
    <author>
      <name>Abhishek Kaul</name>
    </author>
    <author>
      <name>Venkata K. Jandhyala</name>
    </author>
    <author>
      <name>Stergios B. Fotopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03719v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03719v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06021v3</id>
    <updated>2018-06-23T00:42:55Z</updated>
    <published>2017-10-16T22:46:23Z</published>
    <title>Estimating reducible stochastic differential equations by conversion to
  a least-squares problem</title>
    <summary>  Stochastic differential equations (SDEs) are increasingly used in
longitudinal data analysis, compartmental models, growth modelling, and other
applications in a number of disciplines. Parameter estimation, however,
currently requires specialized software packages that can be difficult to use
and understand. This work develops and demonstrates an approach for estimating
reducible SDEs using standard nonlinear least squares or mixed-effects
software. Reducible SDEs are obtained through a change of variables in linear
SDEs, and are sufficiently flexible for modelling many situations. The approach
is based on extending a known technique that converts maximum likelihood
estimation for a Gaussian model with a nonlinear transformation of the
dependent variable into an equivalent least-squares problem. A similar idea can
be used for Bayesian maximum a posteriori estimation. It is shown how to obtain
parameter estimates for reducible SDEs containing both process and observation
noise, including hierarchical models with either fixed or random group
parameters. Code and examples in R are given. Univariate SDEs are discussed in
detail, with extensions to the multivariate case outlined more briefly. The use
of well tested and familiar standard software should make SDE modelling more
transparent and accessible. Keywords: stochastic processes; longitudinal data;
growth curves; compartmental models; mixed-effects; R
</summary>
    <author>
      <name>Oscar García</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06021v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06021v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01401v2</id>
    <updated>2018-06-22T22:12:45Z</updated>
    <published>2018-06-04T21:31:48Z</published>
    <title>On estimation and inference in latent structure random graphs</title>
    <summary>  We define a latent structure model (LSM) random graph as a random dot product
graph (RDPG) in which the latent position distribution incorporates both
probabilistic and geometric constraints, delineated by a family of underlying
distributions on some fixed Euclidean space, and a structural support
submanifold from which the latent positions for the graph are drawn. For a
one-dimensional latent structure model with known structural support, we show
how spectral estimates of the latent positions of an RDPG can be used for
efficient estimation of the paramaters of the LSM. We describe how to estimate
or learn the structural support in cases where it is unknown, with an
illustrative focus on graphs with latent positions along the Hardy-Weinberg
curve. Finally, we use the latent structure model formulation to test bilateral
homology in the Drosophila connectome.
</summary>
    <author>
      <name>Avanti Athreya</name>
    </author>
    <author>
      <name>Minh Tang</name>
    </author>
    <author>
      <name>Youngser Park</name>
    </author>
    <author>
      <name>Carey E. Priebe</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62FXX, 62GXX, 62HXX, 05CXX" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08807v1</id>
    <updated>2018-06-22T18:16:57Z</updated>
    <published>2018-06-22T18:16:57Z</published>
    <title>Estimating the treatment effect in a subgroup defined by an early
  post-baseline biomarker measurement in randomized clinical trials with
  time-to-event endpoint</title>
    <summary>  Biomarker measurements can be relatively easy and quick to obtain and they
are useful to investigate whether a compound works as intended on a
mechanistic, pharmacological level. In some situations, it is realistic to
assume that patients, whose post-baseline biomarker levels indicate that they
do not sufficiently respond to the drug, are also unlikely to respond on
clinically relevant long term outcomes (such as time-to-event). However the
determination of the treatment effect in the subgroup of patients that
sufficiently respond to the drug according to their biomarker levels is not
straightforward: It is unclear which patients on placebo would have responded
had they been given the treatment, so that naive comparisons between treatment
and placebo will not estimate the treatment effect of interest. The purpose of
this paper is to investigate assumptions necessary to obtain causal conclusions
in such a setting, utilizing the formalism of causal inference. Three
approaches for estimation of subgroup effects will be developed and illustrated
using simulations and a case-study.
</summary>
    <author>
      <name>Björn Bornkamp</name>
    </author>
    <author>
      <name>Georgina Bermann</name>
    </author>
    <link href="http://arxiv.org/abs/1806.08807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09674v2</id>
    <updated>2018-06-22T17:15:43Z</updated>
    <published>2018-05-22T19:49:41Z</published>
    <title>A D-vine copula mixed model for joint meta-analysis and comparison of
  diagnostic tests</title>
    <summary>  For a particular disease there may be two diagnostic tests developed, where
each of the tests is subject to several studies. A quadrivariate generalized
linear mixed model (GLMM) has been recently proposed to joint meta-analyse and
compare two diagnostic tests. We propose a D-vine copula mixed model for joint
meta-analysis and comparison of two diagnostic tests. Our general model
includes the quadrivariate GLMM as a special case and can also operate on the
original scale of sensitivities and specificities. The method allows the direct
calculation of sensitivity and specificity for each test, as well as, the
parameters of the summary receiver operator characteristic (SROC) curve, along
with a comparison between the SROCs of each test. Our methodology is
demonstrated with an extensive simulation study and illustrated by
meta-analysing two examples where 2 tests for the diagnosis of a particular
disease are compared. Our study suggests that there can be an improvement on
GLMM in fit to data since our model can also provide tail dependencies and
asymmetries.
</summary>
    <author>
      <name>Aristidis K. Nikoloulopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1506.03920,
  arXiv:1502.07505</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09397v2</id>
    <updated>2018-06-22T15:12:50Z</updated>
    <published>2018-05-23T19:37:47Z</published>
    <title>Identification in Nonparametric Models for Dynamic Treatment Effects</title>
    <summary>  This paper develops a nonparametric model that represents how sequences of
outcomes and treatment choices influence one another in a dynamic manner. In
this setting, we are interested in identifying the average outcome for
individuals in each period, had a particular treatment sequence been assigned.
The identification of this quantity allows us to identify the average treatment
effects (ATE's) and the ATE's on transitions, as well as the optimal treatment
regimes, namely, the regimes that maximize the (weighted) sum of the average
potential outcomes, possibly less the cost of the treatments. The main
contribution of this paper is to relax the sequential randomization assumption
widely used in the biostatistics literature by introducing a flexible
choice-theoretic framework for a sequence of endogenous treatments. We show
that the parameters of interest are identified under each period's two-way
exclusion restriction, i.e., with instruments excluded from the
outcome-determining process and other exogenous variables excluded from the
treatment-selection process. We also consider partial identification in the
case where the latter variables are not available. Lastly, we extend our
results to a setting where treatments do not appear in every period.
</summary>
    <author>
      <name>Sukjin Han</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09397v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09397v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08456v1</id>
    <updated>2018-06-21T23:43:03Z</updated>
    <published>2018-06-21T23:43:03Z</published>
    <title>Bayesian hierarchical models for SNP discovery from genome-wide
  association studies, a semi-supervised machine learning approach</title>
    <summary>  Genome-wide association studies (GWASs) aim to detect genetic risk factors
for complex human diseases by identifying disease-associated single-nucleotide
polymorphisms (SNPs). SNP-wise approach, the standard method for analyzing
GWAS, tests each SNP individually. Then the P-values are adjusted for multiple
testing. Multiple testing adjustment (purely based on p-values) is
over-conservative and causes lack of power in many GWASs, due to insufficiently
modelling the relationship among SNPs. To address this problem, we propose a
novel method, which borrows information across SNPs by grouping SNPs into three
clusters. We pre-specify the patterns of clusters by minor allele frequencies
of SNPs between cases and controls, and enforce the patterns with prior
distributions. Therefore, compared with the traditional approach, it better
controls false discovery rate (FDR) and shows higher sensitivity, which is
confirmed by our simulation studies. We re-analyzed real data studies on
identifying SNPs associated with severe bortezomib-induced peripheral
neuropathy (BiPN) in patients with multiple myeloma. The original analysis in
the literature failed to identify SNPs after FDR adjustment. Our proposed
method not only detected the reported SNPs after FDR adjustment but also
discovered a novel SNP rs4351714 that has been reported to be related to
multiple myeloma in another study.
</summary>
    <author>
      <name>Yan Xu</name>
    </author>
    <author>
      <name>Li Xing</name>
    </author>
    <author>
      <name>Jessica Su</name>
    </author>
    <author>
      <name>Xuekui Zhang</name>
    </author>
    <author>
      <name>Weiliang Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages for the main manuscript (exclude supplementary data). 2
  figures, 2 tables, 5 supplementary figures, 3 supplementary tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08258v1</id>
    <updated>2018-06-21T14:19:18Z</updated>
    <published>2018-06-21T14:19:18Z</published>
    <title>Subgroup Identification using Covariate Adjusted Interaction Trees</title>
    <summary>  We consider the problem of identifying sub-groups of participants in a
clinical trial that have enhanced treatment effect. Recursive partitioning
methods that recursively partition the covariate space based on some measure of
between groups treatment effect difference are popular for such sub-group
identification. The most commonly used recursive partitioning method, the
classification and regression tree algorithm, first creates a large tree by
recursively partitioning the covariate space using some splitting criteria and
then selects the final tree from all subtrees of the large tree. In the context
of subgroup identification, calculation of the splitting criteria and the
evaluation measure used for final tree selection rely on comparing differences
in means between the treatment and control arm. When covariates are prognostic
for the outcome, covariate adjusted estimators have the ability to improve
efficiency compared to using differences in means between the treatment and
control group. This manuscript develops two covariate adjusted estimators that
can be used to both make splitting decisions and for final tree selection. The
performance of the resulting covariate adjusted recursive partitioning
algorithm is evaluated using simulations and by analyzing a clinical trial that
evaluates if motivational interviews improve treatment engagement for substance
abusers.
</summary>
    <author>
      <name>Jon Arni Steingrimsson</name>
    </author>
    <author>
      <name>Jiabei Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1806.08258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08200v1</id>
    <updated>2018-06-21T12:30:12Z</updated>
    <published>2018-06-21T12:30:12Z</published>
    <title>Mixtures of Experts Models</title>
    <summary>  Mixtures of experts models provide a framework in which covariates may be
included in mixture models. This is achieved by modelling the parameters of the
mixture model as functions of the concomitant covariates. Given their mixture
model foundation, mixtures of experts models possess a diverse range of
analytic uses, from clustering observations to capturing parameter
heterogeneity in cross-sectional data. This chapter focuses on delineating the
mixture of experts modelling framework and demonstrates the utility and
flexibility of mixtures of experts models as an analytic tool.
</summary>
    <author>
      <name>Isobel Claire Gormley</name>
    </author>
    <author>
      <name>Sylvia Frühwirth-Schnatter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A chapter prepared for the forthcoming Handbook of Mixture Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07172v2</id>
    <updated>2018-06-21T11:24:39Z</updated>
    <published>2018-06-19T12:01:29Z</published>
    <title>Surrogate Outcomes and Transportability</title>
    <summary>  Identification of causal effects is one of the most fundamental tasks of
causal inference. We consider a variant of the identifiability problem where a
causal effect of interest is not identifiable from observational data alone but
some experimental data is available for the identification task. This
corresponds to a real-world setting where experiments were conducted on a set
of variables, which we call surrogate outcomes, but the variables of interest
were not measured. This problem is a generalization of identifiability using
surrogate experiments and we label it as surrogate outcome identifiability and
show that the concept of transportability provides a sufficient criteria for
determining surrogate outcome identifiability for a large class of queries.
</summary>
    <author>
      <name>Santtu Tikka</name>
    </author>
    <author>
      <name>Juha Karvanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to International Journal of Approximate Reasoning</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07172v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07172v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08144v1</id>
    <updated>2018-06-21T09:52:08Z</updated>
    <published>2018-06-21T09:52:08Z</published>
    <title>Maximal skewness projections for scale mixtures of skew-normal vectors</title>
    <summary>  Multivariate scale mixtures of skew-normal (SMSN) variables are flexible
models that account for non-normality in multivariate data scenarios by tail
weight assessment and a shape vector representing the asymmetry of the model in
a directional fashion. Its stochastic representation involves a skew-normal
(SN) vector and a non negative mixing scalar variable, independent of the SN
vector, that injects kurtosis into the SMSN model. We address the problem of
finding the maximal skewness projection for vectors that follow a SMSN
distribution; when simple conditions on the moments of the mixing variable are
fulfilled, it can be shown that the direction yielding the maximal skewness is
proportional to the shape vector. This finding stresses the directional nature
of the asymmetry in this class of distributions; it also provides the
theoretical foundations for solving the skewness model based projection pursuit
for SMSN vectors. Some examples that show the validity of our theoretical
findings for the most famous distributions within the SMSN family are also
given. For the sake of completeness we carry out a simulation experiment with
artificial data, which sheds light on the usefulness and implications of our
result in the statistical practice.
</summary>
    <author>
      <name>Jorge M. Arevalillo</name>
    </author>
    <author>
      <name>Hilario Navarro</name>
    </author>
    <link href="http://arxiv.org/abs/1806.08144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06054v3</id>
    <updated>2018-06-21T05:07:15Z</updated>
    <published>2018-02-16T17:59:11Z</published>
    <title>Learning Patterns for Detection with Multiscale Scan Statistics</title>
    <summary>  This paper addresses detecting anomalous patterns in images, time-series, and
tensor data when the location and scale of the pattern is unknown a priori. The
multiscale scan statistic convolves the proposed pattern with the image at
various scales and returns the maximum of the resulting tensor. Scale corrected
multiscale scan statistics apply different standardizations at each scale, and
the limiting distribution under the null hypothesis---that the data is only
noise---is known for smooth patterns. We consider the problem of simultaneously
learning and detecting the anomalous pattern from a dictionary of smooth
patterns and a database of many tensors. To this end, we show that the
multiscale scan statistic is a subexponential random variable, and prove a
chaining lemma for standardized suprema, which may be of independent interest.
Then by averaging the statistics over the database of tensors we can learn the
pattern and obtain Bernstein-type error bounds. We will also provide a
construction of an $\epsilon$-net of the location and scale parameters,
providing a computationally tractable approximation with similar error bounds.
</summary>
    <author>
      <name>James Sharpnack</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06054v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06054v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07921v1</id>
    <updated>2018-06-20T18:35:02Z</updated>
    <published>2018-06-20T18:35:02Z</published>
    <title>Beta seasonal autoregressive moving average models</title>
    <summary>  In this paper we introduce the class of beta seasonal autoregressive moving
average ($\beta$SARMA) models for modeling and forecasting time series data
that assume values in the standard unit interval. It generalizes the class of
beta autoregressive moving average models [Rocha and Cribari-Neto, Test, 2009]
by incorporating seasonal dynamics to the model dynamic structure. Besides
introducing the new class of models, we develop parameter estimation,
hypothesis testing inference, and diagnostic analysis tools. We also discuss
out-of-sample forecasting. In particular, we provide closed-form expressions
for the conditional score vector and for the conditional Fisher information
matrix. We also evaluate the finite sample performances of conditional maximum
likelihood estimators and white noise tests using Monte Carlo simulations. An
empirical application is presented and discussed.
</summary>
    <author>
      <name>Fábio M. Bayer</name>
    </author>
    <author>
      <name>Renato J. Cintra</name>
    </author>
    <author>
      <name>Francisco Cribari-Neto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00949655.2018.1491974</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00949655.2018.1491974" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 5 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Statistical Computation and Simulation, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.07921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10, 62Fxx, 91B84" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06784v2</id>
    <updated>2018-06-20T18:01:26Z</updated>
    <published>2018-06-18T15:47:37Z</published>
    <title>Flexible Collaborative Estimation of the Average Causal Effect of a
  Treatment using the Outcome-Highly-Adaptive Lasso</title>
    <summary>  Many estimators of the average causal effect of an intervention require
estimation of the propensity score, the outcome regression, or both. For these
estimators, we must carefully con- sider how to estimate the relevant
regressions. It is often beneficial to utilize flexible techniques such as
semiparametric regression or machine learning. However, optimal estimation of
the regression function does not necessarily lead to optimal estimation of the
average causal effect. Therefore, it is important to consider criteria for
evaluating regression estimators and selecting hyper-parameters. A recent
proposal addressed these issues via the outcome-adaptive lasso, a penalized
regression technique for estimating the propensity score. We build on this
proposal and offer a method that is simultaneously more flexible and more
efficient than the previous pro- posal. We propose the outcome-highly-adaptive
LASSO, a semi-parametric regression estimator designed to down-weight regions
of the confounder space that do not contribute variation to the outcome
regression. We show that tuning this method using collaborative targeted
learning leads to superior finite-sample performance relative to competing
estimators.
</summary>
    <author>
      <name>Cheng Ju</name>
    </author>
    <author>
      <name>David Benkeser</name>
    </author>
    <author>
      <name>Mark J. van der Laan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04793v2</id>
    <updated>2018-06-20T15:40:05Z</updated>
    <published>2017-11-13T19:02:26Z</published>
    <title>Improved Density and Distribution Function Estimation</title>
    <summary>  Given additional distributional information in the form of moment
restrictions, kernel density and distribution function estimators with implied
generalised empirical likelihood probabilities as weights achieve a reduction
in variance due to the systematic use of this extra information. The particular
interest here is the estimation of densities or distributions of (generalised)
residuals in semi-parametric models defined by a finite number of moment
restrictions. Such estimates are of great practical interest, being potentially
of use for diagnostic purposes, including tests of parametric assumptions on an
error distribution, goodness-of-fit tests or tests of overidentifying moment
restrictions. The paper gives conditions for the consistency and describes the
asymptotic mean squared error properties of the kernel density and distribution
estimators proposed in the paper. A simulation study evaluates the small sample
performance of these estimators. Supplements provide analytic examples to
illustrate situations where kernel weighting provides a reduction in variance
together with proofs of the results in the paper.
</summary>
    <author>
      <name>Vitaliy Oryshchenko</name>
    </author>
    <author>
      <name>Richard J. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 3 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04793v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04793v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 62G07, secondary 62G05, 62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6156v4</id>
    <updated>2018-06-20T15:16:51Z</updated>
    <published>2013-05-27T09:35:07Z</published>
    <title>Estimating Average Causal Effects Under General Interference, with
  Application to a Social Network Experiment</title>
    <summary>  This paper presents a randomization-based framework for estimating causal
effects under interference between units, motivated by challenges that arise in
analyzing experiments on social networks. The framework integrates three
components: (i) an experimental design that defines the probability
distribution of treatment assignments, (ii) a mapping that relates experimental
treatment assignments to exposures received by units in the experiment, and
(iii) estimands that make use of the experiment to answer questions of
substantive interest. We develop the case of estimating average unit-level
causal effects from a randomized experiment with interference of arbitrary but
known form. The resulting estimators are based on inverse probability
weighting. We provide randomization-based variance estimators that account for
the complex clustering that can occur when interference is present. We also
establish consistency and asymptotic normality under local dependence
assumptions. We discuss refinements including covariate-adjusted effect
estimators and ratio estimation. We evaluate empirical performance in realistic
settings with a naturalistic simulation using social network data from American
schools. We then present results from a field experiment on the spread of
anti-conflict norms and behavior among school students.
</summary>
    <author>
      <name>Peter M. Aronow</name>
    </author>
    <author>
      <name>Cyrus Samii</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/16-AOAS1005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/16-AOAS1005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Aronow, Peter M.; Samii, Cyrus. Estimating average causal effects
  under general interference, with application to a social network experiment.
  Ann. Appl. Stat. 11 (2017), no. 4, 1912--1947</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.6156v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6156v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07, 62G05, 62P25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02097v2</id>
    <updated>2018-06-20T15:03:05Z</updated>
    <published>2018-04-06T01:02:25Z</published>
    <title>Multi-view Banded Spectral Clustering with Application to ICD9
  Clustering</title>
    <summary>  Despite recent development in methodology, community detection remains a
challenging problem. Existing literature largely focuses on the standard
setting where a network is learned using an observed adjacency matrix from a
single data source. Constructing a shared network from multiple data sources is
more challenging due to the heterogeneity across populations. Additionally, no
existing method leverages the prior distance knowledge available in many
domains to help the discovery of the network structure. To bridge this gap, in
this paper we propose a novel spectral clustering method that optimally
combines multiple data sources while leveraging the prior distance knowledge.
The proposed method combines a banding step guided by the distance knowledge
with a subsequent weighting step to maximize consensus across multiple sources.
Its statistical performance is thoroughly studied under a multi-view stochastic
block model. We also provide a simple yet optimal rule of choosing weights in
practice. The efficacy and robustness of the method is fully demonstrated
through extensive simulations. Finally, we apply the method to cluster the
International classification of diseases, ninth revision (ICD9), codes and
yield a very insightful clustering structure by integrating information from a
large claim database and two healthcare systems.
</summary>
    <author>
      <name>Luwan Zhang</name>
    </author>
    <author>
      <name>Katherine Liao</name>
    </author>
    <author>
      <name>Issac Kohane</name>
    </author>
    <author>
      <name>Tianxi Cai</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02097v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02097v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04991v3</id>
    <updated>2018-06-20T13:28:54Z</updated>
    <published>2018-03-13T18:09:12Z</published>
    <title>Inference on a Distribution from Noisy Draws</title>
    <summary>  We consider a situation where the distribution of a random variable is being
estimated by the empirical distribution of noisy measurements of the random
variable. This is common practice in many settings, including the evaluation of
teacher value-added and the assessment of firm efficiency through
stochastic-frontier models. We use an asymptotic embedding where the noise
shrinks with the sample size to calculate the leading bias in the empirical
distribution arising from the presence of noise. Analytical and jackknife
corrections for the empirical distribution are derived that recenter the limit
distribution and yield confidence intervals with correct coverage in large
samples. A similar adjustment is also presented for the quantile function.
These corrections are non-parametric and easy to implement. Our approach can be
connected to corrections for selection bias and shrinkage estimation and is to
be contrasted with deconvolution. Simulation results confirm the much improved
sampling behavior of the corrected estimators. An empirical illustration on the
estimation of a stochastic-frontier model is also provided.
</summary>
    <author>
      <name>Koen Jochmans</name>
    </author>
    <author>
      <name>Martin Weidner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages main text, 23 pages supplementary appendix, CeMMAP Working
  Paper CWP14/18 (earlier version)</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04991v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04991v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10440v2</id>
    <updated>2018-06-20T11:02:09Z</updated>
    <published>2017-11-28T18:01:06Z</published>
    <title>On the correspondence of deviances and maximum likelihood and interval
  estimates from log-linear to logistic regression modelling</title>
    <summary>  Consider a set of categorical variables $\mathcal{P}$ where at least one,
denoted by $Y$, is binary. The log-linear model that describes the counts in
the resulting contingency table implies a specific logistic regression model,
with the binary variable as the outcome. Extending results in Christensen
(1997), by also considering the case where factors present in the contingency
table disappear from the logistic regression model, we prove that the Maximum
Likelihood Estimate (MLE) for the parameters of the logistic regression equals
the MLE for the corresponding parameters of the log-linear model. We prove
that, asymptotically, standard errors for the two sets of parameters are also
equal. Subsequently, Wald confidence intervals are asymptotically equal. These
results demonstrate the extent to which inferences from the log-linear
framework can be translated to inferences within the logistic regression
framework, on the magnitude of main effects and interactions. Finally, we prove
that the deviance of the log-linear model is equal to the deviance of the
corresponding logistic regression, provided that the latter is fitted to a
dataset where no cell observations are merged when one or more factors in
$\mathcal{P} \setminus \{ Y \}$ become obsolete. We illustrate the derived
results with the analysis of a real dataset.
</summary>
    <author>
      <name>Wei Jing</name>
    </author>
    <author>
      <name>Michail Papathomas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10440v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10440v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05880v4</id>
    <updated>2018-06-20T03:07:52Z</updated>
    <published>2015-08-24T16:51:15Z</published>
    <title>Scalable Bayes via Barycenter in Wasserstein Space</title>
    <summary>  Divide-and-conquer based methods for Bayesian inference provide a general
approach for tractable posterior inference when the sample size is large. These
methods divide the data into smaller subsets, sample from the posterior
distribution of parameters in parallel on all the subsets, and combine
posterior samples from all the subsets to approximate the full data posterior
distribution. The smaller size of any subset compared to the full data implies
that posterior sampling on any subset is computationally more efficient than
sampling from the true posterior distribution. Since the combination step takes
negligible time relative to sampling, posterior computations can be scaled to
massive data by dividing the full data into a sufficiently large number of data
subsets. One such approach relies on the geometry of posterior distributions
estimated across different subsets and combines them through their barycenter
in a Wasserstein space of probability measures. We provide theoretical
guarantees on the accuracy of approximation that are valid in many
applications. We show that the geometric method approximates the full data
posterior distribution better than its competitors across diverse simulations
and reproduces known results when applied to a movie ratings database.
</summary>
    <author>
      <name>Sanvesh Srivastava</name>
    </author>
    <author>
      <name>Cheng Li</name>
    </author>
    <author>
      <name>David B. Dunson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 7 figures, and 11 tables. The updated revision will appear
  in JMLR</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05880v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05880v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1158v3</id>
    <updated>2018-06-20T03:07:43Z</updated>
    <published>2014-07-04T09:09:55Z</published>
    <title>Expandable Factor Analysis</title>
    <summary>  Bayesian sparse factor models have proven useful for characterizing
dependence in multivariate data, but scaling computation to large numbers of
samples and dimensions is problematic. We propose expandable factor analysis
for scalable inference in factor models when the number of factors is unknown.
The method relies on a continuous shrinkage prior for efficient maximum a
posteriori estimation of a low-rank and sparse loadings matrix. The structure
of the prior leads to an estimation algorithm that accommodates uncertainty in
the number of factors. We propose an information criterion to select the
hyperparameters of the prior. Expandable factor analysis has better false
discovery rates and true positive rates than its competitors across diverse
simulations. We apply the proposed approach to a gene expression study of aging
in mice, illustrating superior results relative to four competing methods.
</summary>
    <author>
      <name>Sanvesh Srivastava</name>
    </author>
    <author>
      <name>Barbara E. Engelhardt</name>
    </author>
    <author>
      <name>David B. Dunson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/biomet/asx030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/biomet/asx030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Biometrika. vol. 104. number 3. pp. 649-663. 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.1158v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1158v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07750v2</id>
    <updated>2018-06-20T02:09:07Z</updated>
    <published>2017-12-20T23:56:18Z</published>
    <title>Approximate Bayesian Forecasting</title>
    <summary>  Approximate Bayesian Computation (ABC) has become increasingly prominent as a
method for conducting parameter inference in a range of challenging statistical
problems, most notably those characterized by an intractable likelihood
function. In this paper, we focus on the use of ABC not as a tool for
parametric inference, but as a means of generating probabilistic forecasts; or
for conducting what we refer to as `approximate Bayesian forecasting'. The four
key issues explored are: i) the link between the theoretical behavior of the
ABC posterior and that of the ABC-based predictive; ii) the use of proper
scoring rules to measure the (potential) loss of forecast accuracy when using
an approximate rather than an exact predictive; iii) the performance of
approximate Bayesian forecasting in state space models; and iv) the use of
forecasting criteria to inform the selection of ABC summaries in empirical
settings. The primary finding of the paper is that ABC can provide a
computationally efficient means of generating probabilistic forecasts that are
nearly identical to those produced by the exact predictive, and in a fraction
of the time required to produce predictions via an exact method. y identical to
those produced by the exact predictive, and in a fraction of the time required
to produce predictions via an exact method.
</summary>
    <author>
      <name>David T. Frazier</name>
    </author>
    <author>
      <name>Worapree Maneesoonthorn</name>
    </author>
    <author>
      <name>Gael M. Martin</name>
    </author>
    <author>
      <name>Brendan P. M. McCabe</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07750v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07750v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07422v1</id>
    <updated>2018-06-19T18:48:18Z</updated>
    <published>2018-06-19T18:48:18Z</published>
    <title>Doubly Robust Estimation in Observational Studies with Partial
  Interference</title>
    <summary>  Interference occurs when the treatment (or exposure) of one individual
affects the outcomes of others. In some settings it may be reasonable to assume
individuals can be partitioned into clusters such that there is no interference
between individuals in different clusters, i.e., there is partial interference.
In observational studies with partial interference, inverse probability
weighted (IPW) estimators have been proposed of different possible treatment
effects. However, the validity of IPW estimators depends on the propensity
score being known or correctly modeled. Alternatively, one can estimate the
treatment effect using an outcome regression model. In this paper, we propose
doubly robust (DR) estimators which utilize both models and are consistent and
asymptotically normal if either model, but not necessarily both, is correctly
specified. Empirical results are presented to demonstrate the DR property of
the proposed estimators, as well as the efficiency gain of DR over IPW
estimators when both models are correctly specified. The different estimators
are illustrated using data from a study examining the effects of cholera
vaccination in Bangladesh.
</summary>
    <author>
      <name>Lan Liu</name>
    </author>
    <author>
      <name>Michael G. Hudgens</name>
    </author>
    <author>
      <name>Bradley Saul</name>
    </author>
    <author>
      <name>John D. Clemens</name>
    </author>
    <author>
      <name>Mohammad Ali</name>
    </author>
    <author>
      <name>Michael E. Emch</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06826v2</id>
    <updated>2018-06-19T16:24:28Z</updated>
    <published>2018-05-17T15:39:17Z</published>
    <title>The Blessings of Multiple Causes</title>
    <summary>  Causal inference from observational data often assumes "strong ignorability,"
that all confounders are observed. This assumption is standard yet untestable.
However, many scientific studies involve multiple causes, different variables
whose effects are simultaneously of interest. We propose the deconfounder, an
algorithm that combines unsupervised machine learning and predictive model
checking to perform causal inference in multiple-cause settings. The
deconfounder infers a latent variable as a substitute for unobserved
confounders and then uses that substitute to perform causal inference. We
develop theory for when the deconfounder leads to unbiased causal estimates,
and show that it requires weaker assumptions than classical causal inference.
We analyze its performance in three types of studies: semi-simulated data
around smoking and lung cancer, semi-simulated data around genomewide
association studies, and a real dataset about actors and movie revenue. The
deconfounder provides a checkable approach to estimating close-to-truth causal
effects.
</summary>
    <author>
      <name>Yixin Wang</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">59 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06826v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06826v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07320v1</id>
    <updated>2018-06-19T16:01:48Z</updated>
    <published>2018-06-19T16:01:48Z</published>
    <title>Simultaneous Signal Subspace Rank and Model Selection with an
  Application to Single-snapshot Source Localization</title>
    <summary>  This paper proposes a novel method for model selection in linear regression
by utilizing the solution path of $\ell_1$ regularized least-squares (LS)
approach (i.e., Lasso). This method applies the complex-valued least angle
regression and shrinkage (c-LARS) algorithm coupled with a generalized
information criterion (GIC) and referred to as the c-LARS-GIC method.
c-LARS-GIC is a two-stage procedure, where firstly precise values of the
regularization parameter, called knots, at which a new predictor variable
enters (or leaves) the active set are computed in the Lasso solution path.
Active sets provide a nested sequence of regression models and GIC then selects
the best model. The sparsity order of the chosen model serves as an estimate of
the model order and the LS fit based only on the active set of the model
provides an estimate of the regression parameter vector. We then consider a
source localization problem, where the aim is to detect the number of impinging
source waveforms at a sensor array as well to estimate their
direction-of-arrivals (DoA-s) using only a single-snapshot measurement. We
illustrate via simulations that, after formulating the problem as a grid-based
sparse signal reconstruction problem, the proposed c-LARS-GIC method detects
the number of sources with high probability while at the same time it provides
accurate estimates of source locations.
</summary>
    <author>
      <name>Muhammad Naveed Tabassum</name>
    </author>
    <author>
      <name>Esa Ollila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, To appear in the Proceedings of the 26th European
  Signal Processing Conference (EUSIPCO 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06240v2</id>
    <updated>2018-06-19T15:38:42Z</updated>
    <published>2017-02-21T02:23:41Z</published>
    <title>Simultaneous Inference for Best Linear Predictor of the Conditional
  Average Treatment Effect and Other Structural Functions</title>
    <summary>  This paper provides estimation and inference methods for a structural
function, such as Conditional Average Treatment Effect (CATE), based on modern
machine learning (ML) tools. We assume that such function can be represented as
an expectation g(x) of a signal Y conditional on X that depends on an unknown
nuisance function. In addition to CATE, examples of such functions include
regression function with Partially Missing Outcome and Conditional Average
Partial Derivative. We approximate g(x) by a linear form that is a product of a
vector of the approximating basis functions p(x) and the Best Linear Predictor
(BLP), which we refer to a pseudo-target. Plugging in the first-stage estimate
of the nuisance function into the signal, we estimate BLP via ordinary least
squares. We deliver a high-quality estimate of the pseudo-target function that
features (a) a pointwise Gaussian approximation, (b) a simultaneous Gaussian
approximation, and (c) optimal rate of simultaneous convergence. In the case,
the misspecification error of the linear form decays sufficiently fast, these
approximations automatically hold for the target function g(x) instead of a
pseudo-target. The first stage nuisance parameter is allowed to be
high-dimensional and is estimated by modern ML tools, such as neural networks,
shrinkage estimators, and random forest. Using our method, we estimate the
average price elasticity conditional on income using Yatchew and No (2001) data
and provide simultaneous confidence bands for the target regression function.
</summary>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Vira Semenova</name>
    </author>
    <link href="http://arxiv.org/abs/1702.06240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06393v3</id>
    <updated>2018-06-19T14:38:08Z</updated>
    <published>2017-11-17T04:14:00Z</published>
    <title>A Unified Method for Accurate Inference in Random-effects Meta-analysis</title>
    <summary>  Random-effects meta-analyses have been widely applied in evidence synthesis
for various types of medical studies to adequately address between-studies
heterogeneity. However, standard inference methods for average treatment
effects (e.g., restricted maximum likelihood estimation) usually underestimate
statistical errors and possibly provide highly overconfident results under
realistic situations; for instance, coverage probabilities of confidence
intervals can be substantially below the nominal level. The main reason is that
these inference methods rely on large sample approximations even though the
number of synthesized studies is usually small or moderate in practice. In this
article we solve this problem using a unified inference method based on the
Monte Carlo conditioning method for broad application to random-effects
meta-analysis. The developed method provides accurate confidence intervals with
coverage probabilities that are almost the same as the nominal level. As
specific applications, we provide accurate inference procedures for three types
of meta-analysis: conventional univariate meta-analysis for pairwise treatment
comparisons, meta-analysis of diagnostic test accuracy, and multiple treatment
comparisons via network meta-analysis. We also illustrate the practical
effectiveness of these methods via real data applications.
</summary>
    <author>
      <name>Shonosuke Sugasawa</name>
    </author>
    <author>
      <name>Hisashi Noma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07274v1</id>
    <updated>2018-06-19T14:22:29Z</updated>
    <published>2018-06-19T14:22:29Z</published>
    <title>Efficient data augmentation for multivariate probit models with panel
  data: An application to general practitioner decision-making about
  contraceptives</title>
    <summary>  This article considers the problem of estimating a multivariate probit model
in a panel data setting with emphasis on sampling a high-dimensional
correlation matrix and improving the overall efficiency of the data
augmentation approach. We reparameterise the correlation matrix in a principled
way and then carry out efficient Bayesian inference using Hamiltonian Monte
Carlo. We also propose a novel antithetic variable method to generate samples
from the posterior distribution of the random effects and regression
coefficients, resulting in significant gains in efficiency. We apply the
methodology by analysing stated preference data obtained from Australian
general practitioners evaluating alternative contraceptive products. Our
analysis suggests that the joint probability of discussing long acting
reversible products with a patient shows medical practice variation among the
general practitioners, which indicates some resistance to even discussing these
products, let alone recommending them.
</summary>
    <author>
      <name>Vincent Chin</name>
    </author>
    <author>
      <name>David Gunawan</name>
    </author>
    <author>
      <name>Denzil G. Fiebig</name>
    </author>
    <author>
      <name>Robert Kohn</name>
    </author>
    <author>
      <name>Scott A. Sisson</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07244v1</id>
    <updated>2018-06-19T13:59:29Z</updated>
    <published>2018-06-19T13:59:29Z</published>
    <title>vsgoftest: An Package for Goodness-of-Fit Testing Based on
  Kullback-Leibler Divergence</title>
    <summary>  The R-package vsgoftest performs goodness-of-fit (GOF) tests, based on
Shannon entropy and Kullback-Leibler divergence, developed by Vasicek (1976)
and Song (2002), of various classical families of distributions. The
theoretical framework of the so-called Vasicek-Song (VS) tests is summarized
and followed by a detailed description of the different features of the
package. The power and computational time performances of VS tests are studied
through their comparison with other GOF tests. Application to real datasets
illustrates the easy-to-use functionalities of the vsgoftest package.
</summary>
    <author>
      <name>Justine Lequesne</name>
    </author>
    <author>
      <name>Philippe Regnault</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G07, 62G10, 94A17, 94A15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02502v2</id>
    <updated>2018-06-19T13:20:08Z</updated>
    <published>2018-04-07T03:48:49Z</published>
    <title>Principal Component Analysis: A Natural Approach to Data Exploration</title>
    <summary>  Principal component analysis (PCA) is often used for analyzing data in the
most diverse areas. In this work, we report an integrated approach to several
theoretical and practical aspects of PCA. We start by providing, in an
intuitive and accessible manner, the basic principles underlying PCA and its
applications. Next, we present a systematic, though no exclusive, survey of
some representative works illustrating the potential of PCA applications to a
wide range of areas. An experimental investigation of the ability of PCA for
variance explanation and dimensionality reduction is also developed, which
confirms the efficacy of PCA and also shows that standardizing or not the
original data can have important effects on the obtained results. Overall, we
believe the several covered issues can assist researchers from the most diverse
areas in using and interpreting PCA.
</summary>
    <author>
      <name>Felipe L. Gewers</name>
    </author>
    <author>
      <name>Gustavo R. Ferreira</name>
    </author>
    <author>
      <name>Henrique F. de Arruda</name>
    </author>
    <author>
      <name>Filipi N. Silva</name>
    </author>
    <author>
      <name>Cesar H. Comin</name>
    </author>
    <author>
      <name>Diego R. Amancio</name>
    </author>
    <author>
      <name>Luciano da F. Costa</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02502v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02502v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07176v1</id>
    <updated>2018-06-19T12:13:53Z</updated>
    <published>2018-06-19T12:13:53Z</published>
    <title>Letter to the Editor</title>
    <summary>  Galarza, Lachos and Bandyopadhyay (2017) have recently proposed a method of
estimating linear quantile mixed models (Geraci and Bottai, 2014) based on a
Monte Carlo EM algorithm. They assert that their procedure represents an
improvement over the numerical quadrature and non-smooth optimization approach
implemented by Geraci (2014). The objective of this note is to demonstrate that
this claim is incorrect. We also point out several inaccuracies and
shortcomings in their paper which affect other results and conclusions that can
be drawn.
</summary>
    <author>
      <name>Marco Geraci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2, figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07161v1</id>
    <updated>2018-06-19T11:29:33Z</updated>
    <published>2018-06-19T11:29:33Z</published>
    <title>Identifying Causal Effects with the R Package causaleffect</title>
    <summary>  Do-calculus is concerned with estimating the interventional distribution of
an action from the observed joint probability distribution of the variables in
a given causal structure. All identifiable causal effects can be derived using
the rules of do-calculus, but the rules themselves do not give any direct
indication whether the effect in question is identifiable or not. Shpitser and
Pearl constructed an algorithm for identifying joint interventional
distributions in causal models, which contain unobserved variables and induce
directed acyclic graphs. This algorithm can be seen as a repeated application
of the rules of do-calculus and known properties of probabilities, and it
ultimately either derives an expression for the causal distribution, or fails
to identify the effect, in which case the effect is non-identifiable. In this
paper, the R package causaleffect is presented, which provides an
implementation of this algorithm. Functionality of causaleffect is also
demonstrated through examples.
</summary>
    <author>
      <name>Santtu Tikka</name>
    </author>
    <author>
      <name>Juha Karvanen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18637/jss.v076.i12</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18637/jss.v076.i12" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the version published in the Journal of Statistical Software</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Statistical Software, 76(12):1-30, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.07161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03535v3</id>
    <updated>2018-06-19T11:07:04Z</updated>
    <published>2017-01-13T00:40:44Z</published>
    <title>Fast Bayesian Intensity Estimation for the Permanental Process</title>
    <summary>  The Cox process is a stochastic process which generalises the Poisson process
by letting the underlying intensity function itself be a stochastic process. In
this paper we present a fast Bayesian inference scheme for the permanental
process, a Cox process under which the square root of the intensity is a
Gaussian process. In particular we exploit connections with reproducing kernel
Hilbert spaces, to derive efficient approximate Bayesian inference algorithms
based on the Laplace approximation to the predictive distribution and marginal
likelihood. We obtain a simple algorithm which we apply to toy and real-world
problems, obtaining orders of magnitude speed improvements over previous work.
</summary>
    <author>
      <name>Christian J. Walder</name>
    </author>
    <author>
      <name>Adrian N. Bishop</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 34th International Conference on Machine
  Learning, pages: 3579-3588, 6-11 August 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.03535v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03535v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09413v3</id>
    <updated>2018-06-19T07:00:49Z</updated>
    <published>2016-12-30T07:54:49Z</published>
    <title>Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression</title>
    <summary>  To model categorical response variables given their covariates, we propose a
permuted and augmented stick-breaking (paSB) construction that one-to-one maps
the observed categories to randomly permuted latent sticks. This new
construction transforms multinomial regression into regression analysis of
stick-specific binary random variables that are mutually independent given
their covariate-dependent stick success probabilities, which are parameterized
by the regression coefficients of their corresponding categories. The paSB
construction allows transforming an arbitrary cross-entropy-loss binary
classifier into a Bayesian multinomial one. Specifically, we parameterize the
negative logarithms of the stick failure probabilities with a family of
covariate-dependent softplus functions to construct nonparametric Bayesian
multinomial softplus regression, and transform Bayesian support vector machine
(SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression
models are not only capable of providing probability estimates, quantifying
uncertainty, increasing robustness, and producing nonlinear classification
decision boundaries, but also amenable to posterior simulation. Example results
demonstrate their attractive properties and performance.
</summary>
    <author>
      <name>Quan Zhang</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research, vol. 18, pp. 1-33, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.09413v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09413v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10713v2</id>
    <updated>2018-06-19T03:33:34Z</updated>
    <published>2017-10-29T23:02:26Z</published>
    <title>Bayesian Nonparametric Differential Analysis for Dependent Multigroup
  Data with Application to DNA Methylation Analyses in Cancer</title>
    <summary>  Cancer 'omics datasets involve widely varying sizes and scales, measurement
variables, and correlation structures. An overarching scientific goal in cancer
research is the development of general statistical techniques that can cleanly
sift the signal from the noise in identifying genomic signatures of the disease
across a set of experimental or biological conditions. We propose BayesDiff, a
nonparametric Bayesian approach based on a novel class of first order mixture
models, called the Sticky Poisson-Dirichlet process or multicuisine restaurant
franchise. The BayesDiff methodology flexibly utilizes information from all the
measurements and adaptively accommodates any serial dependence in the data,
accounting for the inter-probe distances, to perform simultaneous inferences on
the variables. The technique is applied to analyze the motivating DNA
methylation gastrointestinal cancer dataset, which displays both serial
correlations and complex interaction patterns. In simulation studies, we
demonstrate the effectiveness of the BayesDiff procedure relative to existing
techniques for differential DNA methylation. Returning to the motivating
dataset, we detect the genomic signature for four types of upper
gastrointestinal cancer. The analysis results support and complement known
features of DNA methylation as well as gene association with gastrointestinal
cancer.
</summary>
    <author>
      <name>Chiyu Gu</name>
    </author>
    <author>
      <name>Veerabhadran Baladandayuthapani</name>
    </author>
    <author>
      <name>Subharup Guha</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10713v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10713v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07016v1</id>
    <updated>2018-06-19T02:33:42Z</updated>
    <published>2018-06-19T02:33:42Z</published>
    <title>Evaluating Ex Ante Counterfactual Predictions Using Ex Post Causal
  Inference</title>
    <summary>  We derive a formal, decision-based method for comparing the performance of
counterfactual treatment regime predictions using the results of experiments
that give relevant information on the distribution of treated outcomes. Our
approach allows us to quantify and assess the statistical significance of
differential performance for optimal treatment regimes estimated from
structural models, extrapolated treatment effects, expert opinion, and other
methods. We apply our method to evaluate optimal treatment regimes for
conditional cash transfer programs across countries where predictions are
generated using data from experimental evaluations in other countries and
pre-program data in the country of interest.
</summary>
    <author>
      <name>Michael Gechter</name>
    </author>
    <author>
      <name>Cyrus Samii</name>
    </author>
    <author>
      <name>Rajeev Dehejia</name>
    </author>
    <author>
      <name>Cristian Pop-Eleches</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06974v1</id>
    <updated>2018-06-18T22:35:36Z</updated>
    <published>2018-06-18T22:35:36Z</published>
    <title>Bayesian monotonic errors-in-variables models with applications to
  pathogen susceptibility testing</title>
    <summary>  Drug dilution (MIC) and disk diffusion (DIA) are the two most common
antimicrobial susceptibility assays used by hospitals and clinics to determine
an unknown pathogen's susceptibility to various antibiotics. Since only one
assay is commonly used, it is important that the two assays give similar
results. Calibration of the DIA assay to the MIC assay is typically done using
the error-rate bounded method, which selects DIA breakpoints that minimize the
observed discrepancies between the two assays. In 2000, Craig proposed a
model-based approach that specifically models the measurement error and
rounding processes of each assay, the underlying pathogen distribution, and the
true monotonic relationship between the two assays. The two assays are then
calibrated by focusing on matching the probabilities of correct classification
(susceptible, indeterminant, and resistant). This approach results in greater
precision and accuracy for estimating DIA breakpoints. In this paper, we expand
the flexibility of the model-based method by introducing a Bayesian
four-parameter logistic model (extending Craig's original three-parameter
model) as well as a Bayesian nonparametric spline model to describe the
relationship between the two assays. We propose two ways to handle spline knot
selection, considering many equally-spaced knots but restricting overfitting
via a random walk prior and treating the number and location of knots as
additional unknown parameters. We demonstrate the two approaches via a series
of simulation studies and apply the methods to two real data sets.
</summary>
    <author>
      <name>Glen DePalma</name>
    </author>
    <author>
      <name>Bruce A. Craig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/sim.7533</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/sim.7533" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics in Medicine. 2018. 37:478-502</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.06974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06959v1</id>
    <updated>2018-06-18T21:30:57Z</updated>
    <published>2018-06-18T21:30:57Z</published>
    <title>High-frequency analysis of parabolic stochastic PDEs</title>
    <summary>  We consider the problem of estimating stochastic volatility for a class of
second-order parabolic stochastic PDEs. Assuming that the solution is observed
at a high temporal frequency, we use limit theorems for multipower variations
and related functionals to construct consistent nonparametric estimators and
asymptotic confidence bounds for the integrated volatility process. As a
byproduct of our analysis, we also obtain feasible estimators for the
regularity of the spatial covariance function of the noise.
</summary>
    <author>
      <name>Carsten Chong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Including supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.10066v2</id>
    <updated>2018-06-18T21:24:54Z</updated>
    <published>2017-09-28T17:13:09Z</published>
    <title>Empirical Bayes Shrinkage and False Discovery Rate Estimation, Allowing
  For Unwanted Variation</title>
    <summary>  We combine two important ideas in the analysis of large-scale genomics
experiments (e.g. experiments that aim to identify genes that are
differentially expressed between two conditions). The first is use of Empirical
Bayes (EB) methods to handle the large number of potentially-sparse effects,
and estimate false discovery rates and related quantities. The second is use of
factor analysis methods to deal with sources of unwanted variation such as
batch effects and unmeasured confounders. We describe a simple modular fitting
procedure that combines key ideas from both these lines of research. This
yields new, powerful EB methods for analyzing genomics experiments that account
for both sparse effects and unwanted variation. In realistic simulations, these
new methods provide significant gains in power and calibration over competing
methods. In real data analysis we find that different methods, while often
conceptually similar, can vary widely in their assessments of statistical
significance. This highlights the need for care in both choice of methods and
interpretation of results. All methods introduced in this paper are implemented
in the R package vicar available at https://github.com/dcgerard/vicar .
</summary>
    <author>
      <name>David Gerard</name>
    </author>
    <author>
      <name>Matthew Stephens</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/biostatistics/kxy029</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/biostatistics/kxy029" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 11 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.10066v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.10066v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00211v3</id>
    <updated>2018-06-18T19:03:03Z</updated>
    <published>2017-04-01T18:52:43Z</published>
    <title>Nonparametric causal effects based on incremental propensity score
  interventions</title>
    <summary>  Most work in causal inference considers deterministic interventions that set
each unit's treatment to some fixed value. However, under positivity violations
these interventions can lead to non-identification, inefficiency, and effects
with little practical relevance. Further, corresponding effects in longitudinal
studies are highly sensitive to the curse of dimensionality, resulting in
widespread use of unrealistic parametric models. We propose a novel solution to
these problems: incremental interventions that shift propensity score values
rather than set treatments to fixed values. Incremental interventions have
several crucial advantages. First, they avoid positivity assumptions entirely.
Second, they require no parametric assumptions and yet still admit a simple
characterization of longitudinal effects, independent of the number of
timepoints. For example, they allow longitudinal effects to be visualized with
a single curve instead of lists of coefficients. After characterizing these
incremental interventions and giving identifying conditions for corresponding
effects, we also develop general efficiency theory, propose efficient
nonparametric estimators that can attain fast convergence rates even when
incorporating flexible machine learning, and propose a bootstrap-based
confidence band and simultaneous test of no treatment effect. Finally we
explore finite-sample performance via simulation, and apply the methods to
study time-varying sociological effects of incarceration on entry into
marriage.
</summary>
    <author>
      <name>Edward H. Kennedy</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00211v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00211v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06802v1</id>
    <updated>2018-06-18T16:11:56Z</updated>
    <published>2018-06-18T16:11:56Z</published>
    <title>Collapsing-Fast-Large-Almost-Matching-Exactly: A Matching Method for
  Causal Inference</title>
    <summary>  We aim to create the highest possible quality of treatment-control matches
for categorical data in the potential outcomes framework. Matching methods are
heavily used in the social sciences due to their interpretability, but most
matching methods in the past do not pass basic sanity checks in that they fail
when irrelevant variables are introduced. Also, past methods tend to be either
computationally slow or produce poor matches. The method proposed in this work
aims to match units on a weighted Hamming distance, taking into account the
relative importance of the covariates; the algorithm aims to match units on as
many relevant variables as possible. To do this, the algorithm creates a
hierarchy of covariate combinations on which to match (similar to downward
closure), in the process solving an optimization problem for each unit in order
to construct the optimal matches. The algorithm uses a single dynamic program
to solve all of optimization problems simultaneously. Notable advantages of our
method over existing matching procedures are its high-quality matches,
versatility in handling different data distributions that may have irrelevant
variables, and ability to handle missing data by matching on as many available
covariates as possible
</summary>
    <author>
      <name>Awa Dieng</name>
    </author>
    <author>
      <name>Yameng Liu</name>
    </author>
    <author>
      <name>Sudeepa Roy</name>
    </author>
    <author>
      <name>Cynthia Rudin</name>
    </author>
    <author>
      <name>Alexander Volfovsky</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06799v1</id>
    <updated>2018-06-18T16:10:22Z</updated>
    <published>2018-06-18T16:10:22Z</published>
    <title>Quantile Regression of Latent Longitudinal Trajectory Features</title>
    <summary>  Quantile regression has demonstrated promising utility in longitudinal data
analysis. Existing work is primarily focused on modeling cross-sectional
outcomes, while outcome trajectories often carry more substantive information
in practice. In this work, we develop a trajectory quantile regression
framework that is designed to robustly and flexibly investigate how latent
individual trajectory features are related to observed subject characteristics.
The proposed models are built under modeling with usual parametric assumptions
lifted or relaxed. We derive our estimation procedure by novelly transforming
the problem at hand to quantile regression with perturbed responses and
adapting the bias correction technique for handling covariate measurement
errors. We establish desirable asymptotic properties of the proposed estimator,
including uniform consistency and weak convergence. Extensive simulation
studies confirm the validity of the proposed method as well as its robustness.
An application to the DURABLE trial uncovers sensible scientific findings and
illustrates the practical value of our proposals.
</summary>
    <author>
      <name>Huijuan Ma</name>
    </author>
    <author>
      <name>Limin Peng</name>
    </author>
    <author>
      <name>Haoda Fu</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5647v2</id>
    <updated>2018-06-18T16:05:12Z</updated>
    <published>2014-12-17T22:10:01Z</published>
    <title>Nonlinear Factor Models for Network and Panel Data</title>
    <summary>  Factor structures or interactive effects are convenient devices to
incorporate latent variables in panel data models. We consider fixed effect
estimation of nonlinear panel single-index models with factor structures in the
unobservables, which include logit, probit, ordered probit and Poisson
specifications. We establish that fixed effect estimators of model parameters
and average partial effects have normal distributions when the two dimensions
of the panel grow large, but might suffer of incidental parameter bias. We show
how models with factor structures can also be applied to capture important
features of network data such as reciprocity, degree heterogeneity, homophily
in latent variables and clustering. We illustrate this applicability with an
empirical example to the estimation of a gravity equation of international
trade between countries using a Poisson model with multiple factors.
</summary>
    <author>
      <name>Mingli Chen</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <author>
      <name>Martin Weidner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5647v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5647v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E20, 62P20, 91B82" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00715v2</id>
    <updated>2018-06-18T15:58:58Z</updated>
    <published>2018-03-02T04:19:33Z</published>
    <title>Robust Multivariate Nonparametric Tests via Projection-Pursuit</title>
    <summary>  In this work, we generalize the Cram\'er-von Mises statistic via projection
pursuit to obtain robust tests for the multivariate two-sample problem. The
proposed tests are consistent against all fixed alternatives, robust to
heavy-tailed data and minimax rate optimal. Our test statistics are completely
free of tuning parameters and are computationally efficient even in high
dimensions. When the dimension tends to infinity, the proposed test is shown to
have identical power to that of the existing high-dimensional mean tests under
certain location models. As a by-product of our approach, we introduce a new
metric called the angular distance which can be thought of as a robust
alternative to the Euclidean distance. Using the angular distance, we connect
the proposed to the reproducing kernel Hilbert space approach. In addition to
the Cram\'er-von Mises statistic, we show that the projection pursuit technique
can be used to define robust, multivariate tests in many other problems.
</summary>
    <author>
      <name>Ilmun Kim</name>
    </author>
    <author>
      <name>Sivaraman Balakrishnan</name>
    </author>
    <author>
      <name>Larry Wasserman</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00715v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00715v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06028v2</id>
    <updated>2018-06-18T15:57:28Z</updated>
    <published>2018-06-15T15:56:58Z</published>
    <title>A new characterization of the Gamma distribution and associated goodness
  of fit tests</title>
    <summary>  We propose a class of weighted $L_2$-type tests of fit to the Gamma
distribution. Our novel procedure is based on a fixed point property of a new
transformation connected to a Steinian characterization of the family of Gamma
distributions. We derive the weak limits of the statistic under the null
hypothesis and under contiguous alternatives. Further, we establish the global
consistency of the tests and apply a parametric bootstrap technique in a Monte
Carlo simulation study to show the competitiveness to existing procedures.
</summary>
    <author>
      <name>Steffen Betsch</name>
    </author>
    <author>
      <name>Bruno Ebner</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06028v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06028v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20, 62G30, 62E10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03708v3</id>
    <updated>2018-06-18T15:40:22Z</updated>
    <published>2018-02-11T08:32:31Z</published>
    <title>A Dynamic Network Perspective on the Latent Group Structure of
  Cryptocurrencies</title>
    <summary>  The latent group structure in the cryptocurrency market yields information on
network risk and dynamics. By forming a dynamic return-based network with coin
attributions, we develop a dynamic covariate-assisted spectral clustering
method to detect communities. We prove its uniform consistency along the
horizons. Applying this new method, we show the return-based network structure
and coin attributions, including algorithm and proof types, jointly determine
the market segmentation. Based on the network model, we propose a novel
"hard-to-value" measure using centrality scores. Further analysis reveals that
the group with a lower centrality score exhibits stronger short-term return
reversals. Cross-sectional return predictability further confirms the economic
meanings of our grouping results and reveal important portfolio management
implications.
</summary>
    <author>
      <name>Li Guo</name>
    </author>
    <author>
      <name>Yubo Tao</name>
    </author>
    <author>
      <name>Wolfgang Karl Härdle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03708v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03708v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30, 62F12 (Primary), 91D30 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06777v1</id>
    <updated>2018-06-18T15:38:54Z</updated>
    <published>2018-06-18T15:38:54Z</published>
    <title>MultiFIT: Multivariate Multiscale Framework for Independence Tests</title>
    <summary>  We present a framework for testing independence between two random vectors
that is scalable to massive data. Taking a "divide-and-conquer" approach, we
break down the nonparametric multivariate test of independence into simple
univariate independence tests on a collection of $2\times 2$ contingency
tables, constructed by sequentially discretizing the original sample space at a
cascade of scales from coarse to fine. This transforms a complex nonparametric
testing problem---that traditionally requires quadratic computational
complexity with respect to the sample size---into a multiple testing problem
that can be addressed with a computational complexity that scales almost
linearly with the sample size. We further consider the scenario when the
dimensionality of the two random vectors also grows large, in which case the
curse of dimensionality arises in the proposed framework through an explosion
in the number of univariate tests to be completed. To overcome this difficulty,
we propose a data-adaptive version of our method that completes a fraction of
the univariate tests, judged to be more likely to contain evidence for
dependency based on exploiting the spatial characteristics of the dependency
structure in the data. We provide an inference recipe based on multiple testing
adjustment that guarantees the inferential validity in terms of properly
controlling the family-wise error rate. We demonstrate the tremendous
computational advantage of the algorithm in comparison to existing approaches
while achieving desirable statistical power through an extensive simulation
study. In addition, we illustrate how our method can be used for learning the
nature of the underlying dependency in addition to hypothesis testing. We
demonstrate the use of our method through analyzing a data set from flow
cytometry.
</summary>
    <author>
      <name>Shai Gorsky</name>
    </author>
    <author>
      <name>Li Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06761v1</id>
    <updated>2018-06-18T15:15:41Z</updated>
    <published>2018-06-18T15:15:41Z</published>
    <title>Optimal Subsampling Algorithms for Big Data Generalized Linear Models</title>
    <summary>  To fast approximate the maximum likelihood estimator with massive data, Wang
et al. (JASA, 2017) proposed an Optimal Subsampling Method under the
A-optimality Criterion (OSMAC) for in logistic regression. This paper extends
the scope of the OSMAC framework to include generalized linear models with
canonical link functions. The consistency and asymptotic normality of the
estimator from a general subsampling algorithm are established, and optimal
subsampling probabilities under the A- and L-optimality criteria are derived.
Furthermore, using Frobenius norm matrix concentration inequality, finite
sample properties of the subsample estimator based on optimal subsampling
probabilities are derived. Since the optimal subsampling probabilities depend
on the full data estimate, an adaptive two-step algorithm is developed.
Asymptotic normality and optimality of the estimator from this adaptive
algorithm are established. The proposed methods are illustrated and evaluated
through numerical experiments on simulated and real datasets.
</summary>
    <author>
      <name>Mingyao Ai</name>
    </author>
    <author>
      <name>Jun Yu</name>
    </author>
    <author>
      <name>Huiming Zhang</name>
    </author>
    <author>
      <name>HaiYing Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06746v1</id>
    <updated>2018-06-18T14:57:13Z</updated>
    <published>2018-06-18T14:57:13Z</published>
    <title>Robust model selection between population growth and multiple merger
  coalescents</title>
    <summary>  We study the effect of biological confounders on the model selection problem
between Kingman coalescents with population growth, and Xi-coalescents
involving simultaneous multiple mergers. We use a low dimensional,
computationally tractable summary statistic, dubbed the singleton-tail
statistic, to carry out approximate likelihood ratio tests between these
models. The singleton-tail statistic has been shown to distinguish between the
two classes with high power in the simple setting of neutrally evolving,
panmictic populations without recombination. We extend this work by showing
that cryptic recombination and selection do not diminish the power of the test,
but that misspecifying population structure does. Furthermore, we demonstrate
that the singleton-tail statistic can also solve the more challenging model
selection problem between multiple mergers due to selective sweeps, and
multiple mergers due to high fecundity with moderate power of up to 60%.
</summary>
    <author>
      <name>Jere Koskela</name>
    </author>
    <author>
      <name>Maite Wilke Berenguer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07014v4</id>
    <updated>2018-06-18T13:40:41Z</updated>
    <published>2016-08-25T04:08:15Z</published>
    <title>Sequential multiple testing with generalized error control: an
  asymptotic optimality theory</title>
    <summary>  The sequential multiple testing problem is considered under two generalized
error metrics. Under the first one, the probability of at least $k$ mistakes,
of any kind, is controlled. Under the second, the probabilities of at least
$k_1$ false positives and at least $k_2$ false negatives are simultaneously
controlled. For each formulation, the optimal expected sample size is
characterized, to a first-order asymptotic approximation as the error
probabilities go to 0, and a novel multiple testing procedure is proposed and
shown to be asymptotically efficient under every signal configuration. These
results are established when the data streams for the various hypotheses are
independent and each local log-likelihood ratio statistic satisfies a certain
Strong Law of Large Numbers. In the special case of i.i.d. observations in each
stream, the gains of the proposed sequential procedures over fixed-sample size
schemes are quantified.
</summary>
    <author>
      <name>Yanglei Song</name>
    </author>
    <author>
      <name>Georgios Fellouris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Annals of Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07014v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07014v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04397v2</id>
    <updated>2018-06-18T09:56:16Z</updated>
    <published>2018-03-12T17:47:00Z</published>
    <title>An information-theoretic Phase I/II design for molecularly targeted
  agents that does not require an assumption of monotonicity</title>
    <summary>  For many years Phase I and Phase II clinical trials were conducted
separately, but there was a recent shift to combine these Phases. While a
variety of Phase~I/II model-based designs for cytotoxic agents were proposed in
the literature, methods for molecularly targeted agents (TA) are just starting
to develop. The main challenge of the TA setting is the unknown dose-efficacy
relation that can have either an increasing, plateau or umbrella shape. To
capture these, approaches with more parameters are needed to model the
dose-efficacy relationship or, alternatively, more orderings of the
dose-efficacy relationship are required to account for the uncertainty in the
curve shape. As a result, designs for more complex clinical trials, for
example, trials looking at schedules of a combination treatment involving TA,
have not been extensively studied yet. We propose a novel regimen-finding
design which is based on a derived efficacy-toxicity trade-off function. Due to
its special properties, an accurate regimen selection can be achieved without
any parametric or monotonicity assumptions. We illustrate how this design can
be applied in the context of a complex combination-schedule clinical trial. We
discuss practical and ethical issues such as coherence, delayed and missing
efficacy responses, safety and futility constraints.
</summary>
    <author>
      <name>Pavel Mozgunov</name>
    </author>
    <author>
      <name>Thomas Jaki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/rssc.12293</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/rssc.12293" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Royal Statistical Society: Series C (Applied
  Statistics), 68 (2), pp 1-21, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.04397v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04397v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06853v1</id>
    <updated>2018-06-18T08:21:13Z</updated>
    <published>2018-06-18T08:21:13Z</published>
    <title>A Hybrid Fuzzy Regression Model for Optimal Loss Reserving in Insurance</title>
    <summary>  In this article, a Hybrid Fuzzy Regression Model with Asymmetric Triangular
Fuzzy Coefficients and optimized $h-$value in Generalized Linear Models (GLM)
framework have been developed. The weighted functions of Fuzzy Numbers rather
than the Expected value of Fuzzy Number is used as a defuzzification procedure.
We perform the new model on a numerical data (Taylor and Ashe, 1983) to predict
incremental payments in loss reserving. We prove that the new Hybrid Model with
the optimized $h-$value produce better results than the classical GLM according
to the Reserve Prediction Error and Reserve Standard Deviation.
</summary>
    <author>
      <name>Woundjiagué Apollinaire</name>
    </author>
    <author>
      <name>Mbele Bidima Martin Le Doux</name>
    </author>
    <author>
      <name>Waweru Mwangi Ronald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1806.04530</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06523v1</id>
    <updated>2018-06-18T07:24:59Z</updated>
    <published>2018-06-18T07:24:59Z</published>
    <title>A Frequency Domain Bootstrap for General Stationary Processes</title>
    <summary>  Existing frequency domain methods for bootstrapping time series have a
limited range. Consider for instance the class of spectral mean statistics
(also called integrated periodograms) which includes many important statistics
in time series analysis, such as sample autocovariances and autocorrelations
among other things. Essentially, such frequency domain bootstrap procedures
cover the case of linear time series with independent innovations, and some
even require the time series to be Gaussian. In this paper we propose a new,
frequency domain bootstrap method which is consistent for a much wider range of
stationary processes and can be applied to a large class of periodogram-based
statistics. It introduces a new concept of convolved periodograms of smaller
samples which uses pseudo periodograms of subsamples generated in a way that
correctly imitates the weak dependence structure of the periodogram. %The new
bootstrap procedure %corrects for those aspects of the distribution of spectral
means that cannot be mimicked by existing procedures. We show consistency for
this procedure for a general class of stationary time series, ranging clearly
beyond linear processes, and for general spectral means and ratio statistics.
Furthermore, and for the class of spectral means, we also show, how, using this
new approach, existing bootstrap methods, which replicate appropriately only
the dominant part of the distribution of interest, can be corrected. The finite
sample performance of the new bootstrap procedure is illustrated via
simulations.
</summary>
    <author>
      <name>Marco Meyer</name>
    </author>
    <author>
      <name>Efstathios Paparoditis</name>
    </author>
    <author>
      <name>Jens-Peter Kreiss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10, 62M15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06489v1</id>
    <updated>2018-06-18T03:36:38Z</updated>
    <published>2018-06-18T03:36:38Z</published>
    <title>Moment-based Bayesian Poisson Mixtures for inferring unobserved units</title>
    <summary>  We exploit a suitable moment-based characterization of the mixture of Poisson
distribution for developing Bayesian inference for the unknown size of a finite
population whose units are subject to multiple occurrences during an
enumeration sampling stage. This is a particularly challenging setting for
which many other attempts have been made for inferring the unknown
characteristics of the population. Here we put particular emphasis on the
construction of a default prior elicitation of the characteristics of the
mixing distribution. We assess the comparative performance of our approach in
real data applications and in a simulation study.
</summary>
    <author>
      <name>Danilo Alunni Fegatelli</name>
    </author>
    <author>
      <name>Luca Tardella</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06468v1</id>
    <updated>2018-06-18T00:45:38Z</updated>
    <published>2018-06-18T00:45:38Z</published>
    <title>Variable Importance Assessments and Backward Variable Selection for
  High-Dimensional Data</title>
    <summary>  Variable selection in high-dimensional scenarios is of great interested in
statistics. One application involves identifying differentially expressed genes
in genomic analysis. Existing methods for addressing this problem have some
limits or disadvantages. In this paper, we propose distance based variable
importance measures to deal with these problems, which is inspired by the
Multi-Response Permutation Procedure (MRPP). The proposed variable importance
assessments can effectively measure the importance of an individual dimension
by quantifying its influence on the differences between multivariate
distributions. A backward selection algorithm is developed that can be used in
high-dimensional variable selection to discover important variables. Both
simulations and real data applications demonstrate that our proposed method
enjoys good properties and has advantages over other methods.
</summary>
    <author>
      <name>Liuhua Peng</name>
    </author>
    <author>
      <name>Long Qu</name>
    </author>
    <author>
      <name>Dan Nettleton</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04085v3</id>
    <updated>2018-06-17T19:06:38Z</updated>
    <published>2018-04-11T16:50:13Z</published>
    <title>Mean and median bias reduction in generalized linear models</title>
    <summary>  This paper presents an integrated framework for estimation and inference from
generalized linear models using adjusted score equations that result in mean
and median bias reduction. The framework unifies the theoretical and
methodological aspects of past research on mean bias reduction and
accommodates, in a natural way, recent advances on median bias reduction. In
particular, general expressions for the mean and median bias-reducing adjusted
score functions are derived in terms of quantities that are readily available
in standard software for fitting generalized linear models. The adjusted score
equations are solved using a unifying quasi-Fisher scoring algorithm that is
shown to be equivalent to iteratively re-weighted least squares with
appropriately adjusted working variates. Median bias reduction of the
regression parameters is shown to be achieved through a simple translation of
the current value of the parameters in iteratively re-weighted least squares
for mean bias reduction. Inference about the model parameters, including
procedures for model comparison, can be performed in a plug-in manner using
Wald statistics based on the resulting estimators. As is the case for mean bias
reduction, we show how the Poisson trick can be used for median bias reduction
in multinomial logistic regression. Core invariance properties, which are
relevant in practice, are used to develop a new mixed adjustment strategy when
the estimation of a dispersion parameter is necessary. The estimates coming out
from mean and median bias reduction are also found to overcome practical issues
related to infinite estimates that can occur with positive probability in
generalized linear models with multinomial or discrete responses, and can
result in valid inferences about a low-dimensional parameter of interest in the
presence of high-dimensional nuisance parameter.
</summary>
    <author>
      <name>Ioannis Kosmidis</name>
    </author>
    <author>
      <name>Euloge Clovis Kenne Pagui</name>
    </author>
    <author>
      <name>Nicola Sartori</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04085v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04085v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J12, 62F03, 62F12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06655v2</id>
    <updated>2018-06-17T13:34:44Z</updated>
    <published>2016-11-21T05:41:18Z</published>
    <title>Sparse Sliced Inverse Regression Via Lasso</title>
    <summary>  For multiple index models, it has recently been shown that the sliced inverse
regression (SIR) is consistent for estimating the sufficient dimension
reduction (SDR) space if and only if $\rho=\lim\frac{p}{n}=0$, where $p$ is the
dimension and $n$ is the sample size. Thus, when $p$ is of the same or a higher
order of $n$, additional assumptions such as sparsity must be imposed in order
to ensure consistency for SIR. By constructing artificial response variables
made up from top eigenvectors of the estimated conditional covariance matrix,
we introduce a simple Lasso regression method to obtain an estimate of the SDR
space. The resulting algorithm, Lasso-SIR, is shown to be consistent and
achieve the optimal convergence rate under certain sparsity conditions when $p$
is of order $o(n^2\lambda^2)$, where $\lambda$ is the generalized
signal-to-noise ratio. We also demonstrate the superior performance of
Lasso-SIR compared with existing approaches via extensive numerical studies and
several real data examples.
</summary>
    <author>
      <name>Qian Lin</name>
    </author>
    <author>
      <name>Zhigen Zhao</name>
    </author>
    <author>
      <name>Jun S. Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06655v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06655v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J02 (Primary), 62H25 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00685v2</id>
    <updated>2018-06-17T07:54:15Z</updated>
    <published>2017-12-03T00:17:23Z</published>
    <title>Bayesian prior elicitation and selection for extreme values</title>
    <summary>  A major issue of extreme value analysis is the determination of the shape
parameter $\xi$ common to Generalized Extreme Value (GEV) and Generalized
Pareto (GP) distributions, which drives the tail behavior, and is of major
impact on the estimation of return levels and periods. Many practitioners make
the choice of a Bayesian framework to conduct this assessment for accounting of
parametric uncertainties, which are typically high in such analyses
characterized by a low number of observations. Nonetheless, such approaches can
provide large credibility domains for $\xi$, including negative and positive
values, which does not allow to conclude on the nature of the tail. Considering
the block maxima framework, a generic approach of the determination of the
value and sign of $\xi$ arises from model selection between the Fr\'echet,
Gumbel and Weibull possible domains of attraction conditionally to
observations. Opposite to the common choice of the GEV as an appropriate model
for {\it sampling} extreme values, this model selection must be conducted with
great care. The elicitation of proper, informative and easy-to use priors is
conducted based on the following principle: for all parameter dimensions they
act as posteriors of noninformative priors and virtual samples. Statistics of
these virtual samples can be assessed from prior predictive information, and a
compatibility rule can be carried out to complete the calibration, even though
they are only semi-conjugated. Besides, the model selection is conducted using
a mixture encompassing framework, which allows to tackle the computation of
Bayes factors. Motivating by a real case-study involving the elicitation of
expert knowledge on meteorological magnitudes, the overall methodology is
illustrated by toy examples too.
</summary>
    <author>
      <name>Nicolas Bousquet</name>
    </author>
    <author>
      <name>Merlin Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Manuscript in preparation</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00685v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00685v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06304v1</id>
    <updated>2018-06-16T22:09:04Z</updated>
    <published>2018-06-16T22:09:04Z</published>
    <title>Post-Lasso Inference for High-Dimensional Regression</title>
    <summary>  Among the most popular variable selection procedures in high-dimensional
regression, Lasso provides a solution path to rank the variables and determines
a cut-off position on the path to select variables and estimate coefficients.
In this paper, we consider variable selection from a new perspective motivated
by the frequently occurred phenomenon that relevant variables are not
completely distinguishable from noise variables on the solution path. We
propose to characterize the positions of the first noise variable and the last
relevant variable on the path. We then develop a new variable selection
procedure to control over-selection of the noise variables ranking after the
last relevant variable, and, at the same time, retain a high proportion of
relevant variables ranking before the first noise variable. Our procedure
utilizes the recently developed covariance test statistic and Q statistic in
post-selection inference. In numerical examples, our method compares favorably
with other existing methods in selection accuracy and the ability to interpret
its results.
</summary>
    <author>
      <name>X. Jessie Jeng</name>
    </author>
    <author>
      <name>Huimin Peng</name>
    </author>
    <author>
      <name>Wenbin Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06297v1</id>
    <updated>2018-06-16T21:11:20Z</updated>
    <published>2018-06-16T21:11:20Z</published>
    <title>A nonparametric spatial test to identify factors that shape a microbiome</title>
    <summary>  The advent of high-throughput sequencing technologies has made data from DNA
material readily available, leading to a surge of microbiome-related research
establishing links between markers of microbiome health and specific outcomes.
However, to harness the power of microbial communities we must understand not
only how they affect us, but also how they can be influenced to improve
outcomes. This area has been dominated by methods that reduce community
composition to summary metrics, which can fail to fully exploit the complexity
of community data. Recently, methods have been developed to model the abundance
of taxa in a community, but they can be computationally intensive and do not
account for spatial effects underlying microbial settlement. These spatial
effects are particularly relevant in the microbiome setting because we expect
communities that are close together to be more similar than those that are far
apart. In this paper, we propose a flexible Bayesian spike-and-slab variable
selection model for presence-absence indicators that accounts for spatial
dependence and cross-dependence between taxa while reducing dimensionality in
both directions. We show by simulation that in the presence of spatial
dependence, popular distance-based hypothesis testing methods fail to preserve
their advertised size, and the proposed method improves variable selection.
Finally, we present an application of our method to an indoor fungal community
found with homes across the contiguous United States.
</summary>
    <author>
      <name>Susheela P. Singh</name>
    </author>
    <author>
      <name>Ana-Maria Staicu</name>
    </author>
    <author>
      <name>Robert R. Dunn</name>
    </author>
    <author>
      <name>Noah Fierer</name>
    </author>
    <author>
      <name>Brian J. Reich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06295v1</id>
    <updated>2018-06-16T21:07:52Z</updated>
    <published>2018-06-16T21:07:52Z</published>
    <title>Detecting intrusions in control systems: a rule of thumb, its
  justification and illustrations</title>
    <summary>  Control systems are exposed to unintentional errors, deliberate intrusions,
false data injection attacks, and various other disruptions. In this paper we
propose, justify, and illustrate a rule of thumb for detecting, or confirming
the absence of, such disruptions. To facilitate the use of the rule, we
rigorously discuss background results that delineate the boundaries of the
rule's applicability. We also discuss ways to further widen the applicability
of the proposed intrusion-detection methodology.
</summary>
    <author>
      <name>Nadezhda Gribkova</name>
    </author>
    <author>
      <name>Ričardas Zitikis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02557v3</id>
    <updated>2018-06-16T14:43:55Z</updated>
    <published>2018-02-07T18:32:47Z</published>
    <title>Neyman-Pearson classification: parametrics and power enhancement</title>
    <summary>  The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers
that achieve a minimal type II error while enforcing the prioritized type I
error under some user-specified level. This paradigm serves naturally in
applications such as severe disease diagnosis and spam detection, where people
have clear priorities over the two error types. Despite recent advances in NP
classification, the NP oracle inequalities, a core theoretical criterion to
evaluate classifiers under the NP paradigm, were established only for
classifiers based on nonparametric assumptions with bounded feature support. In
this work, we conquer the challenges arisen from unbounded feature support in
parametric settings and develop NP classification theory and methodology under
these settings. Concretely, we propose a new parametric NP classifier NP-sLDA
which satisfies the NP oracle inequalities. Furthermore, we construct an
adaptive sample splitting scheme that can be applied universally to existing NP
classifiers and this adaptive strategy greatly enhances the power of these
classifiers. Through extensive numerical experiments and real data studies, we
demonstrate the competence of NP-sLDA and the new sample splitting scheme.
</summary>
    <author>
      <name>Xin Tong</name>
    </author>
    <author>
      <name>Lucy Xia</name>
    </author>
    <author>
      <name>Jiacheng Wang</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02557v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02557v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06231v1</id>
    <updated>2018-06-16T12:19:51Z</updated>
    <published>2018-06-16T12:19:51Z</published>
    <title>Adaptive estimating function inference for non-stationary determinantal
  point processes</title>
    <summary>  Estimating function inference is indispensable for many common point process
models where the joint intensities are tractable while the likelihood function
is not. In this paper we establish asymptotic normality of estimating function
estimators in a very general setting of non-stationary point processes. We then
adapt this result to the case of non-stationary determinantal point processes
which are an important class of models for repulsive point patterns. In
practice often first and second order estimating functions are used. For the
latter it is common practice to omit contributions for pairs of points
separated by a distance larger than some truncation distance which is usually
specified in an ad hoc manner. We suggest instead a data-driven approach where
the truncation distance is adapted automatically to the point process being
fitted and where the approach integrates seamlessly with our asymptotic
framework. The good performance of the adaptive approach is illustrated via
simulation studies for non-stationary determinantal point processes.
</summary>
    <author>
      <name>Frédéric Lavancier</name>
    </author>
    <author>
      <name>Arnaud Poinas</name>
    </author>
    <author>
      <name>Rasmus Waagepetersen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06209v1</id>
    <updated>2018-06-16T08:40:14Z</updated>
    <published>2018-06-16T08:40:14Z</published>
    <title>The Reduced PC-Algorithm: Improved Causal Structure Learning in Large
  Random Networks</title>
    <summary>  We consider the task of estimating a high-dimensional directed acyclic graph,
given observations from a linear structural equation model with arbitrary noise
distribution. By exploiting properties of common random graphs, we develop a
new algorithm that requires conditioning only on small sets of variables. The
proposed algorithm, which is essentially a modified version of the
PC-Algorithm, offers significant gains in both computational complexity and
estimation accuracy. In particular, it results in more efficient and accurate
estimation in large networks containing hub nodes, which are common in
biological systems. We prove the consistency of the proposed algorithm, and
show that it also requires a less stringent faithfulness assumption than the
PC-Algorithm. Simulations in low and high-dimensional settings are used to
illustrate these findings. An application to gene expression data suggests that
the proposed algorithm can identify a greater number of clinically relevant
genes than current methods.
</summary>
    <author>
      <name>Arjun Sondhi</name>
    </author>
    <author>
      <name>Ali Shojaie</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06179v1</id>
    <updated>2018-06-16T04:39:14Z</updated>
    <published>2018-06-16T04:39:14Z</published>
    <title>Semi-supervised Inference for Explained Variance in High-dimensional
  Linear Regression and Its Applications</title>
    <summary>  We consider statistical inference for the explained variance
$\beta^{\intercal}\Sigma \beta$ under the high-dimensional linear model
$Y=X\beta+\epsilon$ in the semi-supervised setting, where $\beta$ is the
regression vector and $\Sigma$ is the design covariance matrix. A calibrated
estimator, which efficiently integrates both labelled and unlabelled data, is
proposed. It is shown that the estimator achieves the minimax optimal rate of
convergence in the general semi-supervised framework. The optimality result
characterizes how the unlabelled data affects the minimax optimal rate.
Moreover, the limiting distribution for the proposed estimator is established
and data-driven confidence intervals for the explained variance are
constructed. We further develop a randomized calibration technique for
statistical inference in the presence of weak signals and apply the obtained
inference results to a range of important statistical problems, including
signal detection and global testing, prediction accuracy evaluation, and
confidence ball construction. The numerical performance of the proposed
methodology is demonstrated in simulation studies and an analysis of estimating
heritability for a yeast segregant data set with multiple traits.
</summary>
    <author>
      <name>T. Tony Cai</name>
    </author>
    <author>
      <name>Zijian Guo</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06964v4</id>
    <updated>2018-06-16T04:06:47Z</updated>
    <published>2018-03-19T14:49:27Z</published>
    <title>A modern maximum-likelihood theory for high-dimensional logistic
  regression</title>
    <summary>  Every student in statistics or data science learns early on that when the
sample size largely exceeds the number of variables, fitting a logistic model
produces estimates that are approximately unbiased. Every student also learns
that there are formulas to predict the variability of these estimates which are
used for the purpose of statistical inference; for instance, to produce
p-values for testing the significance of regression coefficients. Although
these formulas come from large sample asymptotics, we are often told that we
are on reasonably safe grounds when $n$ is large in such a way that $n \ge 5p$
or $n \ge 10p$. This paper shows that this is far from the case, and
consequently, inferences routinely produced by common software packages are
often unreliable.
  Consider a logistic model with independent features in which $n$ and $p$
become increasingly large in a fixed ratio. Then we show that (1) the MLE is
biased, (2) the variability of the MLE is far greater than classically
predicted, and (3) the commonly used likelihood-ratio test (LRT) is not
distributed as a chi-square. The bias of the MLE is extremely problematic as it
yields completely wrong predictions for the probability of a case based on
observed values of the covariates. We develop a new theory, which
asymptotically predicts (1) the bias of the MLE, (2) the variability of the
MLE, and (3) the distribution of the LRT. We empirically also demonstrate that
these predictions are extremely accurate in finite samples. Further, an
appealing feature is that these novel predictions depend on the unknown
sequence of regression coefficients only through a single scalar, the overall
strength of the signal. This suggests very concrete procedures to adjust
inference; we describe one such procedure learning a single parameter from data
and producing accurate inference
</summary>
    <author>
      <name>Pragya Sur</name>
    </author>
    <author>
      <name>Emmanuel J. Candes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 14 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.06964v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06964v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06136v1</id>
    <updated>2018-06-15T21:46:20Z</updated>
    <published>2018-06-15T21:46:20Z</published>
    <title>The choice to define competing risk events as censoring events and
  implications for causal inference</title>
    <summary>  In failure-time settings, a competing risk event is any event that makes it
impossible for the event of interest to occur. Different analytical methods are
available for estimating the effect of a treatment on a failure event of
interest that is subject to competing events. The choice of method depends on
whether or not competing events are defined as censoring events. Though such
definition has key implications for the causal interpretation of a given
estimate, explicit consideration of those implications has been rare in the
statistical literature. As a result, confusion exists as to how to choose
amongst available methods for analyzing data with competing events and how to
interpret effect estimates. This confusion can be alleviated by understanding
that the choice to define a competing event as a censoring event or not
corresponds to a choice between different causal estimands. In this paper, we
describe the assumptions required to identify those causal estimands and
provide a mapping between such estimands and standard terminology from the
statistical literature---in particular, the terms "subdistribution function",
"subdistribution hazard" and "cause-specific hazard". We show that when the
censoring process depends on measured time-varying risk factors, conventional
statistical methods for competing events are not valid and alternative methods
derived from Robins's g-formula may recover the causal estimand of interest.
</summary>
    <author>
      <name>Jessica G. Young</name>
    </author>
    <author>
      <name>Eric J. Tchetgen Tchetgen</name>
    </author>
    <author>
      <name>Miguel A. Hernán</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09387v2</id>
    <updated>2018-06-15T20:50:58Z</updated>
    <published>2018-02-26T15:16:18Z</published>
    <title>Estimating Precipitation Extremes using Log-Histospline</title>
    <summary>  One of the commonly used approaches to modeling extremes is the
peaks-over-threshold (POT) method. The POT method models exceedances over a
threshold that is sufficiently high or low so that the exceedance has
approximately a generalized Pareto distribution (GPD). This method requires the
selection of a threshold that might affect the estimates. Here we propose an
alternative method, the Log-Histospline (LHSpline), to explore modeling the
tail behavior and the remainder of the density in one step using the full range
of the data. LHSpline applies a smoothing spline model to a finely binned
histogram of the log transformed data to estimate its log density. By
construction, a LHSpline estimation is constrained to have polynomial tail
behavior, a feature commonly observed in daily rainfall observations. We
illustrate the LHSpline method by analyzing the precipitation data collected in
Houston, Texas.
</summary>
    <author>
      <name>Whitney K. Huang</name>
    </author>
    <author>
      <name>Douglas W. Nychka</name>
    </author>
    <author>
      <name>Hao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 13 figures, 2 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06159v3</id>
    <updated>2018-06-15T18:25:16Z</updated>
    <published>2017-06-19T20:10:57Z</published>
    <title>Causal Dantzig: fast inference in linear structural equation models with
  hidden variables under additive interventions</title>
    <summary>  Causal inference is known to be very challenging when only observational data
are available. Randomized experiments are often costly and impractical and in
instrumental variable regression the number of instruments has to exceed the
number of causal predictors. It was recently shown in Peters et al. [2016] that
causal inference for the full model is possible when data from distinct
observational environments are available, exploiting that the conditional
distribution of a response variable is invariant under the correct causal
model. Two shortcomings of such an approach are the high computational effort
for large-scale data and the assumed absence of hidden confounders. Here we
show that these two shortcomings can be addressed if one is willing to make a
more restrictive assumption on the type of interventions that generate
different environments. Thereby, we look at a different notion of invariance,
namely inner-product invariance. By avoiding a computationally cumbersome
reverse-engineering approach such as in Peters et al. [2016], it allows for
large-scale causal inference in linear structural equation models. We discuss
identifiability conditions for the causal parameter and derive asymptotic
confidence intervals in the low-dimensional setting. In the case of
non-identifiability we show that the solution set of causal Dantzig has
predictive guarantees under certain interventions. We derive finite-sample
bounds in the high-dimensional setting and investigate its performance on
simulated datasets.
</summary>
    <author>
      <name>Dominik Rothenhäusler</name>
    </author>
    <author>
      <name>Peter Bühlmann</name>
    </author>
    <author>
      <name>Nicolai Meinshausen</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06159v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06159v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00314v4</id>
    <updated>2018-06-15T16:37:09Z</updated>
    <published>2016-04-01T16:22:00Z</published>
    <title>On choosing mixture components via non-local priors</title>
    <summary>  Choosing the number of mixture components remains an elusive challenge. Model
selection criteria can be either overly liberal or conservative and return
poorly-separated components of limited practical use. We formalize non-local
priors (NLPs) for mixtures and show how they lead to well-separated components
with non-negligible weight, interpretable as distinct subpopulations. We also
propose an estimator for posterior model probabilities under local and
non-local priors, showing that Bayes factors are ratios of posterior to prior
empty-cluster probabilities. The estimator is widely applicable and helps set
thresholds to drop unoccupied components in overfitted mixtures. We suggest
default prior parameters based on multi-modality for Normal/T mixtures and
minimal informativeness for categorical outcomes. We characterise theoretically
the NLP-induced sparsity, derive tractable expressions and algorithms. We fully
develop Normal, Binomial and product Binomial mixtures but the theory,
computation and principles hold more generally. We observed a serious lack of
sensitivity of the Bayesian information criterion (BIC), insufficient parsimony
of the AIC and a local prior, and a mixed behavior of the singular BIC. We also
considered overfitted mixtures, their performance was competitive but depended
on tuning parameters. Under our default prior elicitation NLPs offered a good
compromise between sparsity and power to detect meaningfully-separated
components.
</summary>
    <author>
      <name>Jairo Fúquene</name>
    </author>
    <author>
      <name>Mark Steel</name>
    </author>
    <author>
      <name>David Rossell</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00314v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00314v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06229v2</id>
    <updated>2018-06-15T15:05:09Z</updated>
    <published>2018-01-18T20:32:09Z</published>
    <title>Anchor regression: heterogeneous data meets causality</title>
    <summary>  Many traditional statistical prediction methods mainly deal with the problem
of overfitting to the given data set. On the other hand, there is a vast
literature on the estimation of causal parameters for prediction under
interventions. However, both types of estimators can perform poorly when used
for prediction on heterogeneous data. We discuss the delicate trade-off between
predictive performance on the training distribution and perturbed
distributions. In particular, under a linear structural equation model with
exogenous variables, we show that the change in loss under certain
perturbations (interventions) can be written as a convex penalty. This
motivates anchor regression, a regularization scheme that encourages the
estimator to generalize well to perturbed data. The procedure naturally
provides an interpolation between the solution to ordinary least squares and
two-stage least squares, but also has predictive guarantees if the instrumental
variables assumptions are violated. An additional characterization of the
procedure is given in terms of quantiles: If the data follow a Gaussian
distribution, the method minimizes quantiles of the conditional mean squared
error. We derive guarantees of the proposed procedure for predictive
performance under perturbations for the population case and for
high-dimensional data and test its performance on real-world data.
</summary>
    <author>
      <name>Dominik Rothenhäusler</name>
    </author>
    <author>
      <name>Nicolai Meinshausen</name>
    </author>
    <author>
      <name>Peter Bühlmann</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06229v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06229v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03076v2</id>
    <updated>2018-06-15T14:16:46Z</updated>
    <published>2017-09-10T09:47:56Z</published>
    <title>A Grouping Genetic Algorithm for Joint Stratification and Sample
  Allocation Designs</title>
    <summary>  Predicting the cheapest sample size for the optimal stratification in
multivariate survey design is a problem in cases where the population frame is
large. A solution exists that iteratively searches for the minimum sample size
necessary to meet accuracy constraints in partitions of atomic strata created
by the Cartesian product of auxiliary variables into larger strata. The optimal
stratification can be found by testing all possible partitions. However the
number of possible partitions grows exponentially with the number of initial
strata. There are alternative ways of modelling this problem, one of the most
natural is using Genetic Algorithms (GA). These evolutionary algorithms use
recombination, mutation and selection to search for optimal solutions. They
often converge on optimal or near-optimal solution more quickly than exact
methods. We propose a new GA approach to this problem using grouping genetic
operators instead of traditional operators. The results show a significant
improvement in solution quality for similar computational effort, corresponding
to large monetary savings.
</summary>
    <author>
      <name>Mervyn O'Luing</name>
    </author>
    <author>
      <name>Steven Prestwich</name>
    </author>
    <author>
      <name>S. Armagan Tarim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03076v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03076v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05951v1</id>
    <updated>2018-06-15T13:32:51Z</updated>
    <published>2018-06-15T13:32:51Z</published>
    <title>A goodness of fit test for the Pareto distribution</title>
    <summary>  The Zenga (1984) inequality curve is constant in p for Type I Pareto
distributions. This characterizing behavior will be exploited to obtain
graphical and analytical tools for tail analysis and goodness of fit tests. A
testing procedure for Pareto-type behavior based on a regression of technique
will be introduced.
</summary>
    <author>
      <name>Emanuele Taufer</name>
    </author>
    <author>
      <name>Flavio Santi</name>
    </author>
    <author>
      <name>Giuseppe Espa</name>
    </author>
    <author>
      <name>Maria Michela Dickson</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06714v2</id>
    <updated>2018-06-15T13:06:05Z</updated>
    <published>2018-05-17T11:57:31Z</published>
    <title>High-dimensional doubly robust tests for regression parameters</title>
    <summary>  After variable selection, standard inferential procedures for regression
parameters may not be uniformly valid; there is no finite sample size at which
a standard test is guaranteed to attain its nominal size (within pre-specified
error margins). This problem is exacerbated in high-dimensional settings, where
variable selection becomes unavoidable. This has prompted a flurry of activity
in developing uniformly valid hypothesis tests for a low-dimensional regression
parameter (e.g. the causal effect of an exposure A on an outcome Y) in
high-dimensional models. So far there has been limited focus on model
misspecification, although this is inevitable in high-dimensional settings. We
propose tests of the null that are uniformly valid under sparsity conditions
weaker than those typically invoked in the literature, assuming working models
for the exposure and outcome are both correctly specified. When one of the
models is misspecified, by amending the procedure for estimating the nuisance
parameters, our tests continue to be valid; hence they are then doubly robust.
Our proposals are straightforward to implement using existing software for
penalized maximum likelihood estimation and do not require sample-splitting. We
illustrate them in simulations and an analysis of data obtained from the Ghent
University Intensive Care Unit.
</summary>
    <author>
      <name>Oliver Dukes</name>
    </author>
    <author>
      <name>Vahe Avagyan</name>
    </author>
    <author>
      <name>Stijn Vansteelandt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05939v1</id>
    <updated>2018-06-15T13:04:30Z</updated>
    <published>2018-06-15T13:04:30Z</published>
    <title>Generalized Log-Normal Chain-Ladder</title>
    <summary>  We propose an asymptotic theory for distribution forecasting from the log
normal chain-ladder model. The theory overcomes the difficulty of convoluting
log normal variables and takes estimation error into account. The results
differ from that of the over-dispersed Poisson model and from the chain-ladder
based bootstrap. We embed the log normal chain-ladder model in a class of
infinitely divisible distributions called the generalized log normal
chain-ladder model. The asymptotic theory uses small $\sigma$ asymptotics where
the dimension of the reserving triangle is kept fixed while the standard
deviation is assumed to decrease. The resulting asymptotic forecast
distributions follow t distributions. The theory is supported by simulations
and an empirical application.
</summary>
    <author>
      <name>D. Kuang</name>
    </author>
    <author>
      <name>B. Nielsen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05928v1</id>
    <updated>2018-06-15T12:27:25Z</updated>
    <published>2018-06-15T12:27:25Z</published>
    <title>On a property of the inequality curve $λ(p)$</title>
    <summary>  The Zenga (1984) inequality curve is constant in p for Type I Pareto
distributions. We show that this property holds exactly only for the Pareto
distribution and, asymptotically, for distributions with power tail with index
-a, with a greater than 1. Exploiting these properties one can develop powerful
tools to analyze and estimate the tail of a distribution.
</summary>
    <author>
      <name>Emanuele Taufer</name>
    </author>
    <author>
      <name>Flavio Santi</name>
    </author>
    <author>
      <name>Giuseppe Espa</name>
    </author>
    <author>
      <name>Maria Michela Dickson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09179v2</id>
    <updated>2018-06-15T08:17:26Z</updated>
    <published>2017-11-25T02:07:19Z</published>
    <title>Distance Metrics for Measuring Joint Dependence with Application to
  Causal Inference</title>
    <summary>  Many statistical applications require the quantification of joint dependence
among more than two random vectors. In this work, we generalize the notion of
distance covariance to quantify joint dependence among d &gt;= 2 random vectors.
We introduce the high order distance covariance to measure the so-called
Lancaster interaction dependence. The joint distance covariance is then defined
as a linear combination of pairwise distance covariances and their higher order
counterparts which together completely characterize mutual independence. We
further introduce some related concepts including the distance cumulant,
distance characteristic function, and rank-based distance covariance. Empirical
estimators are constructed based on certain Euclidean distances between sample
elements. We study the large sample properties of the estimators and propose a
bootstrap procedure to approximate their sampling distributions. The asymptotic
validity of the bootstrap procedure is justified under both the null and
alternative hypotheses. The new metrics are employed to perform model selection
in causal inference, which is based on the joint independence testing of the
residuals from the fitted structural equation models. The effectiveness of the
method is illustrated via both simulated and real datasets.
</summary>
    <author>
      <name>Shubhadeep Chakraborty</name>
    </author>
    <author>
      <name>Xianyang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1711.09179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05830v1</id>
    <updated>2018-06-15T07:16:00Z</updated>
    <published>2018-06-15T07:16:00Z</published>
    <title>Parametric versus nonparametric: the fitness coefficient</title>
    <summary>  The fitness coefficient, introduced in this paper, results from a competition
between parametric and nonparametric density estimators within the likelihood
of the data. As illustrated on several real datasets, the fitness coefficient
generally agrees with p-values but is easier to compute and interpret. Namely,
the fitness coefficient can be interpreted as the proportion of data coming
from the parametric model. Moreover, the fitness coefficient can be used to
build a semiparamteric compromise which improves inference over the parametric
and nonparametric approaches. From a theoretical perspective, the fitness
coefficient is shown to converge in probability to one if the model is true and
to zero if the model is false. From a practical perspective, the utility of the
fitness coefficient is illustrated on real and simulated datasets.
</summary>
    <author>
      <name>Gildas Mazo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MaIAGE</arxiv:affiliation>
    </author>
    <author>
      <name>François Portier</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07532v4</id>
    <updated>2018-06-14T20:54:14Z</updated>
    <published>2017-04-25T04:31:45Z</published>
    <title>High-dimensional confounding adjustment using continuous spike and slab
  priors</title>
    <summary>  In observational studies, estimation of a causal effect of a treatment on an
outcome relies on proper adjustment for confounding. If the number of the
potential confounders ($p$) is larger than the number of observations ($n$),
then direct control for all potential confounders is infeasible. Existing
approaches for dimension reduction and penalization are generally aimed at
predicting the outcome, and are less suited for estimation of causal effects.
Under standard penalization approaches (e.g. Lasso), if a variable $X_j$ is
strongly associated with the treatment $T$ but weakly with the outcome $Y$, the
coefficient $\beta_j$ will be shrunk towards zero thus leading to confounding
bias.
  Under the assumption of a linear model for the outcome and sparsity, we
propose continuous spike and slab priors on the regression coefficients
$\beta_j$ corresponding to the potential confounders $X_j$. Specifically, we
introduce a prior distribution that does not heavily shrink to zero the
coefficients ($\beta_j$s) of the $X_j$s that are strongly associated with $T$
but weakly associated with $Y$. We compare our proposed approach to several
state of the art methods proposed in the literature. Our proposed approach has
the following features: 1) it reduces confounding bias in high dimensional
settings; 2) it shrinks towards zero coefficients of instrumental variables;
and 3) it achieves good coverages even in small sample sizes. We apply our
approach to the National Health and Nutrition Examination Survey (NHANES) data
to estimate the causal effects of persistent pesticide exposure on triglyceride
levels.
</summary>
    <author>
      <name>Joseph Antonelli</name>
    </author>
    <author>
      <name>Giovanni Parmigiani</name>
    </author>
    <author>
      <name>Francesca Dominici</name>
    </author>
    <link href="http://arxiv.org/abs/1704.07532v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07532v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09414v2</id>
    <updated>2018-06-14T13:09:13Z</updated>
    <published>2016-11-28T22:32:16Z</published>
    <title>Split-door criterion: Identification of causal effects through auxiliary
  outcomes</title>
    <summary>  We present a method for estimating causal effects in time series data when
fine-grained information about the outcome of interest is available.
Specifically, we examine what we call the split-door setting, where the outcome
variable can be split into two parts: one that is potentially affected by the
cause being studied and another that is independent of it, with both parts
sharing the same (unobserved) confounders. We show that under these conditions,
the problem of identification reduces to that of testing for independence among
observed variables, and present a method that uses this approach to
automatically find subsets of the data that are causally identified. We
demonstrate the method by estimating the causal impact of Amazon's recommender
system on traffic to product pages, finding thousands of examples within the
dataset that satisfy the split-door criterion. Unlike past studies based on
natural experiments that were limited to a single product category, our method
applies to a large and representative sample of products viewed on the site. In
line with previous work, we find that the widely-used click-through rate (CTR)
metric overestimates the causal impact of recommender systems; depending on the
product category, we estimate that 50-80\% of the traffic attributed to
recommender systems would have happened even without any recommendations. We
conclude with guidelines for using the split-door criterion as well as a
discussion of other contexts where the method can be applied.
</summary>
    <author>
      <name>Amit Sharma</name>
    </author>
    <author>
      <name>Jake M. Hofman</name>
    </author>
    <author>
      <name>Duncan J. Watts</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09414v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09414v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; I.2.6; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10865v2</id>
    <updated>2018-06-14T12:53:17Z</updated>
    <published>2018-05-28T11:13:35Z</published>
    <title>Pairwise likelihood estimation of latent autoregressive count models</title>
    <summary>  Latent autoregressive models are useful time series models for the analysis
of infectious disease data. Evaluation of the likelihood function of latent
autoregressive models is intractable and its approximation through
simulation-based methods appears as a standard practice. Although simulation
methods may make the inferential problem feasible, they are often
computationally intensive and the quality of the numerical approximation may be
difficult to assess. We consider instead a weighted pairwise likelihood
approach and explore several computational and methodological aspects including
selection of the pairs, estimation of robust standard errors and the role of
numerical integration. The suggested approach is illustrated using monthly data
on invasive meningococcal disease infection in Greece and Italy.
</summary>
    <author>
      <name>Xanthi Pedeli</name>
    </author>
    <author>
      <name>Cristiano Varin</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05471v1</id>
    <updated>2018-06-14T11:26:35Z</updated>
    <published>2018-06-14T11:26:35Z</published>
    <title>Regression with Functional Errors-in-Predictors: A Generalized
  Method-of-Moments Approach</title>
    <summary>  Functional regression is an important topic in functional data analysis.
Traditionally, one often assumes that samples of the functional predictor are
independent realizations of an underlying stochastic process, and are observed
over a grid of points contaminated by independent and identically distributed
measurement errors. In practice, however, the dynamical dependence across
different curves may exist and the parametric assumption on the measurement
error covariance structure could be unrealistic. In this paper, we consider
functional linear regression with serially dependent functional predictors,
when the contamination of predictors by the measurement error is "genuinely
functional" with fully nonparametric covariance structure. Inspired by the fact
that the autocovariance operator of observed functional predictors
automatically filters out the impact from the unobservable measurement error,
we propose a novel autocovariance-based generalized method-of-moments estimate
of the slope parameter. The asymptotic properties of the resulting estimators
under different functional scenarios are established. We also demonstrate that
our proposed method significantly outperforms possible competitors through
intensive simulation studies. Finally, the proposed method is applied to a
public financial dataset, revealing some interesting findings.
</summary>
    <author>
      <name>Xinghao Qiao</name>
    </author>
    <author>
      <name>Cheng Chen</name>
    </author>
    <author>
      <name>Shaojun Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05429v1</id>
    <updated>2018-06-14T09:24:22Z</updated>
    <published>2018-06-14T09:24:22Z</published>
    <title>Improving precipitation forecast using extreme quantile regression</title>
    <summary>  Aiming to predict extreme precipitation forecast quantiles, we propose a
nonparametric regression model that features a constant extreme value index.
Using local linear quantile regression and an extrapolation technique from
extreme value theory, we develop an estimator for conditional quantiles
corresponding to extreme high probability levels. We establish uniform
consistency and asymptotic normality of the estimators. In a simulation study,
we examine the performance of our estimator on finite samples in comparison
with existing methods. On a precipitation data set in the Netherlands, our
estimators have more predictive power compared to the upper member of ensemble
forecasts provided by a numerical weather predication model.
</summary>
    <author>
      <name>Jasper Velthoen</name>
    </author>
    <author>
      <name>Juan-Juan Cai</name>
    </author>
    <author>
      <name>Geurt Jongbloed</name>
    </author>
    <author>
      <name>Maurice Schmeits</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04632v2</id>
    <updated>2018-06-14T06:35:42Z</updated>
    <published>2017-11-13T15:22:45Z</published>
    <title>(Un)Conditional Sample Generation Based on Distribution Element Trees</title>
    <summary>  Recently, distribution element trees (DETs) were introduced as an accurate
and computationally efficient method for density estimation. In this work, we
demonstrate that the DET formulation promotes an easy and inexpensive way to
generate random samples similar to a smooth bootstrap. These samples can be
generated unconditionally, but also, without further complications,
conditionally utilizing available information about certain probability-space
components.
</summary>
    <author>
      <name>Daniel W. Meyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/10618600.2018.1482768</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/10618600.2018.1482768" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published online in the Journal of Computational and Graphical
  Statistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04632v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04632v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F40, 62G09" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05297v1</id>
    <updated>2018-06-13T23:14:45Z</updated>
    <published>2018-06-13T23:14:45Z</published>
    <title>Pattern Dependence Detection using n-TARP Clustering</title>
    <summary>  Consider an experiment involving a potentially small number of subjects. Some
random variables are observed on each subject: a high-dimensional one called
the "observed" random variable, and a one-dimensional one called the "outcome"
random variable. We are interested in the dependencies between the observed
random variable and the outcome random variable. We propose a method to
quantify and validate the dependencies of the outcome random variable on the
various patterns contained in the observed random variable. Different degrees
of relationship are explored (linear, quadratic, cubic, ...). This work is
motivated by the need to analyze educational data, which often involves
high-dimensional data representing a small number of students. Thus our
implementation is designed for a small number of subjects; however, it can be
easily modified to handle a very large dataset. As an illustration, the
proposed method is used to study the influence of certain skills on the course
grade of students in a signal processing class. A valid dependency of the grade
on the different skill patterns is observed in the data.
</summary>
    <author>
      <name>Tarun Yellamraju</name>
    </author>
    <author>
      <name>Mireille Boutin</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05127v1</id>
    <updated>2018-06-13T16:03:00Z</updated>
    <published>2018-06-13T16:03:00Z</published>
    <title>Stratification Trees for Adaptive Randomization in Randomized Controlled
  Trials</title>
    <summary>  This paper proposes a two-stage adaptive randomization procedure for
randomized controlled trials. The method uses data from a first-stage pilot
experiment to determine how to stratify in a second wave of the experiment,
where the objective is to minimize the variance of an estimator for the average
treatment effect (ATE). We consider selection from a class of stratified
randomization procedures which we call stratification trees: these are
procedures whose strata can be represented as decision trees, with differing
treatment assignment probabilities across strata. By using the pilot to
estimate a stratification tree, we simultaneously select which covariates to
use for stratification, how to stratify over these covariates, as well as the
assignment probabilities within these strata. Our main result shows that using
this randomization procedure with an appropriate estimator results in an
asymptotic variance which minimizes the variance bound for estimating the ATE,
over an optimal stratification of the covariate space. Moreover, by extending
techniques developed in Bugni et al. (2018), the results we present are able to
accommodate a large class of assignment mechanisms within strata, including
stratified block randomization. We also present extensions of the procedure to
the setting of multiple treatments, and to the targeting of subgroup-specific
effects. In a simulation study, we find that our method is most effective when
the response model exhibits some amount of "sparsity" with respect to the
covariates, but can be effective in other contexts as well, as long as the
pilot sample size used to estimate the stratification tree is not prohibitively
small. We conclude by applying our method to the study in Karlan and Wood
(2017), where we estimate a stratification tree using the first wave of their
experiment.
</summary>
    <author>
      <name>Max Tabord-Meehan</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10119v2</id>
    <updated>2018-06-13T14:35:22Z</updated>
    <published>2018-03-27T15:03:51Z</published>
    <title>Learning distributions of shape trajectories from longitudinal datasets:
  a hierarchical model on a manifold of diffeomorphisms</title>
    <summary>  We propose a method to learn a distribution of shape trajectories from
longitudinal data, i.e. the collection of individual objects repeatedly
observed at multiple time-points. The method allows to compute an average
spatiotemporal trajectory of shape changes at the group level, and the
individual variations of this trajectory both in terms of geometry and time
dynamics. First, we formulate a non-linear mixed-effects statistical model as
the combination of a generic statistical model for manifold-valued longitudinal
data, a deformation model defining shape trajectories via the action of a
finite-dimensional set of diffeomorphisms with a manifold structure, and an
efficient numerical scheme to compute parallel transport on this manifold.
Second, we introduce a MCMC-SAEM algorithm with a specific approach to shape
sampling, an adaptive scheme for proposal variances, and a log-likelihood
tempering strategy to estimate our model. Third, we validate our algorithm on
2D simulated data, and then estimate a scenario of alteration of the shape of
the hippocampus 3D brain structure during the course of Alzheimer's disease.
The method shows for instance that hippocampal atrophy progresses more quickly
in female subjects, and occurs earlier in APOE4 mutation carriers. We finally
illustrate the potential of our method for classifying pathological
trajectories versus normal ageing.
</summary>
    <author>
      <name>Alexandre Bône</name>
    </author>
    <author>
      <name>Olivier Colliot</name>
    </author>
    <author>
      <name>Stanley Durrleman</name>
    </author>
    <link href="http://arxiv.org/abs/1803.10119v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10119v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04892v1</id>
    <updated>2018-06-13T08:33:43Z</updated>
    <published>2018-06-13T08:33:43Z</published>
    <title>PoARX Modelling for Multivariate Count Time Series</title>
    <summary>  This paper introduces multivariate Poisson autoregressive models with
exogenous covariates (PoARX) for modelling multivariate time series of counts.
We obtain conditions for the PoARX process to be stationary and ergodic before
proposing a computationally efficient procedure for estimation of parameters by
the method of inference functions (IFM) and obtaining asymptotic normality of
these estimators. Lastly, we demonstrate an application to count data for the
number of people entering and exiting a building, and show how the different
aspects of the model combine to produce a strong predictive model. We conclude
by suggesting some further areas of application and by listing directions for
future work.
</summary>
    <author>
      <name>Jamie Halliday</name>
    </author>
    <author>
      <name>Georgi N. Boshnakov</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00771v3</id>
    <updated>2018-06-13T06:26:12Z</updated>
    <published>2017-12-03T14:01:42Z</published>
    <title>Randomized incomplete $U$-statistics in high dimensions</title>
    <summary>  This paper studies inference for the mean vector of a high-dimensional
$U$-statistic. In the era of Big Data, the dimension $d$ of the $U$-statistic
and the sample size $n$ of the observations tend to be both large, and the
computation of the $U$-statistic is prohibitively demanding. Data-dependent
inferential procedures such as the empirical bootstrap for $U$-statistics is
even more computationally expensive. To overcome such computational bottleneck,
incomplete $U$-statistics obtained by sampling fewer terms of the $U$-statistic
are attractive alternatives. In this paper, we introduce randomized incomplete
$U$-statistics with sparse weights whose computational cost can be made
independent of the order of the $U$-statistic. We derive non-asymptotic
Gaussian approximation error bounds for the randomized incomplete
$U$-statistics in high dimensions, namely in cases where the dimension $d$ is
possibly much larger than the sample size $n$, for both non-degenerate and
degenerate kernels. In addition, we propose generic bootstrap methods for the
incomplete $U$-statistics that are computationally much less-demanding than
existing bootstrap methods, and establish finite sample validity of the
proposed bootstrap methods. Our methods are illustrated on the application to
nonparametric testing for the pairwise independence of a high-dimensional
random vector under weaker assumptions than those appearing in the literature.
</summary>
    <author>
      <name>Xiaohui Chen</name>
    </author>
    <author>
      <name>Kengo Kato</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00771v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00771v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E17, 62F40, 62H15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00825v3</id>
    <updated>2018-06-12T22:32:45Z</updated>
    <published>2015-12-02T19:56:38Z</published>
    <title>Data-adaptive estimation of time-varying spectral densities</title>
    <summary>  This paper introduces a data-adaptive non-parametric approach for the
estimation of time-varying spectral densities from nonstationary time series.
Time-varying spectral densities are commonly estimated by local kernel
smoothing. The performance of these nonparametric estimators, however, depends
crucially on the smoothing bandwidths that need to be specified in both time
and frequency direction. As an alternative and extension to traditional
bandwidth selection methods, we propose an iterative algorithm for constructing
localized smoothing kernels data-adaptively. The main idea, inspired by the
concept of propagation-separation (Polzehl and Spokoiny 2006), is to determine
for a point in the time-frequency plane the largest local vicinity over which
smoothing is justified by the data. By shaping the smoothing kernels
nonparametrically, our method not only avoids the problem of bandwidth
selection in the strict sense but also becomes more flexible. It not only
adapts to changing curvature in smoothly varying spectra but also adjusts for
structural breaks in the time-varying spectrum.
</summary>
    <author>
      <name>Anne van Delft</name>
    </author>
    <author>
      <name>Michael Eichler</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00825v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00825v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="6210 (Primary), 62M15 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04743v1</id>
    <updated>2018-06-12T20:08:53Z</updated>
    <published>2018-06-12T20:08:53Z</published>
    <title>INFERNO: Inference-Aware Neural Optimisation</title>
    <summary>  Complex computer simulations are commonly required for accurate data
modelling in many scientific disciplines, making statistical inference
challenging due to the intractability of the likelihood evaluation for the
observed data. Furthermore, sometimes one is interested on inference drawn over
a subset of the generative model parameters while taking into account model
uncertainty or misspecification on the remaining nuisance parameters. In this
work, we show how non-linear summary statistics can be constructed by
minimising inference-motivated losses via stochastic gradient descent.
</summary>
    <author>
      <name>Pablo de Castro</name>
    </author>
    <author>
      <name>Tommaso Dorigo</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05515v2</id>
    <updated>2018-06-12T19:24:01Z</updated>
    <published>2017-09-16T14:12:20Z</published>
    <title>Some variations on Ensembled Random Survival Forest with application to
  Cancer Research</title>
    <summary>  In this paper we describe a novel implementation of adaboost for prediction
of survival function. We take different variations of the algorithm and compare
the algorithms based on system run time and root mean square error. Our
construction includes right censoring data and competing risk data too. We take
different data set to illustrate the performance of the algorithms.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Suhas N.</name>
    </author>
    <author>
      <name>Talasila Sai Teja</name>
    </author>
    <author>
      <name>Anshul Juneja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages; 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05515v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05515v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08229v3</id>
    <updated>2018-06-12T18:01:11Z</updated>
    <published>2018-02-22T18:31:57Z</published>
    <title>A Better (Bayesian) Interval Estimate for Within-Subject Designs</title>
    <summary>  We develop a Bayesian highest-density interval (HDI) for use in
within-subject designs. This credible interval is based on a standard
noninformative prior and a modified posterior distribution that conditions on
both the data and point estimates of the subject-specific random effects.
Conditioning on the estimated random effects removes between-subject variance
and produces intervals that are the Bayesian analogue of the within-subject
confidence interval proposed in Loftus and Masson (1994). We show that the
latter interval can also be derived as a Bayesian within-subject HDI under a
certain improper prior. We argue that the proposed new interval is superior to
the original within-subject confidence interval, on the grounds of (a) it being
based on a more sensible prior, (b) it having a clear and intuitively appealing
interpretation, and (c) because its length is always smaller. A generalization
of the new interval that can be applied to heteroscedastic data is also
derived, and we show that the resulting interval is numerically equivalent to
the normalization method discussed in Franz and Loftus (2012); however, our
work provides a Bayesian formulation for the normalization method, and in doing
so we identify the associated prior distribution.
</summary>
    <author>
      <name>Farouk S. Nathoo</name>
    </author>
    <author>
      <name>Robyn E. Kilshaw</name>
    </author>
    <author>
      <name>Michael E. J. Masson</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08229v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08229v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04420v1</id>
    <updated>2018-06-12T09:51:32Z</updated>
    <published>2018-06-12T09:51:32Z</published>
    <title>Estimating finite mixtures of semi-Markov chains: an application to the
  segmentation of temporal sensory data</title>
    <summary>  In food science, it is of great interest to get information about the
temporal perception of aliments to create new products, to modify existing ones
or more generally to understand the perception mechanisms. Temporal Dominance
of Sensations (TDS) is a technique to measure temporal perception which
consists in choosing sequentially attributes describing a food product over
tasting. This work introduces new statistical models based on finite mixtures
of semi-Markov chains in order to describe data collected with the TDS
protocol, allowing different temporal perceptions for a same product within a
population. The identifiability of the parameters of such mixture models is
discussed. A penalty is added to the log likelihood to ensure numerical
stability and consistency of the EM algorithm used to fit the parameters. The
BIC criterion is employed for determining the number of mixture components.
Then, the individual qualitative trajectories are clustered by considering the
MAP criterion. A simulation study confirms the good behavior of the proposed
estimation procedure. The methodology is illustrated on an example of consumers
perception of a Gouda cheese and assesses the existence of several behaviors in
terms of perception of this product.
</summary>
    <author>
      <name>Hervé Cardot</name>
    </author>
    <author>
      <name>Guillaume Lecuelle</name>
    </author>
    <author>
      <name>Pascal Schlich</name>
    </author>
    <author>
      <name>Michel Visalli</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07278v2</id>
    <updated>2018-06-12T09:48:41Z</updated>
    <published>2018-01-22T19:03:02Z</published>
    <title>On the estimation of variance parameters in non-standard generalised
  linear mixed models: Application to penalised smoothing</title>
    <summary>  We present a novel method for the estimation of variance parameters in
generalised linear mixed models. The method has its roots in Harville (1977)'s
work, but it is able to deal with models that have a precision matrix for the
random-effect vector that is linear in the inverse of the variance parameters
(i.e., the precision parameters). We call the method SOP (Separation of
Overlapping Precision matrices). SOP is based on applying the method of
successive approximations to easy-to-compute estimate updates of the variance
parameters. These estimate updates have an appealing form: they are the ratio
of a (weighted) sum of squares to a quantity related to effective degrees of
freedom. We provide the sufficient and necessary conditions for these estimates
to be strictly positive. An important application field of SOP is penalised
regression estimation of models where multiple quadratic penalties act on the
same regression coefficients. We discuss in detail two of those models:
penalised splines for locally adaptive smoothness and for hierarchical curve
data. Several data examples in these settings are presented.
</summary>
    <author>
      <name>María Xosé Rodríguez-Álvarez</name>
    </author>
    <author>
      <name>Maria Durban</name>
    </author>
    <author>
      <name>Dae-Jin Lee</name>
    </author>
    <author>
      <name>Paul H. C. Eilers</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11222-018-9818-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11222-018-9818-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Computing, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.07278v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07278v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04334v1</id>
    <updated>2018-06-12T05:11:00Z</updated>
    <published>2018-06-12T05:11:00Z</published>
    <title>Bayesian Inference in Nonparanormal Graphical Models</title>
    <summary>  Gaussian graphical models have been used to study intrinsic dependence among
several variables, but the Gaussianity assumption may be restrictive in many
applications. A nonparanormal graphical model is a semiparametric
generalization for continuous variables where it is assumed that the variables
follow a Gaussian graphical model only after some unknown smooth monotone
transformation. We consider a Bayesian approach in the nonparanormal graphical
model by putting priors on the unknown transformations through a random series
based on B-splines where the coefficients are ordered to induce monotonicity. A
truncated normal prior leads to partial conjugacy in the model and is useful
for posterior simulation using Gibbs sampling. On the underlying precision
matrix of the transformed variables, we consider a spike-and-slab prior and use
an efficient posterior Gibbs sampling scheme. We use the Bayesian Information
Criterion to choose the hyperparameters for the spike-and-slab prior. We
present a posterior consistency result on the underlying transformation and the
precision matrix. We study the numerical performance of the proposed method
through an extensive simulation study and finally apply the proposed method on
a real data set.
</summary>
    <author>
      <name>Jami J. Mulgrave</name>
    </author>
    <author>
      <name>Subhashis Ghosal</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04206v1</id>
    <updated>2018-06-11T19:22:10Z</updated>
    <published>2018-06-11T19:22:10Z</published>
    <title>Inference under Covariate-Adaptive Randomization with Multiple
  Treatments</title>
    <summary>  This paper studies inference in randomized controlled trials with
covariate-adaptive randomization when there are multiple treatments. More
specifically, we study inference about the average effect of one or more
treatments relative to other treatments or a control. As in Bugni et al.
(2018), covariate-adaptive randomization refers to randomization schemes that
first stratify according to baseline covariates and then assign treatment
status so as to achieve balance within each stratum. In contrast to Bugni et
al. (2018), we not only allow for multiple treatments, but further allow for
the proportion of units being assigned to each of the treatments to vary across
strata. We first study the properties of estimators derived from a fully
saturated linear regression, i.e., a linear regression of the outcome on all
interactions between indicators for each of the treatments and indicators for
each of the strata. We show that tests based on these estimators using the
usual heteroskedasticity-consistent estimator of the asymptotic variance are
invalid; on the other hand, tests based on these estimators and suitable
estimators of the asymptotic variance that we provide are exact. For the
special case in which the target proportion of units being assigned to each of
the treatments does not vary across strata, we additionally consider tests
based on estimators derived from a linear regression with strata fixed effects,
i.e., a linear regression of the outcome on indicators for each of the
treatments and indicators for each of the strata. We show that tests based on
these estimators using the usual heteroskedasticity-consistent estimator of the
asymptotic variance are conservative, but tests based on these estimators and
suitable estimators of the asymptotic variance that we provide are exact. A
simulation study illustrates the practical relevance of our theoretical
results.
</summary>
    <author>
      <name>Federico A. Bugni</name>
    </author>
    <author>
      <name>Ivan A. Canay</name>
    </author>
    <author>
      <name>Azeem M. Shaikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04119v1</id>
    <updated>2018-06-11T17:39:37Z</updated>
    <published>2018-06-11T17:39:37Z</published>
    <title>Valid Post-selection Inference in Assumption-lean Linear Regression</title>
    <summary>  Construction of valid statistical inference for estimators based on
data-driven selection has received a lot of attention in the recent times. Berk
et al. (2013) is possibly the first work to provide valid inference for
Gaussian homoscedastic linear regression with fixed covariates under arbitrary
covariate/variable selection. The setting is unrealistic and is extended by
Bachoc et al. (2016) by relaxing the distributional assumptions. A major
drawback of the aforementioned works is that the construction of valid
confidence regions is computationally intensive. In this paper, we first prove
that post-selection inference is equivalent to simultaneous inference and then
construct valid post-selection confidence regions which are computationally
simple. Our construction is based on deterministic inequalities and apply to
independent as well as dependent random variables without the requirement of
correct distributional assumptions. Finally, we compare the volume of our
confidence regions with the existing ones and show that under non-stochastic
covariates, our regions are much smaller.
</summary>
    <author>
      <name>Arun Kumar Kuchibhotla</name>
    </author>
    <author>
      <name>Lawrence D. Brown</name>
    </author>
    <author>
      <name>Andreas Buja</name>
    </author>
    <author>
      <name>Edward I. George</name>
    </author>
    <author>
      <name>Linda Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02620v2</id>
    <updated>2018-06-11T14:55:32Z</updated>
    <published>2018-05-07T17:04:41Z</published>
    <title>An Application of PC-AiR Method for Population Structure Inference in
  the Presence of Sample Relatedness</title>
    <summary>  In this paper, we introduce the PC-AiR method for robust inference of
population structure when there exist related individuals. We describe the
PC-AiR approach in detail and especially examine the shrinkage phenomenon when
predicting the PC scores. To evaluate the performance and compare it with other
methods, we apply it as well as competing methods in a simulation study and two
real datasets.
</summary>
    <author>
      <name>Bochao Jia</name>
    </author>
    <author>
      <name>Bingxin Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02620v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02620v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09561v2</id>
    <updated>2018-06-11T13:57:26Z</updated>
    <published>2017-05-26T12:55:36Z</published>
    <title>Differentially private significance tests for regression coefficients</title>
    <summary>  Many data producers seek to provide users access to confidential data without
unduly compromising data subjects' privacy and confidentiality. One general
strategy is to require users to do analyses without seeing the confidential
data; for example, analysts only get access to synthetic data or query systems
that provide disclosure-protected outputs of statistical models. With synthetic
data or redacted outputs, the analyst never really knows how much to trust the
resulting findings. In particular, if the user did the same analysis on the
confidential data, would regression coefficients of interest be statistically
significant or not? We present algorithms for assessing this question that
satisfy differential privacy. We describe conditions under which the algorithms
should give accurate answers about statistical significance. We illustrate the
properties of the proposed methods using artificial and genuine data.
</summary>
    <author>
      <name>Andrés F. Barrientos</name>
    </author>
    <author>
      <name>Jerome P. Reiter</name>
    </author>
    <author>
      <name>Ashwin Machanavajjhala</name>
    </author>
    <author>
      <name>Yan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03954v1</id>
    <updated>2018-06-11T13:23:13Z</updated>
    <published>2018-06-11T13:23:13Z</published>
    <title>Statistics on functional data and covariance operators in linear inverse
  problems</title>
    <summary>  We introduce a framework for the statistical analysis of functional data in a
setting where these objects cannot be fully observed, but only indirect and
noisy measurements are available, namely an inverse problem setting. The
proposed methodology can be applied either to the analysis of indirectly
observed functional data or to the associated covariance operators,
representing second-order information, and thus lying on a non-Euclidean space.
To deal with the ill-posedness of the inverse problem, we exploit the spatial
structure of the sample data by introducing a flexible regularizing term
embedded in the model. Thanks to its efficiency, the proposed model is applied
to MEG data, leading to a novel statistical approach to the investigation of
functional connectivity.
</summary>
    <author>
      <name>Eardi Lila</name>
    </author>
    <author>
      <name>Simon Arridge</name>
    </author>
    <author>
      <name>John A. D. Aston</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03850v1</id>
    <updated>2018-06-11T08:01:53Z</updated>
    <published>2018-06-11T08:01:53Z</published>
    <title>Confidence ellipsoids for regression coefficients by observations from a
  mixture</title>
    <summary>  Confidence ellipsoids for linear regression coefficients are constructed by
observations from a mixture with varying concentrations. Two approaches are
discussed. The first one is the nonparametric approach based on the weighted
least squares technique. The second one is an approximate maximum likelihood
estimation with application of the EM-algorithm for the estimates calculation.
</summary>
    <author>
      <name>Vitalii Miroshnichenko</name>
    </author>
    <author>
      <name>Rostyslav Maiboroda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15559/18-VMSTA105</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15559/18-VMSTA105" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at https://doi.org/10.15559/18-VMSTA105 in the Modern
  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)
  by VTeX (http://www.vtex.lt/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Modern Stochastics: Theory and Applications 2018, Vol. 5, No. 2,
  225-245</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.03850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04136v3</id>
    <updated>2018-06-11T07:56:23Z</updated>
    <published>2017-05-11T12:31:57Z</published>
    <title>Adaptively Transformed Mixed Model Prediction of General Finite
  Population Parameters</title>
    <summary>  For estimating area-specific parameters (quantities) in a finite population,
a mixed model prediction approach is attractive. However, this approach
strongly depends on the normality assumption of the response values although we
often encounter a non-normal case in practice. In such a case, transforming
observations to make them suitable for normality assumption is a useful tool,
but the problem of selecting suitable transformation still remains open. To
overcome the difficulty, we here propose a new empirical best predicting method
by using a parametric family of transformations to estimate a suitable
transformation based on the data. We suggest a simple estimating method for
transformation parameters based on the profile likelihood function, which
achieves consistency under some conditions on transformation functions. For
measuring variability of point prediction, we construct an empirical Bayes
confidence interval of the population parameter of interest. Through simulation
studies, we investigate numerical performance of the proposed methods. Finally,
we apply the proposed method to synthetic income data in Spanish provinces in
which the resulting estimates indicate that the commonly used
log-transformation would not be appropriate.
</summary>
    <author>
      <name>Shonosuke Sugasawa</name>
    </author>
    <author>
      <name>Tatsuya Kubokawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04136v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04136v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02383v2</id>
    <updated>2018-06-11T07:26:31Z</updated>
    <published>2017-12-06T19:32:18Z</published>
    <title>Predictive inference for locally stationary time series with an
  application to climate data</title>
    <summary>  The Model-free Prediction Principle of Politis (2015) has been successfully
applied to general regression problems, as well as problems involving
stationary time series. However, with long time series, e.g. annual temperature
measurements spanning over 100 years or daily financial returns spanning
several years, it may be unrealistic to assume stationarity throughout the span
of the dataset. In the paper at hand, we show how Model-free Prediction can be
applied to handle time series that are only locally stationary, i.e., they can
be assumed to be as stationary only over short time-windows. Surprisingly there
is little literature on point prediction for general locally stationary time
series even in model-based setups and there is no literature on the
construction of prediction intervals of locally stationary time series. We
attempt to fill this gap here as well. Both one-step-ahead point predictors and
prediction intervals are constructed, and the performance of model-free is
compared to model-based prediction using models that incorporate a trend and/or
heteroscedasticity. Both aspects of the paper, model-free and model-based, are
novel in the context of time-series that are locally (but not globally)
stationary. We also demonstrate the application of our Model-based and
Model-free prediction methods to speleothem climate data which exhibits local
stationarity and show that our best model-free point prediction results
outperform that obtained with the RAMPFIT algorithm previously used for
analysis of this data.
</summary>
    <author>
      <name>Srinjoy Das</name>
    </author>
    <author>
      <name>Dimitris N. Politis</name>
    </author>
    <link href="http://arxiv.org/abs/1712.02383v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02383v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03829v1</id>
    <updated>2018-06-11T06:49:50Z</updated>
    <published>2018-06-11T06:49:50Z</published>
    <title>Mixed-Effect Time-Varying Network Model and Application in Brain
  Connectivity Analysis</title>
    <summary>  Time-varying networks are fast emerging in a wide range of scientific and
business disciplines. Most existing dynamic network models are limited to a
single-subject and discrete-time setting. In this article, we propose a
mixed-effect multi-subject continuous-time stochastic blockmodel that
characterizes the time-varying behavior of the network at the population level,
meanwhile taking into account individual subject variability. We develop a
multi-step optimization procedure for a constrained stochastic blockmodel
estimation, and derive the asymptotic property of the estimator. We demonstrate
the effectiveness of our method through both simulations and an application to
a study of brain development in youth.
</summary>
    <author>
      <name>Jingfei Zhang</name>
    </author>
    <author>
      <name>Will Wei Sun</name>
    </author>
    <author>
      <name>Lexin Li</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03745v1</id>
    <updated>2018-06-10T23:38:02Z</updated>
    <published>2018-06-10T23:38:02Z</published>
    <title>Forecast evaluation with imperfect observations and imperfect models</title>
    <summary>  The field of statistics has become one of the mathematical foundations in
forecast evaluations studies, especially in regard to computing scoring rules.
The classical paradigm of proper scoring rules is to discriminate between two
different forecasts by comparing them with observations. The probability
density function of the observed record is assumed to be perfect as a
verification benchmark. In practice, however, observations are almost always
tainted by errors. These may be due to homogenization problems, instrumental
deficiencies, the need for indirect reconstructions from other sources (e.g.,
radar data), model errors in gridded products like reanalysis, or any other
data-recording issues. If the yardstick used to compare forecasts is imprecise,
one can wonder whether such types of errors may or may not have a strong
influence on decisions based on classical scoring rules. Building on the recent
work of Ferro (2017), we propose a new scoring rule scheme in the context of
models that incorporate errors of the verification data, we compare it to
existing methods, and applied it to various setups, mainly a Gaussian additive
noise model and a gamma multiplicative noise model. In addition, we frame the
problem of error verification in datasets as scoring a model that jointly
couples forecasts and observation distributions. This is strongly connected to
the so-called error-in-variables models in statistics.
</summary>
    <author>
      <name>Philippe Naveau</name>
    </author>
    <author>
      <name>Julie Bessac</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03729v1</id>
    <updated>2018-06-10T21:46:41Z</updated>
    <published>2018-06-10T21:46:41Z</published>
    <title>Lost in translation: On the impact of data coding on penalized
  regression with interactions</title>
    <summary>  Penalized regression approaches are standard tools in quantitative genetics.
It is known that the fit of an \emph{ordinary least squares} (OLS) regression
is independent of certain transformations of the coding of the predictor
variables, and that the standard mixed model \emph{ridge regression best linear
unbiased prediction} (RRBLUP) is neither affected by translations of the
variable coding, nor by global scaling. However, it has been reported that an
extended version of this mixed model, which incorporates interactions by
products of markers as additional predictor variables is affected by
translations of the marker coding. In this work, we identify the cause of this
loss of invariance in a general context of penalized regression on polynomials
in the predictor variables. We show that in most cases, translating the coding
of the predictor variables has an impact on effect estimates, with an exception
when only the size of the coefficients of monomials of highest total degree are
penalized. The invariance of RRBLUP can thus be considered as a special case of
this setting, with a polynomial of total degree 1, where the size of the fixed
effect (total degree 0) is not penalized but all coefficients of monomials of
total degree 1 are. The extended RRBLUP, which includes interactions, is not
invariant to translations because it does not only penalize interactions (total
degree 2), but also additive effects (total degree 1). Our observations are not
restricted to ridge regression, but generally valid for penalized regressions,
for instance also for the $\ell_1$ penalty of LASSO.
</summary>
    <author>
      <name>Johannes W R Martini</name>
    </author>
    <author>
      <name>Francisco Rosales</name>
    </author>
    <author>
      <name>Ngoc-Thuy Ha</name>
    </author>
    <author>
      <name>Thomas Kneib</name>
    </author>
    <author>
      <name>Johannes Heise</name>
    </author>
    <author>
      <name>Valentin Wimmer</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J05 62J07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04530v1</id>
    <updated>2018-06-10T19:24:18Z</updated>
    <published>2018-06-10T19:24:18Z</published>
    <title>A Least Squares Estimation of a Hybrid log-Poisson Regression and its
  Goodness of Fit for Optimal Loss Reserves in Insurance</title>
    <summary>  In this article, the parameters of a hybrid log-linear model (log-Poisson)
are estimated using the fuzzy least-squares (FLS) procedures (Celmi\c{n}\v{s},
987a,b, D'Urso and Gastaldi, 2000, DUrso and Gastaldi, 2001). A goodness of fit
have been derived in order to assess and compare this new model and the
classical log-Poisson regression in loss reserving framework (Mack, 1991). Both
the hybrid model and its goodness of fit are performed on a loss reserving
data.
</summary>
    <author>
      <name>Apollinaire Woundjiague</name>
    </author>
    <author>
      <name>Martin Le Doux Mbele Bidima</name>
    </author>
    <author>
      <name>Ronald Waweru Mwangi</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03673v1</id>
    <updated>2018-06-10T15:28:23Z</updated>
    <published>2018-06-10T15:28:23Z</published>
    <title>Sample size calculation and blinded recalculation for analysis of
  covariance models with multiple random covariates</title>
    <summary>  When testing for superiority in a parallel-group setting with a continuous
outcome, adjusting for covariates (e.g., baseline measurements) is usually
recommended, in order to reduce bias and increase power. For this purpose, the
analysis of covariance (ANCOVA) is frequently used, and recently, several exact
and approximate sample size calculation procedures have been proposed. However,
in case of multiple covariates, the planning might pose some practical
challenges and surprising pitfalls, which have not been recognized so far.
Moreover, since a considerable number of parameters have to be specified in
advance, the risk of making erroneous initial assumptions, leading to
substantially over- or underpowered studies, is increased. Therefore, we
propose a method, which allows for re-estimating the sample size at a
prespecified time point during the course of the trial. Extensive simulations
for a broad range of settings, including unbalanced designs, confirm that the
proposed method provides reliable results in many practically relevant
situations. An advantage of the reassessment procedure is that it does not
require unblinding of the data. In order to facilitate the application of the
proposed method, we provide some R code and discuss a real-life data example.
</summary>
    <author>
      <name>Georg Zimmermann</name>
    </author>
    <author>
      <name>Meinhard Kieser</name>
    </author>
    <author>
      <name>Arne Bathke</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03668v1</id>
    <updated>2018-06-10T14:50:12Z</updated>
    <published>2018-06-10T14:50:12Z</published>
    <title>Generalized Goodness-Of-Fit Tests for Correlated Data</title>
    <summary>  This paper concerns the problem of applying the generalized goodness-of-fit
(gGOF) type tests for analyzing correlated data. The gGOF family broadly covers
the maximum-based testing procedures by ordered input $p$-values, such as the
false discovery rate procedure, the Kolmogorov-Smirnov type statistics, the
$\phi$-divergence family, etc. Data analysis framework and a novel $p$-value
calculation approach is developed under the Gaussian mean model and the
generalized linear model (GLM). We reveal the influence of data transformations
to the signal-to-noise ratio and the statistical power under both sparse and
dense signal patterns and various correlation structures. In particular, the
innovated transformation (IT), which is shown equivalent to the marginal
model-fitting under the GLM, is often preferred for detecting sparse signals in
correlated data. We propose a testing strategy called the digGOF, which
combines a double-adaptation procedure (i.e., adapting to both the statistic's
formula and the truncation scheme of the input $p$-values) and the IT within
the gGOF family. It features efficient computation and robust adaptation to the
family-retained advantages for given data. Relevant approaches are assessed by
extensive simulations and by genetic studies of Crohn's disease and amyotrophic
lateral sclerosis. Computations have been included into the R package SetTest
available on CRAN.
</summary>
    <author>
      <name>Hong Zhang</name>
    </author>
    <author>
      <name>Zheyang Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03647v1</id>
    <updated>2018-06-10T12:23:15Z</updated>
    <published>2018-06-10T12:23:15Z</published>
    <title>Determining the dimension of factor structures in non-stationary large
  datasets</title>
    <summary>  We propose a procedure to determine the dimension of the common factor space
in a large, possibly non-stationary, dataset. Our procedure is designed to
determine whether there are (and how many) common factors (i) with linear
trends, (ii) with stochastic trends, (iii) with no trends, i.e. stationary. Our
analysis is based on the fact that the largest eigenvalues of a suitably scaled
covariance matrix of the data (corresponding to the common factor part)
diverge, as the dimension $N$ of the dataset diverges, whilst the others stay
bounded. Therefore, we propose a class of randomised test statistics for the
null that the $p$-th eigenvalue diverges, based directly on the estimated
eigenvalue. The tests only requires minimal assumptions on the data, and no
restrictions on the relative rates of divergence of $N$ and $T$ are imposed.
Monte Carlo evidence shows that our procedure has very good finite sample
properties, clearly dominating competing approaches when no common factors are
present. We illustrate our methodology through an application to US bond yields
with different maturities observed over the last 30 years. A common linear
trend and two common stochastic trends are found and identified as the
classical level, slope and curvature factors.
</summary>
    <author>
      <name>Matteo Barigozzi</name>
    </author>
    <author>
      <name>Lorenzo Trapani</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03471v1</id>
    <updated>2018-06-09T12:55:46Z</updated>
    <published>2018-06-09T12:55:46Z</published>
    <title>A new measure of treatment effect for random-effects meta-analysis of
  comparative binary outcome data</title>
    <summary>  Comparative binary outcome data are of fundamental interest in statistics and
are often pooled in meta-analyses. Here we examine the simplest case where for
each study there are two patient groups and a binary event of interest, giving
rise to a series of $2 \times 2$ tables. A variety of measures of treatment
effect are then available and are conventionally used in meta-analyses, such as
the odds ratio, the risk ratio and the risk difference. Here we propose a new
type of measure of treatment effect for this type of data that is very easily
interpretable by lay audiences. We give the rationale for the new measure and
we present three contrasting methods for computing its within-study variance so
that it can be used in conventional meta-analyses. We then develop three
alternative methods for random-effects meta-analysis that use our measure and
we apply our methodolgy to some real examples. We conclude that our new measure
is a fully viable alternative to existing measures. It has the advantage that
its interpretation is especially simple and direct, so that its meaning can be
more readily understood by those with little or no formal statistical training.
This may be especially valuable when presenting `plain language summaries',
such as those used by Cochrane.
</summary>
    <author>
      <name>Rose Baker</name>
    </author>
    <author>
      <name>Dan Jackson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62 statistics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01449v5</id>
    <updated>2018-06-09T00:37:21Z</updated>
    <published>2017-09-05T15:26:13Z</published>
    <title>Visualization in Bayesian workflow</title>
    <summary>  Bayesian data analysis is about more than just computing a posterior
distribution, and Bayesian visualization is about more than trace plots of
Markov chains. Practical Bayesian data analysis, like all data analysis, is an
iterative process of model building, inference, model checking and evaluation,
and model expansion. Visualization is helpful in each of these stages of the
Bayesian workflow and it is indispensable when drawing inferences from the
types of modern, high-dimensional models that are used by applied researchers.
</summary>
    <author>
      <name>Jonah Gabry</name>
    </author>
    <author>
      <name>Daniel Simpson</name>
    </author>
    <author>
      <name>Aki Vehtari</name>
    </author>
    <author>
      <name>Michael Betancourt</name>
    </author>
    <author>
      <name>Andrew Gelman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 Figures. Includes supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01449v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01449v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03120v1</id>
    <updated>2018-06-08T12:38:21Z</updated>
    <published>2018-06-08T12:38:21Z</published>
    <title>Variational inference for sparse network reconstruction from count data</title>
    <summary>  In multivariate statistics, the question of finding direct interactions can
be formulated as a problem of network inference - or network reconstruction -
for which the Gaussian graphical model (GGM) provides a canonical framework.
Unfortunately, the Gaussian assumption does not apply to count data which are
encountered in domains such as genomics, social sciences or ecology.
  To circumvent this limitation, state-of-the-art approaches use two-step
strategies that first transform counts to pseudo Gaussian observations and then
apply a (partial) correlation-based approach from the abundant literature of
GGM inference. We adopt a different stance by relying on a latent model where
we directly model counts by means of Poisson distributions that are conditional
to latent (hidden) Gaussian correlated variables. In this multivariate Poisson
lognormal-model, the dependency structure is completely captured by the latent
layer. This parametric model enables to account for the effects of covariates
on the counts.
  To perform network inference, we add a sparsity inducing constraint on the
inverse covariance matrix of the latent Gaussian vector. Unlike the usual
Gaussian setting, the penalized likelihood is generally not tractable, and we
resort instead to a variational approach for approximate likelihood
maximization. The corresponding optimization problem is solved by alternating a
gradient ascent on the variational parameters and a graphical-Lasso step on the
covariance matrix.
  We show that our approach is highly competitive with the existing methods on
simulation inspired from microbiological data. We then illustrate on three
various data sets how accounting for sampling efforts via offsets and
integrating external covariates (which is mostly never done in the existing
literature) drastically changes the topology of the inferred network.
</summary>
    <author>
      <name>Julien Chiquet</name>
    </author>
    <author>
      <name>Mahendra Mariadassou</name>
    </author>
    <author>
      <name>Stéphane Robin</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03087v1</id>
    <updated>2018-06-08T11:07:56Z</updated>
    <published>2018-06-08T11:07:56Z</published>
    <title>Estimation of marginal model with subgroup auxiliary information</title>
    <summary>  Marginal model is a popular instrument for studying longitudinal data and
cluster data. This paper investigates the estimator of marginal model with
subgroup auxiliary information. To marginal model, we propose a new type of
auxiliary information, and combine them with the traditional estimating
equations of the quadratic inference function (QIF) method based on the
generalized method of moments (GMM). Thus obtaining a more efficient estimator.
The asymptotic normality and the test statistics of the proposed estimator are
established. The theoretical result shows that the estimator with subgroup
information is more efficient than the conventional QIF one. Simulation studies
are carried out to examine the performance of the proposed method under finite
sample. We apply the proposed method to a real data for illustration.
</summary>
    <author>
      <name>Jie He</name>
    </author>
    <author>
      <name>Xiaogang Duan</name>
    </author>
    <author>
      <name>Shumei Zhang</name>
    </author>
    <author>
      <name>Hui Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01852v2</id>
    <updated>2018-06-08T10:00:44Z</updated>
    <published>2018-05-04T16:51:38Z</published>
    <title>Selective Inference for $L_2$-Boosting</title>
    <summary>  We review several recently proposed post-selection inference frameworks and
assess their transferability to the component-wise functional gradient descent
algorithm (CFGD) under normality assumption for model errors, also known as
$L_2$-Boosting. The CFGD is one of the most versatile toolboxes to analyze data
as it scales well to high-dimensional data sets, allows for a very flexible
definition of additive regression models and incorporates inbuilt variable
selection. Due to the iterative nature, which can repeatedly select the same
component to update, a statistical inference framework for component-wise
boosting algorithms requires adaptations of existing approaches; we propose
tests and confidence intervals for linear, grouped and penalized additive model
components selected by $L_2$-Boosting. We apply our framework to the prostate
cancer data set and investigate the properties of our concepts in simulation
studies.
</summary>
    <author>
      <name>David Rügamer</name>
    </author>
    <author>
      <name>Sonja Greven</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01852v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01852v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06696v4</id>
    <updated>2018-06-08T04:59:13Z</updated>
    <published>2015-06-22T17:34:53Z</published>
    <title>A Theoretical and Empirical Comparison of the Temporal Exponential
  Random Graph Model and the Stochastic Actor-Oriented Model</title>
    <summary>  The temporal exponential random graph model (TERGM) and the stochastic
actor-oriented model (SAOM, e.g., SIENA) are popular models for longitudinal
network analysis. We compare these models theoretically, via simulation, and
through a real-data example in order to assess their relative strengths and
weaknesses. Though we do not aim to make a general claim about either being
superior to the other across all specifications, we highlight several
theoretical differences the analyst might consider and find that with some
specifications, the two models behave very similarly, while each model
out-predicts the other one the more the specific assumptions of the respective
model are met.
</summary>
    <author>
      <name>Philip Leifeld</name>
    </author>
    <author>
      <name>Skyler J. Cranmer</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06696v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06696v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05046v2</id>
    <updated>2018-06-08T02:45:51Z</updated>
    <published>2017-04-17T17:57:51Z</published>
    <title>Counting Process Based Dimension Reduction Methods for Censored Outcomes</title>
    <summary>  We propose a class of dimension reduction methods for right censored survival
data using a counting process representation of the failure process.
Semiparametric estimating equations are constructed to estimate the dimension
reduction subspace for the failure time model. The proposed method addresses
two fundamental limitations of existing approaches. First, using the counting
process formulation, it does not require any estimation of the censoring
distribution to compensate the bias in estimating the dimension reduction
subspace. Second, the nonparametric part in the estimating equations is
adaptive to the structural dimension, hence the approach circumvents the curse
of dimensionality. Asymptotic normality is established for the obtained
estimators. We further propose a computationally efficient approach that
simplifies the estimation equation formulations and requires only a singular
value decomposition to estimate the dimension reduction subspace. Numerical
studies suggest that our new approaches exhibit significantly improved
performance for estimating the true dimension reduction subspace. We further
conduct a real data analysis on a skin cutaneous melanoma dataset from The
Cancer Genome Atlas. The proposed method is implemented in the R package
"orthoDr".
</summary>
    <author>
      <name>Qiang Sun</name>
    </author>
    <author>
      <name>Ruoqing Zhu</name>
    </author>
    <author>
      <name>Tao Wang</name>
    </author>
    <author>
      <name>Donglin Zeng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First version</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05046v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05046v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62N01, 62G08" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02935v1</id>
    <updated>2018-06-08T01:26:46Z</updated>
    <published>2018-06-08T01:26:46Z</published>
    <title>Causal effects based on distributional distances</title>
    <summary>  We develop a novel framework for estimating causal effects based on the
discrepancy between unobserved counterfactual distributions. In our setting a
causal effect is defined in terms of the $L_1$ distance between different
counterfactual outcome distributions, rather than a mean difference in outcome
values. Directly comparing counterfactual outcome distributions can provide
more nuanced and valuable information about causality than a simple comparison
of means. We consider single- and multi-source randomized studies, as well as
observational studies, and analyze error bounds and asymptotic properties of
the proposed estimators. We further propose methods to construct confidence
intervals for the unknown mean distribution distance. Finally, we illustrate
the new methods and verify their effectiveness in empirical studies.
</summary>
    <author>
      <name>Kwangho Kim</name>
    </author>
    <author>
      <name>Jisu Kim</name>
    </author>
    <author>
      <name>Edward H. Kennedy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08841v3</id>
    <updated>2018-06-08T01:06:22Z</updated>
    <published>2018-04-24T05:19:44Z</published>
    <title>Between hard and soft thresholding: optimal iterative thresholding
  algorithms</title>
    <summary>  Iterative thresholding algorithms seek to optimize a differentiable objective
function over a sparsity or rank constraint by alternating between gradient
steps that reduce the objective, and thresholding steps that enforce the
constraint. This work examines the choice of the thresholding operator, and
asks whether it is possible to achieve stronger guarantees than what is
possible with hard thresholding. We develop the notion of relative concavity of
a thresholding operator, a quantity that characterizes the convergence
performance of any thresholding operator on the target optimization problem.
Surprisingly, we find that commonly used thresholding operators, such as hard
thresholding and soft thresholding, are suboptimal in terms of convergence
guarantees. Instead, a general class of thresholding operators, lying between
hard thresholding and soft thresholding, is shown to be optimal with the
strongest possible convergence guarantee among all thresholding operators.
Examples of this general class includes $\ell_q$ thresholding with appropriate
choices of $q$, and a newly defined {\em reciprocal thresholding} operator. We
also investigate the implications of the improved optimization guarantee in the
statistical setting of sparse linear regression, and show that this new class
of thresholding operators attain the optimal rate for computationally efficient
estimators, matching the Lasso.
</summary>
    <author>
      <name>Haoyang Liu</name>
    </author>
    <author>
      <name>Rina Foygel Barber</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08841v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08841v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.04625v3</id>
    <updated>2018-06-08T01:04:36Z</updated>
    <published>2017-02-15T14:36:44Z</published>
    <title>Non-separable Models with High-dimensional Data</title>
    <summary>  This paper studies non-separable models with a continuous treatment when the
dimension of the control variables is high and potentially larger than the
effective sample size. We propose a three-step estimation procedure to estimate
the average, quantile, and marginal treatment effects. In the first stage we
estimate the conditional mean, distribution, and density objects by penalized
local least squares, penalized local maximum likelihood estimation, and
penalized conditional density estimation, respectively, where control variables
are selected via a localized method of $L_{1}$ -penalization at each value of
the continuous treatment. In the second stage we estimate the average and the
marginal distribution of the potential outcome via the plug-in principle. In
the third stage, we estimate the quantile and marginal treatment effects by
inverting the estimated distribution function and using the local linear
regression, respectively. We study the asymptotic properties of these
estimators and propose a weighted-bootstrap method for inference. Using
simulated and real datasets, we demonstrate the proposed estimators perform
well in finite samples.
</summary>
    <author>
      <name>Liangjun Su</name>
    </author>
    <author>
      <name>Takuya Ura</name>
    </author>
    <author>
      <name>Yichong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1702.04625v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.04625v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03646v3</id>
    <updated>2018-06-07T23:48:56Z</updated>
    <published>2017-12-11T05:01:50Z</published>
    <title>Dynamic Mixed Frequency Synthesis for Economic Nowcasting</title>
    <summary>  We develop a novel Bayesian framework for dynamic modeling of mixed frequency
data to nowcast quarterly U.S. GDP growth. The introduced framework utilizes
foundational Bayesian theory and treats data sampled at different frequencies
as latent factors that are later synthesized, allowing flexible methodological
specifications based on interests and utility. Time-varying inter-dependencies
between the mixed frequency data are learnt and effectively mapped onto easily
interpretable parameters. A macroeconomic study of nowcasting quarterly U.S.
GDP growth using a number of monthly economic variables demonstrates
improvements in terms of nowcast performance and interpretability compared to
the standard in the literature. The study further shows that incorporating
information during a quarter markedly improves the performance in terms of both
point and density nowcasts.
</summary>
    <author>
      <name>Kenichiro McAlinn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1601.07463</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03646v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03646v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09159v2</id>
    <updated>2018-06-07T22:05:16Z</updated>
    <published>2018-03-24T20:21:06Z</published>
    <title>Efficient Discovery of Heterogeneous Treatment Effects in Randomized
  Experiments via Anomalous Pattern Detection</title>
    <summary>  In the recent literature on estimating heterogeneous treatment effects, each
proposed method makes its own set of restrictive assumptions about the
intervention's effects and which subpopulations to explicitly estimate.
Moreover, the majority of the literature provides no mechanism to identify
which subpopulations are the most affected--beyond manual inspection--and
provides little guarantee on the correctness of the identified subpopulations.
Therefore, we propose Treatment Effect Subset Scan (TESS), a new method for
discovering which subpopulation in a randomized experiment is most
significantly affected by a treatment. We frame this challenge as a pattern
detection problem where we efficiently maximize a nonparametric scan statistic
over subpopulations. Furthermore, we identify the subpopulation which
experiences the largest distributional change as a result of the intervention,
while making minimal assumptions about the intervention's effects or the
underlying data generating process. In addition to the algorithm, we
demonstrate that the asymptotic Type I and II error can be controlled, and
provide sufficient conditions for detection consistency--i.e., exact
identification of the affected subpopulation. Finally, we validate the efficacy
of the method by discovering heterogeneous treatment effects in simulations and
in real-world data from a well-known program evaluation study.
</summary>
    <author>
      <name>Edward McFowland III</name>
    </author>
    <author>
      <name>Sriram Somanchi</name>
    </author>
    <author>
      <name>Daniel B. Neill</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09159v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09159v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01677v3</id>
    <updated>2018-06-07T18:02:26Z</updated>
    <published>2017-05-04T02:26:40Z</published>
    <title>Optimized Regression Discontinuity Designs</title>
    <summary>  The increasing popularity of regression discontinuity methods for causal
inference in observational studies has led to a proliferation of different
estimating strategies, most of which involve first fitting non-parametric
regression models on both sides of a treatment assignment boundary and then
reporting plug-in estimates for the effect of interest. In applications,
however, it is often difficult to tune the non-parametric regressions in a way
that is well calibrated for the specific target of inference; for example, the
model with the best global in-sample fit may provide poor estimates of the
discontinuity parameter. In this paper, we propose an alternative method for
estimation and statistical inference in regression discontinuity designs that
uses numerical convex optimization to directly obtain the finite-sample-minimax
linear estimator for the regression discontinuity parameter, subject to bounds
on the second derivative of the conditional response function. Given a bound on
the second derivative, our proposed method is fully data-driven, and provides
uniform confidence intervals for the regression discontinuity parameter with
both discrete and continuous running variables. The method also naturally
extends to the case of multiple running variables.
</summary>
    <author>
      <name>Guido Imbens</name>
    </author>
    <author>
      <name>Stefan Wager</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review of Economics and Statistics, forthcoming</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01677v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01677v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02792v1</id>
    <updated>2018-06-07T17:09:56Z</updated>
    <published>2018-06-07T17:09:56Z</published>
    <title>Estimation of Mittag-Leffler Parameters</title>
    <summary>  We propose a procedure for estimating the parameters of the Mittag-Leffler
(ML) and the generalized Mittag-Leffler (GML) distributions. The algorithm is
less restrictive, computationally simple, and necessary to make these models
usable in practice. A comparison with the fractional moment estimator indicated
favorable results for the proposed method.
</summary>
    <author>
      <name>Dexter Cahoy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications in Statistics - Simulation and Computation, Volume
  42, Issue 2, 303-315 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.02792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02774v1</id>
    <updated>2018-06-07T16:43:40Z</updated>
    <published>2018-06-07T16:43:40Z</published>
    <title>Parameter estimation for fractional Poisson processes</title>
    <summary>  The paper proposes a formal estimation procedure for parameters of the
fractional Poisson process (fPp). Such procedures are needed to make the fPp
model usable in applied situations. The basic idea of fPp, motivated by
experimental data with long memory is to make the standard Poisson model more
flexible by permitting non-exponential, heavy-tailed distributions of
interarrival times and different scaling properties. We establish the
asymptotic normality of our estimators for the two parameters appearing in our
fPp model. This fact permits construction of the corresponding confidence
intervals. The properties of the estimators are then tested using simulated
data.
</summary>
    <author>
      <name>Dexter Cahoy</name>
    </author>
    <author>
      <name>Vladimir V. Uchaikin</name>
    </author>
    <author>
      <name>Wojbor A. Woyczynski</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Statistical Planning and Inference, Volume 140, Issue
  11, November 2010, Pages 3106-3120</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.02774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02748v1</id>
    <updated>2018-06-07T16:02:39Z</updated>
    <published>2018-06-07T16:02:39Z</published>
    <title>A stratified age-period-cohort model for spatial heterogeneity in
  all-cause mortality</title>
    <summary>  A common goal in modeling demographic rates is to compare two or more groups.
For ex- ample comparing mortality rates between men and women or between
geographic regions may reveal health inequalities. A popular class of models
for all-cause mortality as well as incidence of specific diseases like cancer
is the age-period-cohort (APC) model. Extending this model to the multivariate
setting is not straightforward because the univariate APC model suffers from
well-known identifiability problems. Often APC models are fit separately for
each strata, and then comparisons are made post hoc. This paper introduces a
stratified APC model to directly assess the sources of heterogeneity in
mortality rates using a Bayesian hierarchical model with matrix-normal priors
that share information on linear and nonlinear aspects of the APC effects
across strata. Computing, model selection, and prior specification are
addressed and the model is then applied to all-cause mortality data from the
European Union.
</summary>
    <author>
      <name>Theresa Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00351v2</id>
    <updated>2018-06-07T15:32:33Z</updated>
    <published>2017-04-30T17:58:02Z</published>
    <title>Nonparametric Cusum Charts for Angular Data with Applications in Health
  Science and Astrophysics</title>
    <summary>  This paper develops non-parametric rotation invariant CUSUMs suited to the
detection of changes in the mean direction as well as changes in the
concentration parameter of angular data. The properties of the CUSUMs are
illustrated by theoretical calculations, Monte Carlo simulation and application
to sequentially observed angular data from health science and astrophysics.
</summary>
    <author>
      <name>F. Lombard</name>
    </author>
    <author>
      <name>Douglas M. Hawkins</name>
    </author>
    <author>
      <name>Cornelis Potgieter</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02730v1</id>
    <updated>2018-06-07T15:25:11Z</updated>
    <published>2018-06-07T15:25:11Z</published>
    <title>A bootstrap test for equality of variances</title>
    <summary>  We introduce a bootstrap procedure to test the hypothesis $H_o$ that $K+1$
variances are homogeneous. The procedure uses a variance-based statistic, and
is derived from a normal-theory test for equality of variances. The test
equivalently expressed the hypothesis as $H_o: \mathbf{\eta}=(
\eta_1,\ldots,\eta_{K+1})^T=\mathbf{0}$, where $\eta_i$'s are log contrasts of
the population variances. A box-type acceptance region is constructed to test
the hypothesis $H_o$. Simulation results indicated that our method is generally
superior to the Shoemaker and Levene tests, and the bootstrapped version of
Levene test in controlling the Type I and Type II errors.
</summary>
    <author>
      <name>Dexter Cahoy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Statistics &amp; Data Analysis, Volume 54, Issue 10, 1
  October 2010, Pages 2306-2316</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.02730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02670v1</id>
    <updated>2018-06-07T13:30:36Z</updated>
    <published>2018-06-07T13:30:36Z</published>
    <title>Scalable Bayesian Nonparametric Clustering and Classification</title>
    <summary>  We develop a scalable multi-step Monte Carlo algorithm for inference under a
large class of nonparametric Bayesian models for clustering and classification.
Each step is "embarrassingly parallel" and can be implemented using the same
Markov chain Monte Carlo sampler. The simplicity and generality of our approach
makes inference for a wide range of Bayesian nonparametric mixture models
applicable to large datasets. Specifically, we apply the approach to inference
under a product partition model with regression on covariates. We show results
for inference with two motivating data sets: a large set of electronic health
records (EHR) and a bank telemarketing dataset. We find interesting clusters
and favorable classification performance relative to other widely used
competing classifiers.
</summary>
    <author>
      <name>Yang Ni</name>
    </author>
    <author>
      <name>Peter Müller</name>
    </author>
    <author>
      <name>Maurice Diesendruck</name>
    </author>
    <author>
      <name>Sinead Williamson</name>
    </author>
    <author>
      <name>Yitan Zhu</name>
    </author>
    <author>
      <name>Yuan Ji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.10547v2</id>
    <updated>2018-06-07T12:48:27Z</updated>
    <published>2018-01-31T16:58:47Z</published>
    <title>On the construction of unbiased estimators for the group testing problem</title>
    <summary>  Debiased estimation has long been an area of research in the group testing
literature. This has led to the development of several estimators with the goal
of bias minimization and, recently, an unbiased estimator based on sequential
binomial sampling. Previous research, however, has focused heavily on the
simple case where no misclassification is assumed and only one trait is to be
tested. In this paper, we consider the problem of unbiased estimation in these
broader areas, giving constructions of such estimators for several cases. We
show that, outside of the standard case addressed previously in the literature,
it is impossible to find any proper unbiased estimator, that is, an estimator
giving only values in the parameter space. This is shown to hold generally
under any binomial or multinomial sampling plans
</summary>
    <author>
      <name>Gregory Haber</name>
    </author>
    <author>
      <name>Yaakov Malinovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised version; Submitted on 7 November 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.10547v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.10547v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F10, 62L12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02666v6</id>
    <updated>2018-06-07T12:21:07Z</updated>
    <published>2015-12-08T21:19:10Z</published>
    <title>Testing-Based Forward Model Selection</title>
    <summary>  This paper introduces and analyzes a procedure called Testing-based forward
model selection (TBFMS) in linear regression problems. This procedure
inductively selects covariates that add predictive power into a working
statistical model before estimating a final regression. The criterion for
deciding which covariate to include next and when to stop including covariates
is derived from a profile of traditional statistical hypothesis tests. This
paper proves probabilistic bounds for prediction error and the number of
selected covariates, which depend on the quality of the tests. The bounds are
then specialized to a case with heteroskedastic data with tests derived from
Huber-Eicker-White standard errors. TBFMS performance is compared to Lasso and
Post-Lasso in simulation studies. TBFMS is then analyzed as a component into
larger post-model selection estimation problems for structural economic
parameters. Finally, TBFMS is used to illustrate an empirical application to
estimating determinants of economic growth.
</summary>
    <author>
      <name>Damian Kozbur</name>
    </author>
    <link href="http://arxiv.org/abs/1512.02666v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02666v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08239v3</id>
    <updated>2018-06-07T11:57:47Z</updated>
    <published>2017-08-28T08:50:51Z</published>
    <title>Power Priors Based on Multiple Historical Studies for Binary Outcomes</title>
    <summary>  Incorporating historical information into the design and analysis of a new
clinical trial has been the subject of much recent discussion. For example, in
the context of clinical trials of antibiotics for drug resistant infections,
where patients with specific infections can be difficult to recruit, there is
often only limited and heterogeneous information available from the historical
trials. To make the best use of the combined information at hand, we consider
an approach based on the multiple power prior which allows the prior weight of
each historical study to be chosen adaptively by empirical Bayes. This choice
of weight has advantages in that it varies commensurably with differences in
the historical and current data and can choose weights near 1 if the data from
the corresponding historical study are similar enough to the data from the
current study. Fully Bayesian approaches are also considered. The methods are
applied to data from antibiotics trials. An analysis of the operating
characteristics in a binomial setting shows that the proposed empirical Bayes
adaptive method works well, compared to several alternative approaches,
including the meta-analytic prior.
</summary>
    <author>
      <name>Isaac Gravestock</name>
    </author>
    <author>
      <name>Leonhard Held</name>
    </author>
    <link href="http://arxiv.org/abs/1708.08239v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08239v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02818v2</id>
    <updated>2018-06-07T10:08:33Z</updated>
    <published>2016-09-09T14:51:54Z</published>
    <title>Network Psychometrics</title>
    <summary>  This chapter provides a general introduction of network modeling in
psychometrics. The chapter starts with an introduction to the statistical model
formulation of pairwise Markov random fields (PMRF), followed by an
introduction of the PMRF suitable for binary data: the Ising model. The Ising
model is a model used in ferromagnetism to explain phase transitions in a field
of particles. Following the description of the Ising model in statistical
physics, the chapter continues to show that the Ising model is closely related
to models used in psychometrics. The Ising model can be shown to be equivalent
to certain kinds of logistic regression models, loglinear models and
multi-dimensional item response theory (MIRT) models. The equivalence between
the Ising model and the MIRT model puts standard psychometrics in a new light
and leads to a strikingly different interpretation of well-known latent
variable models. The chapter gives an overview of methods that can be used to
estimate the Ising model, and concludes with a discussion on the interpretation
of latent variables given the equivalence between the Ising model and MIRT.
</summary>
    <author>
      <name>Sacha Epskamp</name>
    </author>
    <author>
      <name>Gunter K. J. Maris</name>
    </author>
    <author>
      <name>Lourens J. Waldorp</name>
    </author>
    <author>
      <name>Denny Borsboom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Irwing, P., Hughes, D., and Booth, T. (2018). The Wiley Handbook
  of Psychometric Testing, 2 Volume Set: A Multidisciplinary Reference on
  Survey, Scale and Test Development. New York: Wiley</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02818v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02818v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02160v2</id>
    <updated>2018-06-07T09:41:27Z</updated>
    <published>2018-06-06T13:13:46Z</published>
    <title>Deep Bayesian regression models</title>
    <summary>  Regression models are used for inference and prediction in a wide range of
applications providing a powerful scientific tool for researchers and analysts
from different fields. In many research fields the amount of available data as
well as the number of potential explanatory variables is rapidly increasing.
Variable selection and model averaging have become extremely important tools
for improving inference and prediction. However, often linear models are not
sufficient and the complex relationship between input variables and a response
is better described by introducing non-linearities and complex functional
interactions. Deep learning models have been extremely successful in terms of
prediction although they are often difficult to specify and potentially suffer
from overfitting. The aim of this paper is to bring the ideas of deep learning
into a statistical framework which yields more parsimonious models and allows
to quantify model uncertainty. To this end we introduce the class of deep
Bayesian regression models (DBRM) consisting of a generalized linear model
combined with a comprehensive non-linear feature space, where non-linear
features are generated just like in deep learning but combined with variable
selection in order to include only important features. DBRM can easily be
extended to include latent Gaussian variables to model complex correlation
structures between observations, which seems to be not easily possible with
existing deep learning approaches. Two different algorithms based on MCMC are
introduced to fit DBRM and to perform Bayesian inference. The predictive
performance of these algorithms is compared with a large number of state of the
art algorithms. Furthermore we illustrate how DBRM can be used for model
inference in various applications.
</summary>
    <author>
      <name>Aliaksandr Hubin</name>
    </author>
    <author>
      <name>Geir Storvik</name>
    </author>
    <author>
      <name>Florian Frommlet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02160v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02160v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-02, 62-09, 62F07, 62F15, 62J12, 62J05, 62J99, 62M05, 05A16,&#10;  60J22, 92D20, 90C27, 90C59" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.2; G.1.6; G.2.1; G.3; I.2.0; I.2.6; I.2.8; I.5.1; I.6; I.6.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02550v1</id>
    <updated>2018-06-07T07:53:44Z</updated>
    <published>2018-06-07T07:53:44Z</published>
    <title>Undirected network models with degree heterogeneity and homophily</title>
    <summary>  The degree heterogeneity and homophily are two typical features in network
data. In this paper, we formulate a general model for undirected networks with
these two features and present the moment estimation for inferring the degree
and homophily parameters. Our model only specifies a marginal distribution on
each edge in weighted or unweighted graphs and admits the non-independent dyad
structures unlike previous works that assume independent dyads. We establish a
unified theoretical framework under which the consistency of the moment
estimator hold as the size of networks goes to infinity. We also derive its
asymptotic representation that can be used to characterize its limiting
distribution. The asymptotic representation of the estimator of the homophily
parameter contains a bias term. Accurate inference necessitates
bias-correction.Several applications are provided to illustrate the unified
theoretical result.
</summary>
    <author>
      <name>Ting Yan</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02458v1</id>
    <updated>2018-06-06T23:27:39Z</updated>
    <published>2018-06-06T23:27:39Z</published>
    <title>On Bayesian inferential tasks with infinite-state jump processes:
  efficient data augmentation</title>
    <summary>  Advances in sampling schemes for Markov jump processes have recently enabled
multiple inferential tasks. However, in statistical and machine learning
applications, we often require that these continuous-time models find support
on structured and infinite state spaces. In these cases, exact sampling may
only be achieved by often inefficient particle filtering procedures, and
rapidly augmenting observed datasets remains a significant challenge. Here, we
build on the principles of uniformization and present a tractable framework to
address this problem, which greatly improves the efficiency of existing
state-of-the-art methods commonly used in small finite-state systems, and
further scales their use to infinite-state scenarios. We capitalize on the
marginal role of variable subsets in a model hierarchy during the process
jumps, and describe an algorithm that relies on measurable mappings between
pairs of states and carefully designed sets of synthetic jump observations. The
proposed method enables the efficient integration of slice sampling techniques
and it can overcome the existing computational bottleneck. We offer evidence by
means of experiments addressing inference and clustering tasks on both
simulated and real data sets.
</summary>
    <author>
      <name>Iker Perez</name>
    </author>
    <author>
      <name>Lax Chan</name>
    </author>
    <author>
      <name>Mercedes Torres Torres</name>
    </author>
    <author>
      <name>James Goulding</name>
    </author>
    <author>
      <name>Theodore Kypraios</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02430v1</id>
    <updated>2018-06-06T21:16:06Z</updated>
    <published>2018-06-06T21:16:06Z</published>
    <title>Identifying Heritable Communities of Microbiome by Root-Unifrac and
  Wishart Distribution</title>
    <summary>  We introduce a method to identify heritable microbiome communities when the
input is a pairwise dissimilarity matrix among all samples. Current methods
target each taxon individually and are unable to take advantage of their
phylogenetic relationships. In contrast, our approach focuses on community
heritability by using the root-Unifrac to summarize the microbiome samples
through their pairwise dissimilarities while taking the phylogeny into account.
The resulting dissimilarity matrix is then transformed into an outer product
matrix and further modeled through a Wishart distribution with the same set of
variance components as in the univariate model. Directly modeling the entire
dissimilarity matrix allows us to bypass any dimension reduction steps. An
important contribution of our work is to prove the positive definiteness of
such outer product matrix, hence the applicability of the Wishart distribution.
Simulation shows that this community heritability approach has higher power
than existing methods to identify heritable groups of taxa. Empirical results
on the TwinsUK dataset are also provided.
</summary>
    <author>
      <name>Yunfan Tang</name>
    </author>
    <author>
      <name>Dan Nicolae</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01412v2</id>
    <updated>2018-06-06T20:51:05Z</updated>
    <published>2018-06-04T22:11:11Z</published>
    <title>A Fast Algorithm for Maximum Likelihood Estimation of Mixture
  Proportions Using Sequential Quadratic Programming</title>
    <summary>  Maximum likelihood estimation of mixture proportions has a long history in
statistics and continues to play a role in modern statistics, notably in
recently developed nonparametric empirical Bayes (EM) methods. Although this
problem has traditionally been solved by using an EM algorithm, recent work by
Koenker and Mizera shows that modern convex optimization methods can be
substantially faster and more accurate. In particular, they used an interior
point (IP) method to solve a dual formulation of the original optimization
problem. Here we develop a new optimization approach, based on sequential
quadratic programming (SQP), which is substantially faster than the IP method
without sacrificing accuracy. Our approach combines several ideas: first, it
solves a reformulation of the original primal problem rather than the dual;
second, it uses SQP instead of IP, with the goal of making full use of the
expensive gradient and Hessian computations; third, it uses an active set
method to solve the QP subproblem within the SQP algorithm to exploit the
sparse nature of the problem; and fourth, it uses low-rank approximations for
much faster gradient and Hessian computations at virtually no cost to accuracy.
We illustrate all these ideas in experiments on synthetic data sets as well as
on a large genetic association data set. For large data sets (e.g., $n \approx
10^6$ observations and $m \approx 10^3$ mixture components) our implementation
yields at least 100-fold reduction in compute time compared with
state-of-the-art IP methods, and it does so without sacrificing accuracy. Our
methods are implemented in Julia and R, and we have made the source code
available at https://github.com/stephenslab/mixsqp-paper .
</summary>
    <author>
      <name>Youngseok Kim</name>
    </author>
    <author>
      <name>Peter Carbonetto</name>
    </author>
    <author>
      <name>Matthew Stephens</name>
    </author>
    <author>
      <name>Mihai Anitescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01412v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01412v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02419v1</id>
    <updated>2018-06-06T20:48:35Z</updated>
    <published>2018-06-06T20:48:35Z</published>
    <title>A Likelihood-based Alternative to Null Hypothesis Significance Testing</title>
    <summary>  The logical and practical difficulties associated with research
interpretation using P values and null hypothesis significance testing have
been extensively documented. This paper describes an alternative,
likelihood-based approach to P-value interpretation. The P-value and sample
size of a research study are used to derive a likelihood function with a single
parameter, the estimated population effect size, and the method of maximum
likelihood estimation is used to calculate the most likely effect size.
Comparison of the likelihood of the most likely effect size and the likelihood
of the minimum clinically significant effect size using the likelihood ratio
test yields the clinical significance support level (or S-value), a logical and
easily understood metric of research evidence. This clinical significance
likelihood approach has distinct advantages over null hypothesis significance
testing. As motivating examples we demonstrate the calculation and
interpretation of S-values applied to two recent widely publicised trials,
WOMAN from the Lancet and RELIEF from the New England Journal of Medicine.
</summary>
    <author>
      <name>Nicholas Adams</name>
    </author>
    <author>
      <name>Gerard O'Reilly</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02411v1</id>
    <updated>2018-06-06T20:26:03Z</updated>
    <published>2018-06-06T20:26:03Z</published>
    <title>Outcome identification in electronic health records using predictions
  from an enriched Dirichlet process mixture</title>
    <summary>  We propose a novel semiparametric model for the joint distribution of a
continuous longitudinal outcome and the baseline covariates using an enriched
Dirichlet process (EDP) prior. This joint model decomposes into a linear mixed
model for the outcome given the covariates and marginals for the covariates.
The nonparametric EDP prior is placed on the regression and spline
coefficients, the error variance, and the parameters governing the predictor
space. We predict the outcome at unobserved time points for subjects with data
at other time points as well as for new subjects with only baseline covariates.
We find improved prediction over mixed models with Dirichlet process (DP)
priors when there are a large number of covariates. Our method is demonstrated
with electronic health records consisting of initiators of second generation
antipsychotic medications, which are known to increase the risk of diabetes. We
use our model to predict laboratory values indicative of diabetes for each
individual and assess incidence of suspected diabetes from the predicted
dataset. Our model also serves as a functional clustering algorithm in which
subjects are clustered into groups with similar longitudinal trajectories of
the outcome over time.
</summary>
    <author>
      <name>Bret Zeldow</name>
    </author>
    <author>
      <name>James Flory</name>
    </author>
    <author>
      <name>Alisa Stephens-Shields</name>
    </author>
    <author>
      <name>Marsha Raebel</name>
    </author>
    <author>
      <name>Jason Roy</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02304v1</id>
    <updated>2018-06-06T17:09:43Z</updated>
    <published>2018-06-06T17:09:43Z</published>
    <title>ABC Variable Selection with Bayesian Forests</title>
    <summary>  Few problems in statistics are as perplexing as variable selection in the
presence of very many redundant covariates. The variable selection problem is
most familiar in parametric environments such as the linear model or additive
variants thereof. In this work, we abandon the linear model framework, which
can be quite detrimental when the covariates impact the outcome in a non-linear
way, and turn to tree-based methods for variable selection. Such variable
screening is traditionally done by pruning down large trees or by ranking
variables based on some importance measure. Despite heavily used in practice,
these ad-hoc selection rules are not yet well understood from a theoretical
point of view. In this work, we devise a Bayesian tree-based probabilistic
method and show that it is consistent for variable selection when the
regression surface is a smooth mix of $p&gt;n$ covariates. These results are the
first model selection consistency results for Bayesian forest priors.
Probabilistic assessment of variable importance is made feasible by a
spike-and-slab wrapper around sum- of-trees priors. Sampling from posterior
distributions over trees is inherently very difficult. As an alternative to
MCMC, we propose ABC Bayesian Forests, a new ABC sampling method based on
data-splitting that achieves higher ABC acceptance rate while retaining
probabilistic coherence. We show that the method is robust and successful at
finding variables with high marginal inclusion probabilities. Our ABC algorithm
provides a new avenue towards approximating the median probability model in
non-parametric setups where the marginal likelihood is intractable.
</summary>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Veronika Ročková</name>
    </author>
    <author>
      <name>Yuexi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05383v2</id>
    <updated>2018-06-06T15:43:20Z</updated>
    <published>2018-05-14T18:59:04Z</published>
    <title>Spatio-temporal Bayesian On-line Changepoint Detection with Model
  Selection</title>
    <summary>  Bayesian On-line Changepoint Detection is extended to on-line model selection
and non-stationary spatio-temporal processes. We propose spatially structured
Vector Autoregressions (VARs) for modelling the process between changepoints
(CPs) and give an upper bound on the approximation error of such models. The
resulting algorithm performs prediction, model selection and CP detection
on-line. Its time complexity is linear and its space complexity constant, and
thus it is two orders of magnitudes faster than its closest competitor. In
addition, it outperforms the state of the art for multivariate data.
</summary>
    <author>
      <name>Jeremias Knoblauch</name>
    </author>
    <author>
      <name>Theodoros Damoulas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7f figures, to appear in Proceedings of the 35th
  International Conference on Machine Learning 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05383v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05383v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02138v1</id>
    <updated>2018-06-06T12:07:57Z</updated>
    <published>2018-06-06T12:07:57Z</published>
    <title>On high-dimensional modifications of some graph-based two-sample tests</title>
    <summary>  Testing for the equality of two high-dimensional distributions is a
challenging problem, and this becomes even more challenging when the sample
size is small. Over the last few decades, several graph-based two-sample tests
have been proposed in the literature, which can be used for data of arbitrary
dimensions. Most of these test statistics are computed using pairwise Euclidean
distances among the observations. But, due to concentration of pairwise
Euclidean distances, these tests have poor performance in many high-dimensional
problems. Some of them can have powers even below the nominal level when the
scale-difference between two distributions dominates the location-difference.
To overcome these limitations, we introduce a new class of dissimilarity
indices and use it to modify some popular graph-based tests. These modified
tests use the distance concentration phenomenon to their advantage, and as a
result, they outperform the corresponding tests based on the Euclidean distance
in a wide variety of examples. We establish the high-dimensional consistency of
these modified tests under fairly general conditions. Analyzing several
simulated as well as real data sets, we demonstrate their usefulness in high
dimension, low sample size situations.
</summary>
    <author>
      <name>Soham Sarkar</name>
    </author>
    <author>
      <name>Rahul Biswas</name>
    </author>
    <author>
      <name>Anil K. Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02084v1</id>
    <updated>2018-06-06T09:30:11Z</updated>
    <published>2018-06-06T09:30:11Z</published>
    <title>Bayesian varying coefficient models using PC priors</title>
    <summary>  Varying coefficient models arise naturally as a flexible extension of a
simpler model where the effect of the covariate is constant. In this work, we
present varying coefficient models in a unified way using the recently proposed
framework of penalized complexity (PC) priors to build priors that allow proper
shrinkage to the simpler model, avoiding overfitting. We illustrate their
application in two spatial examples where varying coefficient models are
relevant.
</summary>
    <author>
      <name>Maria Franco-Villoria</name>
    </author>
    <author>
      <name>Massimo Ventrucci</name>
    </author>
    <author>
      <name>Håvard Rue</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02068v1</id>
    <updated>2018-06-06T08:52:57Z</updated>
    <published>2018-06-06T08:52:57Z</published>
    <title>Dynamically rescaled Hamiltonian Monte Carlo for Bayesian Hierarchical
  Models</title>
    <summary>  Dynamically rescaled Hamiltonian Monte Carlo (DRHMC) is introduced as a
computationally fast and easily implemented method for performing full Bayesian
analysis in hierarchical statistical models. The method relies on introducing a
modified parameterisation so that the re-parameterised target distribution has
close to constant scaling properties, and thus is easily sampled using standard
(Euclidian metric) Hamiltonian Monte Carlo. Provided that the parameterisations
of the conditional distributions specifying the hierarchical model are
"constant information parameterisations" (CIP), the relation between the
modified- and original parameterisation is bijective, explicitly computed and
admit exploitation of sparsity in the numerical linear algebra involved. CIPs
for a large catalogue of statistical models are presented, and from the
catalogue, it is clear that many CIPs are currently routinely used in
statistical computing. A relation between the proposed methodology and a class
of explicitly integrated Riemann manifold Hamiltonian Monte Carlo methods is
discussed. The methodology is illustrated on several example models, including
a model for inflation rates with multiple levels of non-linearly dependent
latent variables.
</summary>
    <author>
      <name>Tore Selland Kleppe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Includes supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08129v2</id>
    <updated>2018-06-06T06:55:12Z</updated>
    <published>2017-11-22T04:59:12Z</published>
    <title>PULasso: High-dimensional variable selection with presence-only data</title>
    <summary>  In various real-world problems, we are presented with classification problems
with positive and unlabeled data, referred to as presence-only responses. In
this paper, we study variable selection in the context of presence only
responses where the number of features or covariates p is large. The
combination of presence-only responses and high dimensionality presents both
statistical and computational challenges. In this paper, we develop the PUlasso
algorithm for variable selection and classification with positive and unlabeled
responses. Our algorithm involves using the majorization-minimization (MM)
framework which is a generalization of the well-known expectation-maximization
(EM) algorithm. In particular to make our algorithm scalable, we provide two
computational speed-ups to the standard EM algorithm. We provide a theoretical
guarantee where we first show that our algorithm converges to a stationary
point, and then prove that any stationary point within a local neighborhood of
the true parameter achieves the minimax optimal mean-squared error under both
strict sparsity and group sparsity assumptions. We also demonstrate through
simulations that our algorithm out-performs state-of-the-art algorithms in the
moderate p settings in terms of classification performance. Finally, we
demonstrate that our PUlasso algorithm performs well on a biochemistry example.
</summary>
    <author>
      <name>Hyebin Song</name>
    </author>
    <author>
      <name>Garvesh Raskutti</name>
    </author>
    <link href="http://arxiv.org/abs/1711.08129v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08129v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01979v1</id>
    <updated>2018-06-06T02:12:20Z</updated>
    <published>2018-06-06T02:12:20Z</published>
    <title>Spike Sorting by Convolutional Dictionary Learning</title>
    <summary>  Spike sorting refers to the problem of assigning action potentials observed
in extra-cellular recordings of neural activity to the neuron(s) from which
they originate. We cast this problem as one of learning a convolutional
dictionary from raw multi-electrode waveform data, subject to sparsity
constraints. In this context, sparsity refers to the number of neurons that are
allowed to spike simultaneously. The convolutional dictionary setting, along
with its assumptions (e.g. refractoriness) that are motivated by the
spike-sorting problem, let us give theoretical bounds on the sample complexity
of spike sorting as a function of the number of underlying neurons, the rate of
occurrence of simultaneous spiking, and the firing rate of the neurons. We
derive memory/computation-efficient convolutional versions of OMP (cOMP) and
KSVD (cKSVD), popular algorithms for sparse coding and dictionary learning
respectively. We demonstrate via simulations that an algorithm that alternates
between cOMP and cKSVD can recover the underlying spike waveforms successfully,
assuming few neurons spike simultaneously, and is stable in the presence of
noise. We also apply the algorithm to extra-cellular recordings from a tetrode
in the rat Hippocampus.
</summary>
    <author>
      <name>Andrew H. Song</name>
    </author>
    <author>
      <name>Francisco Flores</name>
    </author>
    <author>
      <name>Demba Ba</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01956v1</id>
    <updated>2018-06-05T22:53:21Z</updated>
    <published>2018-06-05T22:53:21Z</published>
    <title>Distribution free goodness of fit tests for regularly varying tail
  distributions</title>
    <summary>  We discuss in this paper a possibility of constructing a whole class of
asymptotic distribution-free tests for testing regularly varying tail
distributions. The idea is that we treat the tails of distributions as members
of a parametric family and using MLE to estimate the exponent. No matter what
the exponent's estimator is, we are able to transform the whole class into a
specific distribution with a prefix exponent so that we are free from choosing
any functional of the tail empirical process as a distribution-free test
statistic. The asymptotic behavior of some new tests, as examples from the
whole class of new tests, are demonstrated as well.
</summary>
    <author>
      <name>Thuong Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01947v1</id>
    <updated>2018-06-05T22:02:53Z</updated>
    <published>2018-06-05T22:02:53Z</published>
    <title>A linear time method for the detection of point and collective anomalies</title>
    <summary>  The challenge of efficiently identifying anomalies in data sequences is an
important statistical problem that now arises in many applications. Whilst
there has been substantial work aimed at making statistical analyses robust to
outliers, or point anomalies, there has been much less work on detecting
anomalous segments, or collective anomalies. By bringing together ideas from
changepoint detection and robust statistics, we introduce Collective And Point
Anomalies (CAPA), a computationally efficient approach that is suitable when
collective anomalies are characterised by either a change in mean, variance, or
both, and distinguishes them from point anomalies. Theoretical results
establish the consistency of CAPA at detecting collective anomalies and
empirical results show that CAPA has close to linear computational cost as well
as being more accurate at detecting and locating collective anomalies than
other approaches. We demonstrate the utility of CAPA through its ability to
detect exoplanets from light curve data from the Kepler telescope.
</summary>
    <author>
      <name>Alexander Tristan Maximilian Fisch</name>
    </author>
    <author>
      <name>Idris Arthur Eckley</name>
    </author>
    <author>
      <name>Paul Fearnhead</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01936v1</id>
    <updated>2018-06-05T21:11:59Z</updated>
    <published>2018-06-05T21:11:59Z</published>
    <title>Selection and Estimation Optimality in High Dimensions with the TWIN
  Penalty</title>
    <summary>  We introduce a novel class of variable selection penalties called TWIN, which
provides sensible data-adaptive penalization. Under a linear sparsity regime
and random Gaussian designs we show that penalties in the TWIN class have a
high probability of selecting the correct model and furthermore result in
minimax optimal estimators. The general shape of penalty functions in the TWIN
class is the key ingredient to its desirable properties and results in improved
theoretical and empirical performance over existing penalties. In this work we
introduce two examples of TWIN penalties that admit simple and efficient
coordinate descent algorithms, making TWIN practical in large data settings. We
demonstrate in challenging and realistic simulation settings with high
correlations between active and inactive variables that TWIN has high power in
variable selection while controlling the number of false discoveries,
outperforming standard penalties.
</summary>
    <author>
      <name>Xiaowu Dai</name>
    </author>
    <author>
      <name>Jared D. Huling</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01760v1</id>
    <updated>2018-06-05T15:56:39Z</updated>
    <published>2018-06-05T15:56:39Z</published>
    <title>Predictive Accuracy of Markers or Risk Scores for Interval Censored
  Survival Data</title>
    <summary>  Methods for the evaluation of the predictive accuracy of biomarkers with
respect to survival outcomes subject to right censoring have been discussed
extensively in the literature. In cancer and other diseases, survival outcomes
are commonly subject to interval censoring by design or due to the follow up
schema. In this paper, we present an estimator for the area under the
time-dependent receiver operating characteristic ROC curve for interval
censored data based on a nonparametric sieve maximum likelihood approach. We
establish the asymptotic properties of the proposed estimator, and illustrate
its finite-sample properties using a simulation study. The application of our
method is illustrated using data from a cancer clinical study. An open-source R
package to implement the proposed method is available on CRAN.
</summary>
    <author>
      <name>Yuan Wu</name>
    </author>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Jiaxing Lin</name>
    </author>
    <author>
      <name>Beilin Jia</name>
    </author>
    <author>
      <name>Kouros Owzar</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00101v2</id>
    <updated>2018-06-05T14:30:28Z</updated>
    <published>2017-10-31T20:42:02Z</published>
    <title>Nonparametric covariance estimation for mixed longitudinal studies, with
  applications in midlife women's health</title>
    <summary>  Motivated by applications of mixed longitudinal studies, where a group of
subjects entering the study at different ages (cross-sectional) are followed
for successive years (longitudinal), we consider nonparametric covariance
estimation with samples of noisy and partially-observed functional
trajectories. To ensure model identifiability and estimation consistency, we
introduce and carefully discuss the reduced rank and neighboring incoherence
condition. The proposed algorithm is based on a sequential-aggregation scheme,
which is non-iterative, with only basic matrix operations and closed-form
solutions in each step. The good performance of the proposed method is
supported by both theory and numerical experiments. We also apply the proposed
procedure to a midlife women's working memory study based on the data from the
Study of Women's Health Across the Nation (SWAN).
</summary>
    <author>
      <name>Anru Zhang</name>
    </author>
    <author>
      <name>Kehui Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1711.00101v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00101v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05530v2</id>
    <updated>2018-06-05T13:58:25Z</updated>
    <published>2018-02-15T13:51:37Z</published>
    <title>Modelling spatial heterogeneity and discontinuities using Voronoi
  tessellations</title>
    <summary>  Many methods for modelling spatial processes assume global smoothness
properties; such assumptions are often violated in practice. We introduce a
method for modelling spatial processes that display heterogeneity or contain
discontinuities. The problem of non-stationarity is dealt with by using a
combination of Voronoi tessellation to partition the input space, and a
separate Gaussian process to model the data on each region of the partitioned
space. Our method is highly flexible because we allow the Voronoi cells to form
relationships with each other, which can enable non-convex and disconnected
regions to be considered. In such problems, identifying the borders between
regions is often of great importance and we propose an adaptive sampling method
to gain extra information along such borders. The method is illustrated with
simulation studies and application to real data.
</summary>
    <author>
      <name>Christopher A. Pope</name>
    </author>
    <author>
      <name>John Paul Gosling</name>
    </author>
    <author>
      <name>Stuart Barber</name>
    </author>
    <author>
      <name>Jill Johnson</name>
    </author>
    <author>
      <name>Takanobu Yamaguchi</name>
    </author>
    <author>
      <name>Graham Feingold</name>
    </author>
    <author>
      <name>Paul Blackwell</name>
    </author>
    <link href="http://arxiv.org/abs/1802.05530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02184v2</id>
    <updated>2018-06-05T13:27:23Z</updated>
    <published>2017-11-06T21:37:43Z</published>
    <title>Semiparametric Estimation of Structural Functions in Nonseparable
  Triangular Models</title>
    <summary>  Triangular systems with nonadditively separable unobserved heterogeneity
provide a theoretically appealing framework for the modelling of complex
structural relationships. However, they are not commonly used in practice due
to the need for exogenous variables with large support for identification, the
curse of dimensionality in estimation, and the lack of inferential tools. This
paper introduces two classes of semiparametric nonseparable triangular models
that address these limitations. They are based on distribution and quantile
regression modelling of the reduced form conditional distributions of the
endogenous variables. We show that average, distribution and quantile
structural functions are identified in these systems through a control function
approach that does not require a large support condition. We propose a
computationally attractive three-stage procedure to estimate the structural
functions where the first two stages consist of quantile or distribution
regressions. We provide asymptotic theory and uniform inference methods for
each stage. In particular, we derive functional central limit theorems and
bootstrap functional central limit theorems for the distribution regression
estimators of the structural functions. These results establish the validity of
the bootstrap for three-stage estimators of structural functions, and lead to
simple inference algorithms. We illustrate the implementation and applicability
of all our methods with numerical simulations and an empirical application to
demand analysis.
</summary>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <author>
      <name>Whitney Newey</name>
    </author>
    <author>
      <name>Sami Stouli</name>
    </author>
    <author>
      <name>Francis Vella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P20, 91B82" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01615v1</id>
    <updated>2018-06-05T11:37:53Z</updated>
    <published>2018-06-05T11:37:53Z</published>
    <title>merlin - a unified modelling framework for data analysis and methods
  development in Stata</title>
    <summary>  merlin can do a lot of things. From simple stuff, like fitting a linear
regression or a Weibull survival model, to a three-level logistic mixed effects
model, or a multivariate joint model of multiple longitudinal outcomes (of
different types) and a recurrent event and survival with non-linear
effects...the list is rather endless. merlin can do things I haven't even
thought of yet. I'll take a single dataset, and attempt to show you the full
range of capabilities of merlin, and discuss some future directions for the
implementation in Stata.
</summary>
    <author>
      <name>Michael J. Crowther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Stata Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03597v2</id>
    <updated>2018-06-05T11:23:11Z</updated>
    <published>2017-06-12T12:31:58Z</published>
    <title>Probabilistic partial least squares model: Identifiability, estimation
  and application</title>
    <summary>  With a rapid increase in volume and complexity of data sets, there is a need
for methods that can extract useful information, for example the relationship
between two data sets measured for the same persons. The Partial Least Squares
(PLS) method can be used for this dimension reduction task. Within life
sciences, results across studies are compared and combined. Therefore,
parameters need to be identifiable, which is not the case for PLS. In addition,
PLS is an algorithm, while epidemiological study designs are often
outcome-dependent and methods to analyze such data require a probabilistic
formulation. Moreover, a probabilistic model provides a statistical framework
for inference. To address these issues, we develop Probabilistic PLS (PPLS). We
derive maximum likelihood estimators that satisfy the identifiability
conditions by using an EM algorithm with a constrained optimization in the M
step. We show that the PPLS parameters are identifiable up to sign. A
simulation study is conducted to study the performance of PPLS compared to
existing methods. The PPLS estimates performed well in various scenarios, even
in high dimensions. Most notably, the estimates seem to be robust against
departures from normality. To illustrate our method, we applied it to IgG
glycan data from two cohorts. Our PPLS model provided insight as well as
interpretable results across the two cohorts.
</summary>
    <author>
      <name>Said el Bouhaddani</name>
    </author>
    <author>
      <name>Hae-Won Uh</name>
    </author>
    <author>
      <name>Caroline Hayward</name>
    </author>
    <author>
      <name>Geurt Jongbloed</name>
    </author>
    <author>
      <name>Jeanine Houwing-Duistermaat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Journal of Multivariate Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03597v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03597v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01558v1</id>
    <updated>2018-06-05T08:44:44Z</updated>
    <published>2018-06-05T08:44:44Z</published>
    <title>Combining covariance tapering and lasso driven low rank decomposition
  for the kriging of large spatial datasets</title>
    <summary>  Large spatial datasets are becoming ubiquitous in environmental sciences with
the explosion in the amount of data produced by sensors that monitor and
measure the Earth system. Consequently, the geostatistical analysis of these
data requires adequate methods. Richer datasets lead to more complex modeling
but may also prevent from using classical techniques. Indeed, the kriging
predictor is not straightforwarldly available as it requires the inversion of
the covariance matrix of the data. The challenge of handling such datasets is
therefore to extract the maximum of information they contain while ensuring the
numerical tractability of the associated inference and prediction algorithms.
The different approaches that have been developed in the literature to address
this problem can be classified into two families, both aiming at making the
inversion of the covariance matrix computationally feasible. The covariance
tapering approach circumvents the problem by enforcing the sparsity of the
covariance matrix, making it invertible in a reasonable computation time. The
second available approach assumes a low rank representation of the covariance
function. While both approaches have their drawbacks, we propose a way to
combine them and benefit from their advantages. The covariance model is assumed
to have the form low rank plus sparse. The choice of the basis functions
sustaining the low rank component is data driven and is achieved through a
selection procedure, thus alleviating the computational burden of the low rank
part. This model expresses as a spatial random effects model and the estimation
of the parameters is conducted through a step by step approach treating each
scale separately. The resulting model can account for second order non
stationarity and handle large volumes of data.
</summary>
    <author>
      <name>Thomas Romary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GEOSCIENCES</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Desassis</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07933v6</id>
    <updated>2018-06-05T07:08:22Z</updated>
    <published>2016-02-25T13:52:28Z</published>
    <title>Bootstrap Inference when Using Multiple Imputation</title>
    <summary>  Many modern estimators require bootstrapping to calculate confidence
intervals because either no analytic standard error is available or the
distribution of the parameter of interest is non-symmetric. It remains however
unclear how to obtain valid bootstrap inference when dealing with multiple
imputation to address missing data. We present four methods which are
intuitively appealing, easy to implement, and combine bootstrap estimation with
multiple imputation. We show that three of the four approaches yield valid
inference, but that the performance of the methods varies with respect to the
number of imputed data sets and the extent of missingness. Simulation studies
reveal the behavior of our approaches in finite samples. A topical analysis
from HIV treatment research, which determines the optimal timing of
antiretroviral treatment initiation in young children, demonstrates the
practical implications of the four methods in a sophisticated and realistic
setting. This analysis suffers from missing data and uses the $g$-formula for
inference, a method for which no standard errors are available.
</summary>
    <author>
      <name>Michael Schomaker</name>
    </author>
    <author>
      <name>Christian Heumann</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07933v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07933v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01460v1</id>
    <updated>2018-06-05T01:49:02Z</updated>
    <published>2018-06-05T01:49:02Z</published>
    <title>Dynamic Function-on-Scalars Regression</title>
    <summary>  We develop a modeling framework for dynamic function-on-scalars regression,
in which a time series of functional data is regressed on a time series of
scalar predictors. The regression coefficient function for each predictor is
allowed to be dynamic, which is essential for applications where the
association between predictors and a (functional) response is time-varying. For
greater modeling flexibility, we design a nonparametric reduced-rank functional
data model with an unknown functional basis expansion, which is both
data-adaptive and, unlike most existing methods, modeled as unknown for
appropriate uncertainty quantification. Within a Bayesian framework, we
introduce shrinkage priors that simultaneously (i) regularize time-varying
regression coefficient functions to be locally static, (ii) effectively remove
unimportant predictor variables from the model, and (iii) reduce sensitivity to
the selected rank of the model. A simulation analysis confirms the importance
of these shrinkage priors, with substantial improvements over existing
alternatives. We develop a novel projection-based Gibbs sampling algorithm,
which offers unrivaled computational scalability for fully Bayesian functional
regression. We apply the proposed methodology (i) to characterize the effects
of demographic predictors on age-specific fertility rates in South and
Southeast Asia, and (ii) to analyze the time-varying impact of macroeconomic
variables on the U.S. yield curve.
</summary>
    <author>
      <name>Daniel R. Kowal</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01458v1</id>
    <updated>2018-06-05T01:37:10Z</updated>
    <published>2018-06-05T01:37:10Z</published>
    <title>The Value of Information in Retrospect</title>
    <summary>  In the course of any statistical analysis, it is necessary to consider issues
of data quality and model appropriateness. Value of information (VOI) methods
were initially put forward in the middle of the twentieth century to understand
how important a portion of data is in the decision making. However, since their
genesis, VOI methods have been largely neglected by statisticians. In this
paper we review and extend existing VOI methods and recommend the use of three
quantities for identifying influential and outlying data: an influence measure
previously suggested by Kempthorne (1986), a related quantity known as the
expected value of sample information that is used to gauge how much influence
we would expect data to have, and the ratio of the two which serves as a
comparison between observed influence and expected influence.
  We study the theoretical properties of those quantities and implement our
proposed approach on two datasets. A data set of employment rates and economic
factors in U.S. (Longley, 1967) is used as an example of the linear regression.
It was also used by Cook (1977) to introduce the Cook's distance, a common
frequentist measure of influence. The HIV surveillance data has been the main
data sources for monitoring the HIV epidemics in low and middle income
countries. The Swaziland HIV prevalence data contains the number of HIV+
patients observed at multiple clinics over years. It is used as an example of
the generalized linear mixed models.
</summary>
    <author>
      <name>Jacob Parsons</name>
    </author>
    <author>
      <name>Le Bao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09846v3</id>
    <updated>2018-06-05T01:28:38Z</updated>
    <published>2017-05-27T17:29:40Z</published>
    <title>Phase Function Density Deconvolution with Heteroscedastic Measurement
  Error of Unknown Type</title>
    <summary>  It is important to properly correct for measurement error when estimating
density functions associated with biomedical variables. These estimators that
adjust for measurement error are broadly referred to as density deconvolution
estimators. While most methods in the literature assume the distribution of the
measurement error to be fully known, a recently proposed method based on the
empirical phase function (EPF) can deal with the situation when the measurement
error distribution is unknown. The EPF density estimator has only been
considered in the context of additive and homoscedastic measurement error;
however, the measurement error of many biomedical variables is heteroscedastic
in nature. In this paper, we developed a phase function approach for density
deconvolution when the measurement error has unknown distribution and is
heteroscedastic. A weighted empirical phase function (WEPF) is proposed where
the weights are used to adjust for heteroscedasticity of measurement error. The
asymptotic properties of the WEPF estimator are evaluated. Simulation results
show that the weighting can result in large decreases in mean integrated
squared error (MISE) when estimating the phase function. The estimation of the
weights from replicate observations is also discussed. Finally, the
construction of a deconvolution density estimator using the WEPF is compared to
an existing deconvolution estimator that adjusts for heteroscedasticity, but
assumes the measurement error distribution to be fully known. The WEPF
estimator proves to be competitive, especially when considering that it relies
on the minimal assumption of the distribution of measurement error.
</summary>
    <author>
      <name>Linh Nghiem</name>
    </author>
    <author>
      <name>Cornelis J. Potgieter</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09846v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09846v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07793v3</id>
    <updated>2018-06-05T00:55:49Z</updated>
    <published>2018-01-23T22:14:45Z</published>
    <title>A New Correlation Coefficient for Aggregating Non-strict and Incomplete
  Rankings</title>
    <summary>  We introduce a correlation coefficient that is designed to deal with a
variety of ranking formats including those containing non-strict (i.e.,
with-ties) and incomplete (i.e., unknown) preferences. The new measure, which
can be regarded as a generalization of the seminal Kendall tau correlation
coefficient, is proven to be equivalent to an axiomatic ranking distance
specifically designed to treat individual rankings equitably when solving the
consensus ranking problem. In an effort to further unify and enhance both
robust ranking methodologies this work proves the equivalence of an additional
axiomatic-distance and correlation-coefficient pairing in the space of
non-strict incomplete rankings. The bridging of these complementary theories
reinforces the singular suitability of the featured correlation coefficient to
solve the general consensus ranking problem. The latter premise is bolstered by
an accompanying set of experiments on random instances, which are generated via
a herein developed sampling technique connected with the classic Mallows
distribution of ranking data. To carry out the featured experiments we devise a
specialized branch and bound algorithm that provides the full set of
alternative optimal solutions, when applicable. Applying the algorithm on the
generated random instances reveals that the featured correlation coefficient
yields relative fewer alternative optimal solutions as data becomes noisier
(i.e., as the input rankings get further from a ground truth).
</summary>
    <author>
      <name>Adolfo R. Escobedo</name>
    </author>
    <author>
      <name>Yeawon Yoo</name>
    </author>
    <author>
      <name>J. Kyle Skolfield</name>
    </author>
    <link href="http://arxiv.org/abs/1801.07793v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07793v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01326v1</id>
    <updated>2018-06-04T19:00:14Z</updated>
    <published>2018-06-04T19:00:14Z</published>
    <title>Post model-fitting exploration via a "Next-Door" analysis</title>
    <summary>  We propose a simple method for evaluating the model that has been chosen by
an adaptive regression procedure, our main focus being the lasso. This
procedure deletes each chosen predictor and refits the lasso to get a set of
models that are "close" to the one chosen, referred to as "base model". If the
deletion of a predictor leads to significant deterioration in the model's
predictive power, the predictor is called indispensable; otherwise, the nearby
model is called acceptable and can serve as a good alternative to the base
model. This provides both an assessment of the predictive contribution of each
variable and a set of alternative models that may be used in place of the
chosen model.
  In this paper, we will focus on the cross-validation (CV) setting and a
model's predictive power is measured by its CV error, with base model tuned by
cross-validation. We propose a method for comparing the error rates of the base
model with that of nearby models, and a p-value for testing whether a predictor
is dispensable. We also propose a new quantity called model score which works
similarly as the p-value for the control of type I error. Our proposal is
closely related to the LOCO (leave-one-covarate-out) methods of ([Rinaldo 2016
Bootstrapping]) and less so, to Stability Selection ([Meinshausen 2010
stability]).
  We call this procedure "Next-Door analysis" since it examines models close to
the base model. It can be applied to Gaussian regression data, generalized
linear models, and other supervised learning problems with $\ell_1$
penalization. It could also be applied to best subset and stepwise regression
procedures. We have implemented it in the R language as a library to accompany
the well-known {\tt glmnet} library.
</summary>
    <author>
      <name>Leying Guan</name>
    </author>
    <author>
      <name>Robert Tibshirani</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01238v1</id>
    <updated>2018-06-04T17:25:38Z</updated>
    <published>2018-06-04T17:25:38Z</published>
    <title>Smooth Cyclically Monotone Interpolation and Empirical Center-Outward
  Distribution Functions</title>
    <summary>  We consider the smooth interpolation problem under cyclical monotonicity
constraint. More precisely, consider finite $n$-tuples
$\mathcal{X}=\{x_1,\ldots,x_n\}$ and $\mathcal{Y}=\{y_1,\ldots,y_n\}$ of points
in $ \mathbb{R}^d$, and assume the existence of a unique bijection
$T:\mathcal{X}\rightarrow \mathcal{Y}$ such that $\{(x,T(x)):\, x\in
\mathcal{X} \}$ is cyclically monotone: our goal is to define continuous,
cyclically monotone maps $\bar{T}:\mathbb{R}^d\to \mathbb{R}^d$ such
that~$\bar{T}(x_i)=y_i$, $i=1,\ldots,n$, extending a classical result by
Rockafellar on the subdifferentials of convex functions. Our solutions
$\bar{T}$ are Lipschitz, and we provide a sharp lower bound for the
corresponding Lipschitz constants. The problem is motivated by, and the
solution naturally applies to, the concept of empirical center-outward
distribution function in $\mathbb{R}^d$ developed in Hallin~(2018). Those
empirical distribution functions indeed are defined at the observations only.
Our interpolation provides a smooth extension, as well as a multivariate,
outward-continuous, jump function version thereof (the latter naturally
generalizes the traditional left-continuous univariate concept); both satisfy a
Glivenko-Cantelli property as $n\to\infty$.
</summary>
    <author>
      <name>Eustasio del Barrio</name>
    </author>
    <author>
      <name>Juan A. Cuesta-Albertos</name>
    </author>
    <author>
      <name>Marc Hallin</name>
    </author>
    <author>
      <name>Carlos Matrán</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01126v1</id>
    <updated>2018-06-04T13:59:55Z</updated>
    <published>2018-06-04T13:59:55Z</published>
    <title>Confidence Interval Estimators for MOS Values</title>
    <summary>  For the quantification of QoE, subjects often provide individual rating
scores on certain rating scales which are then aggregated into Mean Opinion
Scores (MOS). From the observed sample data, the expected value is to be
estimated. While the sample average only provides a point estimator, confidence
intervals (CI) are an interval estimate which contains the desired expected
value with a given confidence level. In subjective studies, the number of
subjects performing the test is typically small, especially in lab
environments. The used rating scales are bounded and often discrete like the
5-point ACR rating scale. Therefore, we review statistical approaches in the
literature for their applicability in the QoE domain for MOS interval
estimation (instead of having only a point estimator, which is the MOS). We
provide a conservative estimator based on the SOS hypothesis and binomial
distributions and compare its performance (CI width, outlier ratio of CI
violating the rating scale bounds) and coverage probability with well known CI
estimators. We show that the provided CI estimator works very well in practice
for MOS interval estimators, while the commonly used studentized CIs suffer
from a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.
As an alternative, bootstrapping, i.e., random sampling of the subjective
ratings with replacement, is an efficient CI estimator leading to typically
smaller CIs, but lower coverage than the proposed estimator.
</summary>
    <author>
      <name>Tobias Hossfeld</name>
    </author>
    <author>
      <name>Poul E. Heegaard</name>
    </author>
    <author>
      <name>Martin Varela</name>
    </author>
    <author>
      <name>Lea Skorin-Kapov</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01094v1</id>
    <updated>2018-06-04T13:17:14Z</updated>
    <published>2018-06-04T13:17:14Z</published>
    <title>groupICA: Independent component analysis for grouped data</title>
    <summary>  We introduce groupICA, a novel independent component analysis (ICA) algorithm
which decomposes linearly mixed multivariate observations into independent
components that are corrupted (and rendered dependent) by hidden group-wise
confounding. It extends the ordinary ICA model in a theoretically sound and
explicit way to incorporate group-wise (or environment-wise) structure in data
and hence provides a justified alternative to the use of ICA on data blindly
pooled across groups. In addition to our theoretical framework, we explain its
causal interpretation and motivation, provide an efficient estimation procedure
and prove identifiability of the unmixing matrix under mild assumptions.
Finally, we illustrate the performance and robustness of our method on
simulated data and run experiments on publicly available EEG datasets
demonstrating the applicability to real-world scenarios. We provide a
scikit-learn compatible pip-installable Python package groupICA as well as R
and Matlab implementations accompanied by a documentation and an audible
example at https://sweichwald.de/groupICA.
</summary>
    <author>
      <name>Niklas Pfister</name>
    </author>
    <author>
      <name>Sebastian Weichwald</name>
    </author>
    <author>
      <name>Peter Bühlmann</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">equal contribution between Pfister and Weichwald</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01083v1</id>
    <updated>2018-06-04T12:51:05Z</updated>
    <published>2018-06-04T12:51:05Z</published>
    <title>Optimal Balancing of Time-Dependent Confounders for Marginal Structural
  Models</title>
    <summary>  Marginal structural models (MSMs) estimate the causal effect of a
time-varying treatment in the presence of time-dependent confounding via
weighted regression. The standard approach of using inverse probability of
treatment weighting (IPTW) can lead to high-variance estimates due to extreme
weights and be sensitive to model misspecification. Various methods have been
proposed to partially address this, including truncation and stabilized-IPTW to
temper extreme weights and covariate balancing propensity score (CBPS) to
address treatment model misspecification. In this paper, we present Kernel
Optimal Weighting (KOW), a convex-optimization-based approach that finds
weights for fitting the MSM that optimally balance time-dependent confounders
while simultaneously controlling for precision, directly addressing the above
limitations. KOW directly minimizes the error in estimation due to
time-dependent confounding via a new decomposition as a functional. We further
extend KOW to control for informative censoring. We evaluate the performance of
KOW in a simulation study, comparing it with IPTW, stabilized-IPTW, and CBPS.
We demonstrate the use of KOW in studying the effect of treatment initiation on
time-to-death among people living with HIV and the effect of negative
advertising on elections in the United States.
</summary>
    <author>
      <name>Nathan Kallus</name>
    </author>
    <author>
      <name>Michele Santacatterina</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01082v1</id>
    <updated>2018-06-04T12:51:03Z</updated>
    <published>2018-06-04T12:51:03Z</published>
    <title>On an extension of the promotion time cure model</title>
    <summary>  We consider the problem of estimating the distribution of time-to-event data
that are subject to censoring and for which the event of interest might never
occur, i.e., some subjects are cured. To model this kind of data in the
presence of covariates, one of the leading semiparametric models is the
promotion time cure model \citep{yakovlev1996}, which adapts the Cox model to
the presence of cured subjects. Estimating the conditional distribution results
in a complicated constrained optimization problem, and inference is difficult
as no closed-formula for the variance is available. We propose a new model,
inspired by the Cox model, that leads to a simple estimation procedure and that
presents a closed formula for the variance. We derive some asymptotic
properties of the estimators and we show the practical behaviour of our
procedure by means of simulations. We also apply our model and estimation
method to a breast cancer data set.
</summary>
    <author>
      <name>François Portier</name>
    </author>
    <author>
      <name>Ingrid Van Keilegom</name>
    </author>
    <author>
      <name>Anouar El Ghouch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01015v1</id>
    <updated>2018-06-04T08:56:54Z</updated>
    <published>2018-06-04T08:56:54Z</published>
    <title>Dynamically borrowing strength from another study</title>
    <summary>  Meta-analytic methods may be used to combine evidence from different sources
of information. Quite commonly, the normal-normal hierarchical model (NNHM)
including a random-effect to account for between-study heterogeneity is
utilized for such analyses. The same modeling framework may also be used to not
only derive a combined estimate, but also to borrow strength for a particular
study from another by deriving a shrinkage estimate. For instance, a
small-scale randomized controlled trial could be supported by a non-randomized
study, e.g. a clinical registry. This would be particularly attractive in the
context of rare diseases. We demonstrate that a meta-analysis still makes sense
in this extreme two-study setup, as illustrated using a recent trial and a
clinical registry in Creutzfeld-Jakob disease. Derivation of a shrinkage
estimate within a Bayesian random-effects meta-analysis may substantially
improve a given estimate even based on only a single additional estimate while
accounting for potential effect heterogeneity between the studies. The proposed
approach is quite generally applicable to combine different types of evidence
originating e.g. from meta-analyses or individual studies. An application of
this more general setup is provided in immunosuppression following liver
transplantation in children.
</summary>
    <author>
      <name>Christian Röver</name>
    </author>
    <author>
      <name>Tim Friede</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04815v3</id>
    <updated>2018-06-04T08:45:24Z</updated>
    <published>2017-08-16T08:56:48Z</published>
    <title>Regression estimator for the tail index</title>
    <summary>  Estimating the tail index parameter is one of the primal objectives in
extreme value theory. For heavy-tailed distributions the Hill estimator is the
most popular way to estimate the tail index parameter. Improving the Hill
estimator was aimed by recent works with different methods, for example by
using bootstrap, or Kolmogorov-Smirnov metric. These methods are asymptotically
consistent, but for tail index $\xi &gt;1$ and smaller sample sizes the estimation
fails to approach the theoretical value for realistic sample sizes. In this
paper, we introduce a new empirical method, which can estimate high tail index
parameters well and might also be useful for relatively small sample sizes.
</summary>
    <author>
      <name>László Németh</name>
    </author>
    <author>
      <name>András Zempléni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, 11 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04815v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04815v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G32, 62F40, 60G70" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00306v2</id>
    <updated>2018-06-04T07:52:56Z</updated>
    <published>2017-07-02T15:29:13Z</published>
    <title>Variable Selection Methods for Model-based Clustering</title>
    <summary>  Model-based clustering is a popular approach for clustering multivariate data
which has seen applications in numerous fields. Nowadays, high-dimensional data
are more and more common and the model-based clustering approach has adapted to
deal with the increasing dimensionality. In particular, the development of
variable selection techniques has received a lot of attention and research
effort in recent years. Even for small size problems, variable selection has
been advocated to facilitate the interpretation of the clustering results. This
review provides a summary of the methods developed for variable selection in
model-based clustering. Existing R packages implementing the different methods
are indicated and illustrated in application to two data analysis examples.
</summary>
    <author>
      <name>Michael Fop</name>
    </author>
    <author>
      <name>Thomas Brendan Murphy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1214/18-SS119</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1214/18-SS119" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics Surveys, 12 (2018) 1-48</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.00306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00954v1</id>
    <updated>2018-06-04T05:08:27Z</updated>
    <published>2018-06-04T05:08:27Z</published>
    <title>MacroPCA: An all-in-one PCA method allowing for missing values as well
  as cellwise and rowwise outliers</title>
    <summary>  Multivariate data are typically represented by a rectangular matrix (table)
in which the rows are the objects (cases) and the columns are the variables
(measurements). When there are many variables one often reduces the dimension
by principal component analysis (PCA), which in its basic form is not robust to
outliers. Much research has focused on handling rowwise outliers, i.e. rows
that deviate from the majority of the rows in the data (for instance, they
might belong to a different population). In recent years also cellwise outliers
are receiving attention. These are suspicious cells (entries) that can occur
anywhere in the table. Even a relatively small proportion of outlying cells can
contaminate over half the rows, which causes rowwise robust methods to break
down. In this paper a new PCA method is constructed which combines the
strengths of two existing robust methods in order to be robust against both
cellwise and rowwise outliers. At the same time, the algorithm can cope with
missing values. As of yet it is the only PCA method that can deal with all
three problems simultaneously. Its name MacroPCA stands for PCA allowing for
Missings And Cellwise &amp; Rowwise Outliers. Several simulations and real data
sets illustrate its robustness. New residual maps are introduced, which help to
determine which variables are responsible for the outlying behavior. The method
is well-suited for online process control.
</summary>
    <author>
      <name>Mia Hubert</name>
    </author>
    <author>
      <name>Peter J. Rousseeuw</name>
    </author>
    <author>
      <name>Wannes Van den Bossche</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00928v1</id>
    <updated>2018-06-04T02:21:24Z</updated>
    <published>2018-06-04T02:21:24Z</published>
    <title>A Causal Exposure Response Function with Local Adjustment for
  Confounding</title>
    <summary>  In the last two decades, ambient levels of air pollution have declined
substantially. Yet, as mandated by the Clean Air Act, we must continue to
address the following question: is exposure to levels of air pollution that are
well below the National Ambient Air Quality Standards (NAAQS) harmful to human
health? Furthermore, the highly contentious nature surrounding environmental
regulations necessitates casting this question within a causal inference
framework. Several parametric and semi-parametric regression modeling
approaches have been developed for estimating the exposure-response (ER) curve.
However, most of these approaches: 1) are not formulated in the context of a
potential outcome framework for causal inference; 2) adjust for the same set of
potential confounders across all levels of exposure; and 3) do not account for
model uncertainty regarding covariate selection and shape of the ER. In this
paper, we introduce a Bayesian framework for the estimation of a causal ER
curve called LERCA (Local Exposure Response Confounding Adjustment). LERCA
allows for: a) different confounders and different strength of confounding at
the different exposure levels; and b) model uncertainty regarding confounders'
selection and the shape of ER. Also, LERCA provides a principled way of
assessing the observed covariates' confounding importance at different exposure
levels. We compare our proposed method with state of the art approaches in
causal inference for ER estimation using simulation studies. We also apply the
proposed method to a large data set that includes health, weather, demographic,
and pollution for 5,362 zip codes and for the years of 2011-2013. An R package
is available at https://github.com/gpapadog/LERCA.
</summary>
    <author>
      <name>Georgia Papadogeorgou</name>
    </author>
    <author>
      <name>Francesca Dominici</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02558v2</id>
    <updated>2018-06-03T19:21:05Z</updated>
    <published>2018-02-07T18:33:20Z</published>
    <title>Intentional Control of Type I Error over Unconscious Data Distortion: a
  Neyman-Pearson Approach to Text Classification</title>
    <summary>  Digital texts have become an increasingly important source of data for social
studies. However, textual data from open platforms are vulnerable to
manipulation (e.g., censorship and information inflation), often leading to
bias in subsequent empirical analysis. This paper investigates the problem of
data distortion in text classification when controlling type I error (a
relevant textual message is classified as irrelevant) is the priority. The
default classical classification paradigm that minimizes the overall
classification error can yield an undesirably large type I error, and data
distortion exacerbates this situation. As a solution, we propose the
Neyman-Pearson (NP) classification paradigm which minimizes type II error under
a user-specified type I error constraint. Theoretically, we show that while the
classical oracle (i.e., optimal classifier) cannot be recovered under unknown
data distortion even if one has the entire post-distortion population, the NP
oracle is unaffected by data distortion and can be recovered under the same
condition. Empirically, we illustrate the advantage of NP classification
methods in a case study that classifies posts about strikes and corruption
published on a leading Chinese blogging platform.
</summary>
    <author>
      <name>Lucy Xia</name>
    </author>
    <author>
      <name>Richard Zhao</name>
    </author>
    <author>
      <name>Yanhui Wu</name>
    </author>
    <author>
      <name>Xin Tong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02558v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02558v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00849v1</id>
    <updated>2018-06-03T18:49:50Z</updated>
    <published>2018-06-03T18:49:50Z</published>
    <title>On estimation for Brownian motion governed by telegraph process with
  multiple off states</title>
    <summary>  Brownian motion whose infinitesimal variance changes according to a
three-state continuous time Markov Chain is studied. This Markov Chain can be
viewed as a telegraph process with one on state and two off states. We first
derive the distribution of occupation time of the on state. Then the result is
used to develop a likelihood estimation procedure when the stochastic process
at hand is observed at discrete, possibly irregularly spaced time points. The
likelihood function is evaluated with the forward algorithm in the general
framework of hidden Markov models. The analytic results are confirmed with
simulation studies. The estimation procedure is applied to analyze the position
data from a mountain lion.
</summary>
    <author>
      <name>Vladimir Pozdnyakov</name>
    </author>
    <author>
      <name>L. Mark Elbroch</name>
    </author>
    <author>
      <name>Chaoran Hu</name>
    </author>
    <author>
      <name>Thomas Meyer</name>
    </author>
    <author>
      <name>Jun Yan</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01310v2</id>
    <updated>2018-06-03T13:48:20Z</updated>
    <published>2016-11-04T10:21:55Z</published>
    <title>Achieving Shrinkage in a Time-Varying Parameter Model Framework</title>
    <summary>  Shrinkage for time-varying parameter (TVP) models is investigated within a
Bayesian framework, with the aim to automatically reduce time-varying
parameters to static ones, if the model is overfitting. This is achieved
through placing the double gamma shrinkage prior on the process variances. An
efficient Markov chain Monte Carlo scheme is developed, exploiting boosting
based on the ancillarity-sufficiency interweaving strategy. The method is
applicable both to TVP models for univariate as well as multivariate time
series. Applications include a TVP generalized Phillips curve for EU area
inflation modelling and a multivariate TVP Cholesky stochastic volatility model
for joint modelling of the returns from the DAX-30 index.
</summary>
    <author>
      <name>Angela Bitto</name>
    </author>
    <author>
      <name>Sylvia Frühwirth-Schnatter</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01310v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01310v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00792v1</id>
    <updated>2018-06-03T13:22:07Z</updated>
    <published>2018-06-03T13:22:07Z</published>
    <title>Jackknife Empirical Likelihood Methods for Gini Correlations and their
  Equality Testing</title>
    <summary>  The Gini correlation plays an important role in measuring dependence of
random variables with heavy tailed distributions, whose properties are a
mixture of Pearson's and Spearman's correlations. Due to the structure of this
dependence measure, there are two Gini correlations between each pair of random
variables, which are not equal in general. Both the Gini correlation and the
equality of the two Gini correlations play important roles in Economics. In the
literature, there are limited papers focusing on the inference of the Gini
correlations and their equality testing. In this paper, we develop the
jackknife empirical likelihood (JEL) approach for the single Gini correlation,
for testing the equality of the two Gini correlations, and for the Gini
correlations' differences of two independent samples. The standard limiting
chi-square distributions of those jackknife empirical likelihood ratio
statistics are established and used to construct confidence intervals,
rejection regions, and to calculate $p$-values of the tests. Simulation studies
show that our methods are competitive to existing methods in terms of coverage
accuracy and shortness of confidence intervals, as well as in terms of power of
the tests. The proposed methods are illustrated in an application on a real
data set from UCI Machine Learning Repository.
</summary>
    <author>
      <name>Yongli Sang</name>
    </author>
    <author>
      <name>Xin Dang</name>
    </author>
    <author>
      <name>Yichuan Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jspi.2018.05.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jspi.2018.05.004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 tables, two figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G35, 62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00731v1</id>
    <updated>2018-06-03T02:50:59Z</updated>
    <published>2018-06-03T02:50:59Z</published>
    <title>Bandwidth selection for kernel density estimators of multivariate level
  sets and highest density regions</title>
    <summary>  We consider bandwidth matrix selection for kernel density estimators (KDEs)
of density level sets in $\mathbb{R}^d$, $d \ge 2$. We also consider estimation
of highest density regions, which differs from estimating level sets in that
one specifies the probability content of the set rather than specifying the
level directly; This complicates the problem. Bandwidth selection for KDEs is
well studied, but the goal of most methods is to minimize a global loss
function for the density or its derivatives. The loss we consider here is
instead the measure of the symmetric difference of the true set and estimated
set. We derive an asymptotic approximation to the corresponding risk. The
approximation depends on unknown quantities which can be estimated, and the
approximation can then be minimized to yield a choice of bandwidth, which we
show in simulations performs well.
</summary>
    <author>
      <name>Charles R. Doss</name>
    </author>
    <author>
      <name>Guangwei Weng</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06897v2</id>
    <updated>2018-06-02T19:39:05Z</updated>
    <published>2017-08-23T06:34:06Z</published>
    <title>Projected support points: a new method for high-dimensional data
  reduction</title>
    <summary>  In an era where big and high-dimensional data is readily available, data
scientists are inevitably faced with the challenge of reducing this data for
expensive downstream computation or analysis. To this end, we present here a
new method for reducing high-dimensional big data into a representative point
set, called projected support points (PSPs). A key ingredient in our method is
the so-called sparsity-inducing (SpIn) kernel, which encourages the
preservation of low-dimensional features when reducing high-dimensional data.
We begin by introducing a unifying theoretical framework for data reduction,
connecting PSPs with fundamental sampling principles from experimental design
and Quasi-Monte Carlo. Through this framework, we then derive sparsity
conditions under which the curse-of-dimensionality in data reduction can be
lifted for our method. Next, we propose two algorithms for one-shot and
sequential reduction via PSPs, both of which exploit big data subsampling and
majorization-minimization for efficient optimization. Finally, we demonstrate
the practical usefulness of PSPs in two real-world applications, the first for
data reduction in kernel learning, and the second for reducing Markov Chain
Monte Carlo (MCMC) chains.
</summary>
    <author>
      <name>Simon Mak</name>
    </author>
    <author>
      <name>V. Roshan Joseph</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06897v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06897v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03744v5</id>
    <updated>2018-06-02T18:08:58Z</updated>
    <published>2017-11-10T09:52:49Z</published>
    <title>Efficient Simulation for Portfolio Credit Risk in Normal Mixture Copula
  Models</title>
    <summary>  This paper considers the problem of measuring the credit risk in portfolios
of loans, bonds, and other instruments subject to possible default under
multi-factor models. Due to the amount of the portfolio, the heterogeneous
effect of obligors, and the phenomena that default events are rare and mutually
dependent, it is difficult to calculate portfolio credit risk either by means
of direct analysis or crude Monte Carlo under such models. To capture the
extreme dependence among obligors, we provide an efficient simulation method
for multi-factor models with a normal mixture copula that allows the
multivariate defaults to have an asymmetric distribution, while most of the
literature focuses on simulating one-dimensional cases. To this end, we first
propose a general account of an importance sampling algorithm based on an
unconventional two-parameter exponential embedding. Note that this innovative
tilting device is more suitable for the multivariate normal mixture model than
traditional one-parameter tilting methods and is of independent interest. Next,
by utilizing a fast computational method for how the rare event occurs and the
proposed importance sampling method, we provide an efficient simulation
algorithm to estimate the probability that the portfolio incurs large losses
under the normal mixture copula. Here the proposed simulation device is based
on importance sampling for a joint probability other than the conditional
probability used in previous studies. Theoretical investigations and simulation
studies, which include an empirical example, are given to illustrate the
method.
</summary>
    <author>
      <name>Cheng-Der Fuh</name>
    </author>
    <author>
      <name>Chuan-Ju Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.03744v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03744v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00550v1</id>
    <updated>2018-06-01T21:48:44Z</updated>
    <published>2018-06-01T21:48:44Z</published>
    <title>Return of the Infinitesimal Jackknife</title>
    <summary>  The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data. The ubiquitous tools of cross validation (CV) and the bootstrap are
examples of this technique. These methods are powerful in large part due to
their model agnosticism but can be slow to run on modern, large data sets due
to the need to repeatedly re-fit the model. In this work, we use a linear
approximation to the dependence of the fitting procedure on the weights,
producing results that can be faster than repeated re-fitting by orders of
magnitude. We provide explicit finite-sample error bounds for the approximation
in terms of a small number of simple, verifiable assumptions. Our results apply
whether the weights and data are stochastic, deterministic, or even
adversarially chosen, and so can be used as a tool for proving the accuracy of
a wide variety of problems. As a corollary, we state mild regularity conditions
under which our approximation consistently estimates true leave-k-out cross
validation for any fixed k. We demonstrate the accuracy of our methods on a
range of simulated and real datasets.
</summary>
    <author>
      <name>Ryan Giordano</name>
    </author>
    <author>
      <name>Will Stephenson</name>
    </author>
    <author>
      <name>Runjing Liu</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <author>
      <name>Tamara Broderick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00446v1</id>
    <updated>2018-06-01T17:04:27Z</updated>
    <published>2018-06-01T17:04:27Z</published>
    <title>Bayesian Logistic Regression for Small Areas with Numerous Households</title>
    <summary>  We analyze binary data, available for a relatively large number (big data) of
families (or households), which are within small areas, from a population-based
survey. Inference is required for the finite population proportion of
individuals with a specific character for each area. To accommodate the binary
data and important features of all sampled individuals, we use a hierarchical
Bayesian logistic regression model with each family (not area) having its own
random effect. This modeling helps to correct for overshrinkage so common in
small area estimation. Because there are numerous families, the computational
time on the joint posterior density using standard Markov chain Monte Carlo
(MCMC) methods is prohibitive. Therefore, the joint posterior density of the
hyper-parameters is approximated using an integrated nested normal
approximation (INNA) via the multiplication rule. This approach provides a
sampling-based method that permits fast computation, thereby avoiding very
time-consuming MCMC methods. Then, the random effects are obtained from the
exact conditional posterior density using parallel computing. The unknown
nonsample features and household sizes are obtained using a nested Bayesian
bootstrap that can be done using parallel computing as well. For relatively
small data sets (e.g., 5000 families), we compare our method with a MCMC method
to show that our approach is reasonable. We discuss an example on health
severity using the Nepal Living Standards Survey (NLSS).
</summary>
    <author>
      <name>Balgobin Nandram</name>
    </author>
    <author>
      <name>Lu Chen</name>
    </author>
    <author>
      <name>Shuting Fu</name>
    </author>
    <author>
      <name>Binod Manandhar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08184v2</id>
    <updated>2018-06-01T15:46:25Z</updated>
    <published>2018-01-24T20:51:14Z</published>
    <title>Uncertainty quantification for computer models with spatial output using
  calibration-optimal bases</title>
    <summary>  The calibration of complex computer codes using uncertainty quantification
(UQ) methods is a rich area of statistical methodological development. When
applying these techniques to simulators with spatial output, it is now standard
to use principal component decomposition to reduce the dimensions of the
outputs in order to allow Gaussian process emulators to predict the output for
calibration. We introduce the `terminal case', in which the model cannot
reproduce observations to within model discrepancy, and for which standard
calibration methods in UQ fail to give sensible results. We show that even when
there is no such issue with the model, the standard decomposition on the
outputs can and usually does lead to a terminal case analysis. We present a
simple test to allow a practitioner to establish whether their experiment will
result in a terminal case analysis, and a methodology for defining
calibration-optimal bases that avoid this whenever it is not inevitable. We
present the optimal rotation algorithm for doing this, and demonstrate its
efficacy for an idealised example for which the usual principal component
methods fail. We apply these ideas to the CanAM4 model to demonstrate the
terminal case issue arising for climate models. We discuss climate model tuning
and the estimation of model discrepancy within this context, and show how the
optimal rotation algorithm can be used in developing practical climate model
tuning tools.
</summary>
    <author>
      <name>James M Salter</name>
    </author>
    <author>
      <name>Daniel B Williamson</name>
    </author>
    <author>
      <name>John Scinocca</name>
    </author>
    <author>
      <name>Viatcheslav Kharin</name>
    </author>
    <link href="http://arxiv.org/abs/1801.08184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01241v3</id>
    <updated>2018-06-01T14:54:45Z</updated>
    <published>2016-11-04T01:50:57Z</published>
    <title>Comparing and weighting imperfect models using D-probabilities</title>
    <summary>  We propose a new approach for assigning weights to models using a
divergence-based method ({\em D-probabilities}), relying on evaluating
parametric models relative to a nonparametric Bayesian reference using
Kullback-Leibler divergence. D-probabilities are useful in goodness-of-fit
assessments, in comparing imperfect models, and in providing model weights to
be used in model aggregation. D-probabilities avoid some of the disadvantages
of Bayesian model probabilities, such as large sensitivity to prior choice, and
tend to place higher weight on a greater diversity of models. In an application
to linear model selection against a Gaussian process reference, we provide
simple analytic forms for routine implementation and show that D-probabilities
automatically penalize model complexity. Some asymptotic properties are
described, and we provide interesting probabilistic interpretations of the
proposed model weights. The framework is illustrated through simulation
examples and an ozone data application.
</summary>
    <author>
      <name>Meng Li</name>
    </author>
    <author>
      <name>David B. Dunson</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01241v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01241v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08400v4</id>
    <updated>2018-06-01T14:32:22Z</updated>
    <published>2016-06-27T18:31:12Z</published>
    <title>Robust and rate-optimal Gibbs posterior inference on the boundary of a
  noisy image</title>
    <summary>  Detection of an image boundary when the pixel intensities are measured with
noise is an important problem in image segmentation, with numerous applications
in medical imaging and engineering. From a statistical point of view, the
challenge is that likelihood-based methods require modeling the pixel
intensities inside and outside the image boundary, even though these are
typically of no practical interest. Since misspecification of the pixel
intensity models can negatively affect inference on the image boundary, it
would be desirable to avoid this modeling step altogether. Towards this, we
develop a robust Gibbs approach that constructs a posterior distribution for
the image boundary directly, without modeling the pixel intensities. We prove
that, for a suitable prior on the image boundary, the Gibbs posterior
concentrates asymptotically at the minimax optimal rate, adaptive to the
boundary smoothness. Monte Carlo computation of the Gibbs posterior is
straightforward, and simulation experiments show that the corresponding
inference is more accurate than that based on existing Bayesian methodology.
</summary>
    <author>
      <name>Nicholas Syring</name>
    </author>
    <author>
      <name>Ryan Martin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08400v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08400v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00275v1</id>
    <updated>2018-06-01T10:31:36Z</updated>
    <published>2018-06-01T10:31:36Z</published>
    <title>Locally $D$-optimal Designs for Non-linear Models on the $k$-dimensional
  Ball</title>
    <summary>  In this paper we construct (locally) $D$-optimal designs for a wide class of
non-linear multiple regression models, when the design region is a
$k$-dimensional ball. For this construction we make use of the concept of
invariance and equivariance in the context of optimal designs. As examples we
consider Poisson and negative binomial regression as well as proportional
hazard models with censoring. By generalisation we can extend these results to
arbitrary ellipsoids.
</summary>
    <author>
      <name>Martin Radloff</name>
    </author>
    <author>
      <name>Rainer Schwabe</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62K05, 62J12, 62N01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00225v1</id>
    <updated>2018-06-01T07:48:28Z</updated>
    <published>2018-06-01T07:48:28Z</published>
    <title>Model-based clustering for populations of networks</title>
    <summary>  We propose a model-based clustering method for populations of networks that
describes the joint distribution of a sequence of networks in a parsimonious
manner, and can be used to identify subpopulations of networks that share
certain topological properties of interest. We discuss how maximum likelihood
estimation can be performed with the EM algorithm and study the performance of
the proposed method on simulated data. We conclude with an example application
to a sequence of face-to-face interaction networks measured in an office
environment.
</summary>
    <author>
      <name>Mirko Signorelli</name>
    </author>
    <author>
      <name>Ernst Wit</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00221v1</id>
    <updated>2018-06-01T07:35:20Z</updated>
    <published>2018-06-01T07:35:20Z</published>
    <title>Lecture Notes: Temporal Point Processes and the Conditional Intensity
  Function</title>
    <summary>  These short lecture notes contain a not too technical introduction to point
processes on the time line. The focus lies on defining these processes using
the conditional intensity function. Furthermore, likelihood inference, methods
of simulation and residual analysis for temporal point processes specified by a
conditional intensity function are considered.
</summary>
    <author>
      <name>Jakob Gulddahl Rasmussen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02321v1</id>
    <updated>2018-06-01T01:34:04Z</updated>
    <published>2018-06-01T01:34:04Z</published>
    <title>Fitting a deeply-nested hierarchical model to a large book review
  dataset using a moment-based estimator</title>
    <summary>  We consider a particular instance of a common problem in recommender systems:
using a database of book reviews to inform user-targeted recommendations. In
our dataset, books are categorized into genres and sub-genres. To exploit this
nested taxonomy, we use a hierarchical model that enables information pooling
across across similar items at many levels within the genre hierarchy. The main
challenge in deploying this model is computational: the data sizes are large,
and fitting the model at scale using off-the-shelf maximum likelihood
procedures is prohibitive. To get around this computational bottleneck, we
extend a moment-based fitting procedure proposed for fitting single-level
hierarchical models to the general case of arbitrarily deep hierarchies. This
extension is an order of magnetite faster than standard maximum likelihood
procedures. The fitting method can be deployed beyond recommender systems to
general contexts with deeply-nested hierarchical generalized linear mixed
models.
</summary>
    <author>
      <name>Ningshan Zhang</name>
    </author>
    <author>
      <name>Kyle Schmaus</name>
    </author>
    <author>
      <name>Patrick O. Perry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04487v2</id>
    <updated>2018-05-31T19:16:47Z</updated>
    <published>2017-12-12T19:51:23Z</published>
    <title>Topological mixture estimation</title>
    <summary>  Density functions that represent sample data are often multimodal, i.e. they
exhibit more than one maximum. Typically this behavior is taken to indicate
that the underlying data deserves a more detailed representation as a mixture
of densities with individually simpler structure. The usual specification of a
component density is quite restrictive, with log-concave the most general case
considered in the literature, and Gaussian the overwhelmingly typical case. It
is also necessary to determine the number of mixture components \emph{a
priori}, and much art is devoted to this. Here, we introduce \emph{topological
mixture estimation}, a completely nonparametric and computationally efficient
solution to the one-dimensional problem where mixture components need only be
unimodal. We repeatedly perturb the unimodal decomposition of Baryshnikov and
Ghrist to produce a topologically and information-theoretically optimal
unimodal mixture. We also detail a smoothing process that optimally exploits
topological persistence of the unimodal category in a natural way when working
directly with sample data. Finally, we illustrate these techniques through
examples.
</summary>
    <author>
      <name>Steve Huntsman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, accepted to ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.04487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00048v1</id>
    <updated>2018-05-31T18:50:58Z</updated>
    <published>2018-05-31T18:50:58Z</published>
    <title>Basis Values Have Questionable Value</title>
    <summary>  Controlling the chance of failure is the aim of design for reliability. Among
a host of uncertainties, material properties constitute one key input to
designing safe structures. However, legally required approaches to
characterizing materials -- basis values -- are fundamentally incompatible with
design for reliability. Namely, basis values prevent designs from achieving
high-reliability, perpetrating orders-of-magnitude higher probabilities of
failure than specified. In this work, we demonstrate this pathology and
recommend a solution: the concept of precision margin. We present a general
definition, operationalizations and efficient estimation procedures, and
illustrative demonstrations on classical reliability problems. The results
vastly outperform the current industrial practice, and illuminate a new tool to
navigate the trade space between design and information costs.
</summary>
    <author>
      <name>Zachary del Rosario</name>
    </author>
    <author>
      <name>Richard W. Fenrich</name>
    </author>
    <author>
      <name>Gianluca Iaccarino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 16 figures, linked GitHub repo</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2610v7</id>
    <updated>2018-05-31T18:00:37Z</updated>
    <published>2013-11-11T21:20:54Z</published>
    <title>Joint Mean and Covariance Modeling of Multiple Health Outcome Measures</title>
    <summary>  Health exams determine a patient's health status by comparing the patient's
measurement with a population reference range, a 95% interval derived from a
homogeneous reference population. Similarly, most of the established relation
among health problems are assumed to hold for the entire population. We use
data from the 2009 - 2010 National Health and Nutrition Examination Survey
(NHANES) on four major health problems in the U.S. and apply a joint mean and
covariance model to study how the reference ranges and associations of those
health outcomes could vary among subpopulations. We discuss guidelines for
model selection and evaluation, using standard criteria such as AIC in
conjunction with posterior predictive checks. The results from the proposed
model can help identify subpopulations in which more data need to be collected
to refine the reference range and to study the specific associations among
those health problems.
</summary>
    <author>
      <name>Xiaoyue Niu</name>
    </author>
    <author>
      <name>Peter D. Hoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.2610v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2610v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08016v2</id>
    <updated>2018-05-31T09:50:06Z</updated>
    <published>2016-04-27T10:59:56Z</published>
    <title>Adaptive Incremental Mixture Markov chain Monte Carlo</title>
    <summary>  We propose Adaptive Incremental Mixture Markov chain Monte Carlo (AIMM), a
novel approach to sample from challenging probability distributions defined on
a general state-space. While adaptive MCMC methods usually update a parametric
proposal kernel with a global rule, AIMM locally adapts a semiparametric
kernel. AIMM is based on an independent Metropolis-Hastings proposal
distribution which takes the form of a finite mixture of Gaussian
distributions. Central to this approach is the idea that the proposal
distribution adapts to the target by locally adding a mixture component when
the discrepancy between the proposal mixture and the target is deemed to be too
large. As a result, the number of components in the mixture proposal is not
fixed in advance. Theoretically, we prove that there exists a process that can
be made arbitrarily close to AIMM and that converges to the correct target
distribution. We also illustrate that it performs well in practice in a variety
of challenging situations, including high-dimensional and multimodal target
distributions.
</summary>
    <author>
      <name>Florian Maire</name>
    </author>
    <author>
      <name>Nial Friel</name>
    </author>
    <author>
      <name>Antonietta Mira</name>
    </author>
    <author>
      <name>Adrian Raftery</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C05 65C40 60G10 93E35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08327v3</id>
    <updated>2018-05-31T09:20:50Z</updated>
    <published>2017-06-26T11:24:51Z</published>
    <title>Informed Sub-Sampling MCMC: Approximate Bayesian Inference for Large
  Datasets</title>
    <summary>  This paper introduces a framework for speeding up Bayesian inference
conducted in presence of large datasets. We design a Markov chain whose
transition kernel uses an (unknown) fraction of (fixed size) of the available
data that is randomly refreshed throughout the algorithm. Inspired by the
Approximate Bayesian Computation (ABC) literature, the subsampling process is
guided by the fidelity to the observed data, as measured by summary statistics.
The resulting algorithm, Informed Sub-Sampling MCMC (ISS-MCMC), is a generic
and flexible approach which, contrary to existing scalable methodologies,
preserves the simplicity of the Metropolis-Hastings algorithm. Even though
exactness is lost, i.e. the chain distribution approximates the posterior, we
study and quantify theoretically this bias and show on a diverse set of
examples that it yields excellent performances when the computational budget is
limited. If available and cheap to compute, we show that setting the summary
statistics as the maximum likelihood estimator is supported by theoretical
arguments.
</summary>
    <author>
      <name>Florian Maire</name>
    </author>
    <author>
      <name>Nial Friel</name>
    </author>
    <author>
      <name>Pierre Alquier</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08327v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08327v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65C40, 65C60, 62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09053v2</id>
    <updated>2018-05-31T02:11:46Z</updated>
    <published>2018-02-25T18:11:03Z</published>
    <title>Evolutionary Spectra Based on the Multitaper Method with Application to
  Stationarity Test</title>
    <summary>  In this work, we propose a new inference procedure for understanding
non-stationary processes, under the framework of evolutionary spectra developed
by Priestley. Among various frameworks of modeling non-stationary processes,
the distinguishing feature of the evolutionary spectra is its focus on the
physical meaning of frequency. The classical estimate of the evolutionary
spectral density is based on a double-window technique consisting of a
short-time Fourier transform and a smoothing. However, smoothing is known to
suffer from the so-called bias leakage problem. By incorporating Thomson's
multitaper method that was originally designed for stationary processes, we
propose an improved estimate of the evolutionary spectral density, and analyze
its bias/variance/resolution tradeoff. As an application of the new estimate,
we further propose a non-parametric rank-based stationarity test, and provide
various experimental studies.
</summary>
    <author>
      <name>Yu Xiang</name>
    </author>
    <author>
      <name>Jie Ding</name>
    </author>
    <author>
      <name>Vahid Tarokh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Signal Processing. A short version
  of this work will appear in ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12256v1</id>
    <updated>2018-05-30T23:06:40Z</updated>
    <published>2018-05-30T23:06:40Z</published>
    <title>Note on the robustification of the Student $t$-test statistic using the
  median and the median absolute deviation</title>
    <summary>  In this note, we propose a robustified analogue of the conventional Student
$t$-test statistic. The proposed statistic is easy to implement and thus
practically useful. We also show that it is a pivotal quantity and converges to
a standard normal distribution.
</summary>
    <author>
      <name>Chanseok Park</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12253v1</id>
    <updated>2018-05-30T22:53:22Z</updated>
    <published>2018-05-30T22:53:22Z</published>
    <title>Sequential Experimental Design for Optimal Structural Intervention in
  Gene Regulatory Networks Based on the Mean Objective Cost of Uncertainty</title>
    <summary>  Scientists are attempting to use models of ever increasing complexity,
especially in medicine, where gene-based diseases such as cancer require better
modeling of cell regulation. Complex models suffer from uncertainty and
experiments are needed to reduce this uncertainty. Because experiments can be
costly and time-consuming it is desirable to determine experiments providing
the most useful information. If a sequence of experiments is to be performed,
experimental design is needed to determine the order. A classical approach is
to maximally reduce the overall uncertainty in the model, meaning maximal
entropy reduction. A recently proposed method takes into account both model
uncertainty and the translational objective, for instance, optimal structural
intervention in gene regulatory networks, where the aim is to alter the
regulatory logic to maximally reduce the long-run likelihood of being in a
cancerous state. The mean objective cost of uncertainty (MOCU) quantifies
uncertainty based on the degree to which model uncertainty affects the
objective. Experimental design involves choosing the experiment that yields the
greatest reduction in MOCU. This paper introduces finite-horizon dynamic
programming for MOCU-based sequential experimental design and compares it to
the greedy approach, which selects one experiment at a time without
consideration of the full horizon of experiments. A salient aspect of the paper
is that it demonstrates the advantage of MOCU-based design over the widely used
entropy-based design for both greedy and dynamic-programming strategies and
investigates the effect of model conditions on the comparative performances.
</summary>
    <author>
      <name>Mahdi Imani</name>
    </author>
    <author>
      <name>Roozbeh Dehghannasiri</name>
    </author>
    <author>
      <name>Ulisses M. Braga-Neto</name>
    </author>
    <author>
      <name>Edward R. Dougherty</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12249v1</id>
    <updated>2018-05-30T22:27:34Z</updated>
    <published>2018-05-30T22:27:34Z</published>
    <title>Optimal Sample Size Planning for the Wilcoxon-Mann-Whitney-Test</title>
    <summary>  There are many different proposed procedures for sample size planning for the
Wilcoxon-Mann-Whitney test at given type-I and type-II error rates $\alpha$ and
$\beta$, respectively. Most methods assume very specific models or types of
data in order to simplify calculations (for example, ordered categorical or
metric data, location shift alternatives, etc.). We present a unified approach
that covers metric data with and without ties, count data, ordered categorical
data, and even dichotomous data. For that, we calculate the unknown theoretical
quantities such as the variances under the null and relevant alternative
hypothesis by considering the following `synthetic data' approach. We evaluate
data whose empirical distribution functions match with the theoretical
distribution functions involved in the computations of the unknown theoretical
quantities. Then well-known relations for the ranks of the data are used for
the calculations.
  In addition to computing the necessary sample size $N$ for a fixed allocation
proportion $t = n_1/N$, where $n_1$ is the sample size in the first group and
$N = n_1 + n_2$ is the total sample size, we provide an interval for the
optimal allocation rate $t$ which minimizes the total sample size $N$. It turns
out that for certain distributions, a balanced design is optimal. We give a
characterization of these distributions. Furthermore we show that the optimal
choice of $t$ depends on the ratio of the two variances which determine the
variance of the Wilcoxon-Mann-Whitney statistic under the alternative. This is
different from an optimal sample size allocation in case of the normal
distribution model.
</summary>
    <author>
      <name>Martin Happ</name>
    </author>
    <author>
      <name>Arne C. Bathke</name>
    </author>
    <author>
      <name>Edgar Brunner</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G99, 62K05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12201v1</id>
    <updated>2018-05-30T19:59:26Z</updated>
    <published>2018-05-30T19:59:26Z</published>
    <title>Bayesian Nonparametric Higher Order Hidden Markov Models</title>
    <summary>  We consider the problem of flexible modeling of higher order hidden Markov
models when the number of latent states and the nature of the serial
dependence, including the true order, are unknown. We propose Bayesian
nonparametric methodology based on tensor factorization techniques that can
characterize any transition probability with a specified maximal order,
allowing automated selection of the important lags and capturing higher order
interactions among the lags. Theoretical results provide insights into
identifiability of the emission distributions and asymptotic behavior of the
posterior. We design efficient Markov chain Monte Carlo algorithms for
posterior computation. In simulation experiments, the method vastly
outperformed its first and higher order competitors not just in higher order
settings, but, remarkably, also in first order cases. Practical utility is
illustrated using real world applications.
</summary>
    <author>
      <name>Abhra Sarkar</name>
    </author>
    <author>
      <name>David B. Dunson</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12179v1</id>
    <updated>2018-05-30T18:59:57Z</updated>
    <published>2018-05-30T18:59:57Z</published>
    <title>U-statistical inference for hierarchical clustering</title>
    <summary>  Clustering methods are a valuable tool for the identification of patterns in
high dimensional data with applications in many scientific problems. However,
quantifying uncertainty in clustering is a challenging problem, particularly
when dealing with High Dimension Low Sample Size (HDLSS) data. We develop here
a U-statistics based clustering approach that assesses statistical significance
in clustering and is specifically tailored to HDLSS scenarios. These
non-parametric methods rely on very few assumptions about the data, and thus
can be applied to a wide range of datasets for which the euclidean distance
captures relevant features. We propose two significance clustering algorithms,
a hierarchical method and a non-nested version. In order to do so, we first
propose an extension of a relevant U-statistics and develop its asymptotic
theory. Our methods are tested through extensive simulations and found to be
more powerful than competing alternatives. They are further showcased in two
applications ranging from genetics to image recognition problems.
</summary>
    <author>
      <name>Marcio Valk</name>
    </author>
    <author>
      <name>Gabriela Bettella Cybis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.12179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00473v1</id>
    <updated>2018-05-30T17:36:25Z</updated>
    <published>2018-05-30T17:36:25Z</published>
    <title>Bayesian nonparametric inference for the covariate-adjusted ROC curve</title>
    <summary>  Accurate diagnosis of disease is of fundamental importance in clinical
practice and medical research. Before a medical diagnostic test is routinely
used in practice, its ability to distinguish between diseased and nondiseased
states must be rigorously assessed through statistical analysis. The receiver
operating characteristic (ROC) curve is the most popular used tool for
evaluating the discriminatory ability of continuous-outcome diagnostic tests.
It has been acknowledged that several factors (e.g., subject-specific
characteristics, such as age and/or gender) can affect the test's accuracy
beyond disease status. Recently, the covariate-adjusted ROC curve has been
proposed and successfully applied as a global summary measure of diagnostic
accuracy that takes covariate information into account. We motivate the use of
the covariate-adjusted ROC curve and develop a highly robust model based on a
combination of B-splines dependent Dirichlet process mixture models and the
Bayesian bootstrap. Multiple simulation studies demonstrate the ability of our
model to successfully recover the true covariate-adjusted ROC curve and to
produce valid inferences in a variety of complex scenarios. Our methods are
motivated by and applied to an endocrine study where the main goal is to assess
the accuracy of the body mass index, adjusted for age and gender, for
predicting clusters of cardiovascular disease risk factors. The R-package AROC,
implementing our proposed methods, is provided.
</summary>
    <author>
      <name>Vanda Inacio de Carvalho</name>
    </author>
    <author>
      <name>Maria Xose Rodriguez-Alvarez</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10421v2</id>
    <updated>2018-05-30T15:38:06Z</updated>
    <published>2017-11-13T16:31:21Z</published>
    <title>A Review of Dynamic Network Models with Latent Variables</title>
    <summary>  We present a selective review of statistical modeling of dynamic networks. We
focus on models with latent variables, specifically, the latent space models
and the latent class models (or stochastic blockmodels), which investigate both
the observed features and the unobserved structure of networks. We begin with
an overview of the static models, and then we introduce the dynamic extensions.
For each dynamic model, we also discuss its applications that have been studied
in the literature, with the data source listed in Appendix. Based on the
review, we summarize a list of open problems and challenges in dynamic network
modeling with latent variables.
</summary>
    <author>
      <name>Bomin Kim</name>
    </author>
    <author>
      <name>Kevin Lee</name>
    </author>
    <author>
      <name>Lingzhou Xue</name>
    </author>
    <author>
      <name>Xiaoyue Niu</name>
    </author>
    <link href="http://arxiv.org/abs/1711.10421v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10421v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07357v2</id>
    <updated>2018-05-30T06:25:52Z</updated>
    <published>2017-11-17T01:38:46Z</published>
    <title>Joint Structural Break Detection and Parameter Estimation in
  High-Dimensional Non-Stationary VAR Models</title>
    <summary>  Assuming stationarity is unrealistic in many time series applications. A more
realistic alternative is to allow for piecewise stationarity, where the model
is allowed to change at given time points. We propose a three-stage procedure
for consistent estimation of both structural change points and parameters of
high-dimensional piecewise vector autoregressive (VAR) models. In the first
step, we reformulate the change point detection problem as a high-dimensional
variable selection one, and propose a penalized least square estimator using a
total variation penalty. We show that the proposed penalized estimation method
over-estimates the number of change points. We then propose a backward
selection criterion in conjunction with a penalized least square estimator to
tackle this issue. In the last step of our procedure, we estimate the VAR
parameters in each of the segments. We prove that the proposed procedure
consistently detects the number of change points and their locations. We also
show that the procedure consistently estimates the VAR parameters. The
performance of the method is illustrated through several simulation scenarios
and real data examples.
</summary>
    <author>
      <name>Abolfazl Safikhani</name>
    </author>
    <author>
      <name>Ali Shojaie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1708.02736</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.07357v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07357v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05544v3</id>
    <updated>2018-05-30T02:05:50Z</updated>
    <published>2018-03-15T00:03:46Z</published>
    <title>Testing the homogeneity of risk differences with sparse count data</title>
    <summary>  In this paper, we consider testing the homogeneity of risk differences in
independent binomial distributions especially when data are sparse. We point
out some drawback of existing tests in either controlling a nominal size or
obtaining powers through theoretical and numerical studies. The proposed test
is designed to avoid such drawback of existing tests. We present the asymptotic
null distributions and asymptotic powers for our proposed test. We also provide
numerical studies including simulations and real data examples showing the
proposed test has reliable results compared to existing testing procedures.
</summary>
    <author>
      <name>Junyong Park</name>
    </author>
    <author>
      <name>Iris Ivy Gauran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There are some errors</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05544v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05544v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06928v4</id>
    <updated>2018-05-29T22:42:11Z</updated>
    <published>2016-12-21T00:30:29Z</published>
    <title>Simultaneous multiple change-point and factor analysis for
  high-dimensional time series</title>
    <summary>  We propose the first comprehensive treatment of high-dimensional time series
factor models with multiple change-points in their second-order structure. We
operate under the most flexible definition of piecewise stationarity, and
estimate the number and locations of change-points consistently as well as
identifying whether they originate in the common or idiosyncratic components.
Through the use of wavelets, we transform the problem of change-point detection
in the second-order structure of a high-dimensional time series, into the
(relatively easier) problem of change-point detection in the means of
high-dimensional panel data. Also, our methodology circumvents the difficult
issue of the accurate estimation of the true number of factors in the presence
of multiple change-points by adopting a screening procedure. We further show
that consistent factor analysis is achieved over each segment defined by the
change-points estimated by the proposed methodology. In extensive simulation
studies, we observe that factor analysis prior to change-point detection
improves the detectability of change-points, and identify and describe an
interesting `spillover' effect in which substantial breaks in the idiosyncratic
components get, naturally enough, identified as change-points in the common
components, which prompts us to regard the corresponding change-points as also
acting as a form of `factors'. Our methodology is implemented in the R package
{\tt factorcpt}, available from CRAN.
</summary>
    <author>
      <name>Matteo Barigozzi</name>
    </author>
    <author>
      <name>Haeran Cho</name>
    </author>
    <author>
      <name>Piotr Fryzlewicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">64 pages, to appear in the Journal of Econometrics</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06928v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06928v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10214v2</id>
    <updated>2018-05-29T16:13:53Z</updated>
    <published>2018-05-25T15:52:12Z</published>
    <title>Bias correction in daily maximum and minimum temperature measurements
  through Gaussian process modeling</title>
    <summary>  The Global Historical Climatology Network-Daily database contains, among
other variables, daily maximum and minimum temperatures from weather stations
around the globe. It is long known that climatological summary statistics based
on daily temperature minima and maxima will not be accurate, if the bias due to
the time at which the observations were collected is not accounted for. Despite
some previous work, to our knowledge, there does not exist a satisfactory
solution to this important problem. In this paper, we carefully detail the
problem and develop a novel approach to address it. Our idea is to impute the
hourly temperatures at the location of the measurements by borrowing
information from the nearby stations that record hourly temperatures, which
then can be used to create accurate summaries of temperature extremes. The key
difficulty is that these imputations of the temperature curves must satisfy the
constraint of falling between the observed daily minima and maxima, and
attaining those values at least once in a twenty-four hour period. We develop a
spatiotemporal Gaussian process model for imputing the hourly measurements from
the nearby stations, and then develop a novel and easy to implement Markov
Chain Monte Carlo technique to sample from the posterior distribution
satisfying the above constraints. We validate our imputation model using hourly
temperature data from four meteorological stations in Iowa, of which one is
hidden and the data replaced with daily minima and maxima, and show that the
imputed temperatures recover the hidden temperatures well. We also demonstrate
that our model can exploit information contained in the data to infer the time
of daily measurements.
</summary>
    <author>
      <name>Maxime Rischard</name>
    </author>
    <author>
      <name>Natesh Pillai</name>
    </author>
    <author>
      <name>Karen A. McKinnon</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10214v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10214v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02673v2</id>
    <updated>2018-05-29T15:07:34Z</updated>
    <published>2017-09-08T12:24:57Z</published>
    <title>Combining cumulative sum change-point detection tests for assessing the
  stationarity of univariate time series</title>
    <summary>  We derive tests of stationarity for univariate time series by combining
change-point tests sensitive to changes in the contemporary distribution with
tests sensitive to changes in the serial dependence. The proposed approach
relies on a general procedure for combining dependent tests based on
resampling. After proving the asymptotic validity of the combining procedure
under the conjunction of null hypotheses, we study rank-based tests of
stationarity by combining cumulative sum change-point tests based on the
contemporary empirical distribution function and on the empirical autocopula at
a given lag. Extensions based on tests solely focusing on second-order
characteristics are proposed next. The finite-sample behaviors of all the
derived statistical procedures for assessing stationarity are investigated in
large-scale Monte Carlo experiments and illustrations on two real data sets are
provided. Extensions to multivariate time series are briefly discussed as well.
</summary>
    <author>
      <name>Axel Bücher</name>
    </author>
    <author>
      <name>Jean-David Fermanian</name>
    </author>
    <author>
      <name>Ivan Kojadinovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 3 tables, 2 figures with supplement</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11505v1</id>
    <updated>2018-05-29T14:36:06Z</updated>
    <published>2018-05-29T14:36:06Z</published>
    <title>Classification with imperfect training labels</title>
    <summary>  We study the effect of imperfect training data labels on the performance of
classification methods. In a general setting, where the probability that an
observation in the training dataset is mislabelled may depend on both the
feature vector and the true label, we bound the excess risk of an arbitrary
classifier trained with imperfect labels in terms of its excess risk for
predicting a noisy label. This reveals conditions under which a classifier
trained with imperfect labels remains consistent for classifying uncorrupted
test data points. Furthermore, under stronger conditions, we derive detailed
asymptotic properties for the popular $k$-nearest neighbour ($k$nn), Support
Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. One
consequence of these results is that the $k$nn and SVM classifiers are robust
to imperfect training labels, in the sense that the rate of convergence of the
excess risks of these classifiers remains unchanged; in fact, it even turns out
that in some cases, imperfect labels may improve the performance of these
methods. On the other hand, the LDA classifier is shown to be typically
inconsistent in the presence of label noise unless the prior probabilities of
each class are equal. Our theoretical results are supported by a simulation
study.
</summary>
    <author>
      <name>Timothy I. Cannings</name>
    </author>
    <author>
      <name>Yingying Fan</name>
    </author>
    <author>
      <name>Richard J. Samworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03791v2</id>
    <updated>2018-05-29T07:29:19Z</updated>
    <published>2018-01-11T14:44:51Z</published>
    <title>On the precision matrix of an irregularly sampled AR(1) process</title>
    <summary>  Irregularly sampled AR(1) processes appear in many computationally demanding
applications. This text provides an analytical expression for the precision
matrix of such a process, and gives efficient algorithms for density evaluation
and simulation, implemented in the R package irregulAR1.
</summary>
    <author>
      <name>Benjamin Allévius</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03791v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03791v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11258v1</id>
    <updated>2018-05-29T06:07:39Z</updated>
    <published>2018-05-29T06:07:39Z</published>
    <title>Iterative Statistical Linear Regression for Gaussian Smoothing in
  Continuous-Time Non-linear Stochastic Dynamic Systems</title>
    <summary>  This paper considers approximate smoothing for discretely observed non-linear
stochastic differential equations. The problem is tackled by developing methods
for linearising stochastic differential equations with respect to an arbitrary
Gaussian process. Two methods are developed based on 1) taking the limit of
statistical linear regression of the discretised process and 2) minimising an
upper bound to a cost functional. Their difference is manifested in the
diffusion of the approximate processes. This in turn gives novel derivations of
pre-existing Gaussian smoothers when Method 1 is used and a new class of
Gaussian smoothers when Method 2 is used. Furthermore, based on the
aforementioned development the iterative Gaussian smoothers in discrete-time
are generalised to the continuous-time setting by iteratively re-linearising
the stochastic differential equation with respect to the current Gaussian
process approximation to the smoothed process. The method is verified in two
challenging tracking problems, a reentry problem and a radar tracked
coordinated turn model with state dependent diffusion. The results show that
the method has better estimation accuracy than state-of-the-art smoothers.
</summary>
    <author>
      <name>Filip Tronarp</name>
    </author>
    <author>
      <name>Simo Särkkä</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11183v1</id>
    <updated>2018-05-28T21:55:02Z</updated>
    <published>2018-05-28T21:55:02Z</published>
    <title>Semi-Implicit Variational Inference</title>
    <summary>  Semi-implicit variational inference (SIVI) is introduced to expand the
commonly used analytic variational distribution family, by mixing the
variational parameter with a flexible distribution. This mixing distribution
can assume any density function, explicit or not, as long as independent random
samples can be generated via reparameterization. Not only does SIVI expand the
variational family to incorporate highly flexible variational distributions,
including implicit ones that have no analytic density functions, but also
sandwiches the evidence lower bound (ELBO) between a lower bound and an upper
bound, and further derives an asymptotically exact surrogate ELBO that is
amenable to optimization via stochastic gradient ascent. With a substantially
expanded variational family and a novel optimization algorithm, SIVI is shown
to closely match the accuracy of MCMC in inferring the posterior in a variety
of Bayesian inference tasks.
</summary>
    <author>
      <name>Mingzhang Yin</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11126v1</id>
    <updated>2018-05-28T18:42:43Z</updated>
    <published>2018-05-28T18:42:43Z</published>
    <title>Statistical Methods in Computed Tomography Image Estimation</title>
    <summary>  There is increasing interest in computed tomography (CT) image estimations
from magnetic resonance (MR) images. This study aims to introduce a novel
statistical learning approach for improving CT estimation from MR images. Prior
knowledges about tissue-types, roughly speaking non-bone and bone tissue-types
from CT images, have been used in collaboration with a Gaussian mixture model
(GMM) to explore CT image estimations from MR images. Due to the introduced
prior knowledges, GMMs were trained for each of the tissue-type. At the
prediction stage, we have no CT image, that is there are no prior knowledges
about the tissue-types and thereby we trained RUSBoost algorithm on the
training dataset in order to estimate the tissue-types from MR images of the
new patient. The estimated RUSBoost algorithm and GMMs were used to predict CT
image from MR images of the new patient. We validated the RUSBoost algorithm by
applying 10-fold cross-validation while the Gaussian mixture models were
validated by using leave-one-out cross-validation of the datasets from the
patients. In comparison with the existing model-based CT image estimation
methods, the proposed method has improved the estimation, especially in bone
tissues. More specifically, our method improved CT image estimation by 23
Hounsfield units (HU) and 6 HU on average for datasets obtained from nine and
five patients, respectively. Bone tissue estimations have been improved by 107
HU and 62 HU on average for datasets from nine and five patients, respectively.
Evaluation of our method shows that it is a promising method to generate CT
image substitutes for the implementation of fully MR-based radiotherapy and
PET/MRI applications.
  Keywords: Computed tomography; magnetic resonance imaging; CT image
estimation; supervised learning; Gaussian mixture model
</summary>
    <author>
      <name>Fekadu L. Bayisa</name>
    </author>
    <author>
      <name>Xijia Liu</name>
    </author>
    <author>
      <name>Anders Garpebring</name>
    </author>
    <author>
      <name>Jun Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08058v2</id>
    <updated>2018-05-28T15:35:51Z</updated>
    <published>2017-06-25T08:25:25Z</published>
    <title>Invariant Causal Prediction for Sequential Data</title>
    <summary>  We investigate the problem of inferring the causal predictors of a response
$Y$ from a set of $d$ explanatory variables $(X^1,\dots,X^d)$. Classical
ordinary least squares regression includes all predictors that reduce the
variance of $Y$. Using only the causal predictors instead leads to models that
have the advantage of remaining invariant under interventions, loosely speaking
they lead to invariance across different "environments" or "heterogeneity
patterns". More precisely, the conditional distribution of $Y$ given its causal
predictors remains invariant for all observations. Recent work exploits such a
stability to infer causal relations from data with different but known
environments. We show that even without having knowledge of the environments or
heterogeneity pattern, inferring causal relations is possible for time-ordered
(or any other type of sequentially ordered) data. In particular, this allows
detecting instantaneous causal relations in multivariate linear time series
which is usually not the case for Granger causality. Besides novel methodology,
we provide statistical confidence bounds and asymptotic detection results for
inferring causal predictors, and present an application to monetary policy in
macroeconomics.
</summary>
    <author>
      <name>Niklas Pfister</name>
    </author>
    <author>
      <name>Peter Bühlmann</name>
    </author>
    <author>
      <name>Jonas Peters</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">55 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08058v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08058v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L05, 62P20, 63J05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11414v1</id>
    <updated>2018-05-28T15:12:14Z</updated>
    <published>2018-05-28T15:12:14Z</published>
    <title>Inference for ergodic diffusions plus noise</title>
    <summary>  We research adaptive maximum likelihood-type estimation for an ergodic
diffusion process where the observation is contaminated by noise. This
methodology leads to the asymptotic independence of the estimators for the
variance of observation noise, the diffusion parameter and the drift one of the
latent diffusion process. Moreover, it can lessen the computational burden
compared to simultaneous maximum likelihood-type estimation. In addition to
adaptive estimation, we propose a test to see if noise exists or not, and
analyse real data as the example such that data contains observation noise with
statistical significance.
</summary>
    <author>
      <name>Shogo H. Nakakita</name>
    </author>
    <author>
      <name>Masayuki Uchida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1711.04462</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03309v2</id>
    <updated>2018-05-28T13:36:40Z</updated>
    <published>2018-05-08T22:25:40Z</published>
    <title>Vecchia approximations of Gaussian-process predictions</title>
    <summary>  Gaussian processes (GPs) are highly flexible function estimators used for
geospatial analysis, nonparametric regression, and machine learning, but they
are computationally infeasible for large datasets. Vecchia approximations of
GPs have been used to enable fast evaluation of the likelihood for parameter
inference. Here, we study Vecchia approximations of spatial predictions at
observed and unobserved locations, including obtaining joint predictive
distributions at large sets of locations. We propose a general Vecchia
framework for GP predictions, which contains some novel and some existing
special cases. We study the accuracy and computational properties of these
approaches theoretically and numerically. We show that our new approaches
exhibit linear computational complexity in the total number of spatial
locations. We also apply our methods to a satellite dataset of chlorophyll
fluorescence.
</summary>
    <author>
      <name>Matthias Katzfuss</name>
    </author>
    <author>
      <name>Joseph Guinness</name>
    </author>
    <author>
      <name>Wenlong Gong</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10890v1</id>
    <updated>2018-05-28T12:38:04Z</updated>
    <published>2018-05-28T12:38:04Z</published>
    <title>Model averaging for robust extrapolation in evidence synthesis</title>
    <summary>  Extrapolation from a source to a target, e.g., from adults to children, is a
promising approach to utilizing external information when data are sparse. In
the context of meta-analysis, one is commonly faced with a small number of
studies, while potentially relevant additional information may also be
available. Here we describe a simple extrapolation strategy using heavy-tailed
mixture priors for effect estimation in meta-analysis, which effectively
results in a model-averaging technique. The described method is robust in the
sense that a potential prior-data conflict, i.e., a discrepancy between source
and target data, is explicitly anticipated. The aim of this paper to develop a
solution for this particular application, to showcase the ease of
implementation by providing R code, and to demonstrate the robustness of the
general approach in simulations.
</summary>
    <author>
      <name>Christian Röver</name>
    </author>
    <author>
      <name>Simon Wandel</name>
    </author>
    <author>
      <name>Tim Friede</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.10890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10854v1</id>
    <updated>2018-05-28T10:29:44Z</updated>
    <published>2018-05-28T10:29:44Z</published>
    <title>One family, six distributions -- A flexible model for insurance claim
  severity</title>
    <summary>  We propose a new class of claim severity distributions with six parameters,
that has the standard two-parameter distributions, the log-normal, the
log-Gamma, the Weibull, the Gamma and the Pareto, as special cases. This
distribution is much more flexible than its special cases, and therefore more
able to to capture important characteristics of claim severity data. Further,
we have investigated how increased parameter uncertainty due to a larger number
of parameters affects the estimate of the reserve. This is done in a large
simulation study, where both the characteristics of the claim size
distributions and the sample size are varied. We have also tried our model on a
set of motor insurance claims from a Norwegian insurance company. The results
from the study show that as long as the amount of data is reasonable, the five-
and six-parameter versions of our model provide very good estimates of both the
quantiles of the claim severity distribution and the reserves, for claim size
distributions ranging from medium to very heavy tailed. However, when the
sample size is small, our model appears to struggle with heavy-tailed data, but
is still adequate for data with more moderate tails.
</summary>
    <author>
      <name>Erik Bølviken</name>
    </author>
    <author>
      <name>Ingrid Hobæk Haff</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10742v1</id>
    <updated>2018-05-28T02:36:02Z</updated>
    <published>2018-05-28T02:36:02Z</published>
    <title>High-dimensional statistical inferences with over-identification:
  confidence set estimation and specification test</title>
    <summary>  Over-identification is a signature feature of the influential Generalized
Method of Moments (Hansen, 1982) that flexibly allows more moment conditions
than the model parameters. Investigating over-identification together with
high-dimensional statistical problems is challenging and remains less explored.
In this paper, we study two high-dimensional statistical problems with
over-identification. The first one concerns statistical inferences associated
with multiple components of the high-dimensional model parameters, and the
second one is on developing a specification test for assessing the validity of
the over-identified moment conditions. For the first problem, we propose to
construct a new set of estimating functions such that the impact from
estimating the nuisance parameters becomes asymptotically negligible. Based on
the new construction, a confidence set is estimated using empirical likelihood
(EL) for the specified components of the model parameters. For the second
problem, we propose a test statistic as the maximum of the marginal EL ratios
respectively calculated from individual components of the high-dimensional
moment conditions. Our theoretical analysis establishes the validity of the
proposed procedures, accommodating exponentially growing data dimensionality,
and our numerical examples demonstrate good performance and potential practical
benefits of our proposed methods with high-dimensional problems.
</summary>
    <author>
      <name>Jinyuan Chang</name>
    </author>
    <author>
      <name>Cheng Yong Tang</name>
    </author>
    <author>
      <name>Tong Tong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02122v4</id>
    <updated>2018-05-28T01:31:25Z</updated>
    <published>2016-10-07T02:17:00Z</published>
    <title>Significance testing in non-sparse high-dimensional linear models</title>
    <summary>  In high-dimensional linear models, the sparsity assumption is typically made,
stating that most of the parameters are equal to zero. Under the sparsity
assumption, estimation and, recently, inference have been well studied.
However, in practice, sparsity assumption is not checkable and more importantly
is often violated; a large number of covariates might be expected to be
associated with the response, indicating that possibly all, rather than just a
few, parameters are non-zero. A natural example is a genome-wide gene
expression profiling, where all genes are believed to affect a common disease
marker. We show that existing inferential methods are sensitive to the sparsity
assumption, and may, in turn, result in the severe lack of control of Type-I
error. In this article, we propose a new inferential method, named CorrT, which
is robust to model misspecification such as heteroscedasticity and lack of
sparsity. CorrT is shown to have Type I error approaching the nominal level for
\textit{any} models and Type II error approaching zero for sparse and many
dense models.
  In fact, CorrT is also shown to be optimal in a variety of frameworks:
sparse, non-sparse and hybrid models where sparse and dense signals are mixed.
Numerical experiments show a favorable performance of the CorrT test compared
to the state-of-the-art methods.
</summary>
    <author>
      <name>Yinchu Zhu</name>
    </author>
    <author>
      <name>Jelena Bradic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02122v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02122v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06302v3</id>
    <updated>2018-05-27T20:45:48Z</updated>
    <published>2017-08-21T16:03:04Z</published>
    <title>A general framework for Vecchia approximations of Gaussian processes</title>
    <summary>  Gaussian processes (GPs) are commonly used as models for functions, time
series, and spatial fields, but they are computationally infeasible for large
datasets. Focusing on the typical setting of modeling spatial data as a GP plus
an additive noise term, we propose a generalization of the Vecchia (1988)
approach as a framework for GP approximations. We show that our general Vecchia
approach contains many popular existing GP approximations as special cases,
allowing for comparisons among the different methods within a unified
framework. Representing the models by directed acyclic graphs, we determine the
sparsity of the matrices necessary for inference, which leads to new insights
regarding the computational properties. Based on these results, we propose a
novel sparse general Vecchia approximation, which ensures computational
feasibility for large spatial datasets but can lead to tremendous improvements
in approximation accuracy over Vecchia's original approach. We provide several
theoretical results and conduct numerical comparisons. We conclude with
guidelines for the use of Vecchia approximations in spatial statistics.
</summary>
    <author>
      <name>Matthias Katzfuss</name>
    </author>
    <author>
      <name>Joseph Guinness</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06302v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06302v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10639v1</id>
    <updated>2018-05-27T15:32:45Z</updated>
    <published>2018-05-27T15:32:45Z</published>
    <title>BIC extensions for order-constrained model selection</title>
    <summary>  The Schwarz or Bayesian information criterion (BIC) is one of the most widely
used tools for model comparison in social science research. The BIC however is
not suitable for evaluating models with order constraints on the parameters of
interest. This paper explores two extensions of the BIC for evaluating order
constrained models, one where a truncated unit information prior is used under
the order-constrained model, and the other where a truncated local unit
information prior is used. The first prior is centered around the maximum
likelihood estimate and the latter prior is centered around a null value.
Several analyses show that the order-constrained BIC based on the location unit
information prior functions better as an Occam's razor for evaluating
order-constrained models and results in lower error probabilities. The
methodology is implemented in the R package `BFpack' which allows researchers
to easily apply the method for order-constrained model selection. The
usefulness of the methodology is illustrated using data from the European
Values Study.
</summary>
    <author>
      <name>Joris Mulder</name>
    </author>
    <author>
      <name>Adrian E. Raftery</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 4, figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.10639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10594v1</id>
    <updated>2018-05-27T08:01:21Z</updated>
    <published>2018-05-27T08:01:21Z</published>
    <title>Spectral Clustering for Multiple Sparse Networks: I</title>
    <summary>  Although much of the focus of statistical works on networks has been on
static networks, multiple networks are currently becoming more common among
network data sets. Usually, a number of network data sets, which share some
form of connection between each other are known as multiple or multi-layer
networks. We consider the problem of identifying the common community
structures for multiple networks. We consider extensions of the spectral
clustering methods for the multiple sparse networks, and give theoretical
guarantee that the spectral clustering methods produce consistent community
detection in case of both multiple stochastic block model and multiple
degree-corrected block models. The methods are shown to work under sufficiently
mild conditions on the number of multiple networks to detect associative
community structures, even if all the individual networks are sparse and most
of the individual networks are below community detectability threshold. We
reinforce the validity of the theoretical results via simulations too.
</summary>
    <author>
      <name>Sharmodeep Bhattacharyya</name>
    </author>
    <author>
      <name>Shirshendu Chatterjee</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F40, 62G09, 62D05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10540v1</id>
    <updated>2018-05-26T21:15:20Z</updated>
    <published>2018-05-26T21:15:20Z</published>
    <title>Reliability Estimation in Coherent Systems</title>
    <summary>  Usually, methods evaluating system reliability require engineers to quantify
the reliability of each of the system components. For series and parallel
systems, there are some options to handle the estimation of each component's
reliability. We will treat the reliability estimation of complex problems of
two classes of coherent systems: series-parallel, and parallel-series. In both
of the cases, the component reliabilities may be unknown. We will present
estimators for reliability functions at all levels of the system (component and
system reliabilities). Nonparametric Bayesian estimators of all
sub-distribution and distribution functions are derived, and a Dirichlet
multivariate process as a prior distribution is presented. Parametric estimator
of the component's reliability based on Weibull model is presented for any kind
of system. Also, some ideas in systems with masked data are discussed.
</summary>
    <author>
      <name>Agatha Rodrigues</name>
    </author>
    <author>
      <name>Carlos Alberto Pereira</name>
    </author>
    <author>
      <name>Adriano Polpo</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05631v2</id>
    <updated>2018-05-26T16:20:57Z</updated>
    <published>2018-02-15T16:07:38Z</published>
    <title>Direct Estimation of Differences in Causal Graphs</title>
    <summary>  We consider the problem of estimating the differences between two causal
directed acyclic graph (DAG) models given i.i.d.~samples from each model. This
is of interest for example in genomics, where changes in the structure or edge
weights of the underlying causal graphs reflect alterations in the gene
regulatory networks. We here provide the first provably consistent method for
directly estimating the differences in a pair of causal DAGs without separately
learning two possibly large and dense DAG models and computing their
difference. Our two-step algorithm first uses invariance tests between
regression coefficients of the two data sets to estimate the skeleton of the
difference graph and then orients some of the edges using invariance tests
between regression residual variances. We demonstrate the properties of our
method through a simulation study and apply it to the analysis of gene
expression data from ovarian cancer and during T-cell activation.
</summary>
    <author>
      <name>Yuhao Wang</name>
    </author>
    <author>
      <name>Chandler Squires</name>
    </author>
    <author>
      <name>Anastasiya Belyaeva</name>
    </author>
    <author>
      <name>Caroline Uhler</name>
    </author>
    <link href="http://arxiv.org/abs/1802.05631v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05631v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.04031v3</id>
    <updated>2018-05-26T12:26:42Z</updated>
    <published>2017-02-14T00:34:06Z</published>
    <title>Maximum likelihood estimation in Gaussian models under total positivity</title>
    <summary>  We analyze the problem of maximum likelihood estimation for Gaussian
distributions that are multivariate totally positive of order two (MTP2). By
exploiting connections to phylogenetics and single-linkage clustering, we give
a simple proof that the maximum likelihood estimator (MLE) for such
distributions exists based on at least 2 observations, irrespective of the
underlying dimension. Slawski and Hein, who first proved this result, also
provided empirical evidence showing that the MTP2 constraint serves as an
implicit regularizer and leads to sparsity in the estimated inverse covariance
matrix, determining what we name the ML graph. We show that we can find an
upper bound for the ML graph by adding edges corresponding to correlations in
excess of those explained by the maximum weight spanning forest of the
correlation matrix. Moreover, we provide globally convergent coordinate descent
algorithms for calculating the MLE under the MTP2 constraint which are
structurally similar to iterative proportional scaling. We conclude the paper
with a discussion of signed MTP2 distributions.
</summary>
    <author>
      <name>Steffen Lauritzen</name>
    </author>
    <author>
      <name>Caroline Uhler</name>
    </author>
    <author>
      <name>Piotr Zwiernik</name>
    </author>
    <link href="http://arxiv.org/abs/1702.04031v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.04031v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60E15, 62H99, 15B48" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04967v2</id>
    <updated>2018-05-26T09:06:47Z</updated>
    <published>2015-06-16T13:42:03Z</published>
    <title>Parsimonious Mixed Models</title>
    <summary>  The analysis of experimental data with mixed-effects models requires
decisions about the specification of the appropriate random-effects structure.
Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal'
models with all possible random effect components included. Estimation of
maximal models, however, may not converge. We show that failure to converge
typically is not due to a suboptimal estimation algorithm, but is a consequence
of attempting to fit a model that is too complex to be properly supported by
the data, irrespective of whether estimation is based on maximum likelihood or
on Bayesian hierarchical modeling with uninformative or weakly informative
priors. Importantly, even under convergence, overparameterization may lead to
uninterpretable models. We provide diagnostic tools for detecting
overparameterization and guiding model simplification.
</summary>
    <author>
      <name>Douglas Bates</name>
    </author>
    <author>
      <name>Reinhold Kliegl</name>
    </author>
    <author>
      <name>Shravan Vasishth</name>
    </author>
    <author>
      <name>Harald Baayen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ArXiv preprint. 21 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04967v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04967v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07042v2</id>
    <updated>2018-05-26T08:11:32Z</updated>
    <published>2018-05-18T04:13:10Z</published>
    <title>Graphon estimation via nearest neighbor algorithm and 2D fused lasso
  denoising</title>
    <summary>  We propose a class of methods for graphon estimation based on exploiting
connections with nonparametric regression. The idea is to construct an ordering
of the nodes in the network, similar in spirit to Chan and Airoldi (2014).
However, rather than only considering orderings based on the empirical degree
as in Chan and Airoldi (2014), we use the nearest neighbor algorithm which is
an approximating solution to the traveling salesman problem. This in turn can
handle general distances $\hat{d}$ between the nodes, something that allows us
to incorporate rich information of the network. Once an ordering is
constructed, we formulate a 2D grid graph denoising problem that we solve
through fused lasso regularization. For particular choices of the metric
$\hat{d}$, we show that the corresponding two-step estimator can attain
competitive rates when the true model is the stochastic block model, and when
the underlying graphon is piecewise H\"{o}lder or it has bounded variation.
</summary>
    <author>
      <name>Oscar Hernan Madrid Padilla</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10406v1</id>
    <updated>2018-05-26T00:39:12Z</updated>
    <published>2018-05-26T00:39:12Z</published>
    <title>Robust Nonparametric Regression under Huber's $ε$-contamination
  Model</title>
    <summary>  We consider the non-parametric regression problem under Huber's
$\epsilon$-contamination model, in which an $\epsilon$ fraction of observations
are subject to arbitrary adversarial noise. We first show that a simple local
binning median step can effectively remove the adversary noise and this median
estimator is minimax optimal up to absolute constants over the H\"{o}lder
function class with smoothness parameters smaller than or equal to 1.
Furthermore, when the underlying function has higher smoothness, we show that
using local binning median as pre-preprocessing step to remove the adversarial
noise, then we can apply any non-parametric estimator on top of the medians. In
particular we show local median binning followed by kernel smoothing and local
polynomial regression achieve minimaxity over H\"{o}lder and Sobolev classes
with arbitrary smoothness parameters. Our main proof technique is a decoupled
analysis of adversary noise and stochastic noise, which can be potentially
applied to other robust estimation problems. We also provide numerical results
to verify the effectiveness of our proposed methods.
</summary>
    <author>
      <name>Simon S. Du</name>
    </author>
    <author>
      <name>Yining Wang</name>
    </author>
    <author>
      <name>Sivaraman Balakrishnan</name>
    </author>
    <author>
      <name>Pradeep Ravikumar</name>
    </author>
    <author>
      <name>Aarti Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05635v4</id>
    <updated>2018-05-25T20:06:19Z</updated>
    <published>2015-12-17T15:41:01Z</published>
    <title>The Sorted Effects Method: Discovering Heterogeneous Effects Beyond
  Their Averages</title>
    <summary>  The partial (ceteris paribus) effects of interest in nonlinear and
interactive linear models are heterogeneous as they can vary dramatically with
the underlying observed or unobserved covariates. Despite the apparent
importance of heterogeneity, a common practice in modern empirical work is to
largely ignore it by reporting average partial effects (or, at best, average
effects for some groups). While average effects provide very convenient scalar
summaries of typical effects, by definition they fail to reflect the entire
variety of the heterogeneous effects. In order to discover these effects much
more fully, we propose to estimate and report sorted effects -- a collection of
estimated partial effects sorted in increasing order and indexed by
percentiles. By construction the sorted effect curves completely represent and
help visualize the range of the heterogeneous effects in one plot. They are as
convenient and easy to report in practice as the conventional average partial
effects. They also serve as a basis for classification analysis, where we
divide the observational units into most or least affected groups and summarize
their characteristics. We provide a quantification of uncertainty (standard
errors and confidence bands) for the estimated sorted effects and related
classification analysis, and provide confidence sets for the most and least
affected groups. The derived statistical results rely on establishing key, new
mathematical results on Hadamard differentiability of a multivariate sorting
operator and a related classification operator, which are of independent
interest. We apply the sorted effects method and classification analysis to
demonstrate several striking patterns in the gender wage gap.
</summary>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Ivan Fernandez-Val</name>
    </author>
    <author>
      <name>Ye Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">62 pages, 9 figures, 8 tables, includes appendix with supplementary
  materials</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05635v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05635v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10229v1</id>
    <updated>2018-05-25T16:11:33Z</updated>
    <published>2018-05-25T16:11:33Z</published>
    <title>Importance sampling for slow-fast diffusions based on moderate
  deviations</title>
    <summary>  We consider systems of slow-fast diffusions with small noise in the slow
component. We construct provably logarithmic asymptotically optimal importance
schemes for the estimation of rare events based on the moderate deviations
principle. Using the subsolution approach we construct schemes and identify
conditions under which the schemes will be asymptotically optimal. Moderate
deviations based importance sampling offers a viable alternative to large
deviations importance sampling when the events are not too rare. In particular,
in many cases of interest one can indeed construct the required change of
measure in closed form, a task which is more complicated using the large
deviations based importance sampling, especially when it comes to multiscale
dynamically evolving processes. The presence of multiple scales and the fact
that we do not make any periodicity assumptions for the coefficients driving
the processes, complicates the design and the analysis of efficient importance
sampling schemes. Simulation studies illustrate the theory.
</summary>
    <author>
      <name>Matthew R. Morse</name>
    </author>
    <author>
      <name>Konstantinos Spiliopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10122v1</id>
    <updated>2018-05-25T12:56:54Z</updated>
    <published>2018-05-25T12:56:54Z</published>
    <title>Function Estimation via Reconstruction</title>
    <summary>  This paper introduces an interpolation-based method, called the
reconstruction approach, for function estimation in nonparametric models. Based
on the fact that interpolation usually has negligible errors compared to
statistical estimation, the reconstruction approach uses an interpolator to
parameterize the unknown function with its values at finite knots, and then
estimates these values by minimizing a regularized empirical risk function.
Some popular methods including kernel ridge regression and kernel support
vector machines can be viewed as its special cases. It is shown that, the
reconstruction idea not only provides different angles to look into existing
methods, but also produces new effective experimental design and estimation
methods for nonparametric models.
</summary>
    <author>
      <name>Shifeng Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.10122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03307v2</id>
    <updated>2018-05-25T10:44:08Z</updated>
    <published>2017-07-11T14:47:43Z</published>
    <title>Fast calibrated additive quantile regression</title>
    <summary>  We propose a novel framework for fitting additive quantile regression models,
which provides well calibrated inference about the conditional quantiles and
fast automatic estimation of the smoothing parameters, for model structures as
diverse as those usable with probabilistic GAMs, while maintaining equivalent
numerical efficiency and stability. The inferential and model fitting framework
proposed here is at the same time statistically rigorous and computationally
efficient, because it adopts the general belief updating framework of Bissiri
et al. (2016) to loss based inference, but computes by adapting the stable
fitting methods of Wood et al. (2016). We enable the use of computationally
efficient methods by proposing a novel smooth generalisation of the pinball
loss, which is the loss function traditionally used in quantile regression. The
new loss is motivated by its relation to kernel quantile estimators, which have
favourable statistical properties relative to empirical quantile estimators.
Further, our inferential framework offers reliable uncertainty estimates for
the fitted conditional quantile, which is achieved by coupling asymptotic
posterior approximations with a novel calibration approach to selection of the
learning rate. Our work was motivated by a probabilistic electricity load
forecasting application, which we use here to demonstrate the proposed
approach. The methods described in this paper are implemented by the qgam R
package, available on the Comprehensive R Archive Network (CRAN).
</summary>
    <author>
      <name>M. Fasiolo</name>
    </author>
    <author>
      <name>Y. Goude</name>
    </author>
    <author>
      <name>R. Nedellec</name>
    </author>
    <author>
      <name>S. N. Wood</name>
    </author>
    <link href="http://arxiv.org/abs/1707.03307v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03307v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10040v1</id>
    <updated>2018-05-25T08:52:38Z</updated>
    <published>2018-05-25T08:52:38Z</published>
    <title>Body and Tail - Separating the distribution function by an efficient
  tail-detecting procedure in risk management</title>
    <summary>  In risk management, tail risks are of crucial importance. The quality of a
tail model, which is determined by data from an unknown distribution, depends
critically on the subset of data used to model the tail. Based on a suitably
weighted mean square error, we present a method that can separate the required
subset. The selected data are used to determine the parameters of the tail
model. Notably, no parameter specifications have to be made to apply the
proposed procedure. Standard goodness of fit tests allow us to evaluate the
quality of the fitted tail model. We apply the method to standard distributions
that are usually considered in the finance and insurance industries. In
addition, for the MSCI World Index, we use historical data to identify the tail
model and to compute the quantiles required for a risk assessment.
</summary>
    <author>
      <name>Ingo Hoffmann</name>
    </author>
    <author>
      <name>Christoph J. Börner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.10040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E17, 62C99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.2; I.6.4; I.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05646v2</id>
    <updated>2018-05-25T04:57:33Z</updated>
    <published>2017-11-15T16:13:01Z</published>
    <title>Spatial Joint Species Distribution Modeling using Dirichlet Processes</title>
    <summary>  Species distribution models usually attempt to explain presence-absence or
abundance of a species at a site in terms of the environmental features
(socalled abiotic features) present at the site. Historically, such models have
considered species individually. However, it is well-established that species
interact to influence presence-absence and abundance (envisioned as biotic
factors). As a result, there has been substantial recent interest in joint
species distribution models with various types of response, e.g.,
presence-absence, continuous and ordinal data. Such models incorporate
dependence between species response as a surrogate for interaction.
  The challenge we focus on here is how to address such modeling in the context
of a large number of species (e.g., order 102) across sites numbering in the
order of 102 or 103 when, in practice, only a few species are found at any
observed site. Again, there is some recent literature to address this; we adopt
a dimension reduction approach. The novel wrinkle we add here is spatial
dependence. That is, we have a collection of sites over a relatively small
spatial region so it is anticipated that species distribution at a given site
would be similar to that at a nearby site. Specifically, we handle dimension
reduction through Dirichlet processes joined with spatial dependence through
Gaussian processes.
  We use both simulated data and a plant communities dataset for the Cape
Floristic Region (CFR) of South Africa to demonstrate our approach. The latter
consists of presence-absence measurements for 639 tree species on 662
locations. Through both data examples we are able to demonstrate improved
predictive performance using the foregoing specification.
</summary>
    <author>
      <name>Shinichiro Shirota</name>
    </author>
    <author>
      <name>Alan E. Gelfand</name>
    </author>
    <author>
      <name>Sudipto Banerjee</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05646v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05646v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09978v1</id>
    <updated>2018-05-25T04:27:58Z</updated>
    <published>2018-05-25T04:27:58Z</published>
    <title>Distributed Cartesian Power Graph Segmentation for Graphon Estimation</title>
    <summary>  We study an extention of total variation denoising over images to over
Cartesian power graphs and its applications to estimating non-parametric
network models. The power graph fused lasso (PGFL) segments a matrix by
exploiting a known graphical structure, $G$, over the rows and columns. Our
main results shows that for any connected graph, under subGaussian noise, the
PGFL achieves the same mean-square error rate as 2D total variation denoising
for signals of bounded variation. We study the use of the PGFL for denoising an
observed network $H$, where we learn the graph $G$ as the $K$-nearest
neighborhood graph of an estimated metric over the vertices. We provide
theoretical and empirical results for estimating graphons, a non-parametric
exchangeable network model, and compare to the state of the art graphon
estimation methods.
</summary>
    <author>
      <name>Shitong Wei</name>
    </author>
    <author>
      <name>Oscar Hernan Madrid-Padilla</name>
    </author>
    <author>
      <name>James Sharpnack</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09937v1</id>
    <updated>2018-05-25T00:13:42Z</updated>
    <published>2018-05-25T00:13:42Z</published>
    <title>Inference Related to Common Breaks in a Multivariate System with Joined
  Segmented Trends with Applications to Global and Hemispheric Temperatures</title>
    <summary>  What transpires from recent research is that temperatures and radiative
forcing seem to be characterized by a linear trend with two changes in the rate
of growth. The first occurs in the early 60s and indicates a very large
increase in the rate of growth of both temperature and radiative forcing
series. This was termed as the "onset of sustained global warming". The second
is related to the more recent so-called hiatus period, which suggests that
temperatures and total radiative forcing have increased less rapidly since the
mid-90s compared to the larger rate of increase from 1960 to 1990. There are
two issues that remain unresolved. The first is whether the breaks in the slope
of the trend functions of temperatures and radiative forcing are common. This
is important because common breaks coupled with the basic science of climate
change would strongly suggest a causal effect from anthropogenic factors to
temperatures. The second issue relates to establishing formally via a proper
testing procedure that takes into account the noise in the series, whether
there was indeed a `hiatus period' for temperatures since the mid 90s. This is
important because such a test would counter the widely held view that the
hiatus is the product of natural internal variability. Our paper provides tests
related to both issues. The results show that the breaks in temperatures and
radiative forcing are common and that the hiatus is characterized by a
significant decrease in their rate of growth. The statistical results are of
independent interest and applicable more generally.
</summary>
    <author>
      <name>Dukpa Kim</name>
    </author>
    <author>
      <name>Tatsushi Oka</name>
    </author>
    <author>
      <name>Francisco Estrada</name>
    </author>
    <author>
      <name>Pierre Perron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 8 tables, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09902v1</id>
    <updated>2018-05-24T21:13:31Z</updated>
    <published>2018-05-24T21:13:31Z</published>
    <title>Generic Conditions for Forecast Dominance</title>
    <summary>  Recent studies have analyzed whether one forecast method dominates another
under a class of consistent scoring functions. For the mean functional, we show
that dominance holds under simple conditions: Both forecasts must be
auto-calibrated, and one forecast must be greater than the other in convex
order. Conditions for quantile forecasts are similar but more complex. Unlike
existing results, the new conditions allow for the case that the forecasts'
underlying information sets are not nested, a situation that is highly relevant
in applications.
</summary>
    <author>
      <name>Fabian Krüger</name>
    </author>
    <author>
      <name>Johanna F. Ziegel</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09876v1</id>
    <updated>2018-05-24T20:00:44Z</updated>
    <published>2018-05-24T20:00:44Z</published>
    <title>Testing small study effects in multivariate meta-analysis</title>
    <summary>  Small study effects occur when smaller studies show different, often larger,
treatment effects than large ones, which may threaten the validity of
systematic reviews and meta-analyses. The most well-known reasons for small
study effects include publication bias, outcome reporting bias and clinical
heterogeneity. Methods to account for small study effects in univariate
meta-analysis have been extensively studied. However, detecting small study
effects in a multivariate meta-analysis setting remains an untouched research
area. One of the complications is that different types of selection processes
can be involved in the reporting of multivariate outcomes. For example, some
studies may be completely unpublished while others may selectively report
multiple outcomes. In this paper, we propose a score test as an overall test of
small study effects in multivariate meta-analysis. Two detailed case studies
are given to demonstrate the advantage of the proposed test over various naive
applications of univariate tests in practice. Through simulation studies, the
proposed test is found to retain nominal Type I error with considerable power
in moderate sample size settings. Finally, we also evaluate the concordance
between the proposed test with the naive application of univariate tests by
evaluating 44 systematic reviews with multiple outcomes from the Cochrane
Database.
</summary>
    <author>
      <name>Chuan Hong</name>
    </author>
    <author>
      <name>Georgia Salanti</name>
    </author>
    <author>
      <name>Sally Morton</name>
    </author>
    <author>
      <name>Richard Riley</name>
    </author>
    <author>
      <name>Haitao Chu</name>
    </author>
    <author>
      <name>Stephen E. Kimmel</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09873v1</id>
    <updated>2018-05-24T19:57:12Z</updated>
    <published>2018-05-24T19:57:12Z</published>
    <title>Concave regression: value-constrained estimation and likelihood
  ratio-based inference</title>
    <summary>  We propose a likelihood ratio statistic for forming hypothesis tests and
confidence intervals for a nonparametrically estimated univariate regression
function, based on the shape restriction of concavity (equivalently,
convexity). Dealing with the likelihood ratio statistic requires studying an
estimator satisfying a null hypothesis, that is, studying a concave
least-squares estimator satisfying a further equality constraint. We study this
null hypothesis least-squares estimator (NLSE) here, and use it to study our
likelihood ratio statistic. The NLSE is the solution to a convex program, and
we find a set of inequality and equality constraints that characterize the
solution. We also study a corresponding limiting version of the convex program
based on observing a Brownian motion with drift. The solution to the limit
problem is a stochastic process. We study the optimality conditions for the
solution to the limit problem and find that they match those we derived for the
solution to the finite sample problem. This allows us to show the limit
stochastic process yields the limit distribution of the (finite sample) NLSE.
We conjecture that the likelihood ratio statistic is asymptotically pivotal,
meaning that it has a limit distribution with no nuisance parameters to be
estimated, which makes it a very effective tool for this difficult inference
problem. We provide a partial proof of this conjecture, and we also provide
simulation evidence strongly supporting this conjecture.
</summary>
    <author>
      <name>Charles R. Doss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09840v1</id>
    <updated>2018-05-24T18:12:10Z</updated>
    <published>2018-05-24T18:12:10Z</published>
    <title>Dynamic Chain Graph Models for Ordinal Time Series Data</title>
    <summary>  This paper introduces sparse dynamic chain graph models for network inference
in high dimensional non-Gaussian time series data. The proposed method
parametrized by a precision matrix that encodes the intra time-slice
conditional independence among variables at a fixed time point, and an
autoregressive coefficient that contains dynamic conditional independences
interactions among time series components across consecutive time steps. The
proposed model is a Gaussian copula vector autoregressive model, which is used
to model sparse interactions in a high-dimensional setting. Estimation is
achieved via a penalized EM algorithm. In this paper, we use an efficient
coordinate descent algorithm to optimize the penalized log-likelihood with the
smoothly clipped absolute deviation penalty. We demonstrate our approach on
simulated and genomic datasets. The method is implemented in an R package
tsnetwork.
</summary>
    <author>
      <name>Pariya Behrouzi</name>
    </author>
    <author>
      <name>Fentaw Abegaz</name>
    </author>
    <author>
      <name>Ernst C. Wit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 2 tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09838v1</id>
    <updated>2018-05-24T18:08:58Z</updated>
    <published>2018-05-24T18:08:58Z</published>
    <title>Strategic Monte Carlo Methods for State and Parameter Estimation in High
  Dimensional Nonlinear Problems</title>
    <summary>  In statistical data assimilation one seeks the largest maximum of the
conditional probability distribution $P(\mathbf{X},\mathbf{p}|\mathbf{Y})$ of
model states, $\mathbf{X}$, and parameters,$\mathbf{p}$, conditioned on
observations $\mathbf{Y}$ through minimizing the `action', $A(\mathbf{X}) =
-\log P(\mathbf{X},\mathbf{p}|\mathbf{Y})$. This determines the dominant
contribution to the expected values of functions of $\mathbf{X}$ but does not
give information about the structure of $P(\mathbf{X},\mathbf{p}|\mathbf{Y})$
away from the maximum. We introduce a Monte Carlo sampling method, called
Strategic Monte Carlo (SMC) sampling, for estimating $P(\mathbf{X},
\mathbf{p}|\mathbf{Y})$ in the neighborhood of its largest maximum to remedy
this limitation. SMC begins with a systematic variational annealing (VA)
procedure for finding the smallest minimum of $A(\mathbf{X})$. SMC generates
accurate estimates for the mean, standard deviation and other higher moments of
$P(\mathbf{X},\mathbf{p}|\mathbf{Y})$. Additionally, the random search allows
for an understanding of any multimodal structure that may underly the dynamics
of the problem. SMC generates a gaussian probability control term based on the
paths determined by VA to minimize a cost function $A(\mathbf{X},\mathbf{p})$.
This probability is sampled during the Monte Carlo search of the cost function
to constrain the search to high probability regions of the surface thus
substantially reducing the time necessary to sufficiently explore the space.
</summary>
    <author>
      <name>Sasha Shirman</name>
    </author>
    <author>
      <name>Henry D. I. Abarbanel</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01805v2</id>
    <updated>2018-05-24T15:50:03Z</updated>
    <published>2017-03-06T10:32:54Z</published>
    <title>Bayesian Estimation of Kendall's tau Using a Latent Normal Approach</title>
    <summary>  The rank-based association between two variables can be modeled by
introducing a latent normal level to ordinal data. We demonstrate how this
approach yields Bayesian inference for Kendall's rank correlation coefficient,
improving on a recent Bayesian solution from asymptotic properties of the test
statistic.
</summary>
    <author>
      <name>Johnny van Doorn</name>
    </author>
    <author>
      <name>Alexander Ly</name>
    </author>
    <author>
      <name>Maarten Marsman</name>
    </author>
    <author>
      <name>Eric-Jan Wagenmakers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09736v1</id>
    <updated>2018-05-24T15:42:39Z</updated>
    <published>2018-05-24T15:42:39Z</published>
    <title>Estimating Population Average Causal Effects in the Presence of
  Non-Overlap: A Bayesian Approach</title>
    <summary>  Most causal inference studies rely on the assumption of positivity, or
overlap, to identify population or sample average causal effects. When this
assumption is violated, these estimands are unidentifiable without some degree
of reliance on model specifications, due to poor data support. Existing methods
to address non-overlap, such as trimming or down-weighting data in regions of
poor support, all suffer from the limitation of changing the estimand. The
change in estimand may diminish the impact of the study results, particularly
for studies intended to influence policy, because estimates may not be
representative of effects in the population of interest to policymakers.
Researchers may be willing to make additional, minimal modeling assumptions in
order to preserve the ability to estimate population average causal effects. We
seek to make two contributions on this topic. First, we propose systematic
definitions of propensity score overlap and non-overlap regions. Second, we
develop a novel Bayesian framework to estimate population average causal
effects with nominal model dependence and appropriately large uncertainties in
the presence of non-overlap and causal effect heterogeneity. In this approach,
the tasks of estimating causal effects in the overlap and non-overlap regions
are delegated to two distinct models. Tree ensembles are selected to
non-parametrically estimate individual causal effects in the overlap region,
where the data can speak for themselves. In the non-overlap region, where
insufficient data support means reliance on model specification is necessary,
individual causal effects are estimated by extrapolating trends from the
overlap region via a spline model. The promising performance of our method is
demonstrated in simulations. Finally, we utilize our method to perform a novel
investigation of the effect of natural gas compressor station exposure on
cancer outcomes.
</summary>
    <author>
      <name>Rachel C. Nethery</name>
    </author>
    <author>
      <name>Fabrizia Mealli</name>
    </author>
    <author>
      <name>Francesca Dominici</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09700v1</id>
    <updated>2018-05-24T14:41:03Z</updated>
    <published>2018-05-24T14:41:03Z</published>
    <title>Convex method for selection of fixed effects in high-dimensional linear
  mixed models</title>
    <summary>  Analysis of high-dimensional data is currently a popular field of research,
thanks to many applications e.g. in genetics (DNA data in genomewide
association studies), spectrometry or web analysis. At the same time, the type
of problems that tend to arise in genetics can often be modelled using linear
mixed models in conjunction with high-dimensional data because linear mixed
models allow us to specify the covariance structure of the models. This enables
us to capture relationships in data such as the population structure, family
relatedness, etc. In this paper we introduce two new convex methods for
variable selection in high-dimensional linear mixed models which, thanks to
convexity, can handle many more variables than existing non-convex methods.
Both methods are compared with existing methods and in the end we suggest an
approach for a wider class of linear mixed models.
</summary>
    <author>
      <name>Jozef Jakubik</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09579v1</id>
    <updated>2018-05-24T09:51:53Z</updated>
    <published>2018-05-24T09:51:53Z</published>
    <title>Model-based inference of conditional extreme value distributions with
  hydrological applications</title>
    <summary>  Multivariate extreme value models are used to estimate joint risk in a number
of applications, with a particular focus on environmental fields ranging from
climatology, hydrology to oceanography. The semi-parametric conditional extreme
value model of Heffernan and Tawn (2004) provides the most suitable of current
statistical models in terms of its flexibility to handle a range of extremal
dependence classes and its ability to deal with high dimensionality. The
standard formulation of the model is highly inefficient as it does not allow
for any partially missing observations, that typically arise with environmental
data, to be included and it suffers from the curse of dimensionality as it
involves a $d-1$-dimensional non-parametric density estimator in
$d$-dimensional problems. A solution to the former was proposed by Keef et al.
(2009) but it is extremely computationally intensive, making its use
prohibitive if the proportion of missing data is non-trivial. We propose to
replace the $d-1$-dimensional non-parametric density estimator with a
model-based copula with univariate marginal densities estimated using kernel
methods. This approach provides statistically and computationally efficient
estimates whatever the degree of missing data or the dimension, $d$, and
improvements in other inferential aspects. The methods are illustrated through
the analysis of UK river flow data at a network of 46 sites.
</summary>
    <author>
      <name>Ross Towe</name>
    </author>
    <author>
      <name>Jonathan Tawn</name>
    </author>
    <author>
      <name>Rob Lamb</name>
    </author>
    <author>
      <name>Chris Sherlock</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09505v1</id>
    <updated>2018-05-24T04:50:10Z</updated>
    <published>2018-05-24T04:50:10Z</published>
    <title>Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering</title>
    <summary>  Standard clustering algorithms usually find regular-structured clusters such
as ellipsoidally- or spherically-dispersed groups, but are more challenged with
groups lacking formal structure or definition. Syncytial clustering is the name
that we introduce for methods that merge groups obtained from standard
clustering algorithms in order to reveal complex group structure in the data.
Here, we develop a distribution-free fully-automated syncytial clustering
algorithm that can be used with $k$-means and other algorithms. Our approach
computes the cumulative distribution function of the normed residuals from an
appropriately fit $k$-groups model and calculates the nonparametric overlap
between each pair of groups. Groups with high pairwise overlaps are merged as
long as the generalized overlap decreases. Our methodology is always a top
performer in identifying groups with regular and irregular structures in
several datasets. The approach is also used to identify the distinct kinds of
gamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and
also the distinct kinds of activation in a functional Magnetic Resonance
Imaging study.
</summary>
    <author>
      <name>Israel Almodóvar-Rivera</name>
    </author>
    <author>
      <name>Ranjan Maitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 19 figures, 6tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09468v1</id>
    <updated>2018-05-24T01:05:14Z</updated>
    <published>2018-05-24T01:05:14Z</published>
    <title>Bayesian predictive densities as an interpretation of a class of
  Skew--Student $t$ distributions with application to medical data</title>
    <summary>  This paper describes a new Bayesian interpretation of a class of
skew--Student $t$ distributions. We consider a hierarchical normal model with
unknown covariance matrix and show that by imposing different restrictions on
the parameter space, corresponding Bayes predictive density estimators under
Kullback-Leibler loss function embrace some well-known skew--Student $t$
distributions. We show that obtained estimators perform better in terms of
frequentist risk function over regular Bayes predictive density estimators. We
apply our proposed methods to estimate future densities of medical data: the
leg-length discrepancy and effect of exercise on the age at which a child
starts to walk.
</summary>
    <author>
      <name>Abdolnasser Sadeghkhani</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60E05, 62F15, 62F30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09460v1</id>
    <updated>2018-05-24T00:17:24Z</updated>
    <published>2018-05-24T00:17:24Z</published>
    <title>Cautious Deep Learning</title>
    <summary>  Most classifiers operate by selecting the maximum of an estimate of the
conditional distribution $p(y|x)$ where $x$ stands for the features of the
instance to be classified and $y$ denotes its label. This often results in a
hubristic bias: overconfidence in the assignment of a definite label. Usually,
the observations are concentrated on a small volume but the classifier provides
definite predictions for the entire space. We propose constructing conformal
prediction sets [vovk2005algorithmic] which contain a set of labels rather than
a single label. These conformal prediction sets contain the true label with
probability $1-\alpha$. Our construction is based on $p(x|y)$ rather than
$p(y|x)$ which results in a classifier that is very cautious: it outputs the
null set - meaning `I don't know' --- when the object does not resemble the
training examples. An important property of our approach is that classes can be
added or removed without having to retrain the classifier. We demonstrate the
performance on the ImageNet ILSVRC dataset using high dimensional features
obtained from state of the art convolutional neural networks.
</summary>
    <author>
      <name>Yotam Hechtlinger</name>
    </author>
    <author>
      <name>Barnabás Póczos</name>
    </author>
    <author>
      <name>Larry Wasserman</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09392v1</id>
    <updated>2018-05-23T19:23:04Z</updated>
    <published>2018-05-23T19:23:04Z</published>
    <title>pMSE Mechanism: Differentially Private Synthetic Data with Maximal
  Distributional Similarity</title>
    <summary>  We propose a method for the release of differentially private synthetic
datasets. In many contexts, data contain sensitive values which cannot be
released in their original form in order to protect individuals' privacy.
Synthetic data is a protection method that releases alternative values in place
of the original ones, and differential privacy (DP) is a formal guarantee for
quantifying the privacy loss. We propose a method that maximizes the
distributional similarity of the synthetic data relative to the original data
using a measure known as the pMSE, while guaranteeing epsilon-differential
privacy. Additionally, we relax common DP assumptions concerning the
distribution and boundedness of the original data. We prove theoretical results
for the privacy guarantee and provide simulations for the empirical failure
rate of the theoretical results under typical computational limitations. We
also give simulations for the accuracy of linear regression coefficients
generated from the synthetic data compared with the accuracy of
non-differentially private synthetic data and other differentially private
methods. Additionally, our theoretical results extend a prior result for the
sensitivity of the Gini Index to include continuous predictors.
</summary>
    <author>
      <name>Joshua Snoke</name>
    </author>
    <author>
      <name>Aleksandra Slavković</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09002v5</id>
    <updated>2018-05-23T18:29:35Z</updated>
    <published>2018-01-26T23:17:08Z</published>
    <title>Median bias reduction in random-effects meta-analysis and
  meta-regression</title>
    <summary>  Random-effects models are frequently used to synthesise information from
different studies in meta-analysis. While likelihood-based inference is
attractive both in terms of limiting properties and of implementation, its
application in random-effects meta-analysis may result in misleading
conclusions, especially when the number of studies is small to moderate. The
current paper shows how methodology that reduces the asymptotic bias of the
maximum likelihood estimator of the variance component can also substantially
improve inference about the mean effect size. The results are derived for the
more general framework of random-effects meta-regression, which allows the mean
effect size to vary with study-specific covariates.
</summary>
    <author>
      <name>Sophia Kyriakou</name>
    </author>
    <author>
      <name>Ioannis Kosmidis</name>
    </author>
    <author>
      <name>Nicola Sartori</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09002v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09002v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F03, 62F12, 62P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08994v2</id>
    <updated>2018-05-23T17:44:07Z</updated>
    <published>2017-01-31T11:16:12Z</published>
    <title>On the geometry of Bayesian inference</title>
    <summary>  We provide a geometric interpretation to Bayesian inference that allows us to
introduce a natural measure of the level of agreement between priors,
likelihoods, and posteriors. The starting point for the construction of our
geometry is the simple observation that the marginal likelihood can be regarded
as an inner product between the prior and the likelihood. A key concept in our
geometry is that of compatibility, a measure which is based on the same
construction principles as Pearson correlation, but which can be used to assess
how much the prior agrees with the likelihood, to gauge the sensitivity of the
posterior to the prior, and to quantify the coherency of the opinions of two
experts. Estimators for all the quantities involved in our geometric setup are
discussed, which can be directly computed from the posterior simulation output.
Some examples are used to illustrate our methods, including data related to
on-the-job drug usage, midge wing length, and prostate cancer.
</summary>
    <author>
      <name>Miguel de Carvalho</name>
    </author>
    <author>
      <name>Garritt L. Page</name>
    </author>
    <author>
      <name>Bradley J. Barney</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08994v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08994v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06181v4</id>
    <updated>2018-05-23T17:11:26Z</updated>
    <published>2017-09-18T22:01:05Z</published>
    <title>On Nesting Monte Carlo Estimators</title>
    <summary>  Many problems in machine learning and statistics involve nested expectations
and thus do not permit conventional Monte Carlo (MC) estimation. For such
problems, one must nest estimators, such that terms in an outer estimator
themselves involve calculation of a separate, nested, estimation. We
investigate the statistical implications of nesting MC estimators, including
cases of multiple levels of nesting, and establish the conditions under which
they converge. We derive corresponding rates of convergence and provide
empirical evidence that these rates are observed in practice. We further
establish a number of pitfalls that can arise from naive nesting of MC
estimators, provide guidelines about how these can be avoided, and lay out
novel methods for reformulating certain classes of nested expectation problems
into single expectations, leading to improved convergence rates. We demonstrate
the applicability of our work by using our results to develop a new estimator
for discrete Bayesian experimental design problems and derive error bounds for
a class of variational objectives.
</summary>
    <author>
      <name>Tom Rainforth</name>
    </author>
    <author>
      <name>Robert Cornish</name>
    </author>
    <author>
      <name>Hongseok Yang</name>
    </author>
    <author>
      <name>Andrew Warrington</name>
    </author>
    <author>
      <name>Frank Wood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at International Conference on Machine Learning 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06181v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06181v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03889v2</id>
    <updated>2018-05-23T14:12:34Z</updated>
    <published>2017-12-11T17:04:39Z</published>
    <title>Statistical sparsity</title>
    <summary>  The main contribution of this paper is a mathematical definition of
statistical sparsity, which is expressed as a limiting property of a sequence
of probability distributions. The limit is characterized by an exceedance
measure~$H$ and a rate parameter~$\rho &gt; 0$, both of which are unrelated to
sample size. The definition is sufficient to encompass all sparsity models that
have been suggested in the signal-detection literature. Sparsity implies that
$\rho$~is small, and a sparse approximation is asymptotic in the rate
parameter, typically with error $o(\rho)$ in the sparse limit $\rho \to 0$. To
first order in sparsity, the sparse signal plus Gaussian noise convolution
depends on the signal distribution only through its rate parameter and
exceedance measure. This is one of several asymptotic approximations implied by
the definition, each of which is most conveniently expressed in terms of the
zeta-transformation of the exceedance measure. One implication is that two
sparse families having the same exceedance measure are inferentially
equivalent, and cannot be distinguished to first order. A converse implication
for methodological strategy is that it may be more fruitful to focus on the
exceedance measure, ignoring aspects of the signal distribution that have
negligible effect on observables and on inferences. From this point of view,
scale models and inverse-power measures seem particularly attractive.
</summary>
    <author>
      <name>Peter McCullagh</name>
    </author>
    <author>
      <name>Nicholas Polson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03889v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03889v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09175v1</id>
    <updated>2018-05-23T13:51:43Z</updated>
    <published>2018-05-23T13:51:43Z</published>
    <title>Detecting SNPs with interactive effects on a quantitative trait</title>
    <summary>  Here we propose a test to detect effects of single nucleotide polymorphisms
(SNPs) on a quantitative trait. Significant SNP-SNP interactions are more
difficult to detect than significant SNPs, partly due to the massive amount of
SNP-SNP combinations. We propose to move away from testing interaction terms,
and move towards testing whether an individual SNP is involved in any
interaction. This reduces the multiple testing burden to one test per SNP, and
allows for interactions with unobserved factors. Analysing one SNP at a time,
we split the individuals into two groups, based on the number of minor alleles.
If the quantitative trait differs in mean between the two groups, the SNP has a
main effect. If the quantitative trait differs in distribution between some
individuals in one group and all other individuals, it possibly has an
interactive effect. We propose a mixture test to detect both types of effects.
Implicitly, the membership probabilities may suggest potential interacting
variables. Analysing simulated and experimental data, we show that the proposed
test is statistically powerful, maintains the type I error rate, and detects
meaningful signals. The R package semisup is available from Bioconductor.
</summary>
    <author>
      <name>Armin Rauschenberger</name>
    </author>
    <author>
      <name>Renee X. Menezes</name>
    </author>
    <author>
      <name>Mark A. van de Wiel</name>
    </author>
    <author>
      <name>Natasja M. van Schoor</name>
    </author>
    <author>
      <name>Marianne A. Jonker</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09091v1</id>
    <updated>2018-05-23T12:30:28Z</updated>
    <published>2018-05-23T12:30:28Z</published>
    <title>Neural networks for post-processing ensemble weather forecasts</title>
    <summary>  Ensemble weather predictions require statistical post-processing of
systematic errors to obtain reliable and accurate probabilistic forecasts.
Traditionally, this is accomplished with distributional regression models in
which the parameters of a predictive distribution are estimated from a training
period. We propose a flexible alternative based on neural networks that can
incorporate nonlinear relationships between arbitrary predictor variables and
forecast distribution parameters that are automatically learned in a
data-driven way rather than requiring pre-specified link functions. In a case
study of 2-meter temperature forecasts at surface stations in Germany, the
neural network approach significantly outperforms benchmark post-processing
methods while being computationally more affordable. Key components to this
improvement are the use of auxiliary predictor variables and station-specific
information with the help of embeddings. Furthermore, the trained neural
network can be used to gain insight into the importance of meteorological
variables thereby challenging the notion of neural networks as uninterpretable
black boxes. Our approach can easily be extended to other statistical
post-processing and forecasting problems. We anticipate that recent advances in
deep learning combined with the ever-increasing amounts of model and
observation data will transform the post-processing of numerical weather
forecasts in the coming decade.
</summary>
    <author>
      <name>Stephan Rasp</name>
    </author>
    <author>
      <name>Sebastian Lerch</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7403v4</id>
    <updated>2018-05-23T05:43:58Z</updated>
    <published>2014-04-29T15:38:41Z</published>
    <title>Selective Sign-Determining Multiple Confidence Intervals with FCR
  Control</title>
    <summary>  Given $m$ unknown parameters with corresponding independent estimators, the
Benjamini-Hochberg (BH) procedure can be used to classify the sign of
parameters such that the expected proportion of erroneous directional decisions
(directional FDR) is controlled at a preset level $q$. More ambitiously, our
goal is to construct sign-determining confidence intervals---instead of only
classifying the sign---such that the expected proportion of non-covering
constructed intervals (FCR) is controlled. We suggest a valid procedure which
adjusts a marginal confidence interval in order to construct a maximum number
of sign-determining confidence intervals. We propose a new marginal confidence
interval, designed specifically for our procedure, which allows to balance a
trade-off between power and length of the constructed intervals, and, in fact,
often enjoy (almost) the best of both worlds. We apply our methods to detect
the sign of correlations in a highly publicized social neuroscience study and,
in a second example, to detect the direction of association for SNPs with
Type-2 Diabetes in GWAS data. In both examples we compare our procedure to
existing methods and obtain encouraging results.
</summary>
    <author>
      <name>Asaf Weinstein</name>
    </author>
    <author>
      <name>Daniel Yekutieli</name>
    </author>
    <link href="http://arxiv.org/abs/1404.7403v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7403v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08931v1</id>
    <updated>2018-05-23T01:48:00Z</updated>
    <published>2018-05-23T01:48:00Z</published>
    <title>On The Estimation of the Hurst Exponent Using Adjusted Rescaled Range
  Analysis, Detrended Fluctuation Analysis and Variance Time Plot: A Case of
  Exponential Distribution</title>
    <summary>  Hurst Exponent has been widely used in different fields as a measure of long
range dependence in time series. It has been studied in hydrology and
geophysics, economics and finance, and recently, it is still a hot topic in the
different areas of research involving DNA sequences, cardiac dynamics, internet
traffic, meteorology and geology. Various methods in the estimation of Hurst
Exponent have been proposed such as Adjusted Rescaled Range Analysis, Detrended
Fluctuation Analysis and Variance Time Plot Analysis. This study explored the
efficiency of the three methods: Adjusted Rescaled Range Analysis, Detrended
Fluctuation Analysis and Variance Time Plot Analysis in the estimation of Hurst
Exponent when data are generated from an exponential distribution. In addition,
the efficiency of the three methods was compared in different sample sizes of
128, 256, 512, 1024 and varying {\lambda} parameter values of 0.1, 0.5, 1.5,
3.0, 5.0 and 7.0. The estimation process for each of the methods using
different sample sizes and {\lambda} parameter values were repeated for 100,
500 and 1000 times to verify the consistency of the result. A Scilab Program
containing different functions was developed for the study to aid in the
simulation process and calculation. The Adjusted Rescaled Range Analysis was
the most efficient method with the smallest Mean Square Error for all {\lambda}
parameter values and different sample sizes.
</summary>
    <author>
      <name>Roel F. Ceballos</name>
    </author>
    <author>
      <name>Fe F. Largo</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Imperial Journal of Interdisciplinary Research 2017 (Volume 3,
  Issue 8, pp. 424-434)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.08931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08110v2</id>
    <updated>2018-05-22T22:22:09Z</updated>
    <published>2018-05-21T15:06:50Z</published>
    <title>On a general structure for hazard-based regression models: an
  application to population-based cancer research</title>
    <summary>  The proportional hazards model represents the most commonly assumed hazard
structure when analysing time to event data using regression models. We study a
general hazard structure which contains, as particular cases, proportional
hazards, accelerated hazards, and accelerated failure time structures, as well
as combinations of these. We propose an approach to apply these different
hazard structures, based on a flexible parametric distribution (Exponentiated
Weibull) for the baseline hazard. This distribution allows us to cover the
basic hazard shapes of interest in practice: constant, bathtub, increasing,
decreasing, and unimodal. In an extensive simulation study, we evaluate our
approach in the context of excess hazard modelling, which is the main quantity
of interest in descriptive cancer epidemiology. This study exhibits good
inferential properties of the proposed model, as well as good performance when
using the Akaike Information Criterion for selecting the hazard structure. An
application on lung cancer data illustrates the usefulness of the proposed
model.
</summary>
    <author>
      <name>Francisco J. Rubio</name>
    </author>
    <author>
      <name>Laurent Remontet</name>
    </author>
    <author>
      <name>Nicholas P. Jewell</name>
    </author>
    <author>
      <name>Aurélien Belot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Statistical Methods in Medical Research. Supplementary
  material and software available here:
  https://sites.google.com/site/fjavierrubio67/home/papers</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08110v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08110v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08883v1</id>
    <updated>2018-05-22T21:57:42Z</updated>
    <published>2018-05-22T21:57:42Z</published>
    <title>Sensitivity of Regular Estimators</title>
    <summary>  This paper studies local asymptotic relationship between two scalar
estimates. We define sensitivity of a target estimate to a control estimate to
be the directional derivative of the target functional with respect to the
gradient direction of the control functional. Sensitivity according to the
information metric on the model manifold is the asymptotic covariance of
regular efficient estimators. Sensitivity according to a general policy metric
on the model manifold can be obtained from influence functions of regular
efficient estimators. Policy sensitivity has a local counterfactual
interpretation, where the ceteris paribus change to a counterfactual
distribution is specified by the combination of a control parameter and a
Riemannian metric on the model manifold.
</summary>
    <author>
      <name>Yaroslav Mukhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 5 figures, includes appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08863v1</id>
    <updated>2018-05-22T20:54:44Z</updated>
    <published>2018-05-22T20:54:44Z</published>
    <title>Langevin Markov Chain Monte Carlo with stochastic gradients</title>
    <summary>  Monte Carlo sampling techniques have broad applications in machine learning,
Bayesian posterior inference, and parameter estimation. Typically the target
distribution takes the form of a product distribution over a dataset with a
large number of entries. For sampling schemes utilizing gradient information it
is cheaper for the derivative to be approximated using a random small subset of
the data, introducing extra noise into the system. We present a new
discretization scheme for underdamped Langevin dynamics when utilizing a
stochastic (noisy) gradient. This scheme is shown to bias computed averages to
second order in the stepsize while giving exact results in the special case of
sampling a Gaussian distribution with a normally distributed stochastic
gradient
</summary>
    <author>
      <name>Charles Matthews</name>
    </author>
    <author>
      <name>Jonathan Weare</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08765v1</id>
    <updated>2018-05-22T17:50:45Z</updated>
    <published>2018-05-22T17:50:45Z</published>
    <title>Multi-model inference through projections in model space</title>
    <summary>  Information criteria have had a profound impact on modern ecological science.
They allow researchers to estimate which probabilistic approximating models are
closest to the generating process. Unfortunately, information criterion
comparison does not tell how good the best model is. Nor do practitioners
routinely test the reliability (e.g. error rates) of information
criterion-based model selection. In this work, we show that these two
shortcomings can be resolved by extending a key observation from Hirotugu
Akaike's original work. Standard information criterion analysis considers only
the divergences of each model from the generating process. It is ignored that
there are also estimable divergence relationships amongst all of the
approximating models. We then show that using both sets of divergences, a model
space can be constructed that includes an estimated location for the generating
process. Thus, not only can an analyst determine which model is closest to the
generating process, she/he can also determine how close to the generating
process the best approximating model is. Properties of the generating process
estimated from these projections are more accurate than those estimated by
model averaging. The applications of our findings extend to all areas of
science where model selection through information criteria is done.
</summary>
    <author>
      <name>Jose-Miguel Ponciano</name>
    </author>
    <author>
      <name>Mark L Taper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 8 figures. Submitted to JRSSB</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08719v1</id>
    <updated>2018-05-22T16:26:42Z</updated>
    <published>2018-05-22T16:26:42Z</published>
    <title>Parsimonious Bayesian deep networks</title>
    <summary>  Combining Bayesian nonparametrics and a forward model selection strategy, we
construct parsimonious Bayesian deep networks (PBDNs) that infer
capacity-regularized network architectures from the data and require neither
cross-validation nor fine-tuning when training the model. One of the two
essential components of a PBDN is the development of a special infinite-wide
single-hidden-layer neural network, whose number of active hidden units can be
inferred from the data. The other one is the construction of a greedy
layer-wise learning algorithm that uses a forward model selection criterion to
determine when to stop adding another hidden layer. We develop both Gibbs
sampling and stochastic gradient descent based maximum a posteriori inference
for PBDNs, providing state-of-the-art classification accuracy and interpretable
data subtypes near the decision boundaries, while maintaining low computational
complexity for out-of-sample prediction.
</summary>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08670v1</id>
    <updated>2018-05-22T15:44:08Z</updated>
    <published>2018-05-22T15:44:08Z</published>
    <title>Regression Analysis of Proportion Outcomes with Random Effects</title>
    <summary>  A regression method for proportional, or fractional, data with mixed effects
is outlined, designed for analysis of datasets in which the outcomes have
substantial weight at the bounds. In such cases a normal approximation is
particularly unsuitable as it can result in incorrect inference. To resolve
this problem, we employ a logistic regression model and then apply a bootstrap
method to correct conservative confidence intervals. This paper outlines the
theory of the method, and demonstrates its utility using simulated data.
Working code for the R platform is provided through the package glmmboot,
available on CRAN.
</summary>
    <author>
      <name>Colman Humphrey</name>
    </author>
    <author>
      <name>Dan Swingley</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08518v1</id>
    <updated>2018-05-22T11:46:01Z</updated>
    <published>2018-05-22T11:46:01Z</published>
    <title>Functional Regression Models with Highly Irregular Designs</title>
    <summary>  In this work we present a new approach, which we call MISFIT, to fitting
functional data models with sparsely and irregularly sampled data. The
limitations of current methods have created major challenges in the fitting of
more complex nonlinear models. Indeed, currently many models cannot be
consistently estimated unless one assumes that the number of observed points
per curve grows sufficiently quickly with the sample size. In contrast, we
demonstrate that MISFIT, which is based on a multiple imputation framework, has
the potential to produce consistent estimates without such an assumption. Just
as importantly, it propagates the uncertainty of not having completely observed
curves, allowing for a more accurate assessment of the uncertainty of parameter
estimates, something that most methods currently cannot accomplish. This work
is motivated by a longitudinal study on macrocephaly, or atypically large head
size, in which electronic medical records allow for the collection of a great
deal of data. However, the sampling is highly variable from child to child.
Using the MISFIT approach we are able to clearly demonstrate that the
development of pathologic conditions related to macrocephaly is associated with
both the overall head circumference of the children as well as the velocity of
their head growth.
</summary>
    <author>
      <name>Justin Petrovich</name>
    </author>
    <author>
      <name>Matthew Reimherr</name>
    </author>
    <author>
      <name>Carrie Daymont</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 19 tables (including the appendix), 46 pages (including
  the appendix)</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08512v1</id>
    <updated>2018-05-22T11:40:32Z</updated>
    <published>2018-05-22T11:40:32Z</published>
    <title>Non-parametric Structural Change Detection in Multivariate Systems</title>
    <summary>  Structural change detection problems are often encountered in analytics and
econometrics, where the performance of a model can be significantly affected by
unforeseen changes in the underlying relationships. Although these problems
have a comparatively long history in statistics, the number of studies done in
the context of multivariate data under nonparametric settings is still small.
In this paper, we propose a consistent method for detecting multiple structural
changes in a system of related regressions over a large dimensional variable
space. In most applications, practitioners also do not have a priori
information on the relevance of different variables, and therefore, both
locations of structural changes as well as the corresponding sparse regression
coefficients need to be estimated simultaneously. The method combines
nonparametric energy distance minimization principle with penalized regression
techniques. After showing asymptotic consistency of the model, we compare the
proposed approach with competing methods in a simulation study. As an example
of a large scale application, we consider structural change point detection in
the context of news analytics during the recent financial crisis period.
</summary>
    <author>
      <name>Pekka Malo</name>
    </author>
    <author>
      <name>Lauri Viitasaari</name>
    </author>
    <author>
      <name>Olga Gorskikh</name>
    </author>
    <author>
      <name>Pauliina Ilmonen</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60-08, 62-xx, 62Gxx, 62Hxx, 65Cxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.2.6; I.6.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07109v2</id>
    <updated>2018-05-22T11:28:23Z</updated>
    <published>2018-05-18T09:25:54Z</published>
    <title>On the Bayesian Solution of Differential Equations</title>
    <summary>  The interpretation of numerical methods, such as finite difference methods
for differential equations, as point estimators allows for formal statistical
quantification of the error due to discretisation in the numerical context.
Competing statistical paradigms can be considered and Bayesian probabilistic
numerical methods (PNMs) are obtained when Bayesian statistical principles are
deployed. Bayesian PNM are closed under composition, such that uncertainty due
to different sources of discretisation can be jointly modelled and rigorously
propagated. However, we argue that no strictly Bayesian PNM for the numerical
solution of ordinary differential equations (ODEs) have yet been developed. To
address this gap, we work at a foundational level, where a novel Bayesian PNM
is proposed as a proof-of-concept. Our proposal is a synthesis of classical Lie
group methods, to exploit the underlying structure of the gradient field, and
non-parametric regression in a transformed solution space for the ODE. The
procedure is presented in detail for first order ODEs and relies on a certain
technical condition -- existence of a solvable Lie algebra -- being satisfied.
Numerical illustrations are provided.
</summary>
    <author>
      <name>Junyang Wang</name>
    </author>
    <author>
      <name>Jon Cockayne</name>
    </author>
    <author>
      <name>Chris Oates</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07109v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07109v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07616v2</id>
    <updated>2018-05-22T10:40:14Z</updated>
    <published>2017-09-22T07:25:36Z</published>
    <title>General Bayesian Updating and the Loss-Likelihood Bootstrap</title>
    <summary>  In this paper we revisit the weighted likelihood bootstrap, a method that
generates samples from an approximate Bayesian posterior of a parametric model.
We show that the same method can be derived, without approximation, under a
Bayesian nonparametric model with the parameter of interest defined as
minimising an expected negative log-likelihood under an unknown sampling
distribution. This interpretation enables us to extend the weighted likelihood
bootstrap to posterior sampling for parameters minimizing an expected loss. We
call this method the loss-likelihood bootstrap. We make a connection between
this and general Bayesian updating, which is a way of updating prior belief
distributions without needing to construct a global probability model, yet
requires the calibration of two forms of loss function. The loss-likelihood
bootstrap is used to calibrate the general Bayesian posterior by matching
asymptotic Fisher information. We demonstrate the methodology on a number of
examples.
</summary>
    <author>
      <name>Simon Lyddon</name>
    </author>
    <author>
      <name>Chris Holmes</name>
    </author>
    <author>
      <name>Stephen Walker</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07616v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07616v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08463v1</id>
    <updated>2018-05-22T09:08:01Z</updated>
    <published>2018-05-22T09:08:01Z</published>
    <title>Variational Learning on Aggregate Outputs with Gaussian Processes</title>
    <summary>  While a typical supervised learning framework assumes that the inputs and the
outputs are measured at the same levels of granularity, many applications,
including global mapping of disease, only have access to outputs at a much
coarser level than that of the inputs. Aggregation of outputs makes
generalization to new inputs much more difficult. We consider an approach to
this problem based on variational learning with a model of output aggregation
and Gaussian processes, where aggregation leads to intractability of the
standard evidence lower bounds. We propose new bounds and tractable
approximations, leading to improved prediction accuracy and scalability to
large datasets, while explicitly taking uncertainty into account. We develop a
framework which extends to several types of likelihoods, including the Poisson
model for aggregated count data. We apply our framework to a challenging and
important problem, the fine-scale spatial modelling of malaria incidence, with
over 1 million observations.
</summary>
    <author>
      <name>Ho Chung Leon Law</name>
    </author>
    <author>
      <name>Dino Sejdinovic</name>
    </author>
    <author>
      <name>Ewan Cameron</name>
    </author>
    <author>
      <name>Tim CD Lucas</name>
    </author>
    <author>
      <name>Seth Flaxman</name>
    </author>
    <author>
      <name>Katherine Battle</name>
    </author>
    <author>
      <name>Kenji Fukumizu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08423v1</id>
    <updated>2018-05-22T06:59:34Z</updated>
    <published>2018-05-22T06:59:34Z</published>
    <title>Fast and Accurate Binary Response Mixed Model Analysis via Expectation
  Propagation</title>
    <summary>  Expectation propagation is a general prescription for approximation of
integrals in statistical inference problems. Its literature is mainly concerned
with Bayesian inference scenarios. However, expectation propagation can also be
used to approximate integrals arising in frequentist statistical inference. We
focus on likelihood-based inference for binary response mixed models and show
that fast and accurate quadrature-free inference can be realized for the probit
link case with multivariate random effects and higher levels of nesting. The
approach is supported by asymptotic theory in which expectation propagation is
seen to provide consistent estimation of the exact likelihood surface.
Numerical studies reveal the availability of fast, highly accurate and scalable
methodology for binary mixed model analysis.
</summary>
    <author>
      <name>P. Hall</name>
    </author>
    <author>
      <name>I. M. Johnstone</name>
    </author>
    <author>
      <name>J. T. Ormerod</name>
    </author>
    <author>
      <name>M. P. Wand</name>
    </author>
    <author>
      <name>J. C. F. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08342v1</id>
    <updated>2018-05-22T01:19:30Z</updated>
    <published>2018-05-22T01:19:30Z</published>
    <title>Nearest neighbor density functional estimation based on inverse Laplace
  transform</title>
    <summary>  A general approach to $L_2$-consistent estimation of various density
functionals using $k$-nearest neighbor distances is proposed, along with the
analysis of convergence rates in mean squared error. The construction of the
estimator is based on inverse Laplace transforms related to the target density
functional, which arises naturally from the convergence of a normalized volume
of $k$-nearest neighbor ball to a Gamma distribution in the sample limit. Some
instantiations of the proposed estimator rediscover existing $k$-nearest
neighbor based estimators of Shannon and Renyi entropies and Kullback--Leibler
and Renyi divergences, and discover new consistent estimators for many other
functionals, such as Jensen--Shannon divergence and generalized entropies and
divergences. A unified finite-sample analysis of the proposed estimator is
presented that builds on a recent result by Gao, Oh, and Viswanath (2017) on
the finite sample behavior of the Kozachenko--Leoneko estimator of entropy.
</summary>
    <author>
      <name>Shouvik Ganguly</name>
    </author>
    <author>
      <name>Jongha Ryu</name>
    </author>
    <author>
      <name>Young-Han Kim</name>
    </author>
    <author>
      <name>Yung-Kyun Noh</name>
    </author>
    <author>
      <name>Daniel D. Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08304v1</id>
    <updated>2018-05-21T22:00:58Z</updated>
    <published>2018-05-21T22:00:58Z</published>
    <title>Anchored Bayesian Gaussian Mixture Models</title>
    <summary>  Finite Gaussian mixtures are a flexible modeling tool for irregularly shaped
densities and samples from heterogeneous populations. When modeling with
mixtures using an exchangeable prior on the component features, the component
labels are arbitrary and are indistinguishable in posterior analysis. This
makes it impossible to attribute any meaningful interpretation to the marginal
posterior distributions of the component features. We present an alternative to
the exchangeable prior: by assuming that a small number of latent class labels
are known a priori, we can make inference on the component features without
post-processing. Our method assigns meaning to the component labels at the
modeling stage and can be justified as a data-dependent informative prior on
the labelings. We show that our method produces interpretable results, often
(but not always) similar to those resulting from relabeling algorithms, with
the added benefit that the marginal inferences originate directly from a well
specified probability model rather than a post hoc manipulation. We provide
practical guidelines for model selection that are motivated by maximizing prior
information about the class labels and we demonstrate our method on real and
simulated data.
</summary>
    <author>
      <name>Deborah Kunkel</name>
    </author>
    <author>
      <name>Mario Peruggia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 6 figures, 3 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08300v1</id>
    <updated>2018-05-21T21:49:26Z</updated>
    <published>2018-05-21T21:49:26Z</published>
    <title>Lassoing Eigenvalues</title>
    <summary>  The properties of penalized sample covariance matrices depend on the choice
of the penalty function. In this paper, we introduce a class of non-smooth
penalty functions for the sample covariance matrix, and demonstrate how this
method results in a grouping of the estimated eigenvalues. We refer to this
method as "lassoing eigenvalues" or as the "elasso".
</summary>
    <author>
      <name>David E. Tyler</name>
    </author>
    <author>
      <name>Mengxi Yi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H12 (primary), 62H25 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08275v1</id>
    <updated>2018-05-21T20:05:36Z</updated>
    <published>2018-05-21T20:05:36Z</published>
    <title>Multiple Treatments with Strategic Interaction</title>
    <summary>  We develop an empirical framework in which we identify and estimate the
effects of treatments on outcomes of interest when the treatments are the
result of strategic interaction (e.g., bargaining, oligopolistic entry, peer
effects). We consider a model where agents play a discrete game with complete
information whose equilibrium actions (i.e., binary treatments) determine a
post-game outcome in a nonseparable model with endogeneity. Due to the
simultaneity in the first stage, the model as a whole is incomplete and the
selection process fails to exhibit the conventional monotonicity. Without
imposing parametric restrictions or large support assumptions, this poses
challenges in recovering treatment parameters. To address these challenges, we
first analytically characterize regions that predict equilibria in the
first-stage game with possibly more than two players, and ascertain a monotonic
pattern of these regions. Based on this finding, we derive bounds on the
average treatment effects (ATE's) under nonparametric shape restrictions and
the existence of excluded exogenous variables. We also introduce and point
identify a multi-treatment version of local average treatment effects (LATE's).
We apply our method to data on airlines and air pollution in cities in the U.S.
We find that (i) the causal effect of each airline on pollution is positive,
and (ii) the effect is increasing in the number of firms but at a decreasing
rate.
</summary>
    <author>
      <name>Jorge Balat</name>
    </author>
    <author>
      <name>Sukjin Han</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08233v1</id>
    <updated>2018-05-21T18:04:53Z</updated>
    <published>2018-05-21T18:04:53Z</published>
    <title>Multilevel Models Allow Modular Specification of What and Where to
  Regularize, Especially in Small Area Estimation</title>
    <summary>  Through the lense of multilevel model (MLM) specification and regularization,
this is a connect-the-dots introductory summary of Small Area Estimation, e.g.
small group prediction informed by a complex sampling design. While a
comprehensive book is (Rao and Molina 2015), the goal of this paper is to get
interested researchers up to speed with some current developments. We first
provide historical context of two kinds of regularization: 1) the
regularization 'within' the components of a predictor and 2) the regularization
'between' outcome and predictor. We focus on the MLM framework as it allows the
analyst to flexibly control the targets of the regularization. The flexible
control is useful when analysts want to overcome shortcomings in design-based
estimates. We'll describe the precision deficiencies (high variance) typical of
design-based estimates of small groups. We then highlight an interesting MLM
example from (Chaudhuri and Ghosh 2011) that integrates both kinds of
regularization (between and within). The key idea is to use the design-based
variance to control the amount of 'between' regularization and prior
information to regularize the components 'within' a predictor. The goal is to
let the design-based estimate have authority (when precise) but defer to a
model-based prediction when imprecise. We conclude by discussing optional
criteria to incorporate into a MLM prediction and possible entrypoints for
extensions.
</summary>
    <author>
      <name>Michael Tzen</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07848v3</id>
    <updated>2018-05-21T17:32:01Z</updated>
    <published>2015-12-24T16:21:39Z</published>
    <title>Model-free inference on extreme dependence via waiting times</title>
    <summary>  A variety of methods have been proposed for inference about extreme
dependence for multivariate or spatially-indexed stochastic processes and time
series. Most of these proceed by first transforming data to some specific
extreme value marginal distribution, often the unit Fr\'echet, then fitting a
family of max-stable processes to the transformed data and exploring dependence
within the framework of that model. The marginal transformation, model
selection, and model fitting are all possible sources of misspecification in
this approach.
  We propose an alternative model-free approach, based on the idea that
substantial information on the strength of tail dependence and its temporal
structure are encoded in the distribution of the waiting times between
exceedances of high thresholds at different locations. We propose quantifying
the strength of extremal dependence and assessing uncertainty by using
statistics based on these waiting times. The method does not rely on any
specific underlying model for the process, nor on asymptotic distribution
theory. The method is illustrated by applications to climatological, financial,
and electrophysiology data.
  To put the proposed approach within the context of the existing literature,
we construct a class of spacetime-indexed stochastic processes whose waiting
time distributions are available in closed form by endowing the support points
in de Haan's spectral representation of max-stable processes with random birth
times, velocities, and lifetimes, and applying Smith's model to these
processes. We show that waiting times in this model are stochatically
decreasing in mean speed, and the sample mean of the waiting times obeys a
central limit theorem with a uniform convergence rate under mild conditions.
This indicates that our procedure can be implemented in this setting using
standard $t$ statistics and associated hypothesis tests.
</summary>
    <author>
      <name>James E. Johndrow</name>
    </author>
    <author>
      <name>Robert L. Wolpert</name>
    </author>
    <link href="http://arxiv.org/abs/1512.07848v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07848v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0100v3</id>
    <updated>2018-05-21T14:03:32Z</updated>
    <published>2013-12-31T06:01:53Z</published>
    <title>Improving forecasting performance using covariate-dependent copula
  models</title>
    <summary>  Copulas provide an attractive approach for constructing multivariate
distributions with flexible marginal distributions and different forms of
dependences. Of particular importance in many areas is the possibility of
explicitly forecasting the tail-dependences. Most of the available approaches
are only able to estimate tail-dependences and correlations via nuisance
parameters, but can neither be used for interpretation, nor for forecasting.
Aiming to improve copula forecasting performance, we propose a general Bayesian
approach for modeling and forecasting tail-dependences and correlations as
explicit functions of covariates. The proposed covariate-dependent copula model
also allows for Bayesian variable selection among covariates from the marginal
models as well as the copula density. The copulas we study include Joe-Clayton
copula, Clayton copula, Gumbel copula and Student's \emph{t}-copula. Posterior
inference is carried out using an efficient MCMC simulation method. Our
approach is applied to both simulated data and the S\&amp;P 100 and S\&amp;P 600 stock
indices. The forecasting performance of the proposed approach is compared with
other modeling strategies based on log predictive scores. Value-at-Risk
evaluation is also preformed for model comparisons.
</summary>
    <author>
      <name>Feng Li</name>
    </author>
    <author>
      <name>Yanfei Kang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ijforecast.2018.01.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ijforecast.2018.01.007" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Forecasting (2018), 34(3), pp. 456-476</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.0100v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0100v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07996v1</id>
    <updated>2018-05-21T11:38:23Z</updated>
    <published>2018-05-21T11:38:23Z</published>
    <title>Comparing Two Partitions of Non-Equal Sets of Units</title>
    <summary>  Rand (1971) proposed what has since become a well-known index for comparing
two partitions obtained on the same set of units. The index takes a value on
the interval between 0 and 1, where a higher value indicates more similar
partitions. Sometimes, e.g. when the units are observed in two time periods,
the splitting and merging of clusters should be considered differently,
according to the operationalization of the stability of clusters. The Rand
Index is symmetric in the sense that both the splitting and merging of clusters
lower the value of the index. In such a non-symmetric case, one of the Wallace
indexes (Wallace, 1983) can be used. Further, there are several cases when one
wants to compare two partitions obtained on different sets of units, where the
intersection of these sets of units is a non-empty set of units. In this
instance, the new units and units which leave the clusters from the first
partition can be considered as a factor lowering the value of the index.
Therefore, a modified Rand index is presented. Because the splitting and
merging of clusters have to be considered differently in some situations, an
asymmetric modified Wallace Index is also proposed. For all presented indices,
the correction for chance is described, which allows different values of a
selected index to be compared.
</summary>
    <author>
      <name>Marjan Cugmas</name>
    </author>
    <author>
      <name>Anuška Ferligoj</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Metodolo\v{s}ki zvezki - Advances in Methodology and Statistics,
  Vol. 15, No. 1, 2018, 1-21. Avaiable at
  http://ibmi.mf.uni-lj.si/mz/2018/no-1/Cugmas2018.pdf</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.07996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07970v1</id>
    <updated>2018-05-21T10:17:31Z</updated>
    <published>2018-05-21T10:17:31Z</published>
    <title>Implicit Probabilistic Integrators for ODEs</title>
    <summary>  We introduce a family of implicit probabilistic integrators for initial value
problems (IVPs) taking as a starting point the multistep Adams-Moulton method.
The implicit construction allows for dynamic feedback from the forthcoming
time-step, by contrast with previous probabilistic integrators, all of which
are based on explicit methods. We begin with a concise survey of the
rapidly-expanding field of probabilistic ODE solvers. We then introduce our
method, which builds on and adapts the work of Conrad et al. (2016) and Teymur
et al. (2016), and provide a rigorous proof of its well-definedness and
convergence. We discuss the problem of the calibration of such integrators and
suggest one approach. We give an illustrative example highlighting the effect
of the use of probabilistic integrators - including our new method - in the
setting of parameter inference within an inverse problem.
</summary>
    <author>
      <name>Onur Teymur</name>
    </author>
    <author>
      <name>Han Cheng Lie</name>
    </author>
    <author>
      <name>Tim Sullivan</name>
    </author>
    <author>
      <name>Ben Calderhead</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02257v2</id>
    <updated>2018-05-20T18:31:59Z</updated>
    <published>2018-05-06T18:16:21Z</published>
    <title>Bayesian Regularization for Graphical Models with Unequal Shrinkage</title>
    <summary>  We consider a Bayesian framework for estimating a high-dimensional sparse
precision matrix, in which adaptive shrinkage and sparsity are induced by a
mixture of Laplace priors. Besides discussing our formulation from the Bayesian
standpoint, we investigate the MAP (maximum a posteriori) estimator from a
penalized likelihood perspective that gives rise to a new non-convex penalty
approximating the $\ell_0$ penalty. Optimal error rates for estimation
consistency in terms of various matrix norms along with selection consistency
for sparse structure recovery are shown for the unique MAP estimator under mild
conditions. For fast and efficient computation, an EM algorithm is proposed to
compute the MAP estimator of the precision matrix and (approximate) posterior
probabilities on the edges of the underlying sparse structure. Through
extensive simulation studies and a real application to a call center data, we
have demonstrated the fine performance of our method compared with existing
alternatives.
</summary>
    <author>
      <name>Lingrui Gan</name>
    </author>
    <author>
      <name>Naveen N. Narisetty</name>
    </author>
    <author>
      <name>Feng Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Journal of the American Statistical Association (Theory
  &amp; Methods)</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02257v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02257v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07800v1</id>
    <updated>2018-05-20T17:18:15Z</updated>
    <published>2018-05-20T17:18:15Z</published>
    <title>Large Sample Theory for Merged Data from Multiple Sources</title>
    <summary>  We develop large sample theory for merged data from multiple sources. Main
statistical issues treated in this paper are (1) the same unit potentially
appears in multiple datasets from overlapping data sources, (2) duplicated
items are not identified, and (3) a sample from the same data source is
dependent due to sampling without replacement. We propose and study a new
weighted empirical process and extend empirical process theory to a dependent
and biased sample with duplication. Specifically, we establish the uniform law
of large numbers and uniform central limit theorem over a class of functions
along with several empirical process results under conditions identical to
those in the i.i.d. setting. As applications, we study infinite-dimensional
M-estimation and develop its consistency, rates of convergence, and asymptotic
normality. Our theoretical results are illustrated with simulation studies and
a real data example.
</summary>
    <author>
      <name>Takumi Saegusa</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E20, 62G20, 62D99, 62N01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07714v1</id>
    <updated>2018-05-20T06:21:11Z</updated>
    <published>2018-05-20T06:21:11Z</published>
    <title>On a General Class of Discrete Bivariate Distributions</title>
    <summary>  In this paper we develop a very general class of bivariate discrete
distributions. The basic idea is very simple. The marginals are obtained by
taking the random geometric sum of a baseline distribution function. The
proposed class of distributions is a very flexible class of distributions in
the sense the marginals can take variety of shapes. It can be multimodal as
well as heavy tailed also. It can be both over dispersed as well as under
dispersed. Moreover, the correlation can be of a wide range. We discuss
different properties of the proposes class of bivariate distributions. The
proposed distribution has some interesting physical interpretations also.
Further, we consider two specific base line distributions namely; Poisson and
negative binomial distributions for illustrative purposes. Both of them are
infinitely divisible. The maximum likelihood estimators of the unknown
parameters cannot be obtained in closed form. They can be obtained by solving
three and five dimensional non-linear optimizations problems, respectively. To
avoid that we propose to use the method of moment estimators and they can be
obtained quite conveniently. The analyses of two real data sets have been
performed to show the effectiveness of the proposed class of models. Finally,
we discuss some open problems and conclude the paper.
</summary>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07684v1</id>
    <updated>2018-05-20T00:56:05Z</updated>
    <published>2018-05-20T00:56:05Z</published>
    <title>Stacked Propensity Score Functions for Observational Cohorts with
  Oversampled Exposed Subjects</title>
    <summary>  Observational cohort studies with oversampled exposed subjects are typically
implemented to understand the causal effect of a rare exposure. Because the
distribution of exposed subjects in the sample differs from the source
population, estimation of a propensity score function (i.e., probability of
exposure given baseline covariates) targets a nonparametrically nonidentifiable
parameter. Consistent estimation of propensity score functions is an important
component of various causal inference estimators, including double robust
machine learning and inverse probability weighted estimators. We propose the
use of the probability of exposure from the source population in
observation-weighted stacking algorithms to produce consistent estimators of
propensity score functions. Simulation studies and a hypothetical health policy
intervention data analysis demonstrate low empirical bias and variance for
these stacked propensity score functions with observation weights.
</summary>
    <author>
      <name>Sherri Rose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07622v1</id>
    <updated>2018-05-19T17:02:33Z</updated>
    <published>2018-05-19T17:02:33Z</published>
    <title>Bayesian Bootstrap Inference for the ROC Surface</title>
    <summary>  Accurate diagnosis of disease is of great importance in clinical practice and
medical research. The receiver operating characteristic (ROC) surface is a
popular tool for evaluating the discriminatory ability of continuous diagnostic
test outcomes when there exist three ordered disease classes (e.g., no disease,
mild disease, advanced disease). We propose the Bayesian bootstrap, a fully
nonparametric method, for conducting inference about the ROC surface and its
functionals, such as the volume under the surface. The proposed method is based
on a simple, yet interesting, representation of the ROC surface in terms of
placement variables. Results from a simulation study demonstrate the ability of
our method to successfully recover the true ROC surface and to produce valid
inferences in a variety of complex scenarios. An application to data from the
Trail Making Test to assess cognitive impairment in Parkinson's disease
patients is provided.
</summary>
    <author>
      <name>Vanda Inacio de Carvalho</name>
    </author>
    <author>
      <name>Miguel de Carvalho</name>
    </author>
    <author>
      <name>Adam Branscum</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07580v1</id>
    <updated>2018-05-19T12:17:51Z</updated>
    <published>2018-05-19T12:17:51Z</published>
    <title>Analytic moment and Laplace transform formulae for the quasi-stationary
  distribution of the Shiryaev diffusion on an interval</title>
    <summary>  We derive analytic closed-form moment and Laplace transform formulae for the
quasi-stationary distribution of the classical Shiryaev diffusion restricted to
the interval $[0,A]$ with absorption at a given $A&gt;0$.
</summary>
    <author>
      <name>Aleksey S. Polunchenko</name>
    </author>
    <author>
      <name>Andrey Pepelyshev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J60, 60J25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07575v1</id>
    <updated>2018-05-19T11:57:54Z</updated>
    <published>2018-05-19T11:57:54Z</published>
    <title>Sequential adaptive elastic net approach for single-snapshot source
  localization</title>
    <summary>  This paper proposes efficient algorithms for accurate recovery of
direction-of-arrival (DoA) of sources from single-snapshot measurements using
compressed beamforming (CBF). In CBF, the conventional sensor array signal
model is cast as an underdetermined complex-valued linear regression model and
sparse signal recovery methods are used for solving the DoA finding problem. We
develop a complex-valued pathwise weighted elastic net (c-PW-WEN) algorithm
that finds solutions at knots of penalty parameter values over a path (or grid)
of EN tuning parameter values. c-PW-WEN also computes Lasso or weighted Lasso
in its path. We then propose a sequential adaptive EN (SAEN) method that is
based on c-PW-WEN algorithm with adaptive weights that depend on the previous
solution. Extensive simulation studies illustrate that SAEN improves the
probability of exact recovery of true support compared to conventional sparse
signal recovery approaches such as Lasso, elastic net or orthogonal matching
pursuit in several challenging multiple target scenarios. The effectiveness of
SAEN is more pronounced in the presence of high mutual coherence.
</summary>
    <author>
      <name>Muhammad Naveed Tabassum</name>
    </author>
    <author>
      <name>Esa Ollila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, in the publication to the Journal of the
  Acoustical Society of America</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01675v5</id>
    <updated>2018-05-18T23:36:28Z</updated>
    <published>2016-11-05T17:04:56Z</published>
    <title>A simple method for implementing Monte Carlo tests</title>
    <summary>  We consider a statistical test whose p-value can only be approximated using
Monte Carlo simulations. We are interested in deciding whether the p-value for
an observed data set lies above or below a given threshold such as 5%. We want
to ensure that the resampling risk, the probability of the (Monte Carlo)
decision being different from the true decision, is uniformly bounded. This
article introduces a simple open-ended method with this property, the
confidence sequence method (CSM). We compare our approach to another algorithm,
SIMCTEST, which also guarantees an (asymptotic) uniform bound on the resampling
risk, as well as to other Monte Carlo procedures without a uniform bound. CSM
is free of tuning parameters and conservative. It has the same theoretical
guarantee as SIMCTEST and, in many settings, similar stopping boundaries. As it
is much simpler than other methods, CSM is a useful method for practical
applications.
</summary>
    <author>
      <name>Dong Ding</name>
    </author>
    <author>
      <name>Axel Gandy</name>
    </author>
    <author>
      <name>Georg Hahn</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01675v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01675v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07413v1</id>
    <updated>2018-05-18T19:43:01Z</updated>
    <published>2018-05-18T19:43:01Z</published>
    <title>Assessing Health Care Interventions via an Interrupted Time Series
  Model: Study Power and Design Considerations</title>
    <summary>  The delivery and assessment of quality health care is complex with many
interacting and interdependent components. In terms of research design and
statistical analysis, this complexity and interdependency makes it difficult to
assess the true impact of interventions designed to improve patient health care
outcomes. Interrupted time series (ITS) is a quasi-experimental design
developed for inferring the effectiveness of a health policy intervention while
accounting for temporal dependence within a single system or unit. Current
standardized ITS methods do not simultaneously analyze data for several units,
nor are there methods to test for the existence of a change point and to assess
statistical power for study planning purposes in this context. To address this
limitation we propose the `Robust Multiple ITS' (R-MITS) model, appropriate for
multi-unit ITS data, that allows for inference regarding the estimation of a
global change point across units in the presence of a potentially lagged (or
anticipatory) treatment effect. Under the R-MITS model, one can formally test
for the existence of a change point and estimate the time delay between the
formal intervention implementation and the over-all-unit intervention effect.
We conducted empirical simulation studies to assess the type one error rate of
the testing procedure, power for detecting specified change-point alternatives,
and accuracy of the proposed estimating methodology. R-MITS is illustrated by
analyzing patient satisfaction data from a hospital that implemented and
evaluated a new care delivery model in multiple units.
</summary>
    <author>
      <name>Maricela Cruz</name>
    </author>
    <author>
      <name>Dan L. Gillen</name>
    </author>
    <author>
      <name>Miriam Bender</name>
    </author>
    <author>
      <name>Hernando Ombao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 6 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07301v1</id>
    <updated>2018-05-18T15:46:56Z</updated>
    <published>2018-05-18T15:46:56Z</published>
    <title>Predictive Modeling of Multivariate Longitudinal Insurance Claims Using
  Pair Copula Construction</title>
    <summary>  The bundling feature of a nonlife insurance contract often leads to multiple
longitudinal measurements of an insurance risk. Assessing the association among
the evolution of the multivariate outcomes is critical to the operation of
property-casualty insurers. One complication in the modeling process is the
non-continuousness of insurance risks.
  Motivated by insurance applications, we propose a general framework for
modeling multivariate repeated measurements. The framework easily accommodates
different types of data, including continuous, discrete, as well as mixed
outcomes. Specifically, the longitudinal observations of each response is
separately modeled using pair copula constructions with a D-vine structure. The
multiple D-vines are then joined by a multivariate copula. A sequential
approach is employed for inference and its performance is investigated under a
simulated setting.
  In the empirical analysis, we examine property risks in a government
multi-peril property insurance program. The proposed method is applied to both
policyholders' claim count and loss cost. The model is validated based on
out-of-sample predictions.
</summary>
    <author>
      <name>Peng Shi</name>
    </author>
    <author>
      <name>Zifeng Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05495v2</id>
    <updated>2018-05-18T15:24:55Z</updated>
    <published>2018-02-15T11:57:08Z</published>
    <title>How Much Data Do You Need? An Operational Metric for Fat-tailedness</title>
    <summary>  This note presents an operational measure of fat-tailedness for univariate
probability distributions, in $[0,1]$ where 0 is maximally thin-tailed
(Gaussian) and 1 is maximally fat-tailed. Among others,1) it helps assess the
sample size needed to establish a comparative $n$ needed for statistical
significance, 2) allows practical comparisons across classes of fat-tailed
distributions, 3) helps understand some inconsistent attributes of the
lognormal, pending on the parametrization of its scale parameter. The
literature is rich for what concerns asymptotic behavior, but there is a large
void for finite values of $n$, those needed for operational purposes.
Conventional measures of fat-tailedness, namely 1) the tail index for the power
law class, and 2) Kurtosis for finite moment distributions fail to apply to
some distributions, and do not allow comparisons across classes and
parametrization, that is between power laws outside the Levy-Stable basin, or
power laws to distributions in other classes, or power laws for different
number of summands. How can one compare a sum of 100 Student T distributed
random variables with 3 degrees of freedom to one in a Levy-Stable or a
Lognormal class? How can one compare a sum of 100 Student T with 3 degrees of
freedom to a single Student T with 2 degrees of freedom? We propose an
operational and heuristic measure that allow us to compare $n$-summed
independent variables under all distributions with finite first moment. The
method is based on the rate of convergence of the Law of Large numbers for
finite sums, $n$-summands specifically. We get either explicit expressions or
simulation results and bounds for the lognormal, exponential, Pareto, and the
Student T distributions in their various calibrations --in addition to the
general Pearson classes.
</summary>
    <author>
      <name>Nassim Nicholas Taleb</name>
    </author>
    <link href="http://arxiv.org/abs/1802.05495v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05495v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07272v1</id>
    <updated>2018-05-18T15:13:21Z</updated>
    <published>2018-05-18T15:13:21Z</published>
    <title>Fast Multivariate Log-Concave Density Estimation</title>
    <summary>  We present a computational approach to log-concave density estimation. The
state-of- the-art approach of Cule et al. (2010) utilizes the piecewise-affine
parametrization of the density induced by the given sample set. The number of
parameters as well as non-smooth subgradient-based convex optimization for
determining the maximum likelihood density estimate cause long runtimes for
dimensions $d \geq 2$ and large sample sets.
  Our approach is based on mildly non-convex smooth approximations of the
objective function and sparse, adaptive piecewise-affine density
parametrization. Established memory-efficient numerical optimization techniques
enable to process larger data sets for dimensions $d \geq 2$. While there is no
guarantee that the algorithm returns the maximum likelihood estimate for every
problem instance, we provide comprehensive numerical evidence that it does,
after significantly shorter runtimes. For example, a sample set of size $n =
10000$ in $\mathbb{R}^2$ is processed in less then a second, rather than in
$\approx 5$ hours required by the approach of (Cule et al., 2010) to terminate.
For higher dimensions, density estimation becomes tractable as well: Processing
a sample set of size $n = 10000$ in $\mathbb{R}^6$ requires 35 minutes. The
software is publicly available as CRAN R package fmlogcondens.
</summary>
    <author>
      <name>Fabian Rathke</name>
    </author>
    <author>
      <name>Christoph Schnörr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07267v1</id>
    <updated>2018-05-18T15:04:08Z</updated>
    <published>2018-05-18T15:04:08Z</published>
    <title>Model reparametrization for improving variational inference</title>
    <summary>  In this article, we propose a strategy to improve variational Bayes inference
for a class of models whose variables can be classified as global (common
across all observations) or local (observation specific) by using a model
reparametrization. In particular, an invertible affine transformation is
applied on the local variables so that their posterior dependency on the global
variables is minimized. The functional form of this transformation is deduced
by approximating the conditional posterior distribution of each local variable
given the global variables by a Gaussian distribution via a second order Taylor
expansion. Variational inference for the reparametrized model is then obtained
using stochastic approximation techniques. Our approach can be readily extended
to large datasets via a divide and recombine strategy. Application of the
methods is illustrated using generalized linear mixed models.
</summary>
    <author>
      <name>Linda S. L. Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03213v4</id>
    <updated>2018-05-18T14:56:42Z</updated>
    <published>2017-03-09T10:19:53Z</published>
    <title>Bootstrapping kernel intensity estimation for nonhomogeneous point
  processes depending on spatial covariates</title>
    <summary>  In the spatial point process context, kernel intensity estimation has been
mainly restricted to exploratory analysis due to its lack of consistency.
Different methods have been analysed to overcome this problem, and the
inclusion of covariates resulted to be one possible solution. In this paper we
focus on de\-fi\-ning a theoretical framework to derive a consistent kernel
intensity estimator using covariates, as well as a consistent smooth bootstrap
procedure. We define two new data-driven bandwidth selectors specifically
designed for our estimator: a rule-of-thumb and a plug-in bandwidth based on
our consistent bootstrap method. A simulation study is accomplished to
understand the performance of our proposals in finite samples. Finally, we
describe an application to a real data set consisting of the wildfires in
Canada during June 2015, using meteorological information as covariates.
</summary>
    <author>
      <name>M. I. Borrajo</name>
    </author>
    <author>
      <name>W. González-Manteiga</name>
    </author>
    <author>
      <name>M. D. Martínez-Miranda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 7 figures (15 images), 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03213v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03213v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G05, 62G09, 62H11, 60G55, 60-08" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03016v3</id>
    <updated>2018-05-18T14:04:25Z</updated>
    <published>2018-04-09T14:21:08Z</published>
    <title>A Bayes-Sard Cubature Method</title>
    <summary>  This paper focusses on the formulation of numerical integration as an
inferential task. To date, research effort has largely focussed on the
development of Bayesian cubature, whose distributional output provides
uncertainty quantification for the integral. However, the point estimators
associated to Bayesian cubature can be inaccurate and acutely sensitive to the
prior when the domain is high-dimensional. To address these drawbacks we
introduce Bayes-Sard cubature, a probabilistic framework that combines the
flexibility of Bayesian cubature with the robustness of classical cubatures
which are well-established. This is achieved by considering a Gaussian process
model for the integrand whose mean is a parametric regression model, with an
improper flat prior on each regression coefficient. The features in the
regression model consist of test functions which are guaranteed to be exactly
integrated, with remaining degrees of freedom afforded to the non-parametric
part. The asymptotic convergence of the Bayes-Sard cubature method is
established and the theoretical results are numerically verified. In
particular, we report two orders of magnitude reduction in error compared to
Bayesian cubature in the context of a high-dimensional financial integral.
</summary>
    <author>
      <name>Toni Karvonen</name>
    </author>
    <author>
      <name>Chris J. Oates</name>
    </author>
    <author>
      <name>Simo Särkkä</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03016v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03016v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07147v1</id>
    <updated>2018-05-18T11:27:33Z</updated>
    <published>2018-05-18T11:27:33Z</published>
    <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome
  Data in Trial-Based Health Economic Evaluations</title>
    <summary>  Trial-based economic evaluations are typically performed on cross-sectional
variables, derived from the responses for only the completers in the study,
using methods that ignore the complexities of utility and cost data (e.g.
skewness and spikes). We present an alternative and more efficient Bayesian
parametric approach to handle missing longitudinal outcomes in economic
evaluations, while accounting for the complexities of the data. We specify a
flexible parametric model for the observed data and partially identify the
distribution of the missing data with partial identifying restrictions and
sensitivity parameters. We explore alternative nonignorable scenarios through
different priors for the sensitivity parameters, calibrated on the observed
data. Our approach is motivated by, and applied to, data from a trial assessing
the cost-effectiveness of a new treatment for intellectual disability and
challenging behaviour.
</summary>
    <author>
      <name>Andrea Gabrio</name>
    </author>
    <author>
      <name>Michael J. Daniels</name>
    </author>
    <author>
      <name>Gianluca Baio</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07092v1</id>
    <updated>2018-05-18T08:32:31Z</updated>
    <published>2018-05-18T08:32:31Z</published>
    <title>Bayesian model reduction</title>
    <summary>  This paper reviews recent developments in statistical structure learning;
namely, Bayesian model reduction. Bayesian model reduction is a special but
ubiquitous case of Bayesian model comparison that, in the setting of
variational Bayes, furnishes an analytic solution for (a lower bound on) model
evidence induced by a change in priors. This analytic solution finesses the
problem of scoring large model spaces in model comparison or structure
learning. This is because each new model can be cast in terms of an alternative
set of priors over model parameters. Furthermore, the reduced free energy (i.e.
evidence bound on the reduced model) finds an expedient application in
hierarchical models, where it plays the role of a summary statistic. In other
words, it contains all the necessary information contained in the posterior
distributions over parameters of lower levels. In this technical note, we
review Bayesian model reduction - in terms of common forms of reduced free
energy - and illustrate recent applications in structure learning, hierarchical
or empirical Bayes and as a metaphor for neurobiological processes like
abductive reasoning and sleep.
</summary>
    <author>
      <name>Karl Friston</name>
    </author>
    <author>
      <name>Thomas Parr</name>
    </author>
    <author>
      <name>Peter Zeidman</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07088v1</id>
    <updated>2018-05-18T08:15:54Z</updated>
    <published>2018-05-18T08:15:54Z</published>
    <title>Strongly Consistent of Kullback-Leibler Divergence Estimator and Tests
  for Model Selection Based on a Bias Reduced Kernel Density Estimator</title>
    <summary>  In this paper, we study the strong consistency of a bias reduced kernel
density estimator and derive a strongly con- sistent Kullback-Leibler
divergence (KLD) estimator. As application, we formulate a goodness-of-fit test
and an asymptotically standard normal test for model selection. The Monte Carlo
simulation show the effectiveness of the proposed estimation methods and
statistical tests.
</summary>
    <author>
      <name>Papa Ngom</name>
    </author>
    <author>
      <name>Freedath Djibril Moussa</name>
    </author>
    <author>
      <name>Jean de Dieu Nkurunziza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F10, 62F12, 62G07, 62G10, 62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07086v1</id>
    <updated>2018-05-18T08:14:26Z</updated>
    <published>2018-05-18T08:14:26Z</published>
    <title>Relationship between the Bregman divergence and beta-divergence and
  their Applications</title>
    <summary>  The Bregman divergence have been the subject of several studies. We do not go
to do an exhaustive study of its subclasses, but propose a proof that shows
that the \b{eta}-divergence are subclasses of the Bregman divergences. It is in
this order of idea that we will make a proposition of demonstration which shows
that the \b{eta}-divergence are particular cases of the Bregman divergence. And
also we will propose algorithms and their applications to show the consistency
of our approach. This is of interest for numerous applications since these
divergences are widely used for instant non-negative matrix factorization
(NMF).
</summary>
    <author>
      <name>Macoumba Ndourand Mactar Ndaw</name>
    </author>
    <author>
      <name>Papa Ngom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06915v1</id>
    <updated>2018-05-17T18:21:58Z</updated>
    <published>2018-05-17T18:21:58Z</published>
    <title>A Note on Coding and Standardization of Categorical Variables in
  (Sparse) Group Lasso Regression</title>
    <summary>  Categorical regressor variables are usually handled by introducing a set of
indicator variables, and imposing a linear constraint to ensure identifiability
in the presence of an intercept, or equivalently, using one of various coding
schemes. As proposed in Yuan and Lin [J. R. Statist. Soc. B, 68 (2006), 49-67],
the group lasso is a natural and computationally convenient approach to perform
variable selection in settings with categorical covariates. As pointed out by
Simon and Tibshirani [Stat. Sin., 22 (2011), 983-1001], "standardization" by
means of block-wise orthonormalization of column submatrices each corresponding
to one group of variables can substantially boost performance. In this note, we
study the aspect of standardization for the special case of categorical
predictors in detail. The main result is that orthonormalization is not
required; column-wise scaling of the design matrix followed by re-scaling and
centering of the coefficients is shown to have exactly the same effect. Similar
reductions can be achieved in the case of interactions. The extension to the
so-called sparse group lasso, which additionally promotes within-group
sparsity, is considered as well. The importance of proper standardization is
illustrated via extensive simulations.
</summary>
    <author>
      <name>Felicitas J. Detmer</name>
    </author>
    <author>
      <name>Martin Slawski</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06855v1</id>
    <updated>2018-05-17T16:58:11Z</updated>
    <published>2018-05-17T16:58:11Z</published>
    <title>Mixed integer linear programming: a new approach for instrumental
  variable quantile regressions and related problems</title>
    <summary>  This paper proposes a new framework for estimating instrumental variable (IV)
quantile models. Our proposal can be cast as a mixed integer linear program
(MILP), which allows us to capitalize on recent progress in mixed integer
optimization. The computational advantage of the proposed method makes it an
attractive alternative to existing estimators in the presence of multiple
endogenous regressors. This is a situation that arises naturally when one
endogenous variable is interacted with several other variables in a regression
equation. In our simulations, the proposed method using MILP with a random
starting point can reliably estimate regressions for a sample size of 1000 with
20 endogenous variables in 90 seconds; for high-dimensional problems, our
formulation can deliver decent estimates within minutes for problems with 550
endogenous regressors. We also establish asymptotic theory and provide an
inference procedure. In our simulations, the asymptotic theory provides an
excellent approximation even if we terminate MILP before a certified global
solution is found. This suggests that MILP in our setting can quickly approach
the global solution. In addition, we show that MILP can also be used for
related problems, including censored regression, censored IV quantile
regression and high-dimensional IV quantile regression. As an empirical
illustration, we examine the heterogeneous treatment effect of Job Training
Partnership Act (JTPA) using a regression with 13 interaction terms of the
treatment variable.
</summary>
    <author>
      <name>Yinchu Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03901v2</id>
    <updated>2018-05-17T14:32:17Z</updated>
    <published>2018-01-11T17:56:06Z</published>
    <title>Maximum Likelihood for Gaussian Process Classification and Generalized
  Linear Mixed Models under Case-Control Sampling</title>
    <summary>  Modern data sets in various domains often include units that were sampled
non-randomly from the population and have a latent correlation structure. Here
we investigate a common form of this setting, where every unit is associated
with a latent variable, all latent variables are correlated, and the
probability of sampling a unit depends on its response. Such settings often
arise in case-control studies, where the sampled units are correlated due to
spatial proximity, family relations, or other sources of relatedness. Maximum
likelihood estimation in such settings is challenging from both a computational
and statistical perspective, necessitating approximations that take the
sampling scheme into account. We propose a family of approximate likelihood
approaches which combine composite likelihood and expectation propagation. We
demonstrate the efficacy of our solutions via extensive simulations. We utilize
them to investigate the genetic architecture of several complex disorders
collected in case-control genetic association studies, where hundreds of
thousands of genetic variants are measured for every individual, and the
underlying disease liabilities of individuals are correlated due to genetic
similarity. Our work is the first to provide a tractable likelihood-based
solution for case-control data with complex dependency structures.
</summary>
    <author>
      <name>Omer Weissbrod</name>
    </author>
    <author>
      <name>Shachar Kaufman</name>
    </author>
    <author>
      <name>David Golan</name>
    </author>
    <author>
      <name>Saharon Rosset</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03901v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03901v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04899v2</id>
    <updated>2018-05-17T14:25:52Z</updated>
    <published>2018-05-13T15:25:02Z</published>
    <title>A Bayesian semiparametric framework for causal inference in
  high-dimensional data</title>
    <summary>  We introduce a Bayesian framework for estimating causal effects of binary and
continuous treatments in high-dimensional data. The proposed framework extends
to high-dimensional settings many of the existing semiparametric estimators
introduced in the causal inference literature. Our approach has the following
features: it 1) considers semiparametric estimators that reduce model
dependence; 2) introduces flexible Bayesian priors for dimension reduction of
the covariate space that accommodates non linearity; 3) provides posterior
distributions of any causal estimator that can broadly be defined as a function
of the treatment and outcome model (e.g. standard doubly robust estimator or
the inverse probability weighted estimator); 4) provides posterior credible
intervals with improved finite sample coverage compared to frequentist measures
of uncertainty which rely on asymptotic properties. We show that the posterior
contraction rate of the proposed doubly robust estimator is the product of the
posterior contraction rates of the treatment and outcome models, allowing for
faster posterior contraction. Via simulation we illustrate the ability of the
proposed estimators to flexibly estimate causal effects in high-dimensions, and
show that it performs well relative to existing approaches. Finally, we apply
our proposed procedure to estimate the effect of continuous environmental
exposures.
</summary>
    <author>
      <name>Joseph Antonelli</name>
    </author>
    <author>
      <name>Francesca Dominici</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04899v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04899v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01862v3</id>
    <updated>2018-05-17T12:54:44Z</updated>
    <published>2018-05-04T17:12:22Z</published>
    <title>Lasso, knockoff and Gaussian covariates: a comparison</title>
    <summary>  Given data $\mathbf{y}$ and $k$ covariates $\mathbf{x}_j$ one problem in
linear regression is to decide which if any of the covariates to include when
regressing the dependent variable $\mathbf{y}$ on the covariates
$\mathbf{x}_j$. In this paper three such methods, lasso, knockoff and Gaussian
covariates are compared using simulations and real data. The Gaussian covariate
method is based on exact probabilities which are valid for all $\mathbf{y}$ and
$\mathbf{x}_j$ making it model free. Moreover the probabilities agree with
those based on the F-distribution for the standard linear model with i.i.d.
Gaussian errors. It is conceptually, mathematically and algorithmically very
simple, it is very fast and makes no use of simulations. It outperforms lasso
and knockoff in all respects by a considerable margin.
</summary>
    <author>
      <name>Laurie Davies</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">79 pages Corrects some errors in the first version and includes files
  to enable the reader to reproduce the comparison</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01862v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01862v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06730v1</id>
    <updated>2018-05-17T12:39:26Z</updated>
    <published>2018-05-17T12:39:26Z</published>
    <title>Birnbaum-Saunders Distribution: A Review of Models, Analysis and
  Applications</title>
    <summary>  Birnbaum and Saunders introduced a two-parameter lifetime distribution to
model fatigue life of a metal, subject to cyclic stress. Since then, extensive
work has been done on this model providing different interpretations,
constructions, generalizations, inferential methods, and extensions to
bivariate, multivariate and matrix-variate cases. More than two hundred papers
and one research monograph have already appeared describing all these aspects
and developments. In this paper, we provide a detailed review of all these
developments and at the same time indicate several open problems that could be
considered for further research.
</summary>
    <author>
      <name>N. Balakrishnan</name>
    </author>
    <author>
      <name>D. Kundu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02385v2</id>
    <updated>2018-05-17T12:28:45Z</updated>
    <published>2017-10-06T13:05:19Z</published>
    <title>Gradient boosting in Markov-switching generalized additive models for
  location, scale and shape</title>
    <summary>  We propose a novel class of flexible latent-state time series regression
models which we call Markov-switching generalized additive models for location,
scale and shape. In contrast to conventional Markov-switching regression
models, the presented methodology allows us to model different state-dependent
parameters of the response distribution - not only the mean, but also variance,
skewness and kurtosis parameters - as potentially smooth functions of a given
set of explanatory variables. In addition, the set of possible distributions
that can be specified for the response is not limited to the exponential family
but additionally includes, for instance, a variety of Box-Cox-transformed,
zero-inflated and mixture distributions. We propose an estimation approach
based on the EM algorithm, where we use the gradient boosting framework to
prevent overfitting while simultaneously performing variable selection. The
feasibility of the suggested approach is assessed in simulation experiments and
illustrated in a real-data setting, where we model the conditional distribution
of the daily average price of energy in Spain over time.
</summary>
    <author>
      <name>Timo Adam</name>
    </author>
    <author>
      <name>Andreas Mayr</name>
    </author>
    <author>
      <name>Thomas Kneib</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02385v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02385v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06640v1</id>
    <updated>2018-05-17T07:53:48Z</updated>
    <published>2018-05-17T07:53:48Z</published>
    <title>Testing for Conditional Mean Independence with Covariates through
  Martingale Difference Divergence</title>
    <summary>  As a crucial problem in statistics is to decide whether additional variables
are needed in a regression model. We propose a new multivariate test to
investigate the conditional mean independence of Y given X conditioning on some
known effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are
linearly related, we reformulate an equivalent notion of conditional mean
independence through transformation, which is approximated in practice. We
apply the martingale difference divergence (Shao and Zhang, 2014) to measure
conditional mean dependence, and show that the estimation error from
approximation is negligible, as it has no impact on the asymptotic distribution
of the test statistic under some regularity assumptions. The implementation of
our test is demonstrated by both simulations and a financial data example.
</summary>
    <author>
      <name>Ze Jin</name>
    </author>
    <author>
      <name>Xiaohan Yan</name>
    </author>
    <author>
      <name>David S. Matteson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06639v1</id>
    <updated>2018-05-17T07:53:09Z</updated>
    <published>2018-05-17T07:53:09Z</published>
    <title>Independent Component Analysis via Energy-based and Kernel-based Mutual
  Dependence Measures</title>
    <summary>  We apply both distance-based (Jin and Matteson, 2017) and kernel-based
(Pfister et al., 2016) mutual dependence measures to independent component
analysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,
minimizing empirical dependence measures as an objective function in both
deflation and parallel manners. Solving this minimization problem, we introduce
Latin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization
method, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization
of the Newton-type local optimization method. The performance of MDMICA is
evaluated in various simulation studies and an image data example. When the ICA
model is correct, MDMICA achieves competitive results compared to existing
approaches. When the ICA model is misspecified, the estimated independent
components are less mutually dependent than the observed components using
MDMICA, while they are prone to be even more mutually dependent than the
observed components using other approaches.
</summary>
    <author>
      <name>Ze Jin</name>
    </author>
    <author>
      <name>David S. Matteson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02459v2</id>
    <updated>2018-05-17T02:23:31Z</updated>
    <published>2017-05-06T08:06:44Z</published>
    <title>Sequential Double Robustness in Right-Censored Longitudinal Models</title>
    <summary>  Consider estimating the G-formula for the counterfactual mean outcome under a
given treatment regime in a longitudinal study. Bang and Robins provided an
estimator for this quantity that relies on a sequential regression formulation
of this parameter. This approach is doubly robust in that it is consistent if
either the outcome regressions or the treatment mechanisms are consistently
estimated. We define a stronger notion of double robustness, termed sequential
double robustness, for estimators of the longitudinal G-formula. The definition
emerges naturally from a more general definition of sequential double
robustness for the outcome regression estimators. An outcome regression
estimator is sequentially doubly robust (SDR) if, at each subsequent time
point, either the outcome regression or the treatment mechanism is consistently
estimated. This form of robustness is exactly what one would anticipate is
attainable by studying the remainder term of a first-order expansion of the
G-formula parameter. We show that a particular implementation of an existing
procedure is SDR. We also introduce a novel SDR estimator, whose development
involves a novel translation of ideas used in targeted minimum loss-based
estimation to the infinite-dimensional setting.
</summary>
    <author>
      <name>Alexander R. Luedtke</name>
    </author>
    <author>
      <name>Oleg Sofrygin</name>
    </author>
    <author>
      <name>Mark J. van der Laan</name>
    </author>
    <author>
      <name>Marco Carone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2 Version 1: May 6, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02459v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02459v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04306v2</id>
    <updated>2018-05-16T23:04:39Z</updated>
    <published>2018-04-12T03:58:48Z</published>
    <title>The effect of a Durbin-Watson pretest on confidence intervals in
  regression</title>
    <summary>  Consider a linear regression model and suppose that our aim is to find a
confidence interval for a specified linear combination of the regression
parameters. In practice, it is common to perform a Durbin-Watson pretest of the
null hypothesis of zero first-order autocorrelation of the random errors
against the alternative hypothesis of positive first-order autocorrelation. If
this null hypothesis is accepted then the confidence interval centred on the
Ordinary Least Squares estimator is used; otherwise the confidence interval
centred on the Feasible Generalized Least Squares estimator is used. We provide
new tools for the computation, for any given design matrix and parameter of
interest, of graphs of the coverage probability functions of the confidence
interval resulting from this two-stage procedure and the confidence interval
that is always centred on the Feasible Generalized Least Squares estimator.
These graphs are used to choose the better confidence interval, prior to any
examination of the observed response vector.
</summary>
    <author>
      <name>Paul Kabaila</name>
    </author>
    <author>
      <name>Samer Alhelli</name>
    </author>
    <author>
      <name>Davide Farchione</name>
    </author>
    <author>
      <name>Nathan Bragg</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02738v7</id>
    <updated>2018-05-16T22:18:09Z</updated>
    <published>2016-10-09T23:01:46Z</published>
    <title>Best Subset Binary Prediction</title>
    <summary>  We consider a variable selection problem for the prediction of binary
outcomes. We study the best subset selection procedure by which the covariates
are chosen by maximizing Manski (1975, 1985)'s maximum score objective function
subject to a constraint on the maximal number of selected variables. We show
that this procedure can be equivalently reformulated as solving a mixed integer
optimization problem, which enables computation of the exact or an approximate
solution with a definite approximation error bound. In terms of theoretical
results, we obtain non-asymptotic upper and lower risk bounds when the
dimension of potential covariates is possibly much larger than the sample size.
Our upper and lower risk bounds are minimax rate-optimal when the maximal
number of selected variables is fixed and does not increase with the sample
size. We illustrate usefulness of the best subset binary prediction approach
via Monte Carlo simulations and an empirical application of the work-trip
transportation mode choice.
</summary>
    <author>
      <name>Le-Yu Chen</name>
    </author>
    <author>
      <name>Sokbae Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">55 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02738v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02738v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30, 62C20, 90C11, 62P20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06539v1</id>
    <updated>2018-05-16T22:04:28Z</updated>
    <published>2018-05-16T22:04:28Z</published>
    <title>Generalized Strucutral Causal Models</title>
    <summary>  Structural causal models are a popular tool to describe causal relations in
systems in many fields such as economy, the social sciences, and biology. In
this work, we show that these models are not flexible enough in general to give
a complete causal representation of equilibrium states in dynamical systems
that do not have a unique stable equilibrium independent of initial conditions.
We prove that our proposed generalized structural causal models do capture the
essential causal semantics that characterize these systems. We illustrate the
power and flexibility of this extension on a dynamical system corresponding to
a basic enzymatic reaction. We motivate our approach further by showing that it
also efficiently describes the effects of interventions on functional laws such
as the ideal gas law.
</summary>
    <author>
      <name>Tineke Blom</name>
    </author>
    <author>
      <name>Joris M. Mooij</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09549v2</id>
    <updated>2018-05-16T20:54:21Z</updated>
    <published>2017-07-29T18:49:34Z</published>
    <title>Sensitivity Analysis for matched pair analysis of binary data: From
  worst case to average case analysis</title>
    <summary>  In matched observational studies where treatment assignment is not
randomized, sensitivity analysis helps investigators determine how sensitive
their estimated treatment effect is to some unmeasured con- founder. The
standard approach calibrates the sensitivity analysis according to the worst
case bias in a pair. This approach will result in a conservative sensitivity
analysis if the worst case bias does not hold in every pair. In this paper, we
show that for binary data, the standard approach can be calibrated in terms of
the average bias in a pair rather than worst case bias. When the worst case
bias and average bias differ, the average bias interpretation results in a less
conservative sensitivity analysis and more power. In many studies, the average
case calibration may also carry a more natural interpretation than the worst
case calibration and may also allow researchers to incorporate additional data
to establish an empirical basis with which to calibrate a sensitivity analysis.
We illustrate this with a study of the effects of cellphone use on the
incidence of automobile accidents. Finally, we extend the average case
calibration to the sensitivity analysis of confidence intervals for
attributable effects.
</summary>
    <author>
      <name>Raiden B. Hasegawa</name>
    </author>
    <author>
      <name>Dylan S. Small</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/biom.12688</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/biom.12688" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">minor corrections/clarifications made</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09549v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09549v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06488v1</id>
    <updated>2018-05-16T18:48:17Z</updated>
    <published>2018-05-16T18:48:17Z</published>
    <title>Valid and Approximately Valid Confidence Intervals for Current Status
  Data</title>
    <summary>  We introduce a new framework for creating point-wise confidence intervals for
the distribution of event times for current status data. Existing methods are
based on asymptotics. Our framework is based on binomial properties and
motivates confidence intervals that are very simple to apply and are valid,
i.e., guarantee nominal coverage. Although these confidence intervals are
necessarily conservative for small sample sizes, asymptotically their coverage
rate approaches the nominal one. This binomial framework also motivates
approximately valid confidence intervals, and simulations show that these
approximate intervals generally have coverage rates closer to the nominal level
with shorter length than existing intervals, including the likelihood
ratio-based confidence interval. Unlike previous asymptotic methods that
require different asymptotic distributions for continuous or grid-based
assessment, the binomial framework can be applied to either type of assessment
distribution.
</summary>
    <author>
      <name>Sungwook Kim</name>
    </author>
    <author>
      <name>Michael P. Fay</name>
    </author>
    <author>
      <name>Michael A. Proschan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, and 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07562v2</id>
    <updated>2018-05-16T17:18:29Z</updated>
    <published>2018-04-20T11:46:17Z</published>
    <title>Bound entangled states fit for robust experimental verification</title>
    <summary>  Preparing and certifying bound entangled states in the laboratory is an
intrinsically hard task, due to both the fact that they typically form narrow
regions in the state space, and that a certificate requires a tomographic
reconstruction of the density matrix. Indeed, the previous experiments that
have reported the preparation of a bound entangled state relied on such
tomographic reconstruction techniques. However, the reliability of these
results crucially depends on the extra assumption of an unbiased
reconstruction. We propose an alternative method for certifying the bound
entangled character of a quantum state that leads to a rigorous claim within a
desired statistical significance, while bypassing a full reconstruction of the
state. The method is comprised by a search for bound entangled states that are
robust for experimental verification, and a hypothesis test tailored for the
detection of bound entanglement that is naturally equipped with a measure of
statistical significance. We apply our method to families of states of $3\times
3$ and $4\times 4$ systems, and find that the experimental certification of
bound entangled states is well within reach.
</summary>
    <author>
      <name>Gael Sentís</name>
    </author>
    <author>
      <name>Johannes N. Greiner</name>
    </author>
    <author>
      <name>Jiangwei Shang</name>
    </author>
    <author>
      <name>Jens Siewert</name>
    </author>
    <author>
      <name>Matthias Kleinmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected the two-ququart examples and improved overall presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07562v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07562v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06432v1</id>
    <updated>2018-05-16T17:15:12Z</updated>
    <published>2018-05-16T17:15:12Z</published>
    <title>Doubly Robust Inference with Non-probability Survey Samples</title>
    <summary>  We establish a general framework for statistical inferences with
non-probability survey samples when relevant auxiliary information is available
from a probability survey sample. We develop a rigorous procedure for
estimating the propensity scores for units in the non-probability sample, and
construct doubly robust estimators for the finite population mean. Variance
estimation is discussed under the proposed framework. Results from simulation
studies show the robustness and the efficiency of our proposed estimators as
compared to existing methods. The proposed method is used to analyze a
non-probability survey sample collected by the Pew Research Center with
auxiliary information from the Behavioral Risk Factor Surveillance System and
the Current Population Survey. Our results illustrate a general approach to
inference with non-probability samples and highlight the importance and
usefulness of auxiliary information from probability survey samples.
</summary>
    <author>
      <name>Yilin Chen</name>
    </author>
    <author>
      <name>Pengfei Li</name>
    </author>
    <author>
      <name>Changbao Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 5 tables, this paper has been submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06364v1</id>
    <updated>2018-05-16T15:14:21Z</updated>
    <published>2018-05-16T15:14:21Z</published>
    <title>Adaptive elastic-net and fused estimators in high-dimensional group
  quantile linear model</title>
    <summary>  In applications, the variables are naturally grouped in a linear quantile
model, the most common example being the multivariate variance analysis. For
this model, with the possibility that the number of groups diverges with sample
size, we introduce and study the adaptive elastic-net estimation method. This
method automatically selects, with a probability converging to one, the
significant groups and, moreover, the non zero parameter estimators are
asymptotically normal. The Monte Carlo simulations, using a subgradient
proposed algorithm, show that the adaptive elastic-net group quantile
estimations are more accurate that other existing group estimations in the
literature. When the number of groups coincides whit the number of
observations, a fused penalty allows automatically detection of consecutive
groups which have the same influence on the response variable. We obtain an
upper bound of the number of the consecutive groups with different estimators.
</summary>
    <author>
      <name>Gabriela Ciuperca</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F12, 62F35, 62J05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09996v2</id>
    <updated>2018-05-16T12:46:10Z</updated>
    <published>2018-02-27T16:13:11Z</published>
    <title>Exact Simulation of reciprocal Archimedean copulas</title>
    <summary>  The decreasing enumeration of the points of a Poisson random measure whose
mean measure has finite survival function on the positive half-axis can be
represented as a non-increasing function of the jump times of a standard
Poisson process. This observation allows to generalize the essential idea from
a well-known exact simulation algorithm for arbitrary extreme-value copulas to
copulas of a more general family of max-infinitely divisible distributions,
with reciprocal Archimedean copulas being a particular example.
</summary>
    <author>
      <name>Jan-Frederik Mai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.spl.2018.05.020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.spl.2018.05.020" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics and Probability Letters 141C, 68-73, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.09996v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09996v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00882v4</id>
    <updated>2018-05-16T09:16:17Z</updated>
    <published>2016-07-04T13:37:22Z</published>
    <title>Modelling Ordinal Responses with Uncertainty: a Hierarchical Marginal
  Model with Latent Uncertainty components</title>
    <summary>  In responding to rating questions, an individual may give answers either
according to his/her knowledge/awareness or to his/her level of
indecision/uncertainty, typically driven by a response style. As ignoring this
dual behaviour may lead to misleading results, we define a multivariate model
for ordinal rating responses, by introducing, for every item, a binary latent
variable that discriminates aware from uncertain responses. Some independence
assumptions among latent and observable variables characterize the uncertain
behaviour and make the model easier to interpret. Uncertain responses are
modelled by specifying probability distributions that can depict different
response styles characterizing the uncertain raters. A marginal parametrization
allows a simple and direct interpretation of the parameters in terms of
association among aware responses and their dependence on explanatory factors.
The effectiveness of the proposed model is attested through an application to
real data and supported by a Monte Carlo study.
</summary>
    <author>
      <name>Roberto Colombi</name>
    </author>
    <author>
      <name>Sabrina Giordano</name>
    </author>
    <author>
      <name>Anna Gottard</name>
    </author>
    <author>
      <name>Maria Iannario</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00882v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00882v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06178v1</id>
    <updated>2018-05-16T07:58:05Z</updated>
    <published>2018-05-16T07:58:05Z</published>
    <title>Chirp-like model and its parameter estimation</title>
    <summary>  We propose a chirp-like signal model as an alternative to a chirp model and a
generalisation of the sinusoidal model, which is a fundamental model in the
statistical signal processing literature. It is observed that the proposed
model can be arbitrarily close to the chirp model. The propounded model is
similar to a chirp model in the sense that here also the frequency changes
linearly with time. However, the parameter estimation of a chirp-like model is
simpler compared to a chirp model. In this paper, we consider the least squares
and the sequential least squares estimation procedures and study the asymptotic
properties of these proposed estimators. These asymptotic results are
corroborated through simulation studies and analysis of four speech signal data
sets have been performed to see the effectiveness of the proposed model, and
the results are quite encouraging.
</summary>
    <author>
      <name>Rhythm Grover</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <author>
      <name>Amit Mitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 5 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06099v1</id>
    <updated>2018-05-16T02:35:56Z</updated>
    <published>2018-05-16T02:35:56Z</published>
    <title>Joint longitudinal and time-to-event models for multilevel hierarchical
  data</title>
    <summary>  Joint modelling of longitudinal and time-to-event data has received much
attention recently. Increasingly, extensions to standard joint modelling
approaches are being proposed to handle complex data structures commonly
encountered in applied research. In this paper we propose a joint model for
hierarchical longitudinal and time-to-event data. Our motivating application
explores the association between tumor burden and progression-free survival in
non-small cell lung cancer patients. We define tumor burden as a function of
the sizes of target lesions clustered within a patient. Since a patient may
have more than one lesion, and each lesion is tracked over time, the data have
a three-level hierarchical structure: repeated measurements taken at time
points (level 1) clustered within lesions (level 2) within patients (level 3).
We jointly model the lesion-specific longitudinal trajectories and
patient-specific risk of death or disease progression by specifying novel
association structures that combine information across lower level clusters
(e.g. lesions) into patient-level summaries (e.g. tumor burden). We provide
user-friendly software for fitting the model under a Bayesian framework.
Lastly, we discuss alternative situations in which additional clustering
factor(s) occur at a level higher in the hierarchy than the patient-level,
since this has implications for the model formulation.
</summary>
    <author>
      <name>Samuel L. Brilleman</name>
    </author>
    <author>
      <name>Michael J. Crowther</name>
    </author>
    <author>
      <name>Margarita Moreno-Betancur</name>
    </author>
    <author>
      <name>Jacqueline Buros Novik</name>
    </author>
    <author>
      <name>James Dunyak</name>
    </author>
    <author>
      <name>Nidal Al-Huniti</name>
    </author>
    <author>
      <name>Robert Fox</name>
    </author>
    <author>
      <name>Jeff Hammerbacher</name>
    </author>
    <author>
      <name>Rory Wolfe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06035v1</id>
    <updated>2018-05-15T21:18:05Z</updated>
    <published>2018-05-15T21:18:05Z</published>
    <title>Confounding caused by causal-effect covariability</title>
    <summary>  Confounding seriously impairs our ability to learn about causal relations
from observational data. Confounding can be defined as a statistical
association between two variables due to inputs from a common source (the
confounder). For example, if $Z\rightarrow Y$ and $Z\rightarrow X$, then $X$
and $Y$ will be statistically dependent, even if there are no causal
connections between the two. There are several approaches available to adjust
for confounding, i.e. to remove, or reduce, the association between two
variables due to the confounder. Common adjustment techniques include
stratifying the analysis on the confounder, and including confounders as
covariates in regression models. Most adjustments rely on the assumption that
the causal effects of confounders, on different variables, do not co-vary. For
example, if the causal effect of $Z$ on $X$ and the causal effect of $Z$ on $Y$
co-vary between observational units, a confounding effect remains after
adjustment for $Z$. This causal-effect covariability and its consequences is
the topic of this paper.
  Causal-effect covariability is first explicated using the framework of
structural causal models. Using this framework it is easy to show that
causal-effect covariability generally leads to confounding that cannot be
adjusted for by standard methods. Evidence from data indicates that the
confounding introduced by causal-effect covariability might be a real concern
in applied work.
</summary>
    <author>
      <name>Anders Ledberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05296v2</id>
    <updated>2018-05-15T16:56:47Z</updated>
    <published>2017-07-17T17:49:19Z</published>
    <title>Piecewise-Deterministic Markov Chain Monte Carlo</title>
    <summary>  A novel class of non-reversible Markov chain Monte Carlo schemes relying on
continuous-time piecewise-deterministic Markov Processes has recently emerged.
In these algorithms, the state of the Markov process evolves according to a
deterministic dynamics which is modified using a Markov transition kernel at
random event times. These methods enjoy remarkable features including the
ability to update only a subset of the state components while other components
implicitly keep evolving and the ability to use an unbiased estimate of the
gradient of the log-target while preserving the target as invariant
distribution. However, they also suffer from important limitations. The
deterministic dynamics used so far do not exploit the structure of the target.
Moreover, exact simulation of the event times is feasible for an important yet
restricted class of problems and, even when it is, it is application specific.
This limits the applicability of these techniques and prevents the development
of a generic software implementation of them. We introduce novel MCMC methods
addressing these shortcomings. In particular, we introduce novel
continuous-time algorithms relying on exact Hamiltonian flows and novel
non-reversible discrete-time algorithms which can exploit complex dynamics such
as approximate Hamiltonian dynamics arising from symplectic integrators while
preserving the attractive features of continuous-time algorithms. We
demonstrate the performance of these schemes on a variety of applications.
</summary>
    <author>
      <name>Paul Vanetti</name>
    </author>
    <author>
      <name>Alexandre Bouchard-Côté</name>
    </author>
    <author>
      <name>George Deligiannidis</name>
    </author>
    <author>
      <name>Arnaud Doucet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05795v1</id>
    <updated>2018-05-15T14:20:53Z</updated>
    <published>2018-05-15T14:20:53Z</published>
    <title>Information-Anchored Sensitivity Analysis: Theory and Application</title>
    <summary>  Analysis of longitudinal randomised controlled trials is frequently
complicated because patients deviate from the protocol. Where such deviations
are relevant for the estimand, we are typically required to make an untestable
assumption about post-deviation behaviour in order to perform our primary
analysis and estimate the treatment effect. In such settings, it is now widely
recognised that we should follow this with sensitivity analyses to explore the
robustness of our inferences to alternative assumptions about post-deviation
behaviour. Although there has been a lot of work on how to conduct such
sensitivity analyses, little attention has been given to the appropriate loss
of information due to missing data within sensitivity analysis. We argue more
attention needs to be given to this issue, showing it is quite possible for
sensitivity analysis to decrease and increase the information about the
treatment effect. To address this critical issue, we introduce the concept of
information-anchored sensitivity analysis. By this we mean sensitivity analysis
in which the proportion of information about the treatment estimate lost due to
missing data is the same as the proportion of information about the treatment
estimate lost due to missing data in the primary analysis. We argue this forms
a transparent, practical starting point for interpretation of sensitivity
analysis. We then derive results showing that, for longitudinal continuous
data, a broad class of controlled and reference-based sensitivity analyses
performed by multiple imputation are information-anchored. We illustrate the
theory with simulations and an analysis of a peer review trial, then discuss
our work in the context of other recent work in this area. Our results give a
theoretical basis for the use of controlled multiple imputation procedures for
sensitivity analysis.
</summary>
    <author>
      <name>Suzie Cro</name>
    </author>
    <author>
      <name>James R Carpenter</name>
    </author>
    <author>
      <name>Michael G Kenward</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05756v1</id>
    <updated>2018-05-15T13:36:05Z</updated>
    <published>2018-05-15T13:36:05Z</published>
    <title>Visualizing Tests for Equality of Covariance Matrices</title>
    <summary>  This paper explores a variety of topics related to the question of testing
the equality of covariance matrices in multivariate linear models, particularly
in the MANOVA setting. The main focus is on graphical methods that can be used
to address the evaluation of this assumption. We introduce some extensions of
data ellipsoids, hypothesis-error (HE) plots and canonical discriminant plots
and demonstrate how they can be applied to the testing of equality of
covariance matrices. Further, a simple plot of the components of Box's M test
is proposed that shows _how_ groups differ in covariance and also suggests
other visualizations and alternative test statistics. These methods are
implemented and freely available in the **heplots** and **candisc** packages
for R. Examples from the paper are available in supplementary materials.
</summary>
    <author>
      <name>Michael Friendly</name>
    </author>
    <author>
      <name>Matthew Sigal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The American Statistician, in press (2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07, 62-09, 62H15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02804v3</id>
    <updated>2018-05-15T09:02:43Z</updated>
    <published>2016-07-11T01:43:15Z</published>
    <title>Estimating the number of species to attain sufficient representation in
  a random sample</title>
    <summary>  The statistical problem of using an initial sample to estimate the number of
species in a larger sample has found important applications in fields far
removed from ecology. Here we address the general problem of estimating the
number of species that will be represented by at least a number r of
observations in a future sample. The number r indicates species with sufficient
observations, which are commonly used as a necessary condition for any robust
statistical inference. We derive a procedure to construct consistent estimators
that apply universally for a given population: once constructed, they can be
evaluated as a simple function of r. Our approach is based on a relation
between the number of species represented at least r times and the higher
derivatives of the expected number of species discovered per unit of time.
Combining this relation with a rational function approximation, we propose
nonparametric estimators that are accurate for both large values of r and
long-range extrapolations. We further show that our estimators retain
asymptotic behaviors that are essential for applications on large-scale
datasets. We evaluate the performance of this approach by both simulation and
real data applications for inferences of the vocabulary of Shakespeare and
Dickens, the topology of a Twitter social network, and molecular diversity in
DNA sequencing data.
</summary>
    <author>
      <name>Chao Deng</name>
    </author>
    <author>
      <name>Timothy Daley</name>
    </author>
    <author>
      <name>Peter Calabrese</name>
    </author>
    <author>
      <name>Jie Ren</name>
    </author>
    <author>
      <name>Andrew D. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02804v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02804v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05606v1</id>
    <updated>2018-05-15T07:32:18Z</updated>
    <published>2018-05-15T07:32:18Z</published>
    <title>Nonparametric Bayesian volatility learning under microstructure noise</title>
    <summary>  Aiming at financial applications, we study the problem of learning the
volatility under market microstructure noise. Specifically, we consider noisy
discrete time observations from a stochastic differential equation and develop
a novel computational method to learn the diffusion coefficient of the
equation. We take a nonparametric Bayesian approach, where we model the
volatility function a priori as piecewise constant. Its prior is specified via
the inverse Gamma Markov chain. Sampling from the posterior is accomplished by
incorporating the Forward Filtering Backward Simulation algorithm in the Gibbs
sampler. Good performance of the method is demonstrated on two representative
synthetic data examples. Finally, we apply the method on the EUR/USD exchange
rate dataset.
</summary>
    <author>
      <name>Shota Gugushvili</name>
    </author>
    <author>
      <name>Frank van der Meulen</name>
    </author>
    <author>
      <name>Moritz Schauer</name>
    </author>
    <author>
      <name>Peter Spreij</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary: 62G20, Secondary: 62M05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05480v1</id>
    <updated>2018-05-14T22:05:38Z</updated>
    <published>2018-05-14T22:05:38Z</published>
    <title>ABC-CDE: Towards Approximate Bayesian Computation with Complex
  High-Dimensional Data and Limited Simulations</title>
    <summary>  Approximate Bayesian Computation (ABC) is typically used when the likelihood
is either unavailable or intractable but where data can be simulated under
different parameter settings using a forward model. Despite the recent interest
in ABC, high-dimensional data and costly simulations still remain a bottleneck.
There is also no consensus as to how to best assess the performance of such
methods without knowing the true posterior. We show how a nonparametric
conditional density estimation (CDE) framework, which we refer to as ABC-CDE,
help address three key challenges in ABC: (i) how to efficiently estimate the
posterior distribution with limited simulations and different types of data,
(ii) how to tune and compare the performance of ABC and related methods in
estimating the posterior itself, rather than just certain properties of the
density, and (iii) how to efficiently choose among a large set of summary
statistics based on a CDE surrogate loss. We provide theoretical and empirical
evidence that justify ABC-CDE procedures that directly estimate and assess the
posterior based on an initial ABC sample, and we describe settings where
standard ABC and regression-based approaches are inadequate.
</summary>
    <author>
      <name>Rafael Izbicki</name>
    </author>
    <author>
      <name>Ann B. Lee</name>
    </author>
    <author>
      <name>Taylor Pospisil</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05786v2</id>
    <updated>2018-05-14T20:30:04Z</updated>
    <published>2017-09-18T06:34:03Z</published>
    <title>Parameter Regimes in Partial Functional Panel Regression</title>
    <summary>  A new partial functional linear regression model for panel data with time
varying parameters is introduced. The parameter vector of the multivariate
model component is allowed to be completely time varying while the
function-valued parameter of the functional model component is assumed to
change over K unknown parameter regimes. Consistency is derived for the
suggested estimators and for the classification procedure used to detect the K
unknown parameter regimes. Additionally, the convergence rates of the
estimators are derived under a double asymptotic differentiating between
asymptotic scenarios depending on the relative order of the panel dimensions n
and T. The statistical model is motivated by a real data application
considering the so-called idiosyncratic volatility puzzle using high frequency
data from the S&amp;P500.
</summary>
    <author>
      <name>Dominik Liebl</name>
    </author>
    <author>
      <name>Fabian Walders</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ecosta.2018.05.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ecosta.2018.05.003" rel="related"/>
    <link href="http://arxiv.org/abs/1709.05786v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05786v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03459v3</id>
    <updated>2018-05-14T18:00:02Z</updated>
    <published>2017-04-11T18:00:00Z</published>
    <title>Dynamic nested sampling: an improved algorithm for parameter estimation
  and evidence calculation</title>
    <summary>  We introduce dynamic nested sampling: a generalisation of the nested sampling
algorithm in which the number of "live points" varies to allocate samples more
efficiently. In empirical tests the new method increases parameter estimation
and Bayesian evidence calculation accuracy by factors of up to ~8 and ~2.6
respectively compared to standard nested sampling with the same number of
samples; this is equivalent to speeding up the computation by factors of ~64
and ~7. We also show that the accuracy of both parameter estimation and
evidence calculations can be improved simultaneously. In addition, unlike in
standard nested sampling, more accurate results can be obtained by continuing
the calculation for longer. Popular standard nested sampling implementations
can be easily adapted to perform dynamic nested sampling, and several dynamic
nested sampling software packages are now publicly available.
</summary>
    <author>
      <name>Edward Higson</name>
    </author>
    <author>
      <name>Will Handley</name>
    </author>
    <author>
      <name>Mike Hobson</name>
    </author>
    <author>
      <name>Anthony Lasenby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages + appendix, 14 figures. Added tests of dynamic nested
  sampling with dyPolyChord on a multimodal Gaussian mixture model</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03459v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03459v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05232v1</id>
    <updated>2018-05-14T15:33:58Z</updated>
    <published>2018-05-14T15:33:58Z</published>
    <title>Bayesian forecasting of many count-valued time series</title>
    <summary>  This paper develops forecasting methodology and application of new classes of
dynamic models for time series of non-negative counts. Novel univariate models
synthesise dynamic generalized linear models for binary and conditionally
Poisson time series, with dynamic random effects for over-dispersion. These
models allow use of dynamic covariates in both binary and non-zero count
components. Sequential Bayesian analysis allows fast, parallel analysis of sets
of decoupled time series. New multivariate models then enable information
sharing in contexts when data at a more highly aggregated level provide more
incisive inferences on shared patterns such as trends and seasonality. A novel
multi-scale approach-- one new example of the concept of decouple/recouple in
time series-- enables information sharing across series. This incorporates
cross-series linkages while insulating parallel estimation of univariate
models, hence enables scalability in the number of series. The major motivating
context is supermarket sales forecasting. Detailed examples drawn from a case
study in multi-step forecasting of sales of a number of related items showcase
forecasting of multiple series, with discussion of forecast accuracy metrics
and broader questions of probabilistic forecast accuracy assessment.
</summary>
    <author>
      <name>Lindsay Berry</name>
    </author>
    <author>
      <name>Mike West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 10 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10, 62F15, 62M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01280v2</id>
    <updated>2018-05-14T14:46:47Z</updated>
    <published>2017-11-03T18:01:22Z</published>
    <title>Causal inference for interfering units with cluster and population level
  treatment allocation programs</title>
    <summary>  Interference arises when an individual's potential outcome depends on the
individual treatment level, but also on the treatment level of others. A common
assumption in the causal inference literature in the presence of interference
is partial interference, implying that the population can be partitioned in
clusters of individuals whose potential outcomes only depend on the treatment
of units within the same cluster. Previous literature has defined average
potential outcomes under counterfactual scenarios where treatments are randomly
allocated to units within a cluster. However, within clusters there may be
units that are more or less likely to receive treatment based on covariates or
neighbors' treatment. We define new estimands that describe average potential
outcomes for realistic counterfactual treatment allocation programs, extending
existing estimands to take into consideration the units' covariates and
dependence between units' treatment assignment. We further propose entirely new
estimands for population-level interventions over the collection of clusters,
which correspond in the motivating setting to regulations at the federal (vs.
cluster or regional) level. We discuss these estimands, propose unbiased
estimators and derive asymptotic results as the number of clusters grows.
Finally, we estimate effects in a comparative effectiveness study of power
plant emission reduction technologies on ambient ozone pollution.
</summary>
    <author>
      <name>Georgia Papadogeorgou</name>
    </author>
    <author>
      <name>Fabrizia Mealli</name>
    </author>
    <author>
      <name>Corwin M. Zigler</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01280v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01280v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02491v2</id>
    <updated>2018-05-14T14:29:15Z</updated>
    <published>2017-08-08T13:52:21Z</published>
    <title>Recovering Covariance from Functional Fragments</title>
    <summary>  We consider nonparametric estimation of a covariance function on the unit
square, given a sample of discretely observed fragments of functional data.
When each sample path is only observed on a subinterval of length ${\delta}&lt;1$,
one has no statistical information on the unknown covariance outside a
${\delta}$-band around the diagonal. The problem seems unidentifiable without
parametric assumptions, but we show that nonparametric estimation is feasible
under suitable smoothness and rank conditions on the unknown covariance. This
remains true even when observation is discrete, and we give precise
deterministic conditions on how fine the observation grid needs to be relative
to the rank and fragment length for identifiability to hold true. We show that
our conditions translate the estimation problem to a low-rank matrix completion
problem, and construct a nonparametric estimator that is consistent in a fully
functional sense; indeed, we obtain convergence rates demonstrating that even a
parametric rate is attainable if the grid is sufficiently dense. We illustrate
the numerical performance of our method on real and simulated data.
</summary>
    <author>
      <name>Marie-Hélène Descary</name>
    </author>
    <author>
      <name>Victor M. Panaretos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Biometrika</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02491v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02491v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05315v3</id>
    <updated>2018-05-14T13:10:02Z</updated>
    <published>2015-07-19T18:20:05Z</published>
    <title>Uniformly Valid Confidence Sets Based on the Lasso</title>
    <summary>  In a linear regression model of fixed dimension $p \leq n$, we construct
confidence regions for the unknown parameter vector based on the Lasso
estimator that uniformly and exactly hold the prescribed in finite samples as
well as in an asymptotic setup. We thereby quantify estimation uncertainty as
well as the "post-model selection error" of this estimator. More concretely, in
finite samples with Gaussian errors and asymptotically in the case where the
Lasso estimator is tuned to perform conservative model selection, we derive
exact formulas for computing the minimal coverage probability over the entire
parameter space for a large class of shapes for the confidence sets, thus
enabling the construction of valid confidence regions based on the Lasso
estimator in these settings. The choice of shape for the confidence sets and
comparison with the confidence ellipse based on the least-squares estimator is
also discussed. Moreover, in the case where the Lasso estimator is tuned to
enable consistent model selection, we give a simple confidence region with
minimal coverage probability converging to one. Finally, we also treat the case
of unknown error variance and present some ideas for extensions.
</summary>
    <author>
      <name>Karl Ewald</name>
    </author>
    <author>
      <name>Ulrike Schneider</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Some typos corrected, updated references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electron. J. Statist. 12 (2018), 1358-1387</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.05315v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05315v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F25 (Primary), 62J07, 62J05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04716v3</id>
    <updated>2018-05-14T12:20:30Z</updated>
    <published>2018-01-15T10:01:53Z</published>
    <title>Robust Inference for Seemingly Unrelated Regression Models</title>
    <summary>  Seemingly unrelated regression models generalize linear regression models by
considering multiple regression equations that are linked by contemporaneously
correlated disturbances. Robust inference for seemingly unrelated regression
models is considered. MM-estimators are introduced to obtain estimators that
have both a high breakdown point and a high normal efficiency. A fast and
robust bootstrap procedure is developed to obtain robust inference for these
estimators. Confidence intervals for the model parameters as well as hypothesis
tests for linear restrictions of the regression coefficients in seemingly
unrelated regression models are constructed. Moreover, in order to evaluate the
need for a seemingly unrelated regression model, a robust procedure is proposed
to test for the presence of correlation among the disturbances. The performance
of the fast and robust bootstrap inference is evaluated empirically in
simulation studies and illustrated on real data.
</summary>
    <author>
      <name>Kris Peremans</name>
    </author>
    <author>
      <name>Stefan Van Aelst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jmva.2018.05.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jmva.2018.05.002" rel="related"/>
    <link href="http://arxiv.org/abs/1801.04716v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04716v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05133v1</id>
    <updated>2018-05-14T12:03:13Z</updated>
    <published>2018-05-14T12:03:13Z</published>
    <title>Model selection with lasso-zero: adding straw to the haystack to better
  find needles</title>
    <summary>  The high-dimensional linear model $y = X \beta^0 + \epsilon$ is considered
and the focus is put on the problem of recovering the support $S^0$ of the
sparse vector $\beta^0.$ We introduce lasso-zero, a new $\ell_1$-based
estimator whose novelty resides in an "overfit, then threshold" paradigm and
the use of noise dictionaries for overfitting the response. The methodology is
supported by theoretical results obtained in the special case where no noise
dictionary is used. In this case, lasso-zero boils down to thresholding the
basis pursuit solution. We prove that this procedure requires weaker conditions
on $X$ and $S^0$ than the lasso for exact support recovery, and controls the
false discovery rate for orthonormal designs when tuned by the quantile
universal threshold. However it requires a high signal-to-noise ratio, and the
use of noise dictionaries addresses this issue. The threshold selection
procedure is based on a pivotal statistic and does not require knowledge of the
noise level. Numerical simulations show that lasso-zero performs well in terms
of support recovery and provides a good trade-off between high true positive
rate and low false discovery rate compared to competitors.
</summary>
    <author>
      <name>Pascaline Descloux</name>
    </author>
    <author>
      <name>Sylvain Sardy</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10397v2</id>
    <updated>2018-05-14T11:47:03Z</updated>
    <published>2018-04-27T08:54:45Z</published>
    <title>Implicit Copulas from Bayesian Regularized Regression Smoothers</title>
    <summary>  We show how to extract the implicit copula of a response vector from a
Bayesian regularized regression smoother with Gaussian disturbances. The copula
can be used to compare smoothers that employ different shrinkage priors and
function bases. We illustrate with three popular choices of shrinkage priors
--- a pairwise prior, the horseshoe prior and a g prior augmented with a point
mass as employed for Bayesian variable selection --- and both univariate and
multivariate function bases. The implicit copulas are high-dimensional, have
flexible dependence structures that are far from that of a Gaussian copula, and
are unavailable in closed form. However, we show how they can be evaluated by
first constructing a Gaussian copula conditional on the regularization
parameters, and then integrating over these. Combined with non-parametric
margins the regularized smoothers can be used to model the distribution of
non-Gaussian univariate responses conditional on the covariates. Efficient
Markov chain Monte Carlo schemes for evaluating the copula are given for this
case. Using both simulated and real data, we show how such copula smoothing
models can improve the quality of resulting function estimates and predictive
distributions.
</summary>
    <author>
      <name>Nadja Klein</name>
    </author>
    <author>
      <name>Michael Stanley Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10397v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10397v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02836v2</id>
    <updated>2018-05-14T06:45:04Z</updated>
    <published>2017-11-08T05:21:26Z</published>
    <title>Multilevel Monte Carlo for Smoothing via Transport Methods</title>
    <summary>  In this article we consider recursive approximations of the smoothing
distribution associated to partially observed stochastic differential equations
(SDEs), which are observed discretely in time. Such models appear in a wide
variety of applications including econometrics, finance and engineering. This
problem is notoriously challenging, as the smoother is not available
analytically and hence require numerical approximation. This usually consists
by applying a time-discretization to the SDE, for instance the Euler method,
and then applying a numerical (e.g. Monte Carlo) method to approximate the
smoother. This has lead to a vast literature on methodology for solving such
problems, perhaps the most popular of which is based upon the particle filter
(PF) e.g. [9]. In the context of filtering for this class of problems, it is
well-known that the particle filter can be improved upon in terms of cost to
achieve a given mean squared error (MSE) for estimates. This in the sense that
the computational effort can be reduced to achieve this target MSE, by using
multilevel (ML) methods [12, 13, 18], via the multilevel particle filter (MLPF)
[16, 20, 21]. For instance, to obtain a MSE of $\mathcal{O}(\epsilon^2)$ for
some $\epsilon &gt; 0$ when approximating filtering distributions associated with
Euler-discretized diffusions with constant diffusion coefficients, the cost of
the PF is $\mathcal{O}(\epsilon^{-3})$ while the cost of the MLPF is
$\mathcal{O}(\epsilon^{-2}\log(\epsilon)^2)$. In this article we consider a new
approach to replace the particle filter, using transport methods in [27]. In
the context of filtering, one expects that the proposed method improves upon
the MLPF by yielding, under assumptions, a MSE of $\mathcal{O}(\epsilon^2)$ for
a cost of $\mathcal{O}(\epsilon^{-2})$. This is established theoretically in an
"ideal" example and numerically in numerous examples.
</summary>
    <author>
      <name>Jeremie Houssineau</name>
    </author>
    <author>
      <name>Ajay Jasra</name>
    </author>
    <author>
      <name>Sumeetpal S. Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02836v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02836v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M05, 60J60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05017v1</id>
    <updated>2018-05-14T06:06:30Z</updated>
    <published>2018-05-14T06:06:30Z</published>
    <title>An efficient and robust method for analyzing population pharmacokinetic
  data in genome-wide pharmacogenomic studies: a generalized estimating
  equation approach</title>
    <summary>  Powerful array-based single-nucleotide polymorphism--typing platforms have
recently heralded a new era in which genome-wide studies are conducted with
increasing frequency. A genetic polymorphism associated with population
pharmacokinetics (PK) is typically analyzed using nonlinear mixed-effect models
(NLMM). Applying NLMM to large-scale data, such as those generated by
genome-wide studies, raises several issues related to the assumption of random
effects, as follows: (i) Computation time: it takes a long time to compute the
marginal likelihood. (ii) Convergence of iterative calculation: an adaptive
Gauss-Hermite quadrature is generally used to estimate NLMM; however, iterative
calculations may not converge in complex models. (iii) Random-effects
misspecification leads to slightly inflated type-I error rates. As an
alternative effective approach to resolving these issues, in this article we
propose a generalized estimating equation (GEE) approach for analyzing
population PK data. In general, GEE analysis does not account for
inter-individual variability in PK parameters; therefore, the usual GEE
estimators cannot be interpreted straightforwardly, and their validities have
not been justified. Here, we propose valid inference methods for using GEE even
under conditions of inter-individual variability, and provide theoretical
justifications of the proposed GEE estimators for population PK data. In
numerical evaluations by simulations, the proposed GEE approach exhibited high
computational speed and stability relative to the NLMM approach. Furthermore,
the NLMM analysis was sensitive to the misspecification of the random-effects
distribution, and the proposed GEE inference is valid for any distributional
form. An illustration is provided using data from a genome-wide pharmacogenomic
study of an anticancer drug.
</summary>
    <author>
      <name>Kengo Nagashima</name>
    </author>
    <author>
      <name>Yasunori Sato</name>
    </author>
    <author>
      <name>Hisashi Noma</name>
    </author>
    <author>
      <name>Chikuma Hamada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/sim.5895</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/sim.5895" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistics in Medicine 2013; 32(27): 4838-4858</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.05017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01669v2</id>
    <updated>2018-05-13T08:41:38Z</updated>
    <published>2018-01-05T08:32:17Z</published>
    <title>Anomaly Detection and Location in Distribution Network: A Data-Driven
  Approach</title>
    <summary>  In this paper, a data-driven approach is developed for the anomaly detection
and location in distribution network. First, spatio-temporal matrices are
formulated by using massive online monitoring data collected through
multiple-time-instant monitoring devices in the distribution network. Based on
the random matrix theory for the spectrum analysis of those data matrices, a
real-time anomaly detection and location algorithm is developed. During the
above process, the linear eigenvalue statistics are defined to indicate data
behavior, and the anomaly latencies are analyzed and located. As for those
low-dimensional data matrices formulated in the distribution network, by using
tensor product, an increasing data dimension method is developed for them to be
analyzed by using the random matrix theory. Theoretical justifications for the
developed approach are provided through Marchenko-Pastur Law and Ring Law. Case
studies both on the synthetic data from IEEE standard bus system and the
real-world online monitoring data in a power grid corroborate the feasibility
and effectiveness of the proposed approach.
</summary>
    <author>
      <name>Xin Shi</name>
    </author>
    <author>
      <name>Robert Qiu</name>
    </author>
    <author>
      <name>Xing He</name>
    </author>
    <author>
      <name>Lei Chu</name>
    </author>
    <author>
      <name>Zenan Ling</name>
    </author>
    <author>
      <name>Haosen Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01669v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01669v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05246v6</id>
    <updated>2018-05-13T02:25:46Z</updated>
    <published>2016-10-17T18:19:49Z</published>
    <title>BET on Independence</title>
    <summary>  We study the problem of nonparametric dependence detection. Many existing
methods suffer severe power loss due to non-uniform consistency, which we
illustrate with a paradox. To avoid such power loss, we approach the
nonparametric test of independence through the new framework of binary
expansion statistics (BEStat) and binary expansion testing (BET), which examine
dependence through a novel binary expansion filtration approximation of the
copula. Through a Hadamard transform, we find that the symmetry statistics in
the filtration are complete sufficient statistics for dependence. These
statistics are also uncorrelated under the null. By utilizing symmetry
statistics, the BET avoids the problem of non-uniform consistency and improves
upon a wide class of commonly used methods (a) by achieving the minimax rate in
sample size requirement for reliable power and (b) by providing clear
interpretations of global relationships upon rejection of independence. The
binary expansion approach also connects the symmetry statistics with the
current computing system to facilitate efficient bitwise implementation. We
illustrate the BET with a study of the distribution of stars in the night sky
and with an exploratory data analysis of the TCGA breast cancer data.
</summary>
    <author>
      <name>Kai Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1610.05246v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05246v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00229v4</id>
    <updated>2018-05-12T11:06:33Z</updated>
    <published>2016-06-01T11:19:21Z</published>
    <title>Uncertainty and filtering of hidden Markov models in discrete time</title>
    <summary>  We consider the problem of filtering an unseen Markov chain from noisy
observations, in the presence of uncertainty regarding the parameters of the
processes involved. Using the theory of nonlinear expectations, we describe the
uncertainty in terms of a penalty function, which can be propagated forward in
time in the place of the filter.
</summary>
    <author>
      <name>Samuel N. Cohen</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00229v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00229v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M20, 60G35, 93E11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04667v1</id>
    <updated>2018-05-12T06:19:06Z</updated>
    <published>2018-05-12T06:19:06Z</published>
    <title>Bayesian Dynamic Modeling and Monitoring of Network Flows</title>
    <summary>  In the context of a motivating study of dynamic network flow data on a
large-scale e-commerce web site, we develop Bayesian models for
on-line/sequential analysis for monitoring and adapting to changes reflected in
node-node traffic. For large-scale networks, we customize core Bayesian time
series analysis methods using dynamic generalized linear models (DGLMs)
integrated into the multivariate network context using the concept of
decouple/recouple recently introduced in multivariate time series. This enables
flexible dynamic modeling of flows on large-scale networks and exploitation of
partial parallelization of analysis while critically maintaining coherence with
an over-arching multivariate dynamic flow model. Development is anchored in a
case-study on internet data, with flows of visitors to a commercial news web
site defining a long time series of node-node counts on over 56,000 node pairs.
Characterizing inherent stochasticity in traffic patterns, understanding
node-node interactions, adapting to dynamic changes in flows and allowing for
sensitive monitoring to flag anomalies are central questions. The methodology
of dynamic network DGLMs will be of interest and utility in broad ranges of
dynamic network flow studies.
</summary>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>David Banks</name>
    </author>
    <author>
      <name>Mike West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 24 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3319v4</id>
    <updated>2018-05-12T03:10:10Z</updated>
    <published>2014-05-13T22:31:09Z</published>
    <title>Fully Bayesian Logistic Regression with Hyper-Lasso Priors for
  High-dimensional Feature Selection</title>
    <summary>  High-dimensional feature selection arises in many areas of modern science.
For example, in genomic research we want to find the genes that can be used to
separate tissues of different classes (e.g. cancer and normal) from tens of
thousands of genes that are active (expressed) in certain tissue cells. To this
end, we wish to fit regression and classification models with a large number of
features (also called variables, predictors). In the past decade, penalized
likelihood methods for fitting regression models based on hyper-LASSO
penalization have received increasing attention in the literature. However,
fully Bayesian methods that use Markov chain Monte Carlo (MCMC) are still in
lack of development in the literature. In this paper we introduce an MCMC
(fully Bayesian) method for learning severely multi-modal posteriors of
logistic regression models based on hyper-LASSO priors (non-convex penalties).
Our MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling
framework; we call our method Bayesian logistic regression with hyper-LASSO
(BLRHL) priors. We have used simulation studies and real data analysis to
demonstrate the superior performance of hyper-LASSO priors, and to investigate
the issues of choosing heaviness and scale of hyper-LASSO priors.
</summary>
    <author>
      <name>Longhai Li</name>
    </author>
    <author>
      <name>Weixin Yao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00949655.2018.1490418</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00949655.2018.1490418" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages. arXiv admin note: substantial text overlap with
  arXiv:1308.4690</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Statistical Computation and Simulation, 2018, 88:14,
  2827-2851</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.3319v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3319v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04584v1</id>
    <updated>2018-05-11T20:40:46Z</updated>
    <published>2018-05-11T20:40:46Z</published>
    <title>Robust Comparison of Kernel Densities on Spherical Domains</title>
    <summary>  While spherical data arises in many contexts, including in directional
statistics, the current tools for density estimation and population comparison
on spheres are quite limited. Popular approaches for comparing populations (on
Euclidean domains) mostly involvea two-step procedure: (1) estimate probability
density functions (pdfs) from their respective samples, most commonly using the
kernel density estimator, and, (2) compare pdfs using a metric such as the L2
norm. However, both the estimated pdfs and their differences depend heavily on
the chosen kernels, bandwidths, and sample sizes. Here we develop a framework
for comparing spherical populations that is robust to these choices.
Essentially, we characterize pdfs on spherical domains by quantifying their
smoothness. Our framework uses a spectral representation, with densities
represented by their coefficients with respect to the eigenfunctions of the
Laplacian operator on a sphere. The change in smoothness, akin to using
different kernel bandwidths, is controlled by exponential decays in coefficient
values. Then we derive a proper distance for comparing pdf coefficients while
equalizing smoothness levels, negating influences of sample size and bandwidth.
This signifies a fair and meaningful comparisons of populations, despite vastly
different sample sizes, and leads to a robust and improved performance. We
demonstrate this framework using examples of variables on S1 and S2, and
evaluate its performance using a number of simulations and real data
experiments.
</summary>
    <author>
      <name>Zhengwu Zhang</name>
    </author>
    <author>
      <name>Eric Klassen</name>
    </author>
    <author>
      <name>Anuj Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09305v3</id>
    <updated>2018-05-11T16:24:40Z</updated>
    <published>2017-03-27T20:47:25Z</published>
    <title>Implementing Monte Carlo Tests with P-value Buckets</title>
    <summary>  Software packages usually report the results of statistical tests using
p-values. Users often interpret these by comparing them to standard thresholds,
e.g. 0.1%, 1% and 5%, which is sometimes reinforced by a star rating (***, **,
*). We consider an arbitrary statistical test whose p-value p is not available
explicitly, but can be approximated by Monte Carlo samples, e.g. by bootstrap
or permutation tests. The standard implementation of such tests usually draws a
fixed number of samples to approximate p. However, the probability that the
exact and the approximated p-value lie on different sides of a threshold (the
resampling risk) can be high, particularly for p-values close to a threshold.
We present a method to overcome this. We consider a finite set of
user-specified intervals which cover [0,1] and which can be overlapping. We
call these p-value buckets. We present algorithms that, with arbitrarily high
probability, return a p-value bucket containing p. We prove that for both a
bounded resampling risk and a finite runtime, overlapping buckets need to be
employed, and that our methods both bound the resampling risk and guarantee a
finite runtime for such overlapping buckets. To interpret decisions with
overlapping buckets, we propose an extension of the star rating system. We
demonstrate that our methods are suitable for use in standard software,
including for low p-value thresholds occurring in multiple testing settings,
and that they can be computationally more efficient than standard
implementations.
</summary>
    <author>
      <name>Axel Gandy</name>
    </author>
    <author>
      <name>Georg Hahn</name>
    </author>
    <author>
      <name>Dong Ding</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09305v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09305v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0651v3</id>
    <updated>2018-05-11T15:32:01Z</updated>
    <published>2012-01-03T14:55:58Z</published>
    <title>On the relationship between the theory of cointegration and the theory
  of phase synchronization</title>
    <summary>  The theory of cointegration has been a leading theory in econometrics with
powerful applications to macroeconomics during the last decades. On the other
hand the theory of phase synchronization for weakly coupled complex oscillators
has been one of the leading theories in physics for many years with many
applications to different areas of science. For example, in neuroscience phase
synchronization is regarded as essential for functional coupling of different
brain regions. In an abstract sense both theories describe the dynamic
fluctuation around some equilibrium. In this paper, we point out that there
exists a very close connection between both theories. Apart from phase jumps, a
stochastic version of the Kuramoto equations can be approximated by a
cointegrated system of difference equations. As one consequence, the rich
theory on statistical inference for cointegrated systems can immediately be
applied for statistical inference on phase synchronization based on empirical
data. This includes tests for phase synchronization, tests for unidirectional
coupling and the identification of the equilibrium from data including phase
shifts. We study two examples on a unidirectionally coupled R\"ossler-Lorenz
system and on electrochemical oscillators. The methods from cointegration may
also be used to investigate phase synchronization in complex networks.
Conversely, there are many interesting results on phase synchronization which
may inspire new research on cointegration.
</summary>
    <author>
      <name>Rainer Dahlhaus</name>
    </author>
    <author>
      <name>István Z. Kiss</name>
    </author>
    <author>
      <name>Jan C. Neddermeyer</name>
    </author>
    <link href="http://arxiv.org/abs/1201.0651v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0651v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04421v1</id>
    <updated>2018-05-11T14:31:46Z</updated>
    <published>2018-05-11T14:31:46Z</published>
    <title>Covariate-Adjusted Tensor Classification in High-Dimensions</title>
    <summary>  In contemporary scientific research, it is of great interest to predict a
categorical response based on a high-dimensional tensor (i.e. multi-dimensional
array) and additional covariates. This mixture of different types of data leads
to challenges in statistical analysis. Motivated by applications in science and
engineering, we propose a comprehensive and interpretable discriminant analysis
model, called CATCH model (in short for Covariate-Adjusted Tensor
Classification in High-dimensions), which efficiently integrates the covariates
and the tensor to predict the categorical outcome. The CATCH model jointly
models the relationships among the covariates, the tensor predictor, and the
categorical response. More importantly, it preserves and utilizes the
structures of the data for maximum interpretability and optimal prediction. To
tackle the new computational and statistical challenges arising from the
intimidating tensor dimensions, we propose a penalized approach to select a
subset of tensor predictor entries that has direct discriminative effect after
adjusting for covariates. We further develop an efficient algorithm that takes
advantage of the tensor structure. Theoretical results confirm that our method
achieves variable selection consistency and optimal classification error, even
when the tensor dimension is much larger than the sample size. The superior
performance of our method over existing methods is demonstrated in extensive
simulated and real data examples.
</summary>
    <author>
      <name>Yuqing Pan</name>
    </author>
    <author>
      <name>Qing Mai</name>
    </author>
    <author>
      <name>Xin Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06210v2</id>
    <updated>2018-05-11T10:58:07Z</updated>
    <published>2017-04-20T16:16:31Z</published>
    <title>Computationally efficient methods for fitting mixed models to electronic
  health records data</title>
    <summary>  Motivated by two case studies using primary care records from the Clinical
Practice Research Datalink, we describe statistical methods that facilitate the
analysis of tall data, with very large numbers of observations. Our focus is on
investigating the association between patient characteristics and an outcome of
interest, while allowing for variation among general practices. We explore ways
to fit mixed effects models to tall data, including predictors of interest and
confounding factors as covariates, and including random intercepts to allow for
heterogeneity in outcome among practices. We introduce: (1) weighted regression
and (2) meta-analysis of estimated regression coefficients from each practice.
Both methods reduce the size of the dataset, thus decreasing the time required
for statistical analysis. We compare the methods to an existing subsampling
approach. All methods give similar point estimates, and weighted regression and
meta-analysis give similar standard errors for point estimates to analysis of
the entire dataset, but the subsampling method gives larger standard errors.
Where all data are discrete, weighted regression is equivalent to fitting the
mixed model to the entire dataset. In the presence of a continuous covariate,
meta-analysis is useful. Both methods are easy to implement in standard
statistical software.
</summary>
    <author>
      <name>Kirsty Rhodes</name>
    </author>
    <author>
      <name>Rebecca Turner</name>
    </author>
    <author>
      <name>Rupert Payne</name>
    </author>
    <author>
      <name>Ian White</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06210v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06210v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10766v2</id>
    <updated>2018-05-11T02:44:59Z</updated>
    <published>2018-03-28T06:22:46Z</published>
    <title>Repeated out of Sample Fusion in the Estimation of Small Tail
  Probabilities</title>
    <summary>  Often, it is required to estimate the probability that a quantity such as
toxicity level, plutonium, temperature, rainfall, damage, wind speed, wave
size, earthquake magnitude, risk, etc., exceeds an unsafe high threshold. The
probability in question is then very small. To estimate such a probability,
information is needed about large values of the quantity of interest. However,
in many cases, the data only contain values below or even far below the
designated threshold, let alone exceedingly large values. It is shown that by
repeated fusion of the data with externally generated random data, more
information about small tail probabilities is obtained with the aid of certain
new statistical functions. This provides relatively short, yet reliable
interval estimates based on moderately large samples. A comparison of the
approach with a method from extreme values theory (Peaks over Threshold, or
POT), using both artificial and real data, points to the merit of repeated out
of sample fusion.
</summary>
    <author>
      <name>Benjamin Kedem</name>
    </author>
    <author>
      <name>Lemeng Pan</name>
    </author>
    <author>
      <name>Paul Smith</name>
    </author>
    <author>
      <name>Chen Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1803.10766v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10766v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04203v1</id>
    <updated>2018-05-10T23:10:49Z</updated>
    <published>2018-05-10T23:10:49Z</published>
    <title>Robust Model-Based Clustering of Voting Records</title>
    <summary>  We explore the possibility of discovering extreme voting patterns in the U.S.
Congressional voting records by drawing ideas from the mixture of contaminated
normal distributions. A mixture of latent trait models via contaminated normal
distributions is proposed. We assume that the low dimensional continuous latent
variable comes from a contaminated normal distribution and, therefore, picks up
extreme patterns in the observed binary data while clustering. We consider in
particular such model for the analysis of voting records. The model is applied
to a U.S. Congressional Voting data set on 16 issues. Note this approach is the
first instance within the literature of a mixture model handling binary data
with possible extreme patterns.
</summary>
    <author>
      <name>Yang Tang</name>
    </author>
    <author>
      <name>Paul D. McNicholas</name>
    </author>
    <author>
      <name>Antonio Punzo</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04187v1</id>
    <updated>2018-05-10T21:58:21Z</updated>
    <published>2018-05-10T21:58:21Z</published>
    <title>Analysis of a Mode Clustering Diagram</title>
    <summary>  Mode-based clustering methods define clusters to be the basins of attraction
of the modes of a density estimate. The most common version is mean shift clus-
tering which uses a gradient ascent algorithm to find the basins. Rodriguez and
Laio (2014) introduced a new method that is faster and simpler than mean shift
clustering. Furthermore, they define a clustering diagram that provides a sim-
ple, two-dimensional summary of the mode clustering information. We study the
statistical properties of this diagram and we propose some improvements and
extensions. In particular, we show a connection between the diagram and robust
linear regression.
</summary>
    <author>
      <name>Isabella Verdinelli</name>
    </author>
    <author>
      <name>Larry Wasserman</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04164v1</id>
    <updated>2018-05-10T20:27:13Z</updated>
    <published>2018-05-10T20:27:13Z</published>
    <title>Bivariate Causal Discovery and its Applications to Gene Expression and
  Imaging Data Analysis</title>
    <summary>  The mainstream of research in genetics, epigenetics and imaging data analysis
focuses on statistical association or exploring statistical dependence between
variables. Despite their significant progresses in genetic research,
understanding the etiology and mechanism of complex phenotypes remains elusive.
Using association analysis as a major analytical platform for the complex data
analysis is a key issue that hampers the theoretic development of genomic
science and its application in practice. Causal inference is an essential
component for the discovery of mechanical relationships among complex
phenotypes. Many researchers suggest making the transition from association to
causation. Despite its fundamental role in science, engineering and
biomedicine, the traditional methods for causal inference require at least
three variables. However, quantitative genetic analysis such as QTL, eQTL,
mQTL, and genomic-imaging data analysis requires exploring the causal
relationships between two variables. This paper will focus on bivariate causal
discovery. We will introduce independence of cause and mechanism (ICM) as a
basic principle for causal inference, algorithmic information theory and
additive noise model (ANM) as major tools for bivariate causal discovery.
Large-scale simulations will be performed to evaluate the feasibility of the
ANM for bivariate causal discovery. To further evaluate their performance for
causal inference, the ANM will be applied to the construction of gene
regulatory networks. Also, the ANM will be applied to trait-imaging data
analysis to illustrate three scenarios: presence of both causation and
association, presence of association while absence of causation, and presence
of causation, while lack of association between two variables.
</summary>
    <author>
      <name>Rong Jiao</name>
    </author>
    <author>
      <name>Nan Lin</name>
    </author>
    <author>
      <name>Zixin Hu</name>
    </author>
    <author>
      <name>David A Bennett</name>
    </author>
    <author>
      <name>Li Jin</name>
    </author>
    <author>
      <name>Momiao Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01054v3</id>
    <updated>2018-05-10T18:09:01Z</updated>
    <published>2018-04-03T16:12:32Z</published>
    <title>Prediction intervals for random-effects meta-analysis: a confidence
  distribution approach</title>
    <summary>  For the inference of random-effects models in meta-analysis, the prediction
interval was proposed as a summary measure of the treatment effects that
explains the heterogeneity in the target population. While the
Higgins-Thompson-Spiegelhalter (HTS) plug-in-type prediction interval has been
widely used, in which the heterogeneity parameter is replaced with its point
estimate, its validity depends on a large sample approximation. Most
meta-analyses, however, include less than 20 studies. It has been revealed that
the validity of the HTS method is not assured under realistic situations, but
no solution to this problem has been proposed in literature. Therefore, in this
article, we describe our proposed prediction interval. Instead of using the
plug-in scheme, we developed a bootstrap approach using an exact confidence
distribution to account for the uncertainty in estimation of the heterogeneity
parameter. Compared to the HTS method, the proposed method provides an accurate
prediction interval that adequately explains the heterogeneity of treatment
effects and the statistical error. Simulation studies demonstrated that the HTS
method had poor coverage performance; by contrast, the coverage probabilities
for the proposed method satisfactorily retained the nominal level. Applications
to three published random-effects meta-analyses are presented.
</summary>
    <author>
      <name>Kengo Nagashima</name>
    </author>
    <author>
      <name>Hisashi Noma</name>
    </author>
    <author>
      <name>Toshi A. Furukawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/0962280218773520</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/0962280218773520" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, accepted by Statistical Methods in Medical
  Research</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01054v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01054v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04010v1</id>
    <updated>2018-05-10T14:58:58Z</updated>
    <published>2018-05-10T14:58:58Z</published>
    <title>A mixture autoregressive model based on Student's $t$-distribution</title>
    <summary>  A new mixture autoregressive model based on Student's $t$-distribution is
proposed. A key feature of our model is that the conditional $t$-distributions
of the component models are based on autoregressions that have multivariate
$t$-distributions as their (low-dimensional) stationary distributions. That
autoregressions with such stationary distributions exist is not immediate. Our
formulation implies that the conditional mean of each component model is a
linear function of past observations and the conditional variance is also time
varying. Compared to previous mixture autoregressive models our model may
therefore be useful in applications where the data exhibits rather strong
conditional heteroskedasticity. Our formulation also has the theoretical
advantage that conditions for stationarity and ergodicity are always met and
these properties are much more straightforward to establish than is common in
nonlinear autoregressive models. An empirical example employing a realized
kernel series based on S&amp;P 500 high-frequency data shows that the proposed
model performs well in volatility forecasting.
</summary>
    <author>
      <name>Mika Meitz</name>
    </author>
    <author>
      <name>Daniel Preve</name>
    </author>
    <author>
      <name>Pentti Saikkonen</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03611v2</id>
    <updated>2018-05-10T14:05:14Z</updated>
    <published>2017-11-09T21:40:27Z</published>
    <title>Robust inference on indirect causal effects</title>
    <summary>  Standard methods for inference about direct and indirect effects require
stringent no unmeasured confounding assumptions which often fail to hold in
practice, particularly in observational studies. The goal of this paper is to
introduce a new form of indirect effect, the population intervention indirect
effect (PIIE), that can be nonparametrically identified in the presence of an
unmeasured common cause of exposure and outcome. This new type of indirect
effect captures the extent to which the effect of exposure is mediated by an
intermediate variable under an intervention which fixes the component of
exposure directly influencing the outcome at its observed value. The PIIE is in
fact the indirect component of the population intervention effect, introduced
by Hubbard and Van der Laan (2008). Interestingly, our identification criterion
relaxes Judea Pearl's front-door criterion as it does not require no direct
effect of exposure not mediated by the intermediate variable. For inference, we
develop both parametric and semiparametric methods, including a doubly robust
semiparametric locally efficient estimator, that perform very well in
simulation studies. Finally, the proposed methods are used to measure the
effectiveness of monetary saving recommendations among women enrolled in a
maternal health program in Tanzania.
</summary>
    <author>
      <name>Isabel R. Fulcher</name>
    </author>
    <author>
      <name>Ilya Shpitser</name>
    </author>
    <author>
      <name>Stella Marealle</name>
    </author>
    <author>
      <name>Eric J. Tchetgen Tchetgen</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03611v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03611v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07039v2</id>
    <updated>2018-05-10T08:35:00Z</updated>
    <published>2017-10-19T08:40:24Z</published>
    <title>Causal inference for binary non-independent outcomes</title>
    <summary>  Causal inference on multiple non-independent outcomes raises serious
challenges, because multivariate techniques that properly account for the
outcome's dependence structure need to be considered. We focus on the case of
binary outcomes framing our discussion in the potential outcome approach to
causal inference. We define causal effects of treatment on joint outcomes
introducing the notion of product outcomes. We also discuss a decomposition of
the causal effect on product outcomes into intrinsic and extrinsic causal
effects, which respectively provide information on treatment effect on the
intrinsic (product) structure of the product outcomes and on the outcomes'
dependence structure. We propose a log-mean linear regression approach for
modeling the distribution of the potential outcomes, which is particularly
appealing because all the causal estimands of interest and the decomposition
into intrinsic and extrinsic causal effects can be easily derived by model
parameters. The method is illustrated in two randomized experiments concerning
(i) the effect of the administration of oral pre-surgery morphine on pain
intensity after surgery; and (ii) the effect of honey on nocturnal cough and
sleep difficulty associated with childhood upper respiratory tract infections.
</summary>
    <author>
      <name>Monia Lupparelli</name>
    </author>
    <author>
      <name>Alessandra Mattei</name>
    </author>
    <link href="http://arxiv.org/abs/1710.07039v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07039v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02740v5</id>
    <updated>2018-05-10T07:16:42Z</updated>
    <published>2016-08-09T09:30:37Z</published>
    <title>Bayesian nonparametric sparse VAR models</title>
    <summary>  In a high dimensional setting, vector autoregressive (VAR) models require a
large number of parameters to be estimated and suffer of inferential problems.
We propose a nonparametric Bayesian framework and introduce a new two-stage
hierarchical Dirichlet process prior (DPP) for VAR models. This prior allows us
to avoid overparametrization and overfitting issues by shrinking the
coefficients toward a small number of random locations and induces a random
partition of the coefficients, which is the main inference target of
nonparametric Bayesian models. We use the posterior random partition to cluster
coefficients into groups and to estimate the number of groups. Our
nonparametric Bayesian model with multiple shrinkage prior is well suited for
extracting Granger causality networks from time series, since it allows to
capture some common features of real-world networks, which are sparsity, blocks
or communities structures, heterogeneity and clustering in the strength or
intensity of the edges. In order to fully capture the richness of the data, it
is therefore crucial that the model used to extract network accounts for
weights associated to the edges. We illustrate the benefits of our approach by
extracting network structures from panel data for shock transmission in
business cycles and in financial markets. Empirical evidences show that our
methodology identifies the most relevant linkages between panel units and
clustering effects in the linkages intensity. Also we find that the centrality
of the nodes changes across intensity levels.
</summary>
    <author>
      <name>Monica Billio</name>
    </author>
    <author>
      <name>Roberto Casarin</name>
    </author>
    <author>
      <name>Luca Rossini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised Version of the paper "Bayesian nonparametric Seemingly
  Unrelated Regression Models" --- Supplementary Material available on request</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02740v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02740v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03807v1</id>
    <updated>2018-05-10T04:18:10Z</updated>
    <published>2018-05-10T04:18:10Z</published>
    <title>Structural Breaks in Time Series</title>
    <summary>  This chapter covers methodological issues related to estimation, testing and
computation for models involving structural changes. Our aim is to review
developments as they relate to econometric applications based on linear models.
Substantial advances have been made to cover models at a level of generality
that allow a host of interesting practical applications. These include models
with general stationary regressors and errors that can exhibit temporal
dependence and heteroskedasticity, models with trending variables and possible
unit roots and cointegrated models, among others. Advances have been made
pertaining to computational aspects of constructing estimates, their limit
distributions, tests for structural changes, and methods to determine the
number of changes present. A variety of topics are covered. The first part
summarizes and updates developments described in an earlier review, Perron
(2006), with the exposition following heavily that of Perron (2008). Additions
are included for recent developments: testing for common breaks, models with
endogenous regressors (emphasizing that simply using least-squares is
preferable over instrumental variables methods), quantile regressions, methods
based on Lasso, panel data models, testing for changes in forecast accuracy,
factors models and methods of inference based on a continuous records
asymptotic framework. Our focus is on the so-called off-line methods whereby
one wants to retrospectively test for breaks in a given sample of data and form
confidence intervals about the break dates. The aim is to provide the readers
with an overview of methods that are of direct usefulness in practice as
opposed to issues that are mostly of theoretical interest.
</summary>
    <author>
      <name>Alessandro Casini</name>
    </author>
    <author>
      <name>Pierre Perron</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03744v1</id>
    <updated>2018-05-09T22:13:33Z</updated>
    <published>2018-05-09T22:13:33Z</published>
    <title>Estimation Methods for Cluster Randomized Trials with Noncompliance: A
  Study of A Biometric Smartcard Payment System in India</title>
    <summary>  Many policy evaluations occur in settings with randomized assignment at the
cluster level and treatment noncompliance at the unit level. For example,
villagers or towns might be assigned to treatment and control, but residents
may choose to not comply with their assigned treatment status. For example, in
the state of Andhra Pradesh, the state government sought to evaluate the use of
biometric smartcards to deliver payments from antipoverty programs. Smartcard
payments were randomized at the village level, but residents could choose to
comply or not. In some villages, more than 90% of residents complied with the
treatment, while in other locations fewer than 15% of the residents complied.
When noncompliance is present, investigators may choose to focus attention on
either intention to treat effects or the causal effect among the units that
comply. When analysts focus on effects among compliers, the instrumental
variables framework can be used to evaluate identify causal effects. We first
review extant methods for instrumental variable estimators in clustered designs
which depend on assumptions that are often unrealistic in applied settings. In
response, we develop a method that allows for possible treatment effect
heterogeneity that is correlated with cluster size and uses finite sample
variance estimator. We evaluate these methods using a series of simulations and
apply them to data from an evaluation of welfare transfers via smartcard
payments in India.
</summary>
    <author>
      <name>Hyunseung Kang</name>
    </author>
    <author>
      <name>Luke Keele</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06661v2</id>
    <updated>2018-05-09T22:12:37Z</updated>
    <published>2017-07-20T18:03:42Z</published>
    <title>The Graphical Horseshoe Estimator for Inverse Covariance Matrices</title>
    <summary>  We develop a new estimator of the inverse covariance matrix for
high-dimensional multivariate normal data using the horseshoe prior. The
proposed graphical horseshoe estimator has attractive properties compared to
other popular estimators, such as the graphical lasso and graphical Smoothly
Clipped Absolute Deviation (SCAD). The most prominent benefit is that when the
true inverse covariance matrix is sparse, the graphical horseshoe provides
estimates with small information divergence from the true sampling
distribution. The posterior mean under the graphical horseshoe prior can also
be almost unbiased under certain conditions. In addition to these theoretical
results, we also provide a full Gibbs sampler for implementing our estimator.
MATLAB code is available for download from github at
http://github.com/liyf1988/GHS. The graphical horseshoe estimator compares
favorably to existing techniques in simulations and in a human gene network
data analysis.
</summary>
    <author>
      <name>Yunfan Li</name>
    </author>
    <author>
      <name>Bruce A. Craig</name>
    </author>
    <author>
      <name>Anindya Bhadra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06661v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06661v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04717v5</id>
    <updated>2018-05-09T18:49:21Z</updated>
    <published>2016-12-14T16:36:30Z</published>
    <title>Network cross-validation by edge sampling</title>
    <summary>  While many statistical models and methods are now available for network
analysis, resampling network data remains a challenging problem.
Cross-validation is a useful general tool for model selection and parameter
tuning, but is not directly applicable to networks since splitting network
nodes into groups requires deleting edges and destroys some of the network
structure. Here we propose a new network resampling strategy based on splitting
edges rather than nodes, applicable to both cross-validation and bootstrap for
a wide range of network model selection tasks. We provide a theoretical
justification for our method in a general setting and examples of how our
method can be used in specific network model selection and parameter tuning
tasks. Numerical results on simulated networks and on a citation network of
statisticians show that this cross-validation approach works well for model
selection.
</summary>
    <author>
      <name>Tianxi Li</name>
    </author>
    <author>
      <name>Elizaveta Levina</name>
    </author>
    <author>
      <name>Ji Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04717v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04717v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08418v2</id>
    <updated>2018-05-09T15:07:50Z</updated>
    <published>2017-06-26T14:46:45Z</published>
    <title>Nonseparable Multinomial Choice Models in Cross-Section and Panel Data</title>
    <summary>  Multinomial choice models are fundamental for empirical modeling of economic
choices among discrete alternatives. We analyze identification of binary and
multinomial choice models when the choice utilities are nonseparable in
observed attributes and multidimensional unobserved heterogeneity with
cross-section and panel data. We show that derivatives of choice probabilities
with respect to continuous attributes are weighted averages of utility
derivatives in cross-section models with exogenous heterogeneity. In the
special case of random coefficient models with an independent additive effect,
we further characterize that the probability derivative at zero is proportional
to the population mean of the coefficients. We extend the identification
results to models with endogenous heterogeneity using either a control function
or panel data. In time stationary panel models with two periods, we find that
differences over time of derivatives of choice probabilities identify utility
derivatives "on the diagonal," i.e. when the observed attributes take the same
values in the two periods. We also show that time stationarity does not
identify structural derivatives "off the diagonal" both in continuous and
multinomial choice panel models.
</summary>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Iván Fernández-Val</name>
    </author>
    <author>
      <name>Whitney Newey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08418v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08418v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03567v1</id>
    <updated>2018-05-09T14:55:33Z</updated>
    <published>2018-05-09T14:55:33Z</published>
    <title>Stochastic Modelling of Urban Structure</title>
    <summary>  The building of mathematical and computer models of cities has a long
history. The core elements are models of flows (spatial interaction) and the
dynamics of structural evolution. In this article, we develop a stochastic
model of urban structure to formally account for uncertainty arising from less
predictable events. Standard practice has been to calibrate the spatial
interaction models independently and to explore the dynamics through
simulation. We present two significant results that will be transformative for
both elements. First, we represent the structural variables through a single
potential function and develop stochastic differential equations (SDEs) to
model the evolution. Secondly, we show that the parameters of the spatial
interaction model can be estimated from the structure alone, independently of
flow data, using the Bayesian inferential framework. The posterior distribution
is doubly intractable and poses significant computational challenges that we
overcome using Markov chain Monte Carlo (MCMC) methods. We demonstrate our
methodology with a case study on the London retail system.
</summary>
    <author>
      <name>L. Ellam</name>
    </author>
    <author>
      <name>M. Girolami</name>
    </author>
    <author>
      <name>G. A. Pavliotis</name>
    </author>
    <author>
      <name>A. Wilson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rspa.2017.0700</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rspa.2017.0700" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://dx.doi.org/10.1098/rspa.2017.0700</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ellam L, Girolami M, Pavliotis GA,Wilson A. 2018 Stochastic
  modelling of urban structure. Proc. R. Soc. A 20170700</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.03567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02950v6</id>
    <updated>2018-05-09T14:54:28Z</updated>
    <published>2017-01-11T12:30:53Z</published>
    <title>Convex Mixture Regression for Quantitative Risk Assessment</title>
    <summary>  There is wide interest in studying how the distribution of a continuous
response changes with a predictor. We are motivated by environmental
applications in which the predictor is the dose of an exposure and the response
is a health outcome. A main focus in these studies is inference on dose levels
associated with a given increase in risk relative to a baseline. Popular
methods either dichotomize the continuous response or focus on modeling changes
with the dose in the expectation of the outcome. Such choices may lead to
information loss and provide inaccurate inference on dose-response
relationships. We instead propose a Bayesian convex mixture regression model
that allows the entire distribution of the health outcome to be unknown and
changing with the dose. To balance flexibility and parsimony, we rely on a
mixture model for the density at the extreme doses, and express the conditional
density at each intermediate dose via a convex combination of these extremal
densities. This representation generalizes classical dose-response models for
quantitative outcomes, and provides a more parsimonious, but still powerful,
formulation compared to nonparametric methods, thereby improving
interpretability and efficiency in inference on risk functions. A Markov chain
Monte Carlo algorithm for posterior inference is developed, and the benefits of
our methods are outlined in simulations, along with a study on the impact of
DDT exposure on gestational age.
</summary>
    <author>
      <name>Antonio Canale</name>
    </author>
    <author>
      <name>Daniele Durante</name>
    </author>
    <author>
      <name>David Dunson</name>
    </author>
    <link href="http://arxiv.org/abs/1701.02950v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02950v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03433v1</id>
    <updated>2018-05-09T09:29:00Z</updated>
    <published>2018-05-09T09:29:00Z</published>
    <title>Spatial Poisson Processes for Fatigue Crack Initiation</title>
    <summary>  In this work we propose a stochastic model for estimating the occurrence of
crack initiations on the surface of metallic specimens in fatigue problems that
can be applied to a general class of geometries. The stochastic model is based
on spatial Poisson processes with intensity function that combines stress-life
(S-N) curves with averaged effective stress, $\sigma_{{\mathrm{eff}}}^{\Delta}
(\mathbf{x})$, which is computed after solving numerically the linear
elasticity equations on the specimen domains using finite element methods.
Here, $\Delta$ is a parameter that characterizes the size of the neighbors
covering the domain boundary. The averaged effective stress, parameterized by
$\Delta$, maps the stress tensor to a scalar field upon the specimen domain.
Data from fatigue experiments on notched and unnotched sheet specimens of
75S-T6 aluminum alloys are used to calibrate the model parameters for the
individual data sets and for their combination. Bayesian and classical
approaches are applied to estimate the survival-probability function for any
specimen tested under a prescribed fatigue experimental setup. Our proposed
model can predict the remaining life of specimens from the same material with
new geometries.
</summary>
    <author>
      <name>Ivo Babuska</name>
    </author>
    <author>
      <name>Zaid Sawlan</name>
    </author>
    <author>
      <name>Marco Scavino</name>
    </author>
    <author>
      <name>Barna Szabó</name>
    </author>
    <author>
      <name>Raúl Tempone</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62N05, 74B05, 62M30, 60G55, 62N01, 62M05, 62P30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03373v1</id>
    <updated>2018-05-09T05:11:44Z</updated>
    <published>2018-05-09T05:11:44Z</published>
    <title>Interpretable Proximate Factors for Large Dimensions</title>
    <summary>  This paper approximates latent statistical factors with sparse and
easy-to-interpret proximate factors. Latent factors in a large-dimensional
factor model can be estimated by principal component analysis, but are usually
hard to interpret. By shrinking factor weights, we obtain proximate factors
that are easier to interpret. We show that proximate factors consisting of
5-10\% of the cross-section observations with the largest absolute loadings are
usually sufficient to almost perfectly replicate the population factors,
without assuming a sparse structure in loadings. We derive lower bounds for the
asymptotic exceedance probability of the generalized correlation between
proximate factors and population factors based on extreme value theory, thus
providing guidance on how to construct the proximate factors. Simulations and
empirical applications to financial single-sorted portfolios and macroeconomic
data illustrate that proximate factors approximate latent factors well while
being interpretable.
</summary>
    <author>
      <name>Markus Pelger</name>
    </author>
    <author>
      <name>Ruoxuan Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03353v1</id>
    <updated>2018-05-09T02:35:52Z</updated>
    <published>2018-05-09T02:35:52Z</published>
    <title>Nonparametric Estimation of Conditional Expectation with Auxiliary
  Information and Dimension Reduction</title>
    <summary>  Nonparametric estimation of the conditional expectation $E(Y | U)$ of an
outcome $Y$ given a covariate vector $U$ is of primary importance in many
statistical applications such as prediction and personalized medicine. In some
problems, there is an additional auxiliary variable $Z$ in the training dataset
used to construct estimators, but $Z$ is not available for future prediction or
selecting patient treatment in personalized medicine. For example, in the
training dataset longitudinal outcomes are observed, but only the last outcome
$Y$ is concerned in the future prediction or analysis. The longitudinal
outcomes other than the last point is then the variable $Z$ that is observed
and related with both $Y$ and $U$. Previous work on how to make use of $Z$ in
the estimation of $E(Y|U)$ mainly focused on using $Z$ in the construction of a
linear function of $U$ to reduce covariate dimension for better estimation.
Using $E(Y|U) = E\{E(Y|U, Z)| U\}$, we propose a two-step estimation of inner
and outer expectations, respectively, with sufficient dimension reduction for
kernel estimation in both steps. The information from $Z$ is utilized not only
in dimension reduction, but also directly in the estimation. Because of the
existence of different ways for dimension reduction, we construct two
estimators that may improve the estimator without using $Z$. The improvements
are shown in the convergence rate of estimators as the sample size increases to
infinity as well as in the finite sample simulation performance. A real data
analysis about the selection of mammography intervention is presented for
illustration.
</summary>
    <author>
      <name>Bingying Xie</name>
    </author>
    <author>
      <name>Jun Shao</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03336v1</id>
    <updated>2018-05-09T01:01:52Z</updated>
    <published>2018-05-09T01:01:52Z</published>
    <title>Semiparametric multivariate D-vine time series model</title>
    <summary>  This paper proposes a novel semiparametric multivariate D-vine time series
model (mDvine) that enables the simultaneous copula-based modeling of both
temporal and cross-sectional dependence for multivariate time series. To
construct the mDvine, we first build a semiparametric univariate D-vine time
series model (uDvine) based on a D-vine. The uDvine generalizes the existing
first-order copula-based Markov chain models to Markov chains of an
arbitrary-order. Building upon the uDvine, we then construct the mDvine by
joining multiple uDvines via another parametric copula. As a simple and
tractable model, the mDvine provides flexible models for marginal behavior of
time series and can also generate sophisticated temporal and cross-sectional
dependence structures. Probabilistic properties of both the uDvine and mDvine
are studied in detail. Furthermore, robust and computationally efficient
procedures, including a sequential model selection method and a two-stage MLE,
are proposed for model estimation and inference, and their statistical
properties are investigated. Numerical experiments are conducted to demonstrate
the flexibility of the mDvine, and to examine the performance of the sequential
model selection procedure and the two-stage MLE. Real data applications on the
Australian electricity price and the Ireland wind speed data demonstrate the
superior performance of the mDvine to traditional multivariate time series
models.
</summary>
    <author>
      <name>Zifeng Zhao</name>
    </author>
    <author>
      <name>Peng Shi</name>
    </author>
    <author>
      <name>Zhengjun Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03317v1</id>
    <updated>2018-05-08T23:17:01Z</updated>
    <published>2018-05-08T23:17:01Z</published>
    <title>Subsampling Sequential Monte Carlo for Static Bayesian Models</title>
    <summary>  Our article shows how to carry out Bayesian inference by combining data
subsampling with Sequential Monte Carlo (SMC). This takes advantage of the
attractive properties of SMC for Bayesian computations with the ability of
subsampling to tackle big data problems. SMC sequentially updates a cloud of
particles through a sequence of densities, beginning with a density that is
easy to sample from such as the prior and ending with the posterior density.
Each update of the particle cloud consists of three steps: reweighting,
resampling, and moving. In the move step, each particle is moved using a Markov
kernel and this is typically the most computationally expensive part,
particularly when the dataset is large. It is crucial to have an efficient move
step to ensure particle diversity. Our article makes two important
contributions. First, in order to speed up the SMC computation, we use an
approximately unbiased and efficient annealed likelihood estimator based on
data subsampling. The subsampling approach is more memory efficient than the
corresponding full data SMC, which is a great advantage for parallel
computation. Second, we use a Metropolis within Gibbs kernel with two
conditional updates. First, a Hamiltonian Monte Carlo update makes distant
moves for the model parameters. Second, a block pseudo-marginal proposal is
used for the particles corresponding to the auxiliary variables for the data
subsampling. We demonstrate the usefulness of the methodology using two large
datasets.
</summary>
    <author>
      <name>David Gunawan</name>
    </author>
    <author>
      <name>Robert Kohn</name>
    </author>
    <author>
      <name>Matias Quiroz</name>
    </author>
    <author>
      <name>Khue-Dung Dang</name>
    </author>
    <author>
      <name>Minh-Ngoc Tran</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03316v1</id>
    <updated>2018-05-08T23:08:46Z</updated>
    <published>2018-05-08T23:08:46Z</published>
    <title>Extremal properties of the extended skew-normal distribution</title>
    <summary>  The skew-normal and related families are flexible and asymmetric parametric
models suitable for modelling a diverse range of systems. We focus on the
highly flexible extended skew-normal distribution, and consider when interest
is in the extreme values that it can produce. We derive the well-known Mills'
inequalities and ratio for the univariate extended skew-normal distribution and
establish the asymptotic extreme value distribution for the maxima of samples
drawn from this distribution. We show that the multivariate maximum of a
high-dimensional extended skew-normal random sample has asymptotically
independent components and derive the speed of convergence of the joint tail.
To describe the possible dependence among the components of the multivariate
maximum, we show that under appropriate conditions an approximate multivariate
extreme-value distribution that leads to a rich dependence structure can be
derived.
</summary>
    <author>
      <name>Boris Beranger</name>
    </author>
    <author>
      <name>Simone A. Padoan</name>
    </author>
    <author>
      <name>Yangfan Xu</name>
    </author>
    <author>
      <name>Scott A. Sisson</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03288v1</id>
    <updated>2018-05-08T21:07:29Z</updated>
    <published>2018-05-08T21:07:29Z</published>
    <title>Fused Density Estimation: Theory and Methods</title>
    <summary>  In this paper we introduce a method for nonparametric density estimation on
geometric networks. We define fused density estimators as solutions to a total
variation regularized maximum-likelihood density estimation problem. We provide
theoretical support for fused density estimation by proving that the squared
Hellinger rate of convergence for the estimator achieves the minimax bound over
univariate densities of log-bounded variation. We reduce the original
variational formulation in order to transform it into a tractable,
finite-dimensional quadratic program. Because random variables on geometric
networks are simple generalizations of the univariate case, this method also
provides a useful tool for univariate density estimation. Lastly, we apply this
method and assess its performance on examples in the univariate and geometric
network setting. We compare the performance of different optimization
techniques to solve the problem, and use these results to inform
recommendations for the computation of fused density estimators.
</summary>
    <author>
      <name>Robert Bassett</name>
    </author>
    <author>
      <name>James Sharpnack</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G07, 62G20, 90C35, 49M37" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09606v2</id>
    <updated>2018-05-08T20:18:17Z</updated>
    <published>2017-09-27T16:24:27Z</published>
    <title>Bayesian Dynamic Tensor Regression</title>
    <summary>  Multidimensional arrays (i.e. tensors) of data are becoming increasingly
available and call for suitable econometric tools. We propose a new dynamic
linear regression model for tensor-valued response variables and covariates
that encompasses some well known multivariate models such as SUR, VAR, VECM,
panel VAR and matrix regression models as special cases. For dealing with the
over-parametrization and over-fitting issues due to the curse of
dimensionality, we exploit a suitable parametrization based on the parallel
factor (PARAFAC) decomposition which enables to achieve both parameter
parsimony and to incorporate sparsity effects. Our contribution is twofold:
first, we provide an extension of multivariate econometric models to account
for both tensor-variate response and covariates; second, we show the
effectiveness of proposed methodology in defining an autoregressive process for
time-varying real economic networks. Inference is carried out in the Bayesian
framework combined with Monte Carlo Markov Chain (MCMC). We show the efficiency
of the MCMC procedure on simulated datasets, with different size of the
response and independent variables, proving computational efficiency even with
high-dimensions of the parameter space. Finally, we apply the model for
studying the temporal evolution of real economic networks.
</summary>
    <author>
      <name>Monica Billio</name>
    </author>
    <author>
      <name>Roberto Casarin</name>
    </author>
    <author>
      <name>Sylvia Kaufmann</name>
    </author>
    <author>
      <name>Matteo Iacopini</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09606v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09606v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06903v4</id>
    <updated>2018-05-08T15:22:41Z</updated>
    <published>2016-07-23T09:05:00Z</published>
    <title>Asymptotic Properties of Approximate Bayesian Computation</title>
    <summary>  Approximate Bayesian computation allows for statistical analysis in models
with intractable likelihoods. In this paper we consider the asymptotic
behaviour of the posterior distribution obtained by this method. We give
general results on the rate at which the posterior distribution concentrates on
sets containing the true parameter, its limiting shape, and the asymptotic
distribution of the posterior mean. These results hold under given rates for
the tolerance used within the method, mild regularity conditions on the summary
statistics, and a condition linked to identification of the true parameters.
Implications for practitioners are discussed.
</summary>
    <author>
      <name>David T. Frazier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Monash University</arxiv:affiliation>
    </author>
    <author>
      <name>Gael M. Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Monash University</arxiv:affiliation>
    </author>
    <author>
      <name>Christian P. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Université Paris-Dauphine PSL and University of Warwick, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Judith Rousseau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Oxford, UK</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This 31 pages paper is a revised version of the paper, including
  supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06903v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06903v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02993v1</id>
    <updated>2018-05-08T13:12:26Z</updated>
    <published>2018-05-08T13:12:26Z</published>
    <title>Bayesian models in geographic profiling</title>
    <summary>  We consider the problem of geographic profiling and offer an approach to
choosing a suitable model for each offender. Based on the analysis of the
examined dataset, we divide offenders into several types with similar behavior.
According to the spatial distribution of the offender's crime sites, each new
criminal is assigned to the corresponding group. Then we choose an appropriate
model for the offender and using Bayesian methods we determine the posterior
distribution for the criminal's anchor point. Our models include
directionality, similar to models of Mohler and Short (2012). Our approach also
provides a way to incorporate two possible situations into the model - when the
criminal is a resident or a non-resident. We test this methodology on a real
data set of offenders from Baltimore County and compare the results with
Rossmo's approach. Our approach leads to substantial improvement over Rossmo's
method, especially in the presence of non-residents.
</summary>
    <author>
      <name>Jana Svobodová</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05341v2</id>
    <updated>2018-05-08T12:49:18Z</updated>
    <published>2017-08-17T15:52:22Z</published>
    <title>An Approximate Likelihood Perspective on ABC Methods</title>
    <summary>  We are living in the big data era, as current technologies and networks allow
for the easy and routine collection of data sets in different disciplines.
Bayesian Statistics offers a flexible modeling approach which is attractive for
describing the complexity of these datasets. These models often exhibit a
likelihood function which is intractable due to the large sample size, high
number of parameters, or functional complexity. Approximate Bayesian
Computational (ABC) methods provides likelihood-free methods for performing
statistical inferences with Bayesian models defined by intractable likelihood
functions. The vastity of the literature on ABC methods created a need to
review and relate all ABC approaches so that scientists can more readily
understand and apply them for their own work. This article provides a unifying
review, general representation, and classification of all ABC methods from the
view of approximate likelihood theory. This clarifies how ABC methods can be
characterized, related, combined, improved, and applied for future research.
Possible future research in ABC is then suggested.
</summary>
    <author>
      <name>George Karabatsos</name>
    </author>
    <author>
      <name>Fabrizio Leisen</name>
    </author>
    <link href="http://arxiv.org/abs/1708.05341v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05341v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02898v1</id>
    <updated>2018-05-08T08:39:35Z</updated>
    <published>2018-05-08T08:39:35Z</published>
    <title>Simulation Study on Local Influence Diagnosis for Poisson Mixed-Effect
  Linear Model</title>
    <summary>  Given that hierarchical count data in many fields are not
Normally-distributed and include random effects, this paper extends the
Generalized Linear Mixed Models (GLMMs) into Poisson Mixed-Effect Linear Model
(PMELM) and do numerical simulation experiments to verify the approach proposed
by Rakhmawati et al. (2016) in detecting outliers. This paper produces random
data based on epilepsy longitudinal data in Thall and Vail (1990), use six ways
to contaminate it and try to use code mentioned in supplementary materials in
previous research to detect the man-made outlier. Output shows that this method
is effective sometimes but does not always work, this is probably because of
the limitation of coding or some other reasons. Even though the data set and
local influence method has been researched and analyzed extensively in previous
papers, this paper makes contributions in data visualization. Figures in this
paper show the effect of each influencial component, which are clearer than the
original output in R and SAS.
</summary>
    <author>
      <name>N. Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Acknowledgement: Financial support from "2017 Shanghai University
  Students Innovation and Entrepreneurship Training Program Model School",
  Shanghai Education Commission</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06350v2</id>
    <updated>2018-05-08T07:33:31Z</updated>
    <published>2018-02-18T08:46:22Z</published>
    <title>Spatial modelling with R-INLA: A review</title>
    <summary>  Coming up with Bayesian models for spatial data is easy, but performing
inference with them can be challenging. Writing fast inference code for a
complex spatial model with realistically-sized datasets from scratch is
time-consuming, and if changes are made to the model, there is little guarantee
that the code performs well. The key advantages of R-INLA are the ease with
which complex models can be created and modified, without the need to write
complex code, and the speed at which inference can be done even for spatial
problems with hundreds of thousands of observations.
  R-INLA handles latent Gaussian models, where fixed effects, structured and
unstructured Gaussian random effects are combined linearly in a linear
predictor, and the elements of the linear predictor are observed through one or
more likelihoods. The structured random effects can be both standard areal
model such as the Besag and the BYM models, and geostatistical models from a
subset of the Mat\'ern Gaussian random fields. In this review, we discuss the
large success of spatial modelling with R-INLA and the types of spatial models
that can be fitted, we give an overview of recent developments for areal
models, and we give an overview of the stochastic partial differential equation
(SPDE) approach and some of the ways it can be extended beyond the assumptions
of isotropy and separability. In particular, we describe how slight changes to
the SPDE approach leads to straight-forward approaches for non-stationary
spatial models and non-separable space-time models.
</summary>
    <author>
      <name>Haakon Bakka</name>
    </author>
    <author>
      <name>Håvard Rue</name>
    </author>
    <author>
      <name>Geir-Arne Fuglstad</name>
    </author>
    <author>
      <name>Andrea Riebler</name>
    </author>
    <author>
      <name>David Bolin</name>
    </author>
    <author>
      <name>Elias Krainski</name>
    </author>
    <author>
      <name>Daniel Simpson</name>
    </author>
    <author>
      <name>Finn Lindgren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extensive update, restructuring of sections</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06350v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06350v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02826v1</id>
    <updated>2018-05-08T04:14:25Z</updated>
    <published>2018-05-08T04:14:25Z</published>
    <title>Optimal Subspace Estimation Using Overidentifying Vectors via
  Generalized Method of Moments</title>
    <summary>  Many statistical models seek relationship between variables via subspaces of
reduced dimensions. For instance, in factor models, variables are roughly
distributed around a low dimensional subspace determined by the loading matrix;
in mixed linear regression models, the coefficient vectors for different
mixtures form a subspace that captures all regression functions; in multiple
index models, the effect of covariates is summarized by the effective dimension
reduction space.
  Such subspaces are typically unknown, and good estimates are crucial for data
visualization, dimension reduction, diagnostics and estimation of unknown
parameters. Usually, we can estimate these subspaces by computing moments from
data. Often, there are many ways to estimate a subspace, by using moments of
different orders, transformed moments, etc. A natural question is: how can we
combine all these moment conditions and achieve optimality for subspace
estimation?
  In this paper, we formulate our problem as estimation of an unknown subspace
$\mathcal{S}$ of dimension $r$, given a set of overidentifying vectors $\{
\mathrm{\bf v}_\ell \}_{\ell=1}^m$ (namely $m \ge r$) that satisfy $\mathbb{E}
\mathrm{\bf v}_{\ell} \in \mathcal{S}$ and have the form $$ \mathrm{\bf v}_\ell
= \frac{1}{n} \sum_{i=1}^n \mathrm{\bf f}_\ell(\mathbf{x}_i, y_i), $$ where
data are i.i.d. and each function $\mathrm{\bf f}_\ell$ is known. By exploiting
certain covariance information related to $\mathrm{\bf v}_\ell$, our estimator
of $\mathcal{S}$ uses an optimal weighting matrix and achieves the smallest
asymptotic error, in terms of canonical angles. The analysis is based on the
generalized method of moments that is tailored to our problem. Our method is
applied to aforementioned models and distributed estimation of heterogeneous
datasets, and may be potentially extended to analyze matrix completion, neural
nets, among others.
</summary>
    <author>
      <name>Jianqing Fan</name>
    </author>
    <author>
      <name>Yiqiao Zhong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08511v2</id>
    <updated>2018-05-07T23:11:55Z</updated>
    <published>2017-10-23T21:33:31Z</published>
    <title>An Expectation Maximization Framework for Preferential Attachment Models</title>
    <summary>  In this paper we develop an Expectation Maximization(EM) algorithm to
estimate the parameter of a Yule-Simon distribution. The Yule-Simon
distribution exhibits the "rich get richer" effect whereby an 80-20 type of
rule tends to dominate. These distributions are ubiquitous in industrial
settings. The EM algorithm presented provides both frequentist and Bayesian
estimates of the $\lambda$ parameter. By placing the estimation method within
the EM framework we are able to derive Standard errors of the resulting
estimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and
study the rate of convergence. An explicit, closed form solution for the rate
of convergence of the algorithm is given.
</summary>
    <author>
      <name>Lucas Roberts</name>
    </author>
    <author>
      <name>Denisa Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.08511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07358v2</id>
    <updated>2018-05-07T22:40:33Z</updated>
    <published>2016-06-23T16:09:36Z</published>
    <title>Selection by Partitioning the Solution Paths</title>
    <summary>  The performance of penalized likelihood approaches depends profoundly on the
selection of the tuning parameter; however, there is no commonly agreed-upon
criterion for choosing the tuning parameter. Moreover, penalized likelihood
estimation based on a single value of the tuning parameter suffers from several
drawbacks. This article introduces a novel approach for feature selection based
on the entire solution paths rather than the choice of a single tuning
parameter, which significantly improves the accuracy of the selection.
Moreover, the approach allows for feature selection using ridge or other
strictly convex penalties. The key idea is to classify variables as relevant or
irrelevant at each tuning parameter and then to select all of the variables
which have been classified as relevant at least once. We establish the
theoretical properties of the method, which requires significantly weaker
conditions than existing methods in the literature. We also illustrate the
advantages of the proposed approach with simulation studies and a data example.
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Peng Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07358v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07358v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03775v4</id>
    <updated>2018-05-07T15:41:02Z</updated>
    <published>2016-07-13T14:46:46Z</published>
    <title>Causal inference to formalize responsibility analyses in road safety
  epidemiology</title>
    <summary>  The last few decades have seen the Structural Causal Model framework provide
valuable tools to assess causal effects from observational data. In this
article, we briefly review recent results regarding the recoverability of
causal effects from selection biased data, and apply them to the case of
responsibility analyses in road safety epidemiology. Our objective is to
formally determine whether causal effects can be unbiasedly estimated through
this type of analyses, when available data are restricted to severe accidents,
as it is commonly the case in practice. However, because speed has a direct
effect on the severity of the accident, we show that the causal odds-ratio of
exposures that influence speed, such as alcohol, is not estimable. We present
numerical results to illustrate our arguments, the magnitude of the bias and to
discuss some recent results from real data.
</summary>
    <author>
      <name>Marine Dufournet</name>
    </author>
    <author>
      <name>Emilie Lanoy</name>
    </author>
    <author>
      <name>Jean-Louis Martin</name>
    </author>
    <author>
      <name>Vivian Viallon</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03775v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03775v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08727v2</id>
    <updated>2018-05-07T14:51:11Z</updated>
    <published>2018-02-23T20:30:44Z</published>
    <title>Bayesian Semiparametric Functional Mixed Models for Serially Correlated
  Functional Data, with Application to Glaucoma Data</title>
    <summary>  Glaucoma, a leading cause of blindness, is characterized by optic nerve
damage related to intraocular pressure (IOP), but its full etiology is unknown.
Researchers at UAB have devised a custom device to measure scleral strain
continuously around the eye under fixed levels of IOP, which here is used to
assess how strain varies around the posterior pole, with IOP, and across
glaucoma risk factors such as age. The hypothesis is that scleral strain
decreases with age, which could alter biomechanics of the optic nerve head and
cause damage that could eventually lead to glaucoma. To evaluate this
hypothesis, we adapted Bayesian Functional Mixed Models to model these complex
data consisting of correlated functions on spherical scleral surface, with
nonparametric age effects allowed to vary in magnitude and smoothness across
the scleral surface, multi-level random effect functions to capture
within-subject correlation, and functional growth curve terms to capture serial
correlation across IOPs that can vary around the scleral surface. Our method
yields fully Bayesian inference on the scleral surface or any aggregation or
transformation thereof, and reveals interesting insights into the biomechanical
etiology of glaucoma. The general modeling framework described is very flexible
and applicable to many complex, high-dimensional functional data.
</summary>
    <author>
      <name>Wonyul Lee</name>
    </author>
    <author>
      <name>Michelle F. Miranda</name>
    </author>
    <author>
      <name>Phlip Rausch</name>
    </author>
    <author>
      <name>Veerbhadran Baladandayuthapani</name>
    </author>
    <author>
      <name>Massimo Fazio</name>
    </author>
    <author>
      <name>J. Crawford Downs</name>
    </author>
    <author>
      <name>Jeffrey S. Morris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper accepted in Journal of the American Statistical Association,
  2018 -- to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08727v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08727v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02547v1</id>
    <updated>2018-05-07T14:38:30Z</updated>
    <published>2018-05-07T14:38:30Z</published>
    <title>Learning Gene Regulatory Networks with High-Dimensional Heterogeneous
  Data</title>
    <summary>  The Gaussian graphical model is a widely used tool for learning gene
regulatory networks with high-dimensional gene expression data. Most existing
methods for Gaussian graphical models assume that the data are homogeneous,
i.e., all samples are drawn from a single Gaussian distribution. However, for
many real problems, the data are heterogeneous, which may contain some
subgroups or come from different resources. This paper proposes to model the
heterogeneous data using a mixture Gaussian graphical model, and apply the
imputation-consistency algorithm, combining with the $\psi$-learning algorithm,
to estimate the parameters of the mixture model and cluster the samples to
different subgroups. An integrated Gaussian graphical network is learned across
the subgroups along with the iterations of the imputation-consistency
algorithm. The proposed method is compared with an existing method for learning
mixture Gaussian graphical models as well as a few other methods developed for
homogeneous data, such as graphical Lasso, nodewise regression and
$\psi$-learning. The numerical results indicate superiority of the proposed
method in all aspects of parameter estimation, cluster identification and
network construction. The numerical results also indicate generality of the
proposed method: it can be applied to homogeneous data without significant
harms.
</summary>
    <author>
      <name>Bochao Jia</name>
    </author>
    <author>
      <name>Faming Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08007v2</id>
    <updated>2018-05-07T14:28:47Z</updated>
    <published>2018-01-24T14:50:49Z</published>
    <title>Financial density forecasts: A comprehensive comparison of risk-neutral
  and historical schemes</title>
    <summary>  We investigate the forecasting ability of the most commonly used benchmarks
in financial economics. We approach the usual caveats of probabilistic
forecasts studies -small samples, limited models and non-holistic validations-
by performing a comprehensive comparison of 15 predictive schemes during a time
period of over 21 years. All densities are evaluated in terms of their
statistical consistency, local accuracy and forecasting errors. Using a new
composite indicator, the Integrated Forecast Score (IFS), we show that
risk-neutral densities outperform historical-based predictions in terms of
information content. We find that the Variance Gamma model generates the
highest out-of-sample likelihood of observed prices and the lowest predictive
errors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts
across the entire density range. In contrast, lognormal densities, the Heston
model or the Breeden-Litzenberger formula yield biased predictions and are
rejected in statistical tests.
</summary>
    <author>
      <name>Ricardo Crisostomo</name>
    </author>
    <author>
      <name>Lorena Couso</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/for.2521</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/for.2521" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Forecasting, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.08007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11217v3</id>
    <updated>2018-05-07T13:36:26Z</updated>
    <published>2017-10-30T19:59:57Z</published>
    <title>Location-adjusted Wald statistics for scalar parameters</title>
    <summary>  Inference about a scalar parameter of interest is a core statistical task
that has attracted immense research in statistics. The Wald statistic is a
prime candidate for the task, on the grounds of the asymptotic validity of the
standard normal approximation to its finite-sample distribution, simplicity and
low computational cost. It is well known, though, that this normal
approximation can be inadequate, especially when the sample size is small or
moderate relative to the number of parameters. We propose a novel, algebraic
adjustment to the Wald statistic that can deliver significant improvements in
inferential performance with only small computational overhead, predominantly
due to additional matrix multiplications. The Wald statistic is viewed as an
estimate of a transformation of the model parameters and is appropriately
adjusted, using either maximum likelihood or reduced-bias estimators, bringing
its expectation asymptotically closer to zero. The location adjustment depends
on the expected information, an approximation of the bias of the estimator, and
the derivatives of the transformation, which are all either readily available
or easily obtainable in standard software for a wealth of models. Ample
analytical and numerical evidence and case-studies in prominent inferential
settings, including logistic regression in the presence of nuisance parameters
and analysis of multiple sclerosis lesions from MRI data, support our
recommendation of adopting location-adjusted Wald statistics in statistical
modelling.
</summary>
    <author>
      <name>C. Di Caterina</name>
    </author>
    <author>
      <name>I. Kosmidis</name>
    </author>
    <link href="http://arxiv.org/abs/1710.11217v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11217v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F05, 62F03, 62J02, 62J12, 62P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02407v1</id>
    <updated>2018-05-07T09:02:40Z</updated>
    <published>2018-05-07T09:02:40Z</published>
    <title>Soft Maximin Aggregation of Heterogeneous Array Data</title>
    <summary>  The extraction of a common signal across many recordings is difficult when
each recording -- in addition to the signal -- contains large, unique variation
components. This is observed for voltage sensitive dye imaging (VDSI), an
imaging technique used to measure neuronal activity, for which the resulting 3D
array data have a highly heterogeneous noise structure. Maximin aggregation
(magging) has previously been proposed as a robust estimation method in the
presence of heterogeneous noise. We propose soft maximin aggregation as a
general methodology for estimating a common signal from heterogeneous data. The
soft maximin loss is introduced as an aggregation of explained variances, and
the estimator is obtained by minimizing the penalized soft maximin loss. For a
convex penalty we show convergence of a proximal gradient based algorithm, and
we demonstrate how tensor structures for array data can be exploited by this
algorithm to achieve time and memory efficiency. An implementation is provided
in the R package SMMA available from CRAN. We demonstrate that soft maximin
aggregation performs well on a VSDI data set with 275 recordings, for which
magging does not work.
</summary>
    <author>
      <name>Adam Lund</name>
    </author>
    <author>
      <name>Søren Wengel Mogensen</name>
    </author>
    <author>
      <name>Niels Richard Hansen</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02306v1</id>
    <updated>2018-05-07T01:12:12Z</updated>
    <published>2018-05-07T01:12:12Z</published>
    <title>Semi-Orthogonal Non-Negative Matrix Factorization</title>
    <summary>  Non-negative Matrix Factorization (NMF) is a popular clustering and dimension
reduction method by decomposing a non-negative matrix into the product of two
lower dimension matrices composed of basis vectors. In this paper, we propose a
semi-orthogonal NMF method that enforces one of the matrices to be orthogonal
with mixed signs, thereby guarantees the rank of the factorization. Our method
preserves strict orthogonality by implementing the Cayley transformation to
force the solution path to be exactly on the Stiefel manifold, as opposed to
the approximated orthogonality solutions in existing literature. We apply a
line search update scheme along with an SVD-based initialization which produces
a rapid convergence of the algorithm compared to other existing approaches. In
addition, we present formulations of our method to incorporate both continuous
and binary design matrices. Through various simulation studies, we show that
our model has an advantage over other NMF variations regarding the accuracy of
the factorization, rate of convergence, and the degree of orthogonality while
being computationally competitive. We also apply our method to a text-mining
data on classifying triage notes, and show the effectiveness of our model in
reducing classification error compared to the conventional bag-of-words model
and other alternative matrix factorization approaches.
</summary>
    <author>
      <name>Jack Yutong Li</name>
    </author>
    <author>
      <name>Ruoqing Zhu</name>
    </author>
    <author>
      <name>Annie Qu</name>
    </author>
    <author>
      <name>Han Ye</name>
    </author>
    <author>
      <name>Zhankun Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09583v2</id>
    <updated>2018-05-06T12:55:54Z</updated>
    <published>2017-09-27T15:35:17Z</published>
    <title>Inference for Impulse Responses under Model Uncertainty</title>
    <summary>  In many macroeconomic applications, impulse responses and their frequentist
confidence intervals are constructed by estimating a VAR model in levels - thus
ignoring uncertainty regarding the true (unknown) cointegration rank. In this
paper we investigate the consequences of ignoring this uncertainty. We adapt
several proposed methods for handling model uncertainty to perform inference in
cointegrated VAR models and highlight their shortcomings in the present
setting. Therefore, we propose a new method - Weighted Inference by Model
Plausibility (WIMP) - that takes rank uncertainty into account in a fully
data-driven way. In a simulation study the WIMP method outperforms all other
methods considered, delivering intervals that are robust to rank uncertainty,
yet not overly conservative. We also study the potential ramifications of rank
uncertainty on applied macroeconomic analysis by re-assessing the effects of
fiscal policy shocks based on a variety of identification schemes. We
demonstrate how sensitive the results are to the treatment of the cointegration
rank, and show how formally accounting for rank uncertainty can affect the
conclusions.
</summary>
    <author>
      <name>Lenard Lieb</name>
    </author>
    <author>
      <name>Stephan Smeekes</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09583v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09583v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01505v4</id>
    <updated>2018-05-05T17:53:12Z</updated>
    <published>2017-05-03T16:44:04Z</published>
    <title>Introduction to finite mixtures</title>
    <summary>  Mixture models have been around for over 150 years, as an intuitively simple
and practical tool for enriching the collection of probability distributions
available for modelling data. In this chapter we describe the basic ideas of
the subject, present several alternative representations and perspectives on
these models, and discuss some of the elements of inference about the unknowns
in the models. Our focus is on the simplest set-up, of finite mixture models,
but we discuss also how various simplifying assumptions can be relaxed to
generate the rich landscape of modelling and inference ideas traversed in the
rest of this book.
</summary>
    <author>
      <name>Peter J. Green</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures, A chapter prepared for the forthcoming Handbook
  of Mixture Analysis. V2 corrects a small but important typographical error,
  and makes other minor edits; V3 makes further minor corrections and updates
  following review; V4 corrects algorithmic details in sec 4.1 and 4.2, and
  removes typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01505v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01505v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G07, 62F15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02087v1</id>
    <updated>2018-05-05T17:12:10Z</updated>
    <published>2018-05-05T17:12:10Z</published>
    <title>A Constraint-Based Algorithm For Causal Discovery with Cycles, Latent
  Variables and Selection Bias</title>
    <summary>  Causal processes in nature may contain cycles, and real datasets may violate
causal sufficiency as well as contain selection bias. No constraint-based
causal discovery algorithm can currently handle cycles, latent variables and
selection bias (CLS) simultaneously. I therefore introduce an algorithm called
Cyclic Causal Inference (CCI) that makes sound inferences with a conditional
independence oracle under CLS, provided that we can represent the cyclic causal
process as a non-recursive linear structural equation model with independent
errors. Empirical results show that CCI outperforms CCD in the cyclic case as
well as rivals FCI and RFCI in the acyclic case.
</summary>
    <author>
      <name>Eric V. Strobl</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02075v1</id>
    <updated>2018-05-05T15:57:04Z</updated>
    <published>2018-05-05T15:57:04Z</published>
    <title>Decentralized Nonparametric Multiple Testing</title>
    <summary>  Consider a big data multiple testing task, where, due to storage and
computational bottlenecks, one is given a very large collection of p-values by
splitting into manageable chunks and distributing over thousands of computer
nodes. This paper is concerned with the following question: How can we find the
full data multiple testing solution by operating completely independently on
individual machines in parallel, without any data exchange between nodes? This
version of the problem tends naturally to arise in a wide range of
data-intensive science and industry applications whose methodological solution
has not appeared in the literature to date; therefore, we feel it is necessary
to undertake such analysis. Based on the nonparametric functional statistical
viewpoint of large-scale inference, started in Mukhopadhyay (2016), this paper
furnishes a new computing model that brings unexpected simplicity to the design
of the algorithm which might otherwise seem daunting using classical approach
and notations.
</summary>
    <author>
      <name>Subhadeep Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised version</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02044v1</id>
    <updated>2018-05-05T11:35:08Z</updated>
    <published>2018-05-05T11:35:08Z</published>
    <title>Conditional and marginal relative risk parameters for a class of
  recursive regression graph models</title>
    <summary>  In linear regression modelling the distortion of effects after marginalizing
over variables of the conditioning set has been widely studied in several
contexts. For Gaussian variables, the relationship between marginal and partial
regression coefficients is well-established and the issue is often addressed as
a result of W. G. Cochran. Possible generalizations beyond the linear Gaussian
case have been developed, nevertheless the case of discrete variables is still
challenging, in particular in medical and social science settings. A
multivariate regression framework is proposed for binary data with regression
coefficients given by the logarithm of relative risks and a multivariate
Relative Risk formula is derived to define the relationship between marginal
and conditional relative risks. The method is illustrated through the analysis
of the morphine data in order to assess the effect of preoperative oral
morphine administration on the postoperative pain relief.
</summary>
    <author>
      <name>Monia Lupparelli</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01960v1</id>
    <updated>2018-05-04T23:14:36Z</updated>
    <published>2018-05-04T23:14:36Z</published>
    <title>Causal programming: inference with structural causal models as finding
  instances of a relation</title>
    <summary>  This paper proposes a causal inference relation and causal programming as
general frameworks for causal inference with structural causal models. A tuple,
$\langle M, I, Q, F \rangle$, is an instance of the relation if a formula, $F$,
computes a causal query, $Q$, as a function of known population probabilities,
$I$, in every model entailed by a set of model assumptions, $M$. Many problems
in causal inference can be viewed as the problem of enumerating instances of
the relation that satisfy given criteria. This unifies a number of previously
studied problems, including causal effect identification, causal discovery and
recovery from selection bias. In addition, the relation supports formalizing
new problems in causal inference with structural causal models, such as the
problem of research design. Causal programming is proposed as a further
generalization of causal inference as the problem of finding optimal instances
of the relation, with respect to a cost function.
</summary>
    <author>
      <name>Joshua Brulé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06913v3</id>
    <updated>2018-05-04T19:52:29Z</updated>
    <published>2015-03-24T04:52:11Z</published>
    <title>Mixtures of g-priors in Generalized Linear Models</title>
    <summary>  Mixtures of Zellner's g-priors have been studied extensively in linear models
and have been shown to have numerous desirable properties for Bayesian variable
selection and model averaging. Several extensions of g-priors to Generalized
Linear Models (GLMs) have been proposed in the literature; however, the choice
of prior distribution of g and resulting properties for inference have received
considerably less attention. In this paper, we unify mixtures of g-priors in
GLMs by assigning the truncated Compound Confluent Hypergeometric (tCCH)
distribution to 1/(1 + g), which encompasses as special cases several mixtures
of g-priors in the literature, such as the hyper-g, Beta-prime, truncated
Gamma, incomplete inverse-Gamma, benchmark, robust, hyper-g/n, and intrinsic
priors. Through an integrated Laplace approximation, the posterior distribution
of 1/(1 + g) is in turn a tCCH distribution, and approximate marginal
likelihoods are thus available analytically, leading to "Compound
Hypergeometric Information Criteria" for model selection. We discuss the local
geometric properties of the g-prior in GLMs and show how the desiderata for
model selection proposed by Bayarri et al, such as asymptotic model selection
consistency, intrinsic consistency, and measurement invariance may be used to
justify the prior and specific choices of the hyper parameters. We illustrate
inference using these priors and contrast them to other approaches via
simulation and real data examples. The methodology is implemented in the R
package BAS and freely available on CRAN.
</summary>
    <author>
      <name>Yingbo Li</name>
    </author>
    <author>
      <name>Merlise A. Clyde</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/01621459.2018.1469992</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/01621459.2018.1469992" rel="related"/>
    <link href="http://arxiv.org/abs/1503.06913v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06913v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01886v1</id>
    <updated>2018-05-04T17:58:51Z</updated>
    <published>2018-05-04T17:58:51Z</published>
    <title>Population-calibrated multiple imputation for a binary/categorical
  covariate in categorical regression models</title>
    <summary>  Multiple imputation (MI) has become popular for analyses with missing data in
medical research. The standard implementation of MI is based on the assumption
of data being missing at random (MAR). However, for missing data generated by
missing not at random (MNAR) mechanisms, MI performed assuming MAR might not be
satisfactory. For an incomplete variable in a given dataset, its corresponding
population marginal distribution might also be available in an external data
source. We show how this information can be readily utilised in the imputation
model to calibrate inference to the population, by incorporating an
appropriately calculated offset termed the `calibrated-$\delta$ adjustment'. We
describe the derivation of this offset from the population distribution of the
incomplete variable and show how in applications it can be used to closely (and
often exactly) match the post-imputation distribution to the population level.
Through analytic and simulation studies, we show that our proposed
calibrated-$\delta$ adjustment MI method can give the same inference as
standard MI when data are MAR, and can produce more accurate inference under
two general MNAR missingness mechanisms. The method is used to impute missing
ethnicity data in a type 2 diabetes prevalence case study using UK primary care
electronic health records, where it results in scientifically relevant changes
in inference for non-White ethnic groups compared to standard MI.
Calibrated-$\delta$ adjustment MI represents a pragmatic approach for utilising
available population-level information in a sensitivity analysis to explore
potential departure from the MAR assumption.
</summary>
    <author>
      <name>Tra My Pham</name>
    </author>
    <author>
      <name>James R Carpenter</name>
    </author>
    <author>
      <name>Tim P Morris</name>
    </author>
    <author>
      <name>Angela M Wood</name>
    </author>
    <author>
      <name>Irene Petersen</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01868v1</id>
    <updated>2018-05-04T17:29:14Z</updated>
    <published>2018-05-04T17:29:14Z</published>
    <title>Algorithmic Decision Making in the Presence of Unmeasured Confounding</title>
    <summary>  On a variety of complex decision-making tasks, from doctors prescribing
treatment to judges setting bail, machine learning algorithms have been shown
to outperform expert human judgments. One complication, however, is that it is
often difficult to anticipate the effects of algorithmic policies prior to
deployment, making the decision to adopt them risky. In particular, one
generally cannot use historical data to directly observe what would have
happened had the actions recommended by the algorithm been taken. One standard
strategy is to model potential outcomes for alternative decisions assuming that
there are no unmeasured confounders (i.e., to assume ignorability). But if this
ignorability assumption is violated, the predicted and actual effects of an
algorithmic policy can diverge sharply. In this paper we present a flexible,
Bayesian approach to gauge the sensitivity of predicted policy outcomes to
unmeasured confounders. We show that this policy evaluation problem is a
generalization of estimating heterogeneous treatment effects in observational
studies, and so our methods can immediately be applied to that setting.
Finally, we show, both theoretically and empirically, that under certain
conditions it is possible to construct near-optimal algorithmic policies even
when ignorability is violated. We demonstrate the efficacy of our methods on a
large dataset of judicial actions, in which one must decide whether defendants
awaiting trial should be required to pay bail or can be released without
payment.
</summary>
    <author>
      <name>Jongbin Jung</name>
    </author>
    <author>
      <name>Ravi Shroff</name>
    </author>
    <author>
      <name>Avi Feller</name>
    </author>
    <author>
      <name>Sharad Goel</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01864v1</id>
    <updated>2018-05-04T17:16:12Z</updated>
    <published>2018-05-04T17:16:12Z</published>
    <title>Mixture Envelope Model for Heterogeneous Genomics Data Analysis</title>
    <summary>  Envelope model also known as multivariate regression model was proposed to
solve the multiple response regression problems. It measures the linear
association between predictors and multiple responses by using the minimal
reducing subspace of the covariance matrix that accommodates the mean function.
However, in many real applications, data may consist many unknown confounding
factors or they just come from different resources. Thus, there might be some
heterogeneous dependency across the whole population and divide them into
different groups. For example, there exists several subtypes across the
population with breast cancer with different gene interaction mechanisms for
each subtype group. In this setting, constructing a single model using all
observations ignores the difference between groups while estimating multiple
models for each group is infeasible due to the unknown group classification. To
deal with this problem, we proposed a mixture envelope model which construct a
groupwise model for heterogeneous data and simultaneously classify them into
different groups by an Imputation-Conditional Consistency (ICC) algorithm.
Simulation results shows that our proposed method outperforms on both
classification and prediction than some existing methods. Finally, we apply our
proposed method into breast cancer analysis to identify patients with
inflammatory breast cancer subtype and evaluate the associations between
micro-RNAs and message RNAs gene expression.
</summary>
    <author>
      <name>Bochao Jia</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10065v3</id>
    <updated>2018-05-04T14:57:40Z</updated>
    <published>2018-02-27T18:43:43Z</published>
    <title>Nonasymptotic Gaussian Approximation for Inference with Stable Noise</title>
    <summary>  The results of a series of theoretical studies are reported, examining the
convergence rate for different approximate representations of $\alpha$-stable
distributions. Although they play a key role in modelling random processes with
jumps and discontinuities, the use of $\alpha$-stable distributions in
inference often leads to analytically intractable problems. The LePage series,
which is a probabilistic representation employed in this work, is used to
transform an intractable, infinite-dimensional inference problem into a
conditionally Gaussian parametric problem. A major component of our approach is
the approximation of the tail of this series by a Gaussian random variable.
Standard statistical techniques, such as Expectation-Maximization, Markov chain
Monte Carlo, and Particle Filtering, can then be applied. In addition to the
asymptotic normality of the tail of this series, we establish explicit,
nonasymptotic bounds on the approximation error. Their proofs follow classical
Fourier-analytic arguments, using Ess\'{e}en's smoothing lemma. Specifically,
we consider the distance between the distributions of: $(i)$~the tail of the
series and an appropriate Gaussian; $(ii)$~the full series and the truncated
series; and $(iii)$~the full series and the truncated series with an added
Gaussian term. In all three cases, sharp bounds are established, and the
theoretical results are compared with the actual distances (computed
numerically) in specific examples of symmetric $\alpha$-stable distributions.
This analysis facilitates the selection of appropriate truncations in practice
and offers theoretical guarantees for the accuracy of resulting estimates. One
of the main conclusions obtained is that, for the purposes of inference, the
use of a truncated series together with an approximately Gaussian error term
has superior statistical properties and is likely a preferable choice in
practice.
</summary>
    <author>
      <name>Marina Riabiz</name>
    </author>
    <author>
      <name>Tohid Ardeshiri</name>
    </author>
    <author>
      <name>Ioannis Kontoyiannis</name>
    </author>
    <author>
      <name>Simon Godsill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">V1: 41 pages, 16 figures. V2: 40 pages, 12 figures. Text typos fixed;
  redundant figures from main text and appendices removed; added references in
  section I; slightly changed section VI and its proofs in Appendices C-D-E
  (redundant proofs removed); improved section IX; removed section X
  (Discussion and Conclusion), reference style changed. V3: title in the
  metadata updated</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10065v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10065v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01729v1</id>
    <updated>2018-05-04T11:58:33Z</updated>
    <published>2018-05-04T11:58:33Z</published>
    <title>Axiomatic Approach to Variable Kernel Density Estimation</title>
    <summary>  Variable kernel density estimation allows the approximation of a probability
density by the mean of differently stretched and rotated kernels centered at
given sampling points $y_n\in\mathbb{R}^d,\ n=1,\dots,N$. Up to now, the choice
of the corresponding bandwidth matrices $h_n$ has relied mainly on asymptotic
arguments, like the minimization of the asymptotic mean integrated squared
error (AMISE), which work well for large numbers of sampling points. However,
in practice, one is often confronted with small to moderately sized sample sets
far below the asymptotic regime, which highly restricts the usability of such
methods. As an alternative to this asymptotic reasoning we suggest an axiomatic
approach which guarantees invariance of the density estimate under linear
transformations of the original density (and the sampling points) as well as
under splitting of the density into several `well-separated' parts. In order to
still ensure proper asymptotic behavior of the estimate, we \emph{postulate}
the typical dependence $h_n\propto N^{-1/(d+4)}$. Further, we derive a new
bandwidths selection rule which satisfies these axioms and performs
considerably better than conventional ones in an artificially intricate
two-dimensional example as well as in a real life example.
</summary>
    <author>
      <name>Ilja Klebanov</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00064v4</id>
    <updated>2018-05-04T10:34:03Z</updated>
    <published>2016-11-30T22:35:55Z</published>
    <title>Objective Priors in the Empirical Bayes Framework</title>
    <summary>  When estimating a probability density within the empirical Bayes framework,
the nonparametric maximum likelihood estimate (NPMLE) tends to overfit the
data. This issue is often taken care of by regularization -- a penalty term is
subtracted from the marginal log-likelihood before the maximization step, so
that the estimate favors smooth densities. The majority of penalizations
currently in use are rather arbitrary brute-force solutions, which lack
invariance under reparametrization. This contradicts the principle that, if the
underlying model has several equivalent formulations, the methods of inductive
inference should lead to consistent results. Motivated by this principle and
following an information-theoretic approach similar to the construction of
reference priors, we suggest a penalty term that guarantees this kind of
invariance. The resulting density estimate constitutes an extension of
reference priors.
</summary>
    <author>
      <name>Ilja Klebanov</name>
    </author>
    <author>
      <name>Alexander Sikorski</name>
    </author>
    <author>
      <name>Christof Schütte</name>
    </author>
    <author>
      <name>Susanna Röblitz</name>
    </author>
    <link href="http://arxiv.org/abs/1612.00064v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00064v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01638v1</id>
    <updated>2018-05-04T07:51:49Z</updated>
    <published>2018-05-04T07:51:49Z</published>
    <title>Estimation of Extreme Survival Probabilities with Cox Model</title>
    <summary>  We propose an extension of the regular Cox's proportional hazards model which
allows the estimation of the probabilities of rare events. It is known that
when the data are heavily censored at the upper end of the survival
distribution, the estimation of the tail of the survival distribution is not
reliable. To estimate the distribution beyond the last observed data, we
suppose that the survival data are in the domain of attraction of the Fr\'echet
distribution conditionally to covariates. Under this condition, by the
Fisher-Tippett-Gnedenko theorem, the tail of the baseline distribution can be
adjusted by a Pareto distribution with parameter $\theta$ beyond a threshold
$\tau$. The survival distributions conditioned to the covariates are easily
computed from the baseline. We also propose an aggregated estimate of the
survival probabilities. A procedure allowing an automatic choice of the
threshold and an application on two data sets are given.
</summary>
    <author>
      <name>Ion Grama</name>
    </author>
    <author>
      <name>Kevin Jaunatre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, poster in SAfJR - Leiden 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06489v2</id>
    <updated>2018-05-04T00:39:49Z</updated>
    <published>2017-08-22T03:45:05Z</published>
    <title>Sequential Monte Carlo algorithms for a class of outer measures</title>
    <summary>  Closed-form stochastic filtering equations can be derived in a general
setting where probability distributions are replaced by some specific outer
measures. In this article, we study how the principles of the sequential Monte
Carlo method can be adapted for the purpose of practical implementation of
these equations. In particular, we explore how sampling can be used to provide
support points for the approximation of these outer measures. This step enables
practical algorithms to be derived in the spirit of particle filters. The
performance of the obtained algorithms is demonstrated in simulations and their
versatility is illustrated through various examples.
</summary>
    <author>
      <name>Jeremie Houssineau</name>
    </author>
    <author>
      <name>Branko Ristic</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01575v1</id>
    <updated>2018-05-03T23:38:21Z</updated>
    <published>2018-05-03T23:38:21Z</published>
    <title>Efficient methods for the estimation of the multinomial parameter for
  the two-trait group testing model</title>
    <summary>  Estimation of a single Bernoulli parameter using pooled sampling is among the
oldest problems in the group testing literature. To carry out such estimation,
an array of efficient estimators have been introduced covering a wide range of
situations routinely encountered in applications. More recently, there has been
growing interest in using group testing to simultaneously estimate the joint
probabilities of two correlated traits using a multinomial model.
Unfortunately, basic estimation results, such as the maximum likelihood
estimator (MLE), have not been adequately addressed in the literature for such
cases. In this paper, we show that finding the MLE for this problem is
equivalent to maximizing a multinomial likelihood with a restricted parameter
space. A solution using the EM algorithm is presented which is guaranteed to
converge to the global maximizer, even on the boundary of the parameter space.
Two additional closed form estimators are presented with the goal of minimizing
the bias and/or mean square error. The methods are illustrated by considering
an application to the joint estimation of transmission prevalence for two
strains of the Potato virus Y by the aphid myzus persicae.
</summary>
    <author>
      <name>Gregory Haber</name>
    </author>
    <author>
      <name>Yaakov Malinovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04813v2</id>
    <updated>2018-05-03T19:21:54Z</updated>
    <published>2017-10-13T05:52:26Z</published>
    <title>On Integrated $L^{1}$ Convergence Rate of an Isotonic Regression
  Estimator for Multivariate Observations</title>
    <summary>  We consider a general monotone regression estimation where we allow for
independent and dependent regressors. We propose a modification of the
classical isotonic least squares estimator and establish its rate of
convergence for the integrated $L_1$-loss function. The methodology captures
the shape of the data without assuming additivity or a parametric form for the
regression function. Furthermore, the degree of smoothing is chosen
automatically and no auxiliary tuning is required for the theoretical analysis.
Some simulations and two real data illustrations complement the study of the
proposed estimator.
</summary>
    <author>
      <name>Konstantinos Fokianos</name>
    </author>
    <author>
      <name>Anne Leucht</name>
    </author>
    <author>
      <name>Michael H. Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1710.04813v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04813v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01417v1</id>
    <updated>2018-05-03T16:42:54Z</updated>
    <published>2018-05-03T16:42:54Z</published>
    <title>A generalized spatial sign covariance matrix</title>
    <summary>  The well-known spatial sign covariance matrix (SSCM) carries out a radial
transform which moves all data points to a sphere, followed by computing the
classical covariance matrix of the transformed data. Its popularity stems from
its robustness to outliers, fast computation, and applications to correlation
and principal component analysis. In this paper we study more general radial
functions. It is shown that the eigenvectors of the generalized SSCM are still
consistent and the ranks of the eigenvalues are preserved. The influence
function of the resulting scatter matrix is derived, and it is shown that its
breakdown value is as high as that of the original SSCM. A simulation study
indicates that the best results are obtained when the inner half of the data
points are not transformed and points lying far away are moved to the center.
</summary>
    <author>
      <name>Jakob Raymaekers</name>
    </author>
    <author>
      <name>Peter J. Rousseeuw</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03574v3</id>
    <updated>2018-05-03T13:34:24Z</updated>
    <published>2016-02-10T23:36:37Z</published>
    <title>A knockoff filter for high-dimensional selective inference</title>
    <summary>  This paper develops a framework for testing for associations in a possibly
high-dimensional linear model where the number of features/variables may far
exceed the number of observational units. In this framework, the observations
are split into two groups, where the first group is used to screen for a set of
potentially relevant variables, whereas the second is used for inference over
this reduced set of variables; we also develop strategies for leveraging
information from the first part of the data at the inference step for greater
power. In our work, the inferential step is carried out by applying the
recently introduced knockoff filter, which creates a knockoff copy-a fake
variable serving as a control-for each screened variable. We prove that this
procedure controls the directional false discovery rate (FDR) in the reduced
model controlling for all screened variables; this says that our
high-dimensional knockoff procedure 'discovers' important variables as well as
the directions (signs) of their effects, in such a way that the expected
proportion of wrongly chosen signs is below the user-specified level (thereby
controlling a notion of Type S error averaged over the selected set). This
result is non-asymptotic, and holds for any distribution of the original
features and any values of the unknown regression coefficients, so that
inference is not calibrated under hypothesized values of the effect sizes. We
demonstrate the performance of our general and flexible approach through
numerical studies, showing more power than existing alternatives. Finally, we
apply our method to a genome-wide association study to find locations on the
genome that are possibly associated with a continuous phenotype.
</summary>
    <author>
      <name>Rina Foygel Barber</name>
    </author>
    <author>
      <name>Emmanuel J. Candes</name>
    </author>
    <link href="http://arxiv.org/abs/1602.03574v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03574v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08215v3</id>
    <updated>2018-05-03T13:14:33Z</updated>
    <published>2017-07-25T20:54:50Z</published>
    <title>Scaled Gaussian Stochastic Process for Computer Model Calibration and
  Prediction</title>
    <summary>  We consider the problem of calibrating an imperfect computer model using
experimental data. To compensate the misspecification of the computer model and
make more accurate predictions, a discrepancy function is often included and
modeled via a Gaussian stochastic process (GaSP). The calibrated computer model
alone, however, sometimes fits the experimental data poorly, as the calibration
parameters become unidentifiable. In this work, we propose the scaled Gaussian
stochastic process (S-GaSP), a novel stochastic process that bridges the gap
between two predominant methods, namely the $L_2$ calibration and the GaSP
calibration. It is shown that our approach performs well in both calibration
and prediction. A computationally feasible approach is introduced for this new
model under the Bayesian paradigm. Compared with the GaSP calibration, the
S-GaSP calibration enables the calibrated computer model itself to predict the
reality well, based on the posterior distribution of the calibration
parameters. Numerical comparisons of the simulated and real data are provided
to illustrate the connections and differences between the proposed S-GaSP and
other alternative approaches.
</summary>
    <author>
      <name>Mengyang Gu</name>
    </author>
    <author>
      <name>Long Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1707.08215v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08215v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01124v1</id>
    <updated>2018-05-03T05:48:54Z</updated>
    <published>2018-05-03T05:48:54Z</published>
    <title>A Coefficient of Determination (R2) for Linear Mixed Models</title>
    <summary>  Extensions of linear models are very commonly used in the analysis of
biological data. Whereas goodness of fit measures such as the coefficient of
determination (R2) or the adjusted R2 are well established for linear models,
it is not obvious how such measures should be defined for generalized linear
and mixed models. There are by now several proposals but no consensus has yet
emerged as to the best unified approach in these settings. In particular, it is
an open question how to best account for heteroscedasticity and for covariance
among observations induced by random effects. This paper proposes a new
approach that addresses this issue and is universally applicable. It is
exemplified using three biological examples.
</summary>
    <author>
      <name>Hans-Peter Piepho</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01010v1</id>
    <updated>2018-05-02T20:35:48Z</updated>
    <published>2018-05-02T20:35:48Z</published>
    <title>Toward a diagnostic toolkit for linear models with Gaussian-process
  distributed random effects</title>
    <summary>  Gaussian processes (GPs) are widely used as distributions of random effects
in linear mixed models, which are fit using the restricted likelihood or the
closely-related Bayesian analysis. This article addresses two problems. First,
we propose tools for understanding how data determine estimates in these
models, using a spectral basis approximation to the GP under which the
restricted likelihood is formally identical to the likelihood for a
gamma-errors GLM with identity link. Second, to examine the data's support for
a covariate and to understand how adding that covariate moves variation in the
outcome y out of the GP and error parts of the fit, we apply a linear-model
diagnostic, the added variable plot (AVP), both to the original observations
and to projections of the data onto the spectral basis functions. The spectral-
and observation-domain AVPs estimate the same coefficient for a covariate but
emphasize low- and high-frequency data features respectively and thus highlight
the covariate's effect on the GP and error parts of the fit respectively. The
spectral approximation applies to data observed on a regular grid; for data
observed at irregular locations, we propose smoothing the data to a grid before
applying our methods. The methods are illustrated using the forest-biomass data
of Finley et al.~(2008).
</summary>
    <author>
      <name>Maitreyee Bose</name>
    </author>
    <author>
      <name>James S. Hodges</name>
    </author>
    <author>
      <name>Sudipto Banerjee</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08036v2</id>
    <updated>2018-05-02T18:21:05Z</updated>
    <published>2017-07-25T15:16:01Z</published>
    <title>Theoretical Properties of Quasistationary Monte Carlo Methods</title>
    <summary>  This paper gives foundational results for the application of
quasistationarity to Monte Carlo inference problems. We prove natural
sufficient conditions for the quasilimiting distribution of a killed diffusion
to coincide with a target density of interest. We also quantify the rate of
convergence to quasistationarity by relating the killed diffusion to an
appropriate Langevin diffusion. As an example, we consider in detail a killed
Ornstein--Uhlenbeck process with Gaussian quasistationary distribution.
</summary>
    <author>
      <name>Andi Q. Wang</name>
    </author>
    <author>
      <name>Martin Kolb</name>
    </author>
    <author>
      <name>David Steinsaltz</name>
    </author>
    <author>
      <name>Gareth O. Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 1 figure. Revised version of paper: presentation of
  material reordered to be more reader-friendly, with various additional
  comments throughout, and a final discussion has been added</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J60, 60J70 (Primary), 47D07, 65C05 (secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00829v1</id>
    <updated>2018-05-02T14:08:37Z</updated>
    <published>2018-05-02T14:08:37Z</published>
    <title>Selection of proposal distributions for generalized importance sampling
  estimators</title>
    <summary>  The standard importance sampling (IS) method uses samples from a single
proposal distribution and assigns weights to them, according to the ratio of
the target and proposal pdfs. This naive IS estimator, generally does not work
well in multiple targets examples as the weights can take arbitrarily large
values making the estimator highly unstable. In such situations, alternative
generalized IS estimators involving samples from multiple proposal
distributions are preferred. Just like the standard IS, the success of these
multiple IS estimators crucially depends on the choice of the proposal
distributions. The selection of these proposal distributions is the focus of
this article. We propose three methods based on (i)~a geometric space filling
coverage criterion, (ii)~a sequential maximum variance approach, and (iii)~a
maximum entropy approach. In particular, we describe these methods in the
context of Doss's (2010) two-stage IS estimator, although the first two methods
are applicable to any multi-proposal IS estimator. Two of the proposed
approaches use estimates of asymptotic variances of Geyer's (1994) reverse
logistic estimator and Doss's (2010) IS estimators. Thus, we provide consistent
spectral variance estimators for these asymptotic variances. We demonstrate the
performance of these spectral variance methods for estimating the size
parameter of a negative binomial generalized linear model. The proposed methods
for selecting proposal densities are illustrated using two detailed examples.
The first example is a robust Bayesian binary regression model, where the
generalized IS method is used to estimate the link function parameter. The
second example involves analysis of count data using the Poisson spatial
generalized linear mixed model.
</summary>
    <author>
      <name>Vivekananda Roy</name>
    </author>
    <author>
      <name>Evangelos Evangelou</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60-08, 60J22" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06578v2</id>
    <updated>2018-05-02T12:43:53Z</updated>
    <published>2017-07-20T15:47:35Z</published>
    <title>On Nonparametric Regression using Data Depth</title>
    <summary>  We investigate nonparametric regression methods based on statistical depth
functions. These nonparametric regression procedures can be used in situations,
where the response is multivariate and the covariate is a random element in a
metric space. This includes regression with functional covariate as a special
case. Our objective is to study different features of the conditional
distribution of the response given the covariate. We construct measures of the
center and the spread of the conditional distribution using depth based
nonparametric regression procedures. We establish the asymptotic consistency of
those measures and develop a test for heteroscedasticity based on the measure
of conditional spread. The usefulness of the methodology is demonstrated in
some real datasets. In one dataset consisting of Italian household expenditure
data for the period 1973 to 1992, we regress the expenditure for different
items on their prices. In another dataset, our responses are the nutritional
contents of different meat samples measured by their protein, fat and moisture
contents, and the functional covariate is the absorbance spectra of the meat
samples.
</summary>
    <author>
      <name>Joydeep Chowdhury</name>
    </author>
    <author>
      <name>Probal Chaudhuri</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06578v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06578v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00753v1</id>
    <updated>2018-05-02T11:59:34Z</updated>
    <published>2018-05-02T11:59:34Z</published>
    <title>Gaussian Process Forecast with multidimensional distributional entries</title>
    <summary>  In this work, we propose to define Gaussian Processes indexed by
multidimensional distributions. In the framework where the distributions can be
modeled as i.i.d realizations of a measure on the set of distributions, we
prove that the kernel defined as the quadratic distance between the
transportation maps, that transport each distribution to the barycenter of the
distributions, provides a valid covariance function. In this framework, we
study the asymptotic properties of this process, proving micro ergodicity of
the parameters.
</summary>
    <author>
      <name>Francois Bachoc</name>
    </author>
    <author>
      <name>Alexandra Suvorikova</name>
    </author>
    <author>
      <name>Jean-Michel Loubes</name>
    </author>
    <author>
      <name>Vladimir Spokoiny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00726v1</id>
    <updated>2018-05-02T10:57:04Z</updated>
    <published>2018-05-02T10:57:04Z</published>
    <title>Emulation of utility functions over a set of permutations: sequencing
  reliability growth tasks</title>
    <summary>  We consider Bayesian design of experiments problems in which we maximise the
prior expectation of a utility function over a set of permutations, for example
when sequencing a number of tasks to perform. When the number of tasks is large
and the expected utility is expensive to compute, it may be unreasonable or
infeasible to evaluate the expected utility of all permutations. We propose an
approach to emulate the expected utility using a surrogate function based on a
parametric probabilistic model for permutations. The surrogate function is
fitted by maximising the correlation with the expected utility over a set of
training points. We propose a suitable transformation of the expected utility
to improve the fit. We provide results linking the correlation between the two
functions and the number of expected utility evaluations to undertake. The
approach is applied to the sequencing of reliability growth tasks in the
development of hardware systems, in which there is a large number of potential
tasks to perform and engineers are interested in meeting a reliability target
subject to minimising costs and time. An illustrative example shows how the
approach can be used and a simulation study demonstrates the performance of the
approach more generally.
</summary>
    <author>
      <name>Kevin J Wilson</name>
    </author>
    <author>
      <name>Daniel A Henderson</name>
    </author>
    <author>
      <name>John Quigley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/00401706.2017.1377637</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/00401706.2017.1377637" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article accepted at Technometrics. The official journal version is
  given here: https://doi.org/10.1080/00401706.2017.1377637</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00649v1</id>
    <updated>2018-05-02T07:01:39Z</updated>
    <published>2018-05-02T07:01:39Z</published>
    <title>Flexible Density Tempering Approaches for State Space Models with an
  Application to Factor Stochastic Volatility Models</title>
    <summary>  Duan (2015) propose a tempering or annealing approach to Bayesian inference
for time series state space models. In such models the likelihood is often
analytically and computationally intractable. Their approach generalizes the
annealed importance sampling (AIS) approach of Neal (2001) and DelMoral (2006)
when the likelihood can be computed analytically. Annealing is a sequential
Monte Carlo approach that moves a collection of parameters and latent state
variables through a number of levels, with each level having its own target
density, in such a way that it is easy to generate both the parameters and
latent state variables at the initial level while the target density at the
final level is the posterior density of interest. A critical component of the
annealing or density tempering method is the Markov move component that is
implemented at every stage of the annealing process. The Markov move component
effectively runs a small number of Markov chain Monte Carlo iterations for each
combination of parameters and latent variables so that they are better
approximations to that level of the tempered target density. Duan (2015) used a
pseudo marginal Metropolis-Hastings approach with the likelihood estimated
unbiasedly in the Markov move component. One of the drawbacks of this approach,
however, is that it is difficult to obtain good proposals when the parameter
space is high dimensional, such as for a high dimensional factor stochastic
volatility models. We propose using instead more flexible Markov move steps
that are based on particle Gibbs and Hamiltonian Monte Carlo and demonstrate
the proposed methods using a high dimensional stochastic volatility factor
model. An estimate of the marginal likelihood is obtained as a byproduct of the
estimation procedure.
</summary>
    <author>
      <name>David Gunawan</name>
    </author>
    <author>
      <name>Robert Kohn</name>
    </author>
    <author>
      <name>Chris Carter</name>
    </author>
    <author>
      <name>Minh Ngoc Tran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 4 tables, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01208v2</id>
    <updated>2018-05-02T00:01:57Z</updated>
    <published>2018-04-04T01:54:37Z</published>
    <title>Should We Adjust for the Test for Pre-trends in Difference-in-Difference
  Designs?</title>
    <summary>  The common practice in difference-in-difference (DiD) designs is to check for
parallel trends prior to treatment assignment, yet typical estimation and
inference does not account for the fact that this test has occurred. I analyze
the properties of the traditional DiD estimator conditional on having passed
(i.e. not rejected) the test for parallel pre-trends. When the DiD design is
valid and the test for pre-trends confirms it, the typical DiD estimator is
unbiased, but traditional standard errors are overly conservative.
Additionally, there exists an alternative unbiased estimator that is more
efficient than the traditional DiD estimator under parallel trends. However,
when in population there is a non-zero pre-trend but we fail to reject the
hypothesis of parallel pre-trends, the DiD estimator is generally biased
relative to the population DiD coefficient. Moreover, if the trend is monotone,
then under reasonable assumptions the bias from conditioning exacerbates the
bias relative to the true treatment effect. I propose new estimation and
inference procedures that account for the test for parallel trends, and compare
their performance to that of the traditional estimator in a Monte Carlo
simulation.
</summary>
    <author>
      <name>Jonathan Roth</name>
    </author>
    <link href="http://arxiv.org/abs/1804.01208v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01208v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00555v1</id>
    <updated>2018-05-01T21:17:45Z</updated>
    <published>2018-05-01T21:17:45Z</published>
    <title>A general framework for modelling zero inflation</title>
    <summary>  We propose a new framework for the modelling of count data exhibiting zero
inflation (ZI). The main part of this framework includes a new and more general
parameterisation for ZI models which naturally includes both over- and
under-inflation. It further sheds new theoretical light on modelling and
inference and permits a simpler alternative, which we term as multiplicative,
in contrast to the dominant mixture and hurdle models. Our approach gives the
statistician access to new types of ZI of which mixture and hurdle are special
cases. We outline a simple parameterised modelling approach which can help to
infer both ZI type and degree and provide an underlying treatment that shows
that current ZI models are themselves typically within the exponential family,
thus permitting much simpler theory, computation and classical inference. We
outline some possibilities for a natural Bayesian framework for inference; and
a rich basis for work on correlated ZI counts.
  The present paper is an incomplete report on the underlying theory. A later
version will include computational issues and provide further examples.
</summary>
    <author>
      <name>John Haslett</name>
    </author>
    <author>
      <name>Andrew Parnell</name>
    </author>
    <author>
      <name>James Sweeney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00550v1</id>
    <updated>2018-05-01T21:04:24Z</updated>
    <published>2018-05-01T21:04:24Z</published>
    <title>Transporting inferences from a randomized trial to a new target
  population</title>
    <summary>  When variables that are treatment effect modifiers also influence the
decision to participate in a clinical trial, the average effect among trial
participants will differ from the effect in other populations of trial-eligible
individuals. In this tutorial, we consider methods for transporting inferences
about a time-fixed treatment from trial participants to a new target population
of trial-eligible individuals, using data from a completed randomized trial
along with baseline covariate data from a sample of non-participants. We
examine methods based on modeling the expectation of the outcome, the
probability of participation, or both (doubly robust). We compare the
finite-sample performance of different methods in a simulation study and
provide example code to implement the methods in software. We illustrate the
application of the methods to the Coronary Artery Surgery Study, a randomized
trial nested within a cohort of trial-eligible patients to compare coronary
artery surgery plus medical therapy versus medical therapy alone for patients
with chronic coronary artery disease. Lastly, we discuss issues that arise when
using the methods in applied transportability analyses.
</summary>
    <author>
      <name>Issa J. Dahabreh</name>
    </author>
    <author>
      <name>Sarah E. Robertson</name>
    </author>
    <author>
      <name>Elizabeth A. Stuart</name>
    </author>
    <author>
      <name>Miguel A. Hernan</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00541v1</id>
    <updated>2018-05-01T20:15:53Z</updated>
    <published>2018-05-01T20:15:53Z</published>
    <title>Scalable Importance Tempering and Bayesian Variable Selection</title>
    <summary>  We propose a Monte Carlo algorithm to sample from high-dimensional
probability distributions that combines Markov chain Monte Carlo (MCMC) and
importance sampling. We provide a careful theoretical analysis, including
guarantees on robustness to high-dimensionality, explicit comparison with
standard MCMC and illustrations of the potential improvements in efficiency.
Simple and concrete intuition is provided for when the novel scheme is expected
to outperform standard schemes. When applied to Bayesian Variable Selection
problems, the novel algorithm is orders of magnitude more efficient than
available alternative sampling schemes and allows to perform fast and reliable
fully Bayesian inferences with tens of thousands regressors.
</summary>
    <author>
      <name>Giacomo Zanella</name>
    </author>
    <author>
      <name>Gareth Roberts</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
