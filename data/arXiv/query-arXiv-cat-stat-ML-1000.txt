<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dcat%3Astat.ML%26id_list%3D%26start%3D0%26max_results%3D1000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=cat:stat.ML&amp;id_list=&amp;start=0&amp;max_results=1000</title>
  <id>http://arxiv.org/api/wBuBguQybTvhhNko17evv5OgM2c</id>
  <updated>2018-09-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">18160</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/1808.04256v2</id>
    <updated>2018-09-05T17:56:08Z</updated>
    <published>2018-08-10T05:33:23Z</published>
    <title>CT Super-resolution GAN Constrained by the Identical, Residual, and
  Cycle Learning Ensemble(GAN-CIRCLE)</title>
    <summary>  Computed tomography (CT) is widely used in screening, diagnosis, and
image-guided therapy for both clinical and research purposes. Since CT involves
ionizing radiation, an overarching thrust of related technical research is
development of novel methods enabling ultrahigh quality imaging with fine
structural details while reducing the X-ray radiation. In this paper, we
present a semi-supervised deep learning approach to accurately recover
high-resolution (HR) CT images from low-resolution (LR) counterparts.
Specifically, with the generative adversarial network (GAN) as the building
block, we enforce the cycle-consistency in terms of the Wasserstein distance to
establish a nonlinear end-to-end mapping from noisy LR input images to denoised
and deblurred HR outputs. We also include the joint constraints in the loss
function to facilitate structural preservation. In this deep imaging process,
we incorporate deep convolutional neural network (CNN), residual learning, and
network in network techniques for feature extraction and restoration. In
contrast to the current trend of increasing network depth and complexity to
boost the CT imaging performance, which limit its real-world applications by
imposing considerable computational and memory overheads, we apply a parallel
$1\times1$ CNN to compress the output of the hidden layer and optimize the
number of layers and the number of filters for each convolutional layer.
Quantitative and qualitative evaluations demonstrate that our proposed model is
accurate, efficient and robust for super-resolution (SR) image restoration from
noisy LR input images. In particular, we validate our composite SR networks on
three large-scale CT datasets, and obtain promising results as compared to the
other state-of-the-art methods.
</summary>
    <author>
      <name>Chenyu You</name>
    </author>
    <author>
      <name>Guang Li</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Xiaoliu Zhang</name>
    </author>
    <author>
      <name>Hongming Shan</name>
    </author>
    <author>
      <name>Shenghong Ju</name>
    </author>
    <author>
      <name>Zhen Zhao</name>
    </author>
    <author>
      <name>Zhuiyang Zhang</name>
    </author>
    <author>
      <name>Wenxiang Cong</name>
    </author>
    <author>
      <name>Michael W. Vannier</name>
    </author>
    <author>
      <name>Punam K. Saha</name>
    </author>
    <author>
      <name>Ge Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04256v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04256v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01628v1</id>
    <updated>2018-09-05T17:13:02Z</updated>
    <published>2018-09-05T17:13:02Z</published>
    <title>Online local pool generation for dynamic classifier selection: an
  extended version</title>
    <summary>  Dynamic Classifier Selection (DCS) techniques have difficulty in selecting
the most competent classifier in a pool, even when its presence is assured.
Since the DCS techniques rely only on local data to estimate a classifier's
competence, the manner in which the pool is generated could affect the choice
of the best classifier for a given sample. That is, the global perspective in
which pools are generated may not help the DCS techniques in selecting a
competent classifier for samples that are likely to be mislabelled. Thus, we
propose in this work an online pool generation method that produces a locally
accurate pool for test samples in difficult regions of the feature space. The
difficulty of a given area is determined by the classification difficulty of
the samples in it. That way, by using classifiers that were generated in a
local scope, it could be easier for the DCS techniques to select the best one
for the difficult samples. For the query samples in easy regions, a simple
nearest neighbors rule is used. In the extended version of this work, a deep
analysis on the correlation between instance hardness and the performance of
DCS techniques is presented. An instance hardness measure that conveys the
degree of local class overlap is then used to decide when the local pool is
used in the proposed scheme. The proposed method yielded significantly greater
recognition rates in comparison to a Bagging-generated pool and two other
global pool generation schemes for all DCS techniques evaluated. The proposed
scheme's performance was also significantly superior to three state-of-the-art
classification models and statistically equivalent to five of them. Moreover,
an extended analysis on the computational complexity of the proposed method and
of several DS techniques is presented in this version. We also provide the
implementation of the proposed technique using the DESLib library on GitHub.
</summary>
    <author>
      <name>Mariana A. Souza</name>
    </author>
    <author>
      <name>George D. C. Cavalcanti</name>
    </author>
    <author>
      <name>Rafael M. O. Cruz</name>
    </author>
    <author>
      <name>Robert Sabourin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the paper: M. A. Souza, G. D. Cavalcanti, R. M.
  Cruz, R. Sabourin, Online local pool generation for dynamic classifier
  selection, Pattern Recognition 85 (2019) 132 - 148</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01625v1</id>
    <updated>2018-09-05T17:09:00Z</updated>
    <published>2018-09-05T17:09:00Z</published>
    <title>Gene Shaving using influence function of a kernel method</title>
    <summary>  Identifying significant subsets of the genes, gene shaving is an essential
and challenging issue for biomedical research for a huge number of genes and
the complex nature of biological networks,. Since positive definite kernel
based methods on genomic information can improve the prediction of diseases, in
this paper we proposed a new method, "kernel gene shaving (kernel canonical
correlation analysis (kernel CCA) based gene shaving). This problem is
addressed using the influence function of the kernel CCA. To investigate the
performance of the proposed method in a comparison of three popular gene
selection methods (T-test, SAM and LIMMA), we were used extensive simulated and
real microarray gene expression datasets. The performance measures AUC was
computed for each of the methods. The achievement of the proposed method has
improved than the three well-known gene selection methods. In real data
analysis, the proposed method identified a subsets of $210$ genes out of $2000$
genes. The network of these genes has significantly more interactions than
expected, which indicates that they may function in a concerted effort on colon
cancer.
</summary>
    <author>
      <name>Md. Ashad Alam</name>
    </author>
    <author>
      <name>Mohammad Shahjama</name>
    </author>
    <author>
      <name>Md. Ferdush Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, submitted to ICCIT2018, Bangladesh</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09638v2</id>
    <updated>2018-09-05T16:57:25Z</updated>
    <published>2018-08-29T05:01:00Z</published>
    <title>Replay attack spoofing detection system using replay noise by multi-task
  learning</title>
    <summary>  In this paper, we propose a spoofing detection system for replay attack using
replay noise. In many previous studies across various domains, noise has been
reduced. However, in replay attack, we hypothesize that noise is the prominent
feature which is different with original signal and it can be one of the keys
to find whether a signal has been spoofed. We define the noise that is caused
by the replay attack as replay noise. Specifically, the noise of playback
devices, recording environments, and recording devices, is included in the
replay noise. We explore the effectiveness of training a deep neural network
simultaneously for replay attack spoofing detection and replay noise
classification. Multi-task learning was exploited to embed spoofing detection
and replay noise classification in the code layer. The experiment results on
the ASVspoof2017 datasets demonstrate that the performance of our proposed
system is relatively improved 30% on the evaluation set.
</summary>
    <author>
      <name>Hye-Jin Shim</name>
    </author>
    <author>
      <name>Jee-weon Jung</name>
    </author>
    <author>
      <name>Hee-Soo Heo</name>
    </author>
    <author>
      <name>Sunghyun Yoon</name>
    </author>
    <author>
      <name>Ha-Jin Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to 6th IEEE Global Conference on Signal and
  Information Processing (GlobalSIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09638v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09638v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02893v5</id>
    <updated>2018-09-05T16:55:28Z</updated>
    <published>2017-09-09T01:45:43Z</published>
    <title>Convolutional Dictionary Learning: A Comparative Review and New
  Algorithms</title>
    <summary>  Convolutional sparse representations are a form of sparse representation with
a dictionary that has a structure that is equivalent to convolution with a set
of linear filters. While effective algorithms have recently been developed for
the convolutional sparse coding problem, the corresponding dictionary learning
problem is substantially more challenging. Furthermore, although a number of
different approaches have been proposed, the absence of thorough comparisons
between them makes it difficult to determine which of them represents the
current state of the art. The present work both addresses this deficiency and
proposes some new approaches that outperform existing ones in certain contexts.
A thorough set of performance comparisons indicates a very wide range of
performance differences among the existing and proposed methods, and clearly
identifies those that are the most effective.
</summary>
    <author>
      <name>Cristina Garcia-Cardona</name>
    </author>
    <author>
      <name>Brendt Wohlberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCI.2018.2840334</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCI.2018.2840334" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos in Eq. (18) and (19)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computational Imaging, vol. 4, no. 3, pp.
  366-381, Sep 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.02893v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02893v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01357v3</id>
    <updated>2018-09-05T16:50:09Z</updated>
    <published>2018-07-31T08:15:06Z</published>
    <title>A recurrent multi-scale approach to RBG-D Object Recognition</title>
    <summary>  Technological development aims to produce generations of increasingly
efficient robots able to perform complex tasks. This requires considerable
efforts, from the scientific community, to find new algorithms that solve
computer vision problems, such as object recognition. The diffusion of RGB-D
cameras directed the study towards the research of new architectures able to
exploit the RGB and Depth information. The project that is developed in this
thesis concerns the realization of a new end-to-end architecture for the
recognition of RGB-D objects called RCFusion. Our method generates compact and
highly discriminative multi-modal features by combining complementary RGB and
depth information representing different levels of abstraction. We evaluate our
method on standard object recognition datasets, RGB-D Object Dataset and
JHUIT-50. The experiments performed show that our method outperforms the
existing approaches and establishes new state-of-the-art results for both
datasets.
</summary>
    <author>
      <name>Mirco Planamente</name>
    </author>
    <author>
      <name>Mohammad Reza Loghmani</name>
    </author>
    <author>
      <name>Barbara Caputo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master thesis extracted from the paper arXiv:1806.01673 submitted to
  accv 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01357v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01357v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01534v1</id>
    <updated>2018-09-05T16:44:04Z</updated>
    <published>2018-09-05T16:44:04Z</published>
    <title>Utilizing Character and Word Embeddings for Text Normalization with
  Sequence-to-Sequence Models</title>
    <summary>  Text normalization is an important enabling technology for several NLP tasks.
Recently, neural-network-based approaches have outperformed well-established
models in this task. However, in languages other than English, there has been
little exploration in this direction. Both the scarcity of annotated data and
the complexity of the language increase the difficulty of the problem. To
address these challenges, we use a sequence-to-sequence model with
character-based attention, which in addition to its self-learned character
embeddings, uses word embeddings pre-trained with an approach that also models
subword information. This provides the neural model with access to more
linguistic information especially suitable for text normalization, without
large parallel corpora. We show that providing the model with word-level
features bridges the gap for the neural network approach to achieve a
state-of-the-art F1 score on a standard Arabic language correction shared task
dataset.
</summary>
    <author>
      <name>Daniel Watson</name>
    </author>
    <author>
      <name>Nasser Zalmout</name>
    </author>
    <author>
      <name>Nizar Habash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01610v1</id>
    <updated>2018-09-05T16:30:08Z</updated>
    <published>2018-09-05T16:30:08Z</published>
    <title>Bimodal network architectures for automatic generation of image
  annotation from text</title>
    <summary>  Medical image analysis practitioners have embraced big data methodologies.
This has created a need for large annotated datasets. The source of big data is
typically large image collections and clinical reports recorded for these
images. In many cases, however, building algorithms aimed at segmentation and
detection of disease requires a training dataset with markings of the areas of
interest on the image that match with the described anomalies. This process of
annotation is expensive and needs the involvement of clinicians. In this work
we propose two separate deep neural network architectures for automatic marking
of a region of interest (ROI) on the image best representing a finding
location, given a textual report or a set of keywords. One architecture
consists of LSTM and CNN components and is trained end to end with images,
matching text, and markings of ROIs for those images. The output layer
estimates the coordinates of the vertices of a polygonal region. The second
architecture uses a network pre-trained on a large dataset of the same image
types for learning feature representations of the findings of interest. We show
that for a variety of findings from chest X-ray images, both proposed
architectures learn to estimate the ROI, as validated by clinical annotations.
There is a clear advantage obtained from the architecture with pre-trained
imaging network. The centroids of the ROIs marked by this network were on
average at a distance equivalent to 5.1% of the image width from the centroids
of the ground truth ROIs.
</summary>
    <author>
      <name>Mehdi Moradi</name>
    </author>
    <author>
      <name>Ali Madani</name>
    </author>
    <author>
      <name>Yaniv Gur</name>
    </author>
    <author>
      <name>Yufan Guo</name>
    </author>
    <author>
      <name>Tanveer Syeda-Mahmood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to MICCAI 2018, LNCS 11070</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science (LNCS 11070), Proceedings of
  Medical Image Computing &amp; Computer Assisted Intervention (MICCAI 2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.01610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00252v3</id>
    <updated>2018-09-05T16:26:22Z</updated>
    <published>2016-05-01T13:35:15Z</published>
    <title>Fast Rates for General Unbounded Loss Functions: from ERM to Generalized
  Bayes</title>
    <summary>  We present new excess risk bounds for general unbounded loss functions
including log loss and squared loss, where the distribution of the losses may
be heavy-tailed. The bounds hold for general estimators, but they are optimized
when applied to $\eta$-generalized Bayesian, MDL, and empirical risk
minimization estimators. In the case of log loss, the bounds imply convergence
rates for generalized Bayesian inference under misspecification in terms of a
generalization of the Hellinger metric as long as the learning rate $\eta$ is
set correctly. For general loss functions, our bounds rely on two separate
conditions: the $v$-GRIP (generalized reversed information projection)
conditions, which control the lower tail of the excess loss; and the newly
introduced witness condition, which controls the upper tail. The parameter $v$
in the $v$-GRIP conditions determines the achievable rate and is akin to the
exponent in the Tsybakov margin condition and the Bernstein condition for
bounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in
combination with small model complexity leads to $\tilde{O}(1/n)$ rates. The
witness condition allows us to connect the excess risk to an 'annealed' version
thereof, by which we generalize several previous results connecting Hellinger
and R\'enyi divergence to KL divergence.
</summary>
    <author>
      <name>Peter D. Grünwald</name>
    </author>
    <author>
      <name>Nishant A. Mehta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">79 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00252v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00252v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01605v1</id>
    <updated>2018-09-05T16:21:05Z</updated>
    <published>2018-09-05T16:21:05Z</published>
    <title>Anomaly Detection in the Presence of Missing Values</title>
    <summary>  Standard methods for anomaly detection assume that all features are observed
at both learning time and prediction time. Such methods cannot process data
containing missing values. This paper studies five strategies for handling
missing values in test queries: (a) mean imputation, (b) MAP imputation, (c)
reduction (reduced-dimension anomaly detectors via feature bagging), (d)
marginalization (for density estimators only), and (e) proportional
distribution (for tree-based methods only). Our analysis suggests that MAP
imputation and proportional distribution should give better results than mean
imputation, reduction, and marginalization. These hypotheses are largely
confirmed by experimental studies on synthetic data and on anomaly detection
benchmark data sets using the Isolation Forest (IF), LODA, and EGMM anomaly
detection algorithms. However, marginalization worked surprisingly well for
EGMM, and there are exceptions where reduction works well on some benchmark
problems. We recommend proportional distribution for IF, MAP imputation for
LODA, and marginalization for EGMM.
</summary>
    <author>
      <name>Thomas G. Dietterich</name>
    </author>
    <author>
      <name>Tadesse Zemicheal</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00232v2</id>
    <updated>2018-09-05T15:55:45Z</updated>
    <published>2017-12-01T08:41:28Z</published>
    <title>Optimal Algorithms for Distributed Optimization</title>
    <summary>  In this paper, we study the optimal convergence rate for distributed convex
optimization problems in networks. We model the communication restrictions
imposed by the network as a set of affine constraints and provide optimal
complexity bounds for four different setups, namely: the function $F(\xb)
\triangleq \sum_{i=1}^{m}f_i(\xb)$ is strongly convex and smooth, either
strongly convex or smooth or just convex. Our results show that Nesterov's
accelerated gradient descent on the dual problem can be executed in a
distributed manner and obtains the same optimal rates as in the centralized
version of the problem (up to constant or logarithmic factors) with an
additional cost related to the spectral gap of the interaction matrix. Finally,
we discuss some extensions to the proposed setup such as proximal friendly
functions, time-varying graphs, improvement of the condition numbers.
</summary>
    <author>
      <name>César A. Uribe</name>
    </author>
    <author>
      <name>Soomin Lee</name>
    </author>
    <author>
      <name>Alexander Gasnikov</name>
    </author>
    <author>
      <name>Angelia Nedić</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01587v1</id>
    <updated>2018-09-05T15:51:50Z</updated>
    <published>2018-09-05T15:51:50Z</published>
    <title>GAN Lab: Understanding Complex Deep Generative Models using Interactive
  Visual Experimentation</title>
    <summary>  Recent success in deep learning has generated immense interest among
practitioners and students, inspiring many to learn about this new technology.
While visual and interactive approaches have been successfully developed to
help people more easily learn deep learning, most existing tools focus on
simpler models. In this work, we present GAN Lab, the first interactive
visualization tool designed for non-experts to learn and experiment with
Generative Adversarial Networks (GANs), a popular class of complex deep
learning models. With GAN Lab, users can interactively train generative models
and visualize the dynamic training process's intermediate results. GAN Lab
tightly integrates an model overview graph that summarizes GAN's structure, and
a layered distributions view that helps users interpret the interplay between
submodels. GAN Lab introduces new interactive experimentation features for
learning complex deep learning models, such as step-by-step training at
multiple levels of abstraction for understanding intricate training dynamics.
Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web
browsers, without the need for installation or specialized hardware, overcoming
a major practical challenge in deploying interactive tools for deep learning.
</summary>
    <author>
      <name>Minsuk Kahng</name>
    </author>
    <author>
      <name>Nikhil Thorat</name>
    </author>
    <author>
      <name>Duen Horng Chau</name>
    </author>
    <author>
      <name>Fernanda Viégas</name>
    </author>
    <author>
      <name>Martin Wattenberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVCG.2018.2864500</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVCG.2018.2864500" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be published in the IEEE Transactions on
  Visualization and Computer Graphics, 25(1), January 2019, and presented at
  IEEE VAST 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01571v1</id>
    <updated>2018-09-05T15:14:08Z</updated>
    <published>2018-09-05T15:14:08Z</published>
    <title>Knowledge Integrated Classifier Design Based on Utility Optimization</title>
    <summary>  This paper proposes a systematic framework to design a classification model
that yields a classifier which optimizes a utility function based on prior
knowledge. Specifically, as the data size grows, we prove that the produced
classifier asymptotically converges to the optimal classifier, an extended
version of the Bayes rule, which maximizes the utility function. Therefore, we
provide a meaningful theoretical interpretation for modeling with the knowledge
incorporated. Our knowledge incorporation method allows domain experts to guide
the classifier towards correctly classifying data that they think to be more
significant.
</summary>
    <author>
      <name>Shaohan Chen</name>
    </author>
    <author>
      <name>Chuanhou Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08323v2</id>
    <updated>2018-09-05T15:09:06Z</updated>
    <published>2018-02-22T22:08:14Z</published>
    <title>Deep learning algorithm for data-driven simulation of noisy dynamical
  system</title>
    <summary>  We present a deep learning model, DE-LSTM, for the simulation of a stochastic
process with an underlying nonlinear dynamics. The deep learning model aims to
approximate the probability density function of a stochastic process via
numerical discretization and the underlying nonlinear dynamics is modeled by
the Long Short-Term Memory (LSTM) network. It is shown that, when the numerical
discretization is used, the function estimation problem can be solved by a
multi-label classification problem. A penalized maximum log likelihood method
is proposed to impose a smoothness condition in the prediction of the
probability distribution. We show that the time evolution of the probability
distribution can be computed by a high-dimensional integration of the
transition probability of the LSTM internal states. A Monte Carlo algorithm to
approximate the high-dimensional integration is outlined. The behavior of
DE-LSTM is thoroughly investigated by using the Ornstein-Uhlenbeck process and
noisy observations of nonlinear dynamical systems; Mackey-Glass time series and
forced Van der Pol oscillator. It is shown that DE-LSTM makes a good prediction
of the probability distribution without assuming any distributional properties
of the stochastic process. For a multiple-step forecast of the Mackey-Glass
time series, the prediction uncertainty, denoted by the 95\% confidence
interval, first grows, then dynamically adjusts following the evolution of the
system, while in the simulation of the forced Van der Pol oscillator, the
prediction uncertainty does not grow in time even for a 3,000-step forecast.
</summary>
    <author>
      <name>Kyongmin Yeo</name>
    </author>
    <author>
      <name>Igor Melnyk</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08323v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08323v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01564v1</id>
    <updated>2018-09-05T15:03:23Z</updated>
    <published>2018-09-05T15:03:23Z</published>
    <title>Traffic Density Estimation using a Convolutional Neural Network</title>
    <summary>  The goal of this project is to introduce and present a machine learning
application that aims to improve the quality of life of people in Singapore. In
particular, we investigate the use of machine learning solutions to tackle the
problem of traffic congestion in Singapore. In layman's terms, we seek to make
Singapore (or any other city) a smoother place. To accomplish this aim, we
present an end-to-end system comprising of 1. A traffic density estimation
algorithm at traffic lights/junctions and 2. a suitable traffic signal control
algorithms that make use of the density information for better traffic control.
Traffic density estimation can be obtained from traffic junction images using
various machine learning techniques (combined with CV tools). After research
into various advanced machine learning methods, we decided on convolutional
neural networks (CNNs). We conducted experiments on our algorithms, using the
publicly available traffic camera dataset published by the Land Transport
Authority (LTA) to demonstrate the feasibility of this approach. With these
traffic density estimates, different traffic algorithms can be applied to
minimize congestion at traffic junctions in general.
</summary>
    <author>
      <name>Julian Nubert</name>
    </author>
    <author>
      <name>Nicholas Giai Truong</name>
    </author>
    <author>
      <name>Abel Lim</name>
    </author>
    <author>
      <name>Herbert Ilhan Tanujaya</name>
    </author>
    <author>
      <name>Leah Lim</name>
    </author>
    <author>
      <name>Mai Anh Vu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning Project National University of Singapore. 6 pages, 5
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01560v1</id>
    <updated>2018-09-05T14:56:09Z</updated>
    <published>2018-09-05T14:56:09Z</published>
    <title>Reinforcement Learning under Threats</title>
    <summary>  In several reinforcement learning (RL) scenarios, mainly in security
settings, there may be adversaries trying to interfere with the reward
generating process. In this paper, we introduce Threatened Markov Decision
Processes (TMDPs), which provide a framework to support a decision maker
against a potential adversary in RL. Furthermore, we propose a level-$k$
thinking scheme resulting in a new learning framework to deal with TMDPs. After
introducing our framework and deriving theoretical results, relevant empirical
evidence is given via extensive experiments, showing the benefits of accounting
for adversaries while the agent learns.
</summary>
    <author>
      <name>Víctor Gallego</name>
    </author>
    <author>
      <name>Roi Naveiro</name>
    </author>
    <author>
      <name>David Ríos Insua</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07233v3</id>
    <updated>2018-09-05T14:10:46Z</updated>
    <published>2018-08-22T06:07:03Z</published>
    <title>Neural Architecture Optimization</title>
    <summary>  Automatic neural architecture design has shown its potential in discovering
powerful neural network architectures. Existing methods, no matter based on
reinforcement learning or evolutionary algorithms (EA), conduct architecture
search in a discrete space, which is highly inefficient. In this paper, we
propose a simple and efficient method to automatic neural architecture design
based on continuous optimization. We call this new approach neural architecture
optimization (NAO). There are three key components in our proposed approach:
(1) An encoder embeds/maps neural network architectures into a continuous
space. (2) A predictor takes the continuous representation of a network as
input and predicts its accuracy. (3) A decoder maps a continuous representation
of a network back to its architecture. The performance predictor and the
encoder enable us to perform gradient based optimization in the continuous
space to find the embedding of a new architecture with potentially better
accuracy. Such a better embedding is then decoded to a network by the decoder.
Experiments show that the architecture discovered by our method is very
competitive for image classification task on CIFAR-10 and language modeling
task on PTB, outperforming or on par with the best results of previous
architecture search methods with a significantly reduction of computational
resources. Specifically we obtain $2.07\%$ test set error rate for CIFAR-10
image classification task and $55.9$ test set perplexity of PTB language
modeling task. The best discovered architectures on both tasks are successfully
transferred to other tasks such as CIFAR-100 and WikiText-2.
</summary>
    <author>
      <name>Renqian Luo</name>
    </author>
    <author>
      <name>Fei Tian</name>
    </author>
    <author>
      <name>Tao Qin</name>
    </author>
    <author>
      <name>Enhong Chen</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ongoing work. Will appear at NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07233v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07233v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09540v2</id>
    <updated>2018-09-05T13:49:52Z</updated>
    <published>2018-08-28T20:53:01Z</published>
    <title>Lipschitz regularized Deep Neural Networks converge and generalize</title>
    <summary>  Lipschitz regularized neural networks augment the usual fidelity term used in
training with a regularization term corresponding the excess Lipschitz constant
of the network compared to the Lipschitz constant of the data. We prove that
Lipschitz regularized neural networks converge, and provide a rate, in the
limit as the number of data points $n\to\infty$. We consider the regime where
perfect fitting of data is possible, which means the size of the network grows
with $n$. There are two regimes: in the case of perfect labels, we prove
convergence to the label function which corresponds to zero loss. In the case
of corrupted labels which occurs when the Lipschitz constant of the data blows
up, we prove convergence to a regularized label function which is the solution
of a limiting variational problem.
</summary>
    <author>
      <name>Adam M Oberman</name>
    </author>
    <author>
      <name>Jeff Calder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures. Added a figure and a reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09540v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09540v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05377v2</id>
    <updated>2018-09-05T13:49:26Z</updated>
    <published>2018-08-16T08:45:01Z</published>
    <title>Neural Architecture Search: A Survey</title>
    <summary>  Deep Learning has enabled remarkable progress over the last years on a
variety of tasks, such as image recognition, speech recognition, and machine
translation. One crucial aspect for this progress are novel neural
architectures. Currently employed architectures have mostly been developed
manually by human experts, which is a time-consuming and error-prone process.
Because of this, there is growing interest in automated neural architecture
search methods. We provide an overview of existing work in this field of
research and categorize them according to three dimensions: search space,
search strategy, and performance estimation strategy.
</summary>
    <author>
      <name>Thomas Elsken</name>
    </author>
    <author>
      <name>Jan Hendrik Metzen</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01506v1</id>
    <updated>2018-09-05T13:44:12Z</updated>
    <published>2018-09-05T13:44:12Z</published>
    <title>Deep Reinforcement Learning in High Frequency Trading</title>
    <summary>  The ability to give a precise and fast prediction for the price movement of
stocks is the key to profitability in High Frequency Trading. The main
objective of this paper is to propose a novel way of modeling the high
frequency trading problem using Deep Reinforcement Learning and to argue why
Deep RL can have a lot of potential in the field of High Frequency Trading. We
have analyzed the model's performance based on it's prediction accuracy as well
as prediction speed across full-day trading simulations.
</summary>
    <author>
      <name>Prakhar Ganesh</name>
    </author>
    <author>
      <name>Puneet Rakheja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted in ACM CoDs-COMAD 2019, to be held in kolkata, India in
  January 2019. arXiv admin note: text overlap with arXiv:1708.05866 by other
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01485v1</id>
    <updated>2018-09-05T13:24:35Z</updated>
    <published>2018-09-05T13:24:35Z</published>
    <title>Blind Community Detection from Low-rank Excitations of a Graph Filter</title>
    <summary>  This paper considers a novel framework to detect communities in a graph from
the observation of signals at its nodes. We model the observed signals as noisy
outputs of an unknown network process -- represented as a graph filter -- that
is excited by a set of low-rank inputs. Rather than learning the precise
parameters of the graph itself, the proposed method retrieves the community
structure directly; Furthermore, as in blind system identification methods, it
does not require knowledge of the system excitation. The paper shows that
communities can be detected by applying spectral clustering to the low-rank
output covariance matrix obtained from the graph signals. The performance
analysis indicates that the community detection accuracy depends on the
spectral properties of the graph filter considered. Furthermore, we show that
the accuracy can be improved via a low-rank matrix decomposition method when
the excitation signals are known. Numerical experiments demonstrate that our
approach is effective for analyzing network data from diffusion, consumers, and
social dynamics.
</summary>
    <author>
      <name>Hoi-To Wai</name>
    </author>
    <author>
      <name>Santiago Segarra</name>
    </author>
    <author>
      <name>Asuman E. Ozdaglar</name>
    </author>
    <author>
      <name>Anna Scaglione</name>
    </author>
    <author>
      <name>Ali Jadbabaie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05052v6</id>
    <updated>2018-09-05T12:43:31Z</updated>
    <published>2018-05-14T08:08:33Z</published>
    <title>Machine Learning: Basic Principles</title>
    <summary>  This tutorial is based on the lecture notes for the courses "Machine
Learning: Basic Principles" and "Artificial Intelligence", which I have
(co-)taught since 2015 at Aalto University. The aim is to provide an accessible
introduction to some of the main concepts and methods within machine learning.
Many of the current systems which are considered (artificial) intelligent are
based on combinations of few basic machine learning methods. After formalizing
the main building blocks of a machine learning problem, some popular
algorithmic design patterns for machine learning methods are discussed in some
detail.
</summary>
    <author>
      <name>Alexander Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05052v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05052v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07594v3</id>
    <updated>2018-09-05T11:53:24Z</updated>
    <published>2018-05-19T13:51:34Z</published>
    <title>Generalizing Point Embeddings using the Wasserstein Space of Elliptical
  Distributions</title>
    <summary>  Embedding complex objects as vectors in low dimensional spaces is a
longstanding problem in machine learning. We propose in this work an extension
of that approach, which consists in embedding objects as elliptical probability
distributions, namely distributions whose densities have elliptical level sets.
We endow these measures with the 2-Wasserstein metric, with two important
benefits: (i) For such measures, the squared 2-Wasserstein metric has a closed
form, equal to the sum of the squared Euclidean distance between means and the
squared Bures metric between covariance matrices. The latter is a Riemannian
metric between positive semi-definite matrices, which turns out to be Euclidean
on a suitable factor representation of such matrices, which is valid on the
entire geodesic between these matrices. (ii) The 2-Wasserstein distance boils
down to the usual Euclidean metric when comparing Diracs, and therefore
provides the natural framework to extend point embeddings. We show that for
these reasons Wasserstein elliptical embeddings are more intuitive and yield
tools that are better behaved numerically than the alternative choice of
Gaussian embeddings with the Kullback-Leibler divergence. In particular, and
unlike previous work based on the KL geometry, we learn elliptical
distributions that are not necessarily diagonal. We demonstrate the advantages
of elliptical embeddings by using them for visualization, to compute embeddings
of words, and to reflect entailment or hypernymy.
</summary>
    <author>
      <name>Boris Muzellec</name>
    </author>
    <author>
      <name>Marco Cuturi</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07594v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07594v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01434v1</id>
    <updated>2018-09-05T11:06:06Z</updated>
    <published>2018-09-05T11:06:06Z</published>
    <title>Stellar Cluster Detection using GMM with Deep Variational Autoencoder</title>
    <summary>  Detecting stellar clusters have always been an important research problem in
Astronomy. Although images do not convey very detailed information in detecting
stellar density enhancements, we attempt to understand if new machine learning
techniques can reveal patterns that would assist in drawing better inferences
from the available image data. This paper describes an unsupervised approach in
detecting star clusters using Deep Variational Autoencoder combined with a
Gaussian Mixture Model. We show that our method works significantly well in
comparison with state-of-the-art detection algorithm in recognizing a variety
of star clusters even in the presence of noise and distortion.
</summary>
    <author>
      <name>Arnab Karmakar</name>
    </author>
    <author>
      <name>Deepak Mishra</name>
    </author>
    <author>
      <name>Anandmayee Tej</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures, under review in IEEE RAICS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03832v3</id>
    <updated>2018-09-05T10:34:02Z</updated>
    <published>2018-02-11T22:35:28Z</published>
    <title>Quadrature-based features for kernel approximation</title>
    <summary>  We consider the problem of improving kernel approximation via randomized
feature maps. These maps arise as Monte Carlo approximation to integral
representations of kernel functions and scale up kernel methods for larger
datasets. Based on an efficient numerical integration technique, we propose a
unifying approach that reinterprets the previous random features methods and
extends to better estimates of the kernel approximation. We derive the
convergence behaviour and conduct an extensive empirical study that supports
our hypothesis.
</summary>
    <author>
      <name>Marina Munkhoeva</name>
    </author>
    <author>
      <name>Yermek Kapushev</name>
    </author>
    <author>
      <name>Evgeny Burnaev</name>
    </author>
    <author>
      <name>Ivan Oseledets</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, Appendix: 4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03832v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03832v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09627v3</id>
    <updated>2018-09-05T09:08:43Z</updated>
    <published>2017-06-29T08:42:44Z</published>
    <title>Deep learning bank distress from news and numerical financial data</title>
    <summary>  In this paper we focus our attention on the exploitation of the information
contained in financial news to enhance the performance of a classifier of bank
distress. Such information should be analyzed and inserted into the predictive
model in the most efficient way and this task deals with all the issues related
to text analysis and specifically analysis of news media. Among the different
models proposed for such purpose, we investigate one of the possible deep
learning approaches, based on a doc2vec representation of the textual data, a
kind of neural network able to map the sequential and symbolic text input onto
a reduced latent semantic space. Afterwards, a second supervised neural network
is trained combining news data with standard financial figures to classify
banks whether in distressed or tranquil states, based on a small set of known
distress events. Then the final aim is not only the improvement of the
predictive performance of the classifier but also to assess the importance of
news data in the classification process. Does news data really bring more
useful information not contained in standard financial variables? Our results
seem to confirm such hypothesis.
</summary>
    <author>
      <name>Paola Cerchiello</name>
    </author>
    <author>
      <name>Giancarlo Nicola</name>
    </author>
    <author>
      <name>Samuel Ronnqvist</name>
    </author>
    <author>
      <name>Peter Sarlin</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09627v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09627v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00084v4</id>
    <updated>2018-09-05T08:36:22Z</updated>
    <published>2015-10-01T01:24:20Z</published>
    <title>A Direct Approach for Sparse Quadratic Discriminant Analysis</title>
    <summary>  Quadratic discriminant analysis (QDA) is a standard tool for classification
due to its simplicity and flexibility. Because the number of its parameters
scales quadratically with the number of the variables, QDA is not practical,
however, when the dimensionality is relatively large. To address this, we
propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional
data. Formulated in a simple and coherent framework, DA-QDA aims to directly
estimate the key quantities in the Bayes discriminant function including
quadratic interactions and a linear index of the variables for classification.
Under appropriate sparsity assumptions, we establish consistency results for
estimating the interactions and the linear index, and further demonstrate that
the misclassification rate of our procedure converges to the optimal Bayes
risk, even when the dimensionality is exponentially high with respect to the
sample size. An efficient algorithm based on the alternating direction method
of multipliers (ADMM) is developed for finding interactions, which is much
faster than its competitor in the literature. The promising performance of
DA-QDA is illustrated via extensive simulation studies and the analysis of four
real datasets.
</summary>
    <author>
      <name>Binyan Jiang</name>
    </author>
    <author>
      <name>Xiangyu Wang</name>
    </author>
    <author>
      <name>Chenlei Leng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated to the JMLR format</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00084v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00084v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01382v1</id>
    <updated>2018-09-05T08:32:01Z</updated>
    <published>2018-09-05T08:32:01Z</published>
    <title>Anytime Hedge achieves optimal regret in the stochastic regime</title>
    <summary>  This paper is about a surprising fact: we prove that the anytime Hedge
algorithm with decreasing learning rate, which is one of the simplest algorithm
for the problem of prediction with expert advice, is actually both worst-case
optimal and adaptive to the easier stochastic and adversarial with a gap
problems. This runs counter to the common belief in the literature that this
algorithm is overly conservative, and that only new adaptive algorithms can
simultaneously achieve minimax regret and adapt to the difficulty of the
problem. Moreover, our analysis exhibits qualitative differences with other
variants of the Hedge algorithm, based on the so-called "doubling trick", and
the fixed-horizon version (with constant learning rate).
</summary>
    <author>
      <name>Jaouad Mourtada</name>
    </author>
    <author>
      <name>Stéphane Gaïffas</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01369v1</id>
    <updated>2018-09-05T07:56:11Z</updated>
    <published>2018-09-05T07:56:11Z</published>
    <title>Towards quantitative methods to assess network generative models</title>
    <summary>  Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
</summary>
    <author>
      <name>Vahid Mostofi</name>
    </author>
    <author>
      <name>Sadegh Aliakbary</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01357v1</id>
    <updated>2018-09-05T07:13:30Z</updated>
    <published>2018-09-05T07:13:30Z</published>
    <title>Zero Shot Learning for Code Education: Rubric Sampling with Deep
  Learning Inference</title>
    <summary>  In modern computer science education, massive open online courses (MOOCs) log
thousands of hours of data about how students solve coding challenges. Being so
rich in data, these platforms have garnered the interest of the machine
learning community, with many new algorithms attempting to autonomously provide
feedback to help future students learn. But what about those first hundred
thousand students? In most educational contexts (i.e. classrooms), assignments
do not have enough historical data for supervised learning. In this paper, we
introduce a human-in-the-loop "rubric sampling" approach to tackle the "zero
shot" feedback challenge. We are able to provide autonomous feedback for the
first students working on an introductory programming assignment with accuracy
that substantially outperforms data-hungry algorithms and approaches human
level fidelity. Rubric sampling requires minimal teacher effort, can associate
feedback with specific parts of a student's solution and can articulate a
student's misconceptions in the language of the instructor. Deep learning
inference enables rubric sampling to further improve as more assignment
specific student data is acquired. We demonstrate our results on a novel
dataset from Code.org, the world's largest programming education platform.
</summary>
    <author>
      <name>Mike Wu</name>
    </author>
    <author>
      <name>Milan Mosse</name>
    </author>
    <author>
      <name>Noah Goodman</name>
    </author>
    <author>
      <name>Chris Piech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01354v1</id>
    <updated>2018-09-05T06:50:24Z</updated>
    <published>2018-09-05T06:50:24Z</published>
    <title>Semantic Human Matting</title>
    <summary>  Human matting, high quality extraction of humans from natural images, is
crucial for a wide variety of applications. Since the matting problem is
severely under-constrained, most previous methods require user interactions to
take user designated trimaps or scribbles as constraints. This user-in-the-loop
nature makes them difficult to be applied to large scale data or time-sensitive
scenarios. In this paper, instead of using explicit user input constraints, we
employ implicit semantic constraints learned from data and propose an automatic
human matting algorithm (SHM). SHM is the first algorithm that learns to
jointly fit both semantic information and high quality details with deep
networks. In practice, simultaneously learning both coarse semantics and fine
details is challenging. We propose a novel fusion strategy which naturally
gives a probabilistic estimation of the alpha matte. We also construct a very
large dataset with high quality annotations consisting of 35,513 unique
foregrounds to facilitate the learning and evaluation of human matting.
Extensive experiments on this dataset and plenty of real images show that SHM
achieves comparable results with state-of-the-art interactive matting methods.
</summary>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Tiezheng Ge</name>
    </author>
    <author>
      <name>Yanyu Xu</name>
    </author>
    <author>
      <name>Zhiqiang Zhang</name>
    </author>
    <author>
      <name>Xinxin Yang</name>
    </author>
    <author>
      <name>Kun Gai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01353v1</id>
    <updated>2018-09-05T06:49:12Z</updated>
    <published>2018-09-05T06:49:12Z</published>
    <title>IKA: Independent Kernel Approximator</title>
    <summary>  This paper describes a new method for low rank kernel approximation called
IKA. The main advantage of IKA is that it produces a function $\psi(x)$ defined
as a linear combination of arbitrarily chosen functions. In contrast the
approximation produced by Nystr\"om method is a linear combination of kernel
evaluations. The proposed method consistently outperformed Nystr\"om method in
a comparison on the STL-10 dataset. Numerical results are reproducible using
the source code available at https://gitlab.com/matteo-ronchetti/IKA
</summary>
    <author>
      <name>Matteo Ronchetti</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01341v1</id>
    <updated>2018-09-05T06:07:31Z</updated>
    <published>2018-09-05T06:07:31Z</published>
    <title>Embedding Multimodal Relational Data for Knowledge Base Completion</title>
    <summary>  Representing entities and relations in an embedding space is a well-studied
approach for machine learning on relational data. Existing approaches, however,
primarily focus on simple link structure between a finite set of entities,
ignoring the variety of data types that are often used in knowledge bases, such
as text, images, and numerical values. In this paper, we propose multimodal
knowledge base embeddings (MKBE) that use different neural encoders for this
variety of observed data, and combine them with existing relational models to
learn embeddings of the entities and multimodal data. Further, using these
learned embedings and different neural decoders, we introduce a novel
multimodal imputation model to generate missing multimodal values, like text
and images, from information in the knowledge base. We enrich existing
relational datasets to create two novel benchmarks that contain additional
information such as textual descriptions and images of the original entities.
We demonstrate that our models utilize this additional information effectively
to provide more accurate link prediction, achieving state-of-the-art results
with a considerable gap of 5-7% over existing methods. Further, we evaluate the
quality of our generated multimodal values via a user study. We have release
the datasets and the open-source implementation of our models at
https://github.com/pouyapez/mkbe.
</summary>
    <author>
      <name>Pouya Pezeshkpour</name>
    </author>
    <author>
      <name>Liyan Chen</name>
    </author>
    <author>
      <name>Sameer Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05978v3</id>
    <updated>2018-09-05T05:37:03Z</updated>
    <published>2018-06-15T13:55:18Z</published>
    <title>Bayesian Convolutional Neural Networks</title>
    <summary>  We introduce Bayesian Convolutional Neural Networks (BayesCNNs), a variant of
Convolutional Neural Networks (CNNs) which is built upon Bayes by Backprop. We
demonstrate how this novel reliable variational inference method can serve as a
fundamental construct for various network architectures. On multiple datasets
in supervised learning settings (MNIST, CIFAR-10, CIFAR-100, and STL-10), our
proposed variational inference method achieves performances equivalent to
frequentist inference in identical architectures, while a measurement for
uncertainties and a regularisation are incorporated naturally. In the past,
Bayes by Backprop has been successfully implemented in feedforward and
recurrent neural networks, but not in convolutional ones. This work symbolises
the extension of Bayesian neural networks which encompasses all three
aforementioned types of network architectures now.
</summary>
    <author>
      <name>Kumar Shridhar</name>
    </author>
    <author>
      <name>Felix Laumann</name>
    </author>
    <author>
      <name>Adrian Llopart Maurin</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1704.02798 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05978v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05978v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01319v1</id>
    <updated>2018-09-05T04:44:55Z</updated>
    <published>2018-09-05T04:44:55Z</published>
    <title>Cross validation residuals for generalised least squares and other
  correlated data models</title>
    <summary>  Cross validation residuals are well known for the ordinary least squares
model. Here leave-M-out cross validation is extended to generalised least
squares. The relationship between cross validation residuals and Cook's
distance is demonstrated, in terms of an approximation to the difference in the
generalised residual sum of squares for a model fit to all the data (training
and test) and a model fit to a reduced dataset (training data only). For
generalised least squares, as for ordinary least squares, there is no need to
refit the model to reduced size datasets as all the values for K fold cross
validation are available after fitting the model to all the data.
</summary>
    <author>
      <name>Ingrid Annette Baade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02081v2</id>
    <updated>2018-09-05T04:25:45Z</updated>
    <published>2018-04-05T23:41:11Z</published>
    <title>Adaptive Diffusions for Scalable Learning over Graphs</title>
    <summary>  Diffusion-based classifiers such as those relying on the Personalized
PageRank and the Heat kernel, enjoy remarkable classification accuracy at
modest computational requirements. Their performance however is affected by the
extent to which the chosen diffusion captures a typically unknown label
propagation mechanism, that can be specific to the underlying graph, and
potentially different for each class. The present work introduces a
disciplined, data-efficient approach to learning class-specific diffusion
functions adapted to the underlying network topology. The novel learning
approach leverages the notion of "landing probabilities" of class-specific
random walks, which can be computed efficiently, thereby ensuring scalability
to large graphs. This is supported by rigorous analysis of the properties of
the model as well as the proposed algorithms. Furthermore, a robust version of
the classifier facilitates learning even in noisy environments.
  Classification tests on real networks demonstrate that adapting the diffusion
function to the given graph and observed labels, significantly improves the
performance over fixed diffusions; reaching -- and many times surpassing -- the
classification accuracy of computationally heavier state-of-the-art competing
methods, that rely on node embeddings and deep neural networks.
</summary>
    <author>
      <name>Dimitris Berberidis</name>
    </author>
    <author>
      <name>Athanasios N. Nikolakopoulos</name>
    </author>
    <author>
      <name>Georgios B. Giannakis</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01316v1</id>
    <updated>2018-09-05T04:15:13Z</updated>
    <published>2018-09-05T04:15:13Z</published>
    <title>Learning User Preferences and Understanding Calendar Contexts for Event
  Scheduling</title>
    <summary>  With online calendar services gaining popularity worldwide, calendar data has
become one of the richest context sources for understanding human behavior.
However, event scheduling is still time-consuming even with the development of
online calendars. Although machine learning based event scheduling models have
automated scheduling processes to some extent, they often fail to understand
subtle user preferences and complex calendar contexts with event titles written
in natural language. In this paper, we propose Neural Event Scheduling
Assistant (NESA) which learns user preferences and understands calendar
contexts, directly from raw online calendars for fully automated and highly
effective event scheduling. We leverage over 593K calendar events for NESA to
learn scheduling personal events, and we further utilize NESA for
multi-attendee event scheduling. NESA successfully incorporates deep neural
networks such as Bidirectional Long Short-Term Memory, Convolutional Neural
Network, and Highway Network for learning the preferences of each user and
understanding calendar context based on natural languages. The experimental
results show that NESA significantly outperforms previous baseline models in
terms of various evaluation metrics on both personal and multi-attendee event
scheduling tasks. Our qualitative analysis demonstrates the effectiveness of
each layer in NESA and learned user preferences.
</summary>
    <author>
      <name>Donghyeon Kim</name>
    </author>
    <author>
      <name>Jinhyuk Lee</name>
    </author>
    <author>
      <name>Donghee Choi</name>
    </author>
    <author>
      <name>Jaehoon Choi</name>
    </author>
    <author>
      <name>Jaewoo Kang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271712</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271712" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CIKM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50, 68U35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01293v1</id>
    <updated>2018-09-05T01:55:28Z</updated>
    <published>2018-09-05T01:55:28Z</published>
    <title>Stochastic Particle-Optimization Sampling and the Non-Asymptotic
  Convergence Theory</title>
    <summary>  Particle-optimization sampling (POS) is a recently developed technique to
generate high-quality samples from a target distribution by iteratively
updating a set of interactive particles. A representative algorithm is the
Stein variational gradient descent (SVGD). Though obtaining significant
empirical success, the {\em non-asymptotic} convergence behavior of SVGD
remains unknown. In this paper, we generalize POS to a stochasticity setting by
injecting random noise in particle updates, called stochastic
particle-optimization sampling (SPOS). Standard SVGD can be regarded as a
special case of our framework. Notably, for the first time, we develop
non-asymptotic convergence theory for the SPOS framework (which includes SVGD),
characterizing the bias of a sample approximation w.r.t. the numbers of
particles and iterations under both convex- and noncovex-energy-function
settings. Remarkably, we provide theoretical understand of a pitfall of SVGD
that can be avoided in the proposed SPOS framework, i.e., particles tent to
collapse to a local mode in SVGD under some particular conditions. Our theory
is based on the analysis of nonlinear stochastic differential equations, which
serves as an extension and a complemented development to the asymptotic
convergence theory for SVGD such as [1].
</summary>
    <author>
      <name>Jianyi Zhang</name>
    </author>
    <author>
      <name>Ruiyi Zhang</name>
    </author>
    <author>
      <name>Changyou Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Use NIPS template</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00403v2</id>
    <updated>2018-09-05T01:45:01Z</updated>
    <published>2018-09-02T22:17:52Z</published>
    <title>Effective Exploration for Deep Reinforcement Learning via Bootstrapped
  Q-Ensembles under Tsallis Entropy Regularization</title>
    <summary>  Recently deep reinforcement learning (DRL) has achieved outstanding success
on solving many difficult and large-scale RL problems. However the high sample
cost required for effective learning often makes DRL unaffordable in
resource-limited applications. With the aim of improving sample efficiency and
learning performance, we will develop a new DRL algorithm in this paper that
seamless integrates entropy-induced and bootstrap-induced techniques for
efficient and deep exploration of the learning environment. Specifically, a
general form of Tsallis entropy regularizer will be utilized to drive
entropy-induced exploration based on efficient approximation of optimal
action-selection policies. Different from many existing works that rely on
action dithering strategies for exploration, our algorithm is efficient in
exploring actions with clear exploration value. Meanwhile, by employing an
ensemble of Q-networks under varied Tsallis entropy regularization, the
diversity of the ensemble can be further enhanced to enable effective
bootstrap-induced exploration. Experiments on Atari game playing tasks clearly
demonstrate that our new algorithm can achieve more efficient and effective
exploration for DRL, in comparison to recently proposed exploration methods
including Bootstrapped Deep Q-Network and UCB Q-Ensemble.
</summary>
    <author>
      <name>Gang Chen</name>
    </author>
    <author>
      <name>Yiming Peng</name>
    </author>
    <author>
      <name>Mengjie Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00403v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00403v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02436v7</id>
    <updated>2018-09-04T22:39:50Z</updated>
    <published>2017-05-06T03:13:21Z</published>
    <title>Nonlinear Information Bottleneck</title>
    <summary>  Information bottleneck [IB] is a technique for extracting information in some
`input' random variable that is relevant for predicting some different 'output'
random variable. IB works by encoding the input in a compressed 'bottleneck
variable' from which the output can then be accurately decoded. IB can be
difficult to compute in practice, and has been mainly developed for two limited
cases: (1) discrete random variables with small state spaces, and (2)
continuous random variables that are jointly Gaussian distributed (in which
case the encoding and decoding maps are linear). We propose a method to perform
IB in more general domains. Our approach can be applied to discrete or
continuous inputs and outputs, and allows for nonlinear encoding and decoding
maps. The method uses a novel upper bound on the IB objective, derived using a
non-parametric estimator of mutual information and a variational approximation.
We show how to implement the method using neural networks and gradient-based
optimization, and demonstrate its performance on the MNIST dataset.
</summary>
    <author>
      <name>Artemy Kolchinsky</name>
    </author>
    <author>
      <name>Brendan D. Tracey</name>
    </author>
    <author>
      <name>David H. Wolpert</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02436v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02436v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06834v2</id>
    <updated>2018-09-04T22:36:18Z</updated>
    <published>2018-05-17T16:04:39Z</published>
    <title>Subspace Estimation from Incomplete Observations: A High-Dimensional
  Analysis</title>
    <summary>  We present a high-dimensional analysis of three popular algorithms, namely,
Oja's method, GROUSE and PETRELS, for subspace estimation from streaming and
highly incomplete observations. We show that, with proper time scaling, the
time-varying principal angles between the true subspace and its estimates given
by the algorithms converge weakly to deterministic processes when the ambient
dimension $n$ tends to infinity. Moreover, the limiting processes can be
exactly characterized as the unique solutions of certain ordinary differential
equations (ODEs). A finite sample bound is also given, showing that the rate of
convergence towards such limits is $\mathcal{O}(1/\sqrt{n})$. In addition to
providing asymptotically exact predictions of the dynamic performance of the
algorithms, our high-dimensional analysis yields several insights, including an
asymptotic equivalence between Oja's method and GROUSE, and a precise scaling
relationship linking the amount of missing data to the signal-to-noise ratio.
By analyzing the solutions of the limiting ODEs, we also establish phase
transition phenomena associated with the steady-state performance of these
techniques.
</summary>
    <author>
      <name>Chuang Wang</name>
    </author>
    <author>
      <name>Yonina C. Eldar</name>
    </author>
    <author>
      <name>Yue M. Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06834v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06834v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06296v2</id>
    <updated>2018-09-04T22:25:10Z</updated>
    <published>2018-08-20T03:46:15Z</published>
    <title>Universal Stagewise Learning for Non-Convex Problems with Convergence on
  Averaged Solutions</title>
    <summary>  Although stochastic gradient descent (SGD) method and its variants (e.g.,
stochastic momentum methods, AdaGrad) are the choice of algorithms for solving
non-convex problems (especially deep learning), there still remain big gaps
between the theory and the practice with many questions unresolved. For
example, there is still a lack of theories of convergence for SGD and its
variants that use stagewise step size and return an averaged solution in
practice. In addition, theoretical insights of why adaptive step size of
AdaGrad could improve non-adaptive step size of {\sgd} is still missing for
non-convex optimization. This paper aims to address these questions and fill
the gap between theory and practice. We propose a universal stagewise
optimization framework for a broad family of {\bf non-smooth non-convex}
(namely weakly convex) problems with the following key features: (i) at each
stage any suitable stochastic convex optimization algorithms (e.g., SGD or
AdaGrad) that return an averaged solution can be employed for minimizing a
regularized convex problem; (ii) the step size is decreased in a stagewise
manner; (iii) an averaged solution is returned as the final solution that is
selected from all stagewise averaged solutions with sampling probabilities {\it
increasing} as the stage number. Our theoretical results of stagewise AdaGrad
exhibit its adaptive convergence, therefore shed insights on its faster
convergence for problems with sparse stochastic gradients than stagewise SGD.
To the best of our knowledge, these new results are the first of their kind for
addressing the unresolved issues of existing theories mentioned earlier.
</summary>
    <author>
      <name>Zaiyi Chen</name>
    </author>
    <author>
      <name>Tianbao Yang</name>
    </author>
    <author>
      <name>Jinfeng Yi</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <author>
      <name>Enhong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added some new results</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03487v2</id>
    <updated>2018-09-04T20:58:56Z</updated>
    <published>2018-02-10T00:49:17Z</published>
    <title>Spurious local minima in neural networks: a critical view</title>
    <summary>  We investigate the loss surface of nonlinear neural networks. We prove that
even for networks with one hidden layer and "slightest" nonlinearity, there can
be spurious local minima. Our results thus indicate that in general "no
spurious local minima" is a property limited to deep linear networks.
Specifically, for ReLU(-like) networks we prove that for almost all (in
contrast to previous results) practical datasets there exist infinitely many
local minima. We also present a counterexample for more general activation
functions (such as sigmoid, tanh, arctan, ReLU, etc.), for which there exists a
local minimum strictly inferior to the global minimum. Our results make the
least restrictive assumptions relative to the existing results on local
optimality in neural networks. We complete our discussion by presenting a
comprehensive characterization of global optimality for deep linear networks.
Our results unify and subsume other results on this topic.
</summary>
    <author>
      <name>Chulhee Yun</name>
    </author>
    <author>
      <name>Suvrit Sra</name>
    </author>
    <author>
      <name>Ali Jadbabaie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages. Updates from v1: change of the title and the organization
  of the paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01551v3</id>
    <updated>2018-09-04T20:40:14Z</updated>
    <published>2016-12-05T21:18:00Z</published>
    <title>Deep learning in color: towards automated quark/gluon jet discrimination</title>
    <summary>  Artificial intelligence offers the potential to automate challenging
data-processing tasks in collider physics. To establish its prospects, we
explore to what extent deep learning with convolutional neural networks can
discriminate quark and gluon jets better than observables designed by
physicists. Our approach builds upon the paradigm that a jet can be treated as
an image, with intensity given by the local calorimeter deposits. We supplement
this construction by adding color to the images, with red, green and blue
intensities given by the transverse momentum in charged particles, transverse
momentum in neutral particles, and pixel-level charged particle counts.
Overall, the deep networks match or outperform traditional jet variables. We
also find that, while various simulations produce different quark and gluon
jets, the neural networks are surprisingly insensitive to these differences,
similar to traditional observables. This suggests that the networks can extract
robust physical information from imperfect simulations.
</summary>
    <author>
      <name>Patrick T. Komiske</name>
    </author>
    <author>
      <name>Eric M. Metodiev</name>
    </author>
    <author>
      <name>Matthew D. Schwartz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/JHEP01(2017)110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/JHEP01(2017)110" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 9 figures, updated to JHEP version, added table of
  contents, minor typos fixed</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JHEP 01 (2017) 110</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.01551v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01551v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01229v1</id>
    <updated>2018-09-04T20:09:01Z</updated>
    <published>2018-09-04T20:09:01Z</published>
    <title>t-Exponential Memory Networks for Question-Answering Machines</title>
    <summary>  Recent advances in deep learning have brought to the fore models that can
make multiple computational steps in the service of completing a task; these
are capable of describ- ing long-term dependencies in sequential data. Novel
recurrent attention models over possibly large external memory modules
constitute the core mechanisms that enable these capabilities. Our work
addresses learning subtler and more complex underlying temporal dynamics in
language modeling tasks that deal with sparse sequential data. To this end, we
improve upon these recent advances, by adopting concepts from the field of
Bayesian statistics, namely variational inference. Our proposed approach
consists in treating the network parameters as latent variables with a prior
distribution imposed over them. Our statistical assumptions go beyond the
standard practice of postulating Gaussian priors. Indeed, to allow for handling
outliers, which are prevalent in long observed sequences of multivariate data,
multivariate t-exponential distributions are imposed. On this basis, we proceed
to infer corresponding posteriors; these can be used for inference and
prediction at test time, in a way that accounts for the uncertainty in the
available sparse training data. Specifically, to allow for our approach to best
exploit the merits of the t-exponential family, our method considers a new
t-divergence measure, which generalizes the concept of the Kullback-Leibler
divergence. We perform an extensive experimental evaluation of our approach,
using challenging language modeling benchmarks, and illustrate its superiority
over existing state-of-the-art techniques.
</summary>
    <author>
      <name>Kyriakos Tolias</name>
    </author>
    <author>
      <name>Sotirios Chatzis</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01225v1</id>
    <updated>2018-09-04T19:58:06Z</updated>
    <published>2018-09-04T19:58:06Z</published>
    <title>Compositional Stochastic Average Gradient for Machine Learning and
  Related Applications</title>
    <summary>  Many machine learning, statistical inference, and portfolio optimization
problems require minimization of a composition of expected value functions
(CEVF). Of particular interest is the finite-sum versions of such compositional
optimization problems (FS-CEVF). Compositional stochastic variance reduced
gradient (C-SVRG) methods that combine stochastic compositional gradient
descent (SCGD) and stochastic variance reduced gradient descent (SVRG) methods
are the state-of-the-art methods for FS-CEVF problems. We introduce
compositional stochastic average gradient descent (C-SAG) a novel extension of
the stochastic average gradient method (SAG) to minimize composition of
finite-sum functions. C-SAG, like SAG, estimates gradient by incorporating
memory of previous gradient information. We present theoretical analyses of
C-SAG which show that C-SAG, like SAG, and C-SVRG, achieves a linear
convergence rate when the objective function is strongly convex; However, C-CAG
achieves lower oracle query complexity per iteration than C-SVRG. Finally, we
present results of experiments showing that C-SAG converges substantially
faster than full gradient (FG), as well as C-SVRG.
</summary>
    <author>
      <name>Tsung-Yu Hsieh</name>
    </author>
    <author>
      <name>Yasser EL-Manzalawy</name>
    </author>
    <author>
      <name>Yiwei Sun</name>
    </author>
    <author>
      <name>Vasant Honavar</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01185v1</id>
    <updated>2018-09-04T18:24:37Z</updated>
    <published>2018-09-04T18:24:37Z</published>
    <title>DeepPINK: reproducible feature selection in deep neural networks</title>
    <summary>  Deep learning has become increasingly popular in both supervised and
unsupervised machine learning thanks to its outstanding empirical performance.
However, because of their intrinsic complexity, most deep learning methods are
largely treated as black box tools with little interpretability. Even though
recent attempts have been made to facilitate the interpretability of deep
neural networks (DNNs), existing methods are susceptible to noise and lack of
robustness.
  Therefore, scientists are justifiably cautious about the reproducibility of
the discoveries, which is often related to the interpretability of the
underlying statistical models. In this paper, we describe a method to increase
the interpretability and reproducibility of DNNs by incorporating the idea of
feature selection with controlled error rate. By designing a new DNN
architecture and integrating it with the recently proposed knockoffs framework,
we perform feature selection with a controlled error rate, while maintaining
high power. This new method, DeepPINK (Deep feature selection using
Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data
sets to demonstrate its empirical utility.
</summary>
    <author>
      <name>Yang Young Lu</name>
    </author>
    <author>
      <name>Jinchi Lv</name>
    </author>
    <author>
      <name>Yingying Fan</name>
    </author>
    <author>
      <name>William Stafford Noble</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06546v2</id>
    <updated>2018-09-04T17:57:45Z</updated>
    <published>2018-05-16T22:29:15Z</published>
    <title>Joint Classification and Prediction CNN Framework for Automatic Sleep
  Stage Classification</title>
    <summary>  Correctly identifying sleep stages is important in diagnosing and treating
sleep disorders. This work proposes a joint classification-and-prediction
framework based on CNNs for automatic sleep staging, and, subsequently,
introduces a simple yet efficient CNN architecture to power the framework.
Given a single input epoch, the novel framework jointly determines its label
(classification) and its neighboring epochs' labels (prediction) in the
contextual output. While the proposed framework is orthogonal to the widely
adopted classification schemes, which take one or multiple epochs as contextual
inputs and produce a single classification decision on the target epoch, we
demonstrate its advantages in several ways. First, it leverages the dependency
among consecutive sleep epochs while surpassing the problems experienced with
the common classification schemes. Second, even with a single model, the
framework has the capacity to produce multiple decisions, which are essential
in obtaining a good performance as in ensemble-of-models methods, with very
little induced computational overhead. Probabilistic aggregation techniques are
then proposed to leverage the availability of multiple decisions. We conducted
experiments on two public datasets: Sleep-EDF Expanded with 20 subjects, and
Montreal Archive of Sleep Studies dataset with 200 subjects. The proposed
framework yields an overall classification accuracy of 82.3% and 83.6%,
respectively. We also show that the proposed framework not only is superior to
the baselines based on the common classification schemes but also outperforms
existing deep-learning approaches. To our knowledge, this is the first work
going beyond the standard single-output classification to consider multitask
neural networks for automatic sleep staging. This framework provides avenues
for further studies of different neural-network architectures for automatic
sleep staging.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Fernando Andreotti</name>
    </author>
    <author>
      <name>Navin Cooray</name>
    </author>
    <author>
      <name>Oliver Y. Chén</name>
    </author>
    <author>
      <name>Maarten De Vos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication.
  Changes in this version include additional experiments on Sleep-EDF Expanded,
  comparison with ensemble methods, as well as several other minor changes.
  Source code is available online</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06546v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06546v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08438v2</id>
    <updated>2018-09-04T17:14:57Z</updated>
    <published>2018-04-23T13:54:47Z</published>
    <title>A Spoofing Benchmark for the 2018 Voice Conversion Challenge: Leveraging
  from Spoofing Countermeasures for Speech Artifact Assessment</title>
    <summary>  Voice conversion (VC) aims at conversion of speaker characteristic without
altering content. Due to training data limitations and modeling imperfections,
it is difficult to achieve believable speaker mimicry without introducing
processing artifacts; performance assessment of VC, therefore, usually involves
both speaker similarity and quality evaluation by a human panel. As a
time-consuming, expensive, and non-reproducible process, it hinders rapid
prototyping of new VC technology. We address artifact assessment using an
alternative, objective approach leveraging from prior work on spoofing
countermeasures (CMs) for automatic speaker verification. Therein, CMs are used
for rejecting `fake' inputs such as replayed, synthetic or converted speech but
their potential for automatic speech artifact assessment remains unknown. This
study serves to fill that gap. As a supplement to subjective results for the
2018 Voice Conversion Challenge (VCC'18) data, we configure a standard
constant-Q cepstral coefficient CM to quantify the extent of processing
artifacts. Equal error rate (EER) of the CM, a confusability index of VC
samples with real human speech, serves as our artifact measure. Two clusters of
VCC'18 entries are identified: low-quality ones with detectable artifacts (low
EERs), and higher quality ones with less artifacts. None of the VCC'18 systems,
however, is perfect: all EERs are &lt; 30 % (the `ideal' value would be 50 %). Our
preliminary findings suggest potential of CMs outside of their original
application, as a supplemental optimization and benchmarking tool to enhance VC
technology.
</summary>
    <author>
      <name>Tomi Kinnunen</name>
    </author>
    <author>
      <name>Jaime Lorenzo-Trueba</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <author>
      <name>Tomoki Toda</name>
    </author>
    <author>
      <name>Daisuke Saito</name>
    </author>
    <author>
      <name>Fernando Villavicencio</name>
    </author>
    <author>
      <name>Zhenhua Ling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Correction (bug fix) of a published ODYSSEY 2018 publication with the
  same title and author list; more details in footnote in page 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08438v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08438v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01093v1</id>
    <updated>2018-09-04T16:59:53Z</updated>
    <published>2018-09-04T16:59:53Z</published>
    <title>Adversarial Attacks on Node Embeddings</title>
    <summary>  The goal of network representation learning is to learn low-dimensional node
embeddings that capture the graph structure and are useful for solving
downstream tasks. However, despite the proliferation of such methods there is
currently no study of their robustness to adversarial attacks. We provide the
first adversarial vulnerability analysis on the widely used family of methods
based on random walks. We derive efficient adversarial perturbations that
poison the network structure and have a negative effect on both the quality of
the embeddings and the downstream tasks. We further show that our attacks are
transferable -- they generalize to many models -- and are successful even when
the attacker has restricted actions.
</summary>
    <author>
      <name>Aleksandar Bojcheski</name>
    </author>
    <author>
      <name>Stephan Günnemann</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01090v1</id>
    <updated>2018-09-04T16:53:04Z</updated>
    <published>2018-09-04T16:53:04Z</published>
    <title>A Quantum Spatial Graph Convolutional Neural Network using Quantum
  Passing Information</title>
    <summary>  In this paper, we develop a new Quantum Spatial Graph Convolutional Neural
Network (QSGCNN) model that can directly learn a classification function for
graphs of arbitrary sizes. Unlike state-of-the-art Graph Convolutional Neural
Network (GCN) models, the proposed QSGCNN model incorporates the process of
identifying transitive aligned vertices between graphs, and transforms
arbitrary sized graphs into fixed-sized aligned vertex grid structures. To
further learn representative graph characteristics, a new quantum spatial graph
convolution is proposed and employed to extract multi-scale vertex features, in
terms of quantum passing information between grid vertices of each graph. Since
the quantum spatial convolution preserves the property of the input grid
structures, the proposed QSGCNN model allows to directly employ the traditional
convolutional neural network to further learn from the global graph topology,
providing an end-to-end deep learning architecture that integrates the graph
representation and learning in the quantum spatial graph convolution layer and
the traditional convolutional layer for graph classifications. We demonstrate
the effectiveness of the proposed QSGCNN model in terms of the theoretical
connections to state-of-the-art methods. The proposed QSGCNN model addresses
the shortcomings of information loss and imprecise information representation
arising in existing GCN models associated with SortPooling or SumPooling
layers. Experimental results on benchmark graph classification datasets
demonstrate the effectiveness of the proposed QSGCNN model.
</summary>
    <author>
      <name>Lu Bai</name>
    </author>
    <author>
      <name>Yuhang Jiao</name>
    </author>
    <author>
      <name>Luca Rossi</name>
    </author>
    <author>
      <name>Lixin Cui</name>
    </author>
    <author>
      <name>Jian Cheng</name>
    </author>
    <author>
      <name>Edwin R. Hancock</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01079v1</id>
    <updated>2018-09-04T16:38:01Z</updated>
    <published>2018-09-04T16:38:01Z</published>
    <title>Chi-Square Test Neural Network: A New Binary Classifier based on
  Backpropagation Neural Network</title>
    <summary>  We introduce the chi-square test neural network: a single hidden layer
backpropagation neural network using chi-square test theorem to redefine the
cost function and the error function. The weights and thresholds are modified
using standard backpropagation algorithm. The proposed approach has the
advantage of making consistent data distribution over training and testing
sets. It can be used for binary classification. The experimental results on
real world data sets indicate that the proposed algorithm can significantly
improve the classification accuracy comparing to related approaches.
</summary>
    <author>
      <name>Yuan Wu</name>
    </author>
    <author>
      <name>Lingling Li</name>
    </author>
    <author>
      <name>Lian Li</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03230v2</id>
    <updated>2018-09-04T16:20:07Z</updated>
    <published>2018-08-09T16:46:51Z</published>
    <title>Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal
  densities?</title>
    <summary>  Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of
Markov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity
of HMC algorithms is their excellent performance as the dimension $d$ of the
target becomes large: under conditions that are satisfied for many common
statistical models, optimally-tuned HMC algorithms have a running time that
scales like $d^{0.25}$. In stark contrast, the running time of the usual
Random-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This
superior scaling of the HMC algorithm with dimension is attributed to the fact
that it, unlike RWM, incorporates the gradient information in the proposal
distribution. In this paper, we investigate a different scaling question: does
HMC beat RWM for highly $\textit{multimodal}$ targets? We find that the answer
is often $\textit{no}$. We compute the spectral gaps for both the algorithms
for a specific class of multimodal target densities, and show that they are
identical. The key reason is that, within one mode, the gradient is effectively
ignorant about other modes, thus negating the advantage the HMC algorithm
enjoys in unimodal targets. We also give heuristic arguments suggesting that
the above observation may hold quite generally. Our main tool for answering
this question is a novel simple formula for the conductance of HMC using
Liouville's theorem. This result allows us to compute the spectral gap of HMC
algorithms, for both the classical HMC with isotropic momentum and the recent
Riemannian HMC, for multimodal targets.
</summary>
    <author>
      <name>Oren Mangoubi</name>
    </author>
    <author>
      <name>Natesh S. Pillai</name>
    </author>
    <author>
      <name>Aaron Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03230v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03230v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01022v1</id>
    <updated>2018-09-04T14:31:42Z</updated>
    <published>2018-09-04T14:31:42Z</published>
    <title>A Neural Network Aided Approach for LDPC Coded DCO-OFDM with Clipping
  Distortion</title>
    <summary>  In this paper, a neural network-aided bit-interleaved coded modulation
(NN-BICM) receiver is designed to mitigate the nonlinear clipping distortion in
the LDPC coded direct currentbiased optical orthogonal frequency division
multiplexing (DCOOFDM) systems. Taking the cross-entropy as loss function, a
feed forward network is trained by backpropagation algorithm to output the
condition probability through the softmax activation function, thereby
assisting in a modified log-likelihood ratio (LLR) improvement. To reduce the
complexity, this feed-forward network simplifies the input layer with a single
symbol and the corresponding Gaussian variance instead of focusing on the
inter-carrier interference between multiple subcarriers. On the basis of the
neural network-aided BICM with Gray labelling, we propose a novel stacked
network architecture of the bitinterleaved coded modulation with iterative
decoding (NN-BICMID). Its performance has been improved further by calculating
the condition probability with the aid of a priori probability that derived
from the extrinsic LLRs in the LDPC decoder at the last iteration, at the
expense of customizing neural network detectors at each iteration time
separately. Utilizing the optimal DC bias as the midpoint of the dynamic
region, the simulation results demonstrate that both the NN-BICM and NN-BICM-ID
schemes achieve noticeable performance gains than other counterparts, in which
the NN-BICM-ID clearly outperforms NN-BICM with various modulation and coding
schemes.
</summary>
    <author>
      <name>Yuan He</name>
    </author>
    <author>
      <name>Ming Jiang</name>
    </author>
    <author>
      <name>Chunming Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01018v1</id>
    <updated>2018-09-04T14:24:19Z</updated>
    <published>2018-09-04T14:24:19Z</published>
    <title>Parameter Transfer Extreme Learning Machine based on Projective Model</title>
    <summary>  Recent years, transfer learning has attracted much attention in the community
of machine learning. In this paper, we mainly focus on the tasks of parameter
transfer under the framework of extreme learning machine (ELM). Unlike the
existing parameter transfer approaches, which incorporate the source model
information into the target by regularizing the di erence between the source
and target domain parameters, an intuitively appealing projective-model is
proposed to bridge the source and target model parameters. Specifically, we
formulate the parameter transfer in the ELM networks by the means of parameter
projection, and train the model by optimizing the projection matrix and
classifier parameters jointly. Further more, the `L2,1-norm structured sparsity
penalty is imposed on the source domain parameters, which encourages the joint
feature selection and parameter transfer. To evaluate the e ectiveness of the
proposed method, comprehensive experiments on several commonly used domain
adaptation datasets are presented. The results show that the proposed method
significantly outperforms the non-transfer ELM networks and other classical
transfer learning methods.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Boyuan Jiang</name>
    </author>
    <author>
      <name>Xinyu Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted as an oral paper by IJCNN 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00114v2</id>
    <updated>2018-09-04T14:19:57Z</updated>
    <published>2018-02-28T22:26:43Z</published>
    <title>SQL-Rank: A Listwise Approach to Collaborative Ranking</title>
    <summary>  In this paper, we propose a listwise approach for constructing user-specific
rankings in recommendation systems in a collaborative fashion. We contrast the
listwise approach to previous pointwise and pairwise approaches, which are
based on treating either each rating or each pairwise comparison as an
independent instance respectively. By extending the work of (Cao et al. 2007),
we cast listwise collaborative ranking as maximum likelihood under a
permutation model which applies probability mass to permutations based on a low
rank latent score matrix. We present a novel algorithm called SQL-Rank, which
can accommodate ties and missing data and can run in linear time. We develop a
theoretical framework for analyzing listwise ranking methods based on a novel
representation theory for the permutation model. Applying this framework to
collaborative ranking, we derive asymptotic statistical rates as the number of
users and items grow together. We conclude by demonstrating that our SQL-Rank
method often outperforms current state-of-the-art algorithms for implicit
feedback such as Weighted-MF and BPR and achieve favorable results when
compared to explicit feedback algorithms such as matrix factorization and
collaborative ranking.
</summary>
    <author>
      <name>Liwei Wu</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <author>
      <name>James Sharpnack</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05800v2</id>
    <updated>2018-09-04T13:14:38Z</updated>
    <published>2018-07-16T11:41:32Z</published>
    <title>Deep Generative Model using Unregularized Score for Anomaly Detection
  with Heterogeneous Complexity</title>
    <summary>  Accurate and automated detection of anomalous samples in a natural image
dataset can be accomplished with a probabilistic model for end-to-end modeling
of images. Such images have heterogeneous complexity, however, and a
probabilistic model overlooks simply shaped objects with small anomalies. This
is because the probabilistic model assigns undesirably lower likelihoods to
complexly shaped objects that are nevertheless consistent with set standards.
To overcome this difficulty, we propose an unregularized score for deep
generative models (DGMs), which are generative models leveraging deep neural
networks. We found that the regularization terms of the DGMs considerably
influence the anomaly score depending on the complexity of the samples. By
removing these terms, we obtain an unregularized score, which we evaluated on a
toy dataset and real-world manufacturing datasets. Empirical results
demonstrate that the unregularized score is robust to the inherent complexity
of samples and can be used to better detect anomalies.
</summary>
    <author>
      <name>Takashi Matsubara</name>
    </author>
    <author>
      <name>Kenta Hama</name>
    </author>
    <author>
      <name>Ryosuke Tachibana</name>
    </author>
    <author>
      <name>Kuniaki Uehara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended version of a manuscript in Proc. of The 2018
  International Joint Conference on Neural Networks (IJCNN2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09586v2</id>
    <updated>2018-09-04T11:30:12Z</updated>
    <published>2018-07-13T13:43:15Z</published>
    <title>Perturb and Combine to Identify Influential Spreaders in Real-World
  Networks</title>
    <summary>  Recent research has shown that graph degeneracy algorithms, which decompose a
network into a hierarchy of nested subgraphs of decreasing size and increasing
density, are very effective at detecting the good spreaders in a network.
However, it is also known that degeneracy-based decompositions of a graph are
unstable to small perturbations of the network structure. In Machine Learning,
the performance of unstable classification and regression methods, such as
fully-grown decision trees, can be greatly improved by using Perturb and
Combine (P&amp;C) strategies such as bagging (bootstrap aggregating). Therefore, we
propose a P&amp;C procedure for networks that (1) creates many perturbed versions
of a given graph, (2) applies a node scoring function separately to each graph
(such as a degeneracy-based one), and (3) combines the results. We conduct
real-world experiments on the tasks of identifying influential spreaders in
large social networks, and influential words (keywords) in small word
co-occurrence networks. We use the k-core, generalized k-core, and PageRank
algorithms as our vertex scoring functions. In each case, using the aggregated
scores brings significant improvements compared to using the scores computed on
the original graphs. Finally, a bias-variance analysis suggests that our P&amp;C
procedure works mainly by reducing bias, and that therefore, it should be
capable of improving the performance of all vertex scoring functions, not only
unstable ones.
</summary>
    <author>
      <name>Antoine J. -P. Tixier</name>
    </author>
    <author>
      <name>Maria-Evgenia G. Rossi</name>
    </author>
    <author>
      <name>Fragkiskos D. Malliaros</name>
    </author>
    <author>
      <name>Jesse Read</name>
    </author>
    <author>
      <name>Michalis Vazirgiannis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">More compact format</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09586v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09586v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01133v1</id>
    <updated>2018-09-04T10:26:37Z</updated>
    <published>2018-09-04T10:26:37Z</published>
    <title>Automated bird sound recognition in realistic settings</title>
    <summary>  We evaluated the effectiveness of an automated bird sound identification
system in a situation that emulates a realistic, typical application. We
trained classification algorithms on a crowd-sourced collection of bird audio
recording data and restricted our training methods to be completely free of
manual intervention. The approach is hence directly applicable to the analysis
of multiple species collections, with labelling provided by crowd-sourced
collection. We evaluated the performance of the bird sound recognition system
on a realistic number of candidate classes, corresponding to real conditions.
We investigated the use of two canonical classification methods, chosen due to
their widespread use and ease of interpretation, namely a k Nearest Neighbour
(kNN) classifier with histogram-based features and a Support Vector Machine
(SVM) with time-summarisation features. We further investigated the use of a
certainty measure, derived from the output probabilities of the classifiers, to
enhance the interpretability and reliability of the class decisions. Our
results demonstrate that both identification methods achieved similar
performance, but we argue that the use of the kNN classifier offers somewhat
more flexibility. Furthermore, we show that employing an outcome certainty
measure provides a valuable and consistent indicator of the reliability of
classification results. Our use of generic training data and our investigation
of probabilistic classification methodologies that can flexibly address the
variable number of candidate species/classes that are expected to be
encountered in the field, directly contribute to the development of a practical
bird sound identification system with potentially global application. Further,
we show that certainty measures associated with identification outcomes can
significantly contribute to the practical usability of the overall system.
</summary>
    <author>
      <name>Timos Papadopoulos</name>
    </author>
    <author>
      <name>Stephen J. Roberts</name>
    </author>
    <author>
      <name>Katherine J. Willis</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00862v1</id>
    <updated>2018-09-04T09:54:25Z</updated>
    <published>2018-09-04T09:54:25Z</published>
    <title>Handwriting styles: benchmarks and evaluation metrics</title>
    <summary>  Evaluating the style of handwriting generation is a challenging problem,
since it is not well defined. It is a key component in order to develop in
developing systems with more personalized experiences with humans. In this
paper, we propose baseline benchmarks, in order to set anchors to estimate the
relative quality of different handwriting style methods. This will be done
using deep learning techniques, which have shown remarkable results in
different machine learning tasks, learning classification, regression, and most
relevant to our work, generating temporal sequences. We discuss the challenges
associated with evaluating our methods, which is related to evaluation of
generative models in general. We then propose evaluation metrics, which we find
relevant to this problem, and we discuss how we evaluate the evaluation
metrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,
there is no work done before in generating handwriting (either in terms of
methodology or the performance metrics), our in exploring styles using this
dataset.
</summary>
    <author>
      <name>Omar Mohammed</name>
    </author>
    <author>
      <name>Gerard Bailly</name>
    </author>
    <author>
      <name>Damien Pellier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE International Workshop on Deep and Transfer
  Learning (DTL 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00852v1</id>
    <updated>2018-09-04T09:18:19Z</updated>
    <published>2018-09-04T09:18:19Z</published>
    <title>Multi-target Unsupervised Domain Adaptation without Exactly Shared
  Categories</title>
    <summary>  Unsupervised domain adaptation (UDA) aims to learn the unlabeled target
domain by transferring the knowledge from the labeled source domain. To date,
most of the existing works focus on the scenario of one source domain and one
target domain (1S1T), and just a few works concern UDA of multiple source
domains and one target domain (mS1T) for solving the insufficient knowledge
problem with single source domain. While, to the best of our knowledge, almost
no work concerns the scenario of one source domain and multiple target domains
(1SmT). In the 1SmT, these unlabeled target domains may not necessarily share
the same categories, therefore, in contrast to mS1T, 1SmT is more challenging.
In this paper, we study such a new UDA scenario, and accordingly propose a UDA
framework (PA-1SmT) through the model parameter adaptation among these target
domains and the source domain. A key ingredient of our framework is that we
firstly construct a model parameter dictionary which is shared not only between
the source domain and the individual target domains but also among the multiple
target domains. Then we use it to sparsely represent individual target
parameters, which attains knowledge transfer among the domains. Such a new
knowledge transfer is different from existing popular methods for UDA, such as
subspace alignment, distribution matching etc., and can also be directly used
for DA of privacy protection due to the fact that the knowledge is transferred
just via the model parameters rather than data itself. Finally, our
experimental results on three domain adaptation benchmark datasets demonstrate
the superiority of our framework.
</summary>
    <author>
      <name>Huanhuan Yu</name>
    </author>
    <author>
      <name>Menglei Hu</name>
    </author>
    <author>
      <name>Songcan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00846v1</id>
    <updated>2018-09-04T09:01:10Z</updated>
    <published>2018-09-04T09:01:10Z</published>
    <title>Understanding Regularization in Batch Normalization</title>
    <summary>  Batch Normalization (BN) makes output of hidden neuron had zero mean and unit
variance, improving convergence and generalization when training neural
networks. This work understands these phenomena theoretically. We analyze BN by
using a building block of neural networks, which consists of a weight layer, a
BN layer, and a nonlinear activation function. This simple network helps us
understand the characteristics of BN, where the results are generalized to deep
models in numerical studies. We explore BN in three aspects. First, by viewing
BN as a stochastic process, an analytical form of regularization inherited in
BN is derived. Second, the optimization dynamic with this regularization shows
that BN enables training converged with large maximum and effective learning
rates. Third, BN's generalization with regularization is explored by using
random matrix theory and statistical mechanics. Both simulations and
experiments support our analyses.
</summary>
    <author>
      <name>Ping Luo</name>
    </author>
    <author>
      <name>Xinjiang Wang</name>
    </author>
    <author>
      <name>Wenqi Shao</name>
    </author>
    <author>
      <name>Zhanglin Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Work in progress. 17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00804v2</id>
    <updated>2018-09-04T08:59:26Z</updated>
    <published>2018-06-03T14:53:20Z</published>
    <title>NAM: Non-Adversarial Unsupervised Domain Mapping</title>
    <summary>  Several methods were recently proposed for the task of translating images
between domains without prior knowledge in the form of correspondences. The
existing methods apply adversarial learning to ensure that the distribution of
the mapped source domain is indistinguishable from the target domain, which
suffers from known stability issues. In addition, most methods rely heavily on
`cycle' relationships between the domains, which enforce a one-to-one mapping.
In this work, we introduce an alternative method: Non-Adversarial Mapping
(NAM), which separates the task of target domain generative modeling from the
cross-domain mapping task. NAM relies on a pre-trained generative model of the
target domain, and aligns each source image with an image synthesized from the
target domain, while jointly optimizing the domain mapping function. It has
several key advantages: higher quality and resolution image translations,
simpler and more stable training and reusable target models. Extensive
experiments are presented validating the advantages of our method.
</summary>
    <author>
      <name>Yedid Hoshen</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00836v1</id>
    <updated>2018-09-04T08:41:53Z</updated>
    <published>2018-09-04T08:41:53Z</published>
    <title>A Recurrent Neural Network for Sentiment Quantification</title>
    <summary>  Quantification is a supervised learning task that consists in predicting,
given a set of classes C and a set D of unlabelled items, the prevalence (or
relative frequency) p(c|D) of each class c in C. Quantification can in
principle be solved by classifying all the unlabelled items and counting how
many of them have been attributed to each class. However, this "classify and
count" approach has been shown to yield suboptimal quantification accuracy;
this has established quantification as a task of its own, and given rise to a
number of methods specifically devised for it. We propose a recurrent neural
network architecture for quantification (that we call QuaNet) that observes the
classification predictions to learn higher-order "quantification embeddings",
which are then refined by incorporating quantification predictions of simple
classify-and-count-like methods. We test {QuaNet on sentiment quantification on
text, showing that it substantially outperforms several state-of-the-art
baselines.
</summary>
    <author>
      <name>Andrea Esuli</name>
    </author>
    <author>
      <name>Alejandro Moreo Fernández</name>
    </author>
    <author>
      <name>Fabrizio Sebastiani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3269287</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3269287" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at CIKM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01575v1</id>
    <updated>2018-09-04T08:36:09Z</updated>
    <published>2018-09-04T08:36:09Z</published>
    <title>Bounded Rational Decision-Making with Adaptive Neural Network Priors</title>
    <summary>  Bounded rationality investigates utility-optimizing decision-makers with
limited information-processing power. In particular, information theoretic
bounded rationality models formalize resource constraints abstractly in terms
of relative Shannon information, namely the Kullback-Leibler Divergence between
the agents' prior and posterior policy. Between prior and posterior lies an
anytime deliberation process that can be instantiated by sample-based
evaluations of the utility function through Markov Chain Monte Carlo (MCMC)
optimization. The most simple model assumes a fixed prior and can relate
abstract information-theoretic processing costs to the number of sample
evaluations. However, more advanced models would also address the question of
learning, that is how the prior is adapted over time such that generated prior
proposals become more efficient. In this work we investigate generative neural
networks as priors that are optimized concurrently with anytime sample-based
decision-making processes such as MCMC. We evaluate this approach on toy
examples.
</summary>
    <author>
      <name>Heinke Hihn</name>
    </author>
    <author>
      <name>Sebastian Gottwald</name>
    </author>
    <author>
      <name>Daniel A. Braun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-99978-4-17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-99978-4-17" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ANNPR 2018: Artificial Neural Networks in Pattern
  Recognition</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pancioni L., Schwenker F., Trentin E. (eds) Artificial Neural
  Networks in Pattern Recognition. ANNPR 2018. Lecture Notes in Computer
  Science, vol 11081. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.01575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04673v3</id>
    <updated>2018-09-04T08:33:45Z</updated>
    <published>2017-09-14T09:04:41Z</published>
    <title>Analysis of Set-Valued Stochastic Approximations: Applications to Noisy
  Approximate Value and Fixed point Iterations</title>
    <summary>  The main aim of this paper is the development of Lyapunov function based
sufficient conditions for stability (almost sure boundedness) and convergence
of stochastic approximation algorithms (SAAs) with set-valued mean-fields, a
class of model-free algorithms that have become important in recent times. In
this paper we provide a complete analysis of such algorithms under three
different, yet related set of sufficient conditions, based on the existence of
an associated global/local Lyapunov function. Unlike previous Lyapunov function
based approaches, we provide a simple recipe for explicitly constructing the
Lyapunov function needed for analysis. Our work builds on the works of
Abounadi, Bertsekas and Borkar (2002), Munos (2005) and Ramaswamy and Bhatnagar
(2016). An important motivation to the flavor of our assumptions comes from the
need to understand approximate dynamic programming and reinforcement learning
algorithms, that use deep neural networks (DNNs) for function approximations
and parameterizations. These algorithms are popularly known as deep
reinforcement learning algorithms. As an important application of our theory we
provide a complete analysis of the stochastic approximation counterpart of
approximate value iteration (AVI), an important dynamic programming method
designed to tackle Bellman's curse of dimensionality. Although motivated by the
need to understand deep reinforcement learning algorithms our theory is more
generally applicable. It is further used to develop the first SAA for finding
fixed points of contractive set-valued maps and provide a comprehensive
analysis of the same.
</summary>
    <author>
      <name>Arunselvan Ramaswamy</name>
    </author>
    <author>
      <name>Shalabh Bhatnagar</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04673v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04673v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L20, 93E35, 37B25, 34A60, 90C39, 37C25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00832v1</id>
    <updated>2018-09-04T08:31:21Z</updated>
    <published>2018-09-04T08:31:21Z</published>
    <title>Improving the Expressiveness of Deep Learning Frameworks with Recursion</title>
    <summary>  Recursive neural networks have widely been used by researchers to handle
applications with recursively or hierarchically structured data. However,
embedded control flow deep learning frameworks such as TensorFlow, Theano,
Caffe2, and MXNet fail to efficiently represent and execute such neural
networks, due to lack of support for recursion. In this paper, we add recursion
to the programming model of existing frameworks by complementing their design
with recursive execution of dataflow graphs as well as additional APIs for
recursive definitions. Unlike iterative implementations, which can only
understand the topological index of each node in recursive data structures, our
recursive implementation is able to exploit the recursive relationships between
nodes for efficient execution based on parallel computation. We present an
implementation on TensorFlow and evaluation results with various recursive
neural network models, showing that our recursive implementation not only
conveys the recursive nature of recursive neural networks better than other
implementations, but also uses given resources more effectively to reduce
training and inference time.
</summary>
    <author>
      <name>Eunji Jeong</name>
    </author>
    <author>
      <name>Joo Seong Jeong</name>
    </author>
    <author>
      <name>Soojeong Kim</name>
    </author>
    <author>
      <name>Gyeong-In Yu</name>
    </author>
    <author>
      <name>Byung-Gon Chun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3190508.3190530</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3190508.3190530" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in EuroSys 2018. 13 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EuroSys 2018: Thirteenth EuroSys Conference, April 23-26, 2018,
  Porto, Portugal</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.00832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00811v1</id>
    <updated>2018-09-04T07:03:58Z</updated>
    <published>2018-09-04T07:03:58Z</published>
    <title>A Deep Learning Spatiotemporal Prediction Framework for Mobile
  Crowdsourced Services</title>
    <summary>  This papers presents a deep learning-based framework to predict crowdsourced
service availability spatially and temporally. A novel two-stage prediction
model is introduced based on historical spatio-temporal traces of mobile
crowdsourced services. The prediction model first clusters mobile crowdsourced
services into regions. The availability prediction of a mobile crowdsourced
service at a certain location and time is then formulated as a classification
problem. To determine the availability duration of predicted mobile
crowdsourced services, we formulate a forecasting task of time series using the
Gramian Angular Field. We validated the effectiveness of the proposed framework
through multiple experiments.
</summary>
    <author>
      <name>Ahmed Ben Said</name>
    </author>
    <author>
      <name>Abdelkarim Erradi</name>
    </author>
    <author>
      <name>Azadeh Ghari Neiat</name>
    </author>
    <author>
      <name>Athman Bouguettaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11036-018-1105-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11036-018-1105-0" rel="related"/>
    <link href="http://arxiv.org/abs/1809.00811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00800v1</id>
    <updated>2018-09-04T05:33:00Z</updated>
    <published>2018-09-04T05:33:00Z</published>
    <title>Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse
  Linguistic Expressions</title>
    <summary>  In this paper, we propose a new kernel-based co-occurrence measure that can
be applied to sparse linguistic expressions (e.g., sentences) with a very short
learning time, as an alternative to pointwise mutual information (PMI). As well
as deriving PMI from mutual information, we derive this new measure from the
Hilbert--Schmidt independence criterion (HSIC); thus, we call the new measure
the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of
PMI that allows various similarity metrics (e.g., sentence embeddings) to be
plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast
(linear in the size of the data) matrix calculations regardless of whether we
use linear or nonlinear kernels. Empirically, in a dialogue response selection
task, PHSIC is learned thousands of times faster than an RNN-based PMI while
outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is
beneficial as a criterion of a data selection task for machine translation
owing to its ability to give high (low) scores to a consistent (inconsistent)
pair with other pairs.
</summary>
    <author>
      <name>Sho Yokoi</name>
    </author>
    <author>
      <name>Sosuke Kobayashi</name>
    </author>
    <author>
      <name>Kenji Fukumizu</name>
    </author>
    <author>
      <name>Jun Suzuki</name>
    </author>
    <author>
      <name>Kentaro Inui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05174v3</id>
    <updated>2018-09-04T03:25:13Z</updated>
    <published>2015-11-16T21:07:38Z</published>
    <title>Cross-scale predictive dictionaries</title>
    <summary>  Sparse representations using data dictionaries provide an efficient model
particularly for signals that do not enjoy alternate analytic sparsifying
transformations. However, solving inverse problems with sparsifying
dictionaries can be computationally expensive, especially when the dictionary
under consideration has a large number of atoms. In this paper, we incorporate
additional structure on to dictionary-based sparse representations for visual
signals to enable speedups when solving sparse approximation problems. The
specific structure that we endow onto sparse models is that of a multi-scale
modeling where the sparse representation at each scale is constrained by the
sparse representation at coarser scales. We show that this cross-scale
predictive model delivers significant speedups, often in the range of
10-60$\times$, with little loss in accuracy for linear inverse problems
associated with images, videos, and light fields.
</summary>
    <author>
      <name>Vishwanath Saragadam</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Aswin Sankaranarayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05174v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05174v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01129v1</id>
    <updated>2018-09-04T03:12:40Z</updated>
    <published>2018-09-04T03:12:40Z</published>
    <title>Lipschitz Networks and Distributional Robustness</title>
    <summary>  Robust risk minimisation has several advantages: it has been studied with
regards to improving the generalisation properties of models and robustness to
adversarial perturbation. We bound the distributionally robust risk for a model
class rich enough to include deep neural networks by a regularised empirical
risk involving the Lipschitz constant of the model. This allows us to
interpretand quantify the robustness properties of a deep neural network. As an
application we show the distributionally robust risk upperbounds the
adversarial training risk.
</summary>
    <author>
      <name>Zac Cranko</name>
    </author>
    <author>
      <name>Simon Kornblith</name>
    </author>
    <author>
      <name>Zhan Shi</name>
    </author>
    <author>
      <name>Richard Nock</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00770v1</id>
    <updated>2018-09-04T02:13:37Z</updated>
    <published>2018-09-04T02:13:37Z</published>
    <title>Transferring Deep Reinforcement Learning with Adversarial Objective and
  Augmentation</title>
    <summary>  In the past few years, deep reinforcement learning has been proven to solve
problems which have complex states like video games or board games. The next
step of intelligent agents would be able to generalize between tasks, and using
prior experience to pick up new skills more quickly. However, most
reinforcement learning algorithms for now are often suffering from catastrophic
forgetting even when facing a very similar target task. Our approach enables
the agents to generalize knowledge from a single source task, and boost the
learning progress with a semisupervised learning method when facing a new task.
We evaluate this approach on Atari games, which is a popular reinforcement
learning benchmark, and show that it outperforms common baselines based on
pre-training and fine-tuning.
</summary>
    <author>
      <name>Shu-Hsuan Hsu</name>
    </author>
    <author>
      <name>I-Chao Shen</name>
    </author>
    <author>
      <name>Bing-Yu Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00758v1</id>
    <updated>2018-09-04T00:52:25Z</updated>
    <published>2018-09-04T00:52:25Z</published>
    <title>End-to-end Multimodal Emotion and Gender Recognition with Dynamic
  Weights of Joint Loss</title>
    <summary>  Multi-task learning (MTL) is one of the method for improving generalizability
of multiple tasks. In order to perform multiple classification tasks with one
neural network model, the losses of each task should be combined. Previous
studies have mostly focused on prediction of multiple tasks using joint loss
with static weights for training model. Choosing weights between tasks have not
taken any considerations while it is set by uniformly or empirically. In this
study, we propose a method to make joint loss using dynamic weights to improve
total performance not an individual performance of tasks, and apply this method
to end-to-end multimodal emotion and gender recognition model using audio and
video data. This approach provides proper weights for each loss of the tasks
when training ends. In our experiment, a performance of emotion and gender
recognition with proposed method shows lower joint loss which is computed as
negative log-likelihood than the one with static weights of joint loss. Also,
our proposed model shows better generalizability than compared models. In our
best knowledge, this research shows the strength of dynamic weights of joint
loss for maximizing total performance at first in emotion and gender
recognition task.
</summary>
    <author>
      <name>Myungsu Chae</name>
    </author>
    <author>
      <name>Tae-Ho Kim</name>
    </author>
    <author>
      <name>Young Hoon Shin</name>
    </author>
    <author>
      <name>Jun-Woo Kim</name>
    </author>
    <author>
      <name>Soo-Young Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IROS 2018 Workshop on Crossmodal Learning for
  Intelligent Robotics</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10728v2</id>
    <updated>2018-09-03T21:20:50Z</updated>
    <published>2018-06-28T01:12:32Z</published>
    <title>Deep Echo State Networks with Uncertainty Quantification for
  Spatio-Temporal Forecasting</title>
    <summary>  Long-lead forecasting for spatio-temporal systems can often entail complex
nonlinear dynamics that are difficult to specify it a priori. Current
statistical methodologies for modeling these processes are often highly
parameterized and thus, challenging to implement from a computational
perspective. One potential parsimonious solution to this problem is a method
from the dynamical systems and engineering literature referred to as an echo
state network (ESN). ESN models use so-called {\it reservoir computing} to
efficiently compute recurrent neural network (RNN) forecasts. Moreover,
so-called "deep" models have recently been shown to be successful at predicting
high-dimensional complex nonlinear processes, particularly those with multiple
spatial and temporal scales of variability (such as we often find in
spatio-temporal environmental data). Here we introduce a deep ensemble ESN
(D-EESN) model. We present two versions of this model for spatio-temporal
processes that both produce forecasts and associated measures of uncertainty.
The first approach utilizes a bootstrap ensemble framework and the second is
developed within a hierarchical Bayesian framework (BD-EESN). This more general
hierarchical Bayesian framework naturally accommodates non-Gaussian data types
and multiple levels of uncertainties. The methodology is first applied to a
data set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical
system simulation model and then to a long-lead United States (U.S.) soil
moisture forecasting application.
</summary>
    <author>
      <name>Patrick L. McDermott</name>
    </author>
    <author>
      <name>Christopher K. Wikle</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10728v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10728v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00710v1</id>
    <updated>2018-09-03T20:13:25Z</updated>
    <published>2018-09-03T20:13:25Z</published>
    <title>A Dual Approach for Optimal Algorithms in Distributed Optimization over
  Networks</title>
    <summary>  We study the optimal convergence rates for distributed convex optimization
problems over networks, where the objective is to minimize the sum
$\sum_{i=1}^{m}f_i(z)$ of local functions of the nodes in the network. We
provide optimal complexity bounds for four different cases, namely: the case
when each function $f_i$ is strongly convex and smooth, the cases when it is
either strongly convex or smooth and the case when it is convex but neither
strongly convex nor smooth. Our approach is based on the dual of an
appropriately formulated primal problem, which includes the underlying static
graph that models the communication restrictions. Our results show distributed
algorithms that achieve the same optimal rates as their centralized
counterparts (up to constant and logarithmic factors), with an additional cost
related to the spectral gap of the interaction matrix that captures the local
communications of the nodes in the network.
</summary>
    <author>
      <name>César A. Uribe</name>
    </author>
    <author>
      <name>Soomin Lee</name>
    </author>
    <author>
      <name>Alexander Gasnikov</name>
    </author>
    <author>
      <name>Angelia Nedić</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work is an extended version of the manuscript: Optimal
  Algorithms for Distributed Optimization} arXiv:1712.00232</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02808v2</id>
    <updated>2018-09-03T19:24:16Z</updated>
    <published>2018-04-09T04:00:30Z</published>
    <title>Latent Space Policies for Hierarchical Reinforcement Learning</title>
    <summary>  We address the problem of learning hierarchical deep neural network policies
for reinforcement learning. In contrast to methods that explicitly restrict or
cripple lower layers of a hierarchy to force them to use higher-level
modulating signals, each layer in our framework is trained to directly solve
the task, but acquires a range of diverse strategies via a maximum entropy
reinforcement learning objective. Each layer is also augmented with latent
random variables, which are sampled from a prior distribution during the
training of that layer. The maximum entropy objective causes these latent
variables to be incorporated into the layer's policy, and the higher level
layer can directly control the behavior of the lower layer through this latent
space. Furthermore, by constraining the mapping from latent variables to
actions to be invertible, higher layers retain full expressivity: neither the
higher layers nor the lower layers are constrained in their behavior. Our
experimental evaluation demonstrates that we can improve on the performance of
single-layer policies on standard benchmark tasks simply by adding additional
layers, and that our method can solve more complex sparse-reward tasks by
learning higher-level policies on top of high-entropy skills optimized for
simple low-level objectives.
</summary>
    <author>
      <name>Tuomas Haarnoja</name>
    </author>
    <author>
      <name>Kristian Hartikainen</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018; Videos: https://sites.google.com/view/latent-space-deep-rl
  Code: https://github.com/haarnoja/sac</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02808v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02808v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10101v3</id>
    <updated>2018-09-03T18:22:30Z</updated>
    <published>2018-08-30T03:35:50Z</published>
    <title>DP-ADMM: ADMM-based Distributed Learning with Differential Privacy</title>
    <summary>  Privacy-preserving distributed machine learning has become more important
than ever due to the high demand of large-scale data processing. This paper
focuses on a class of machine learning problems that can be formulated as
regularized empirical risk minimization, and develops a privacy-preserving
approach to such learning problems. We use Alternating Direction Method of
Multipliers (ADMM) to decentralize the learning algorithm, and apply Gaussian
mechanisms to provide local differential privacy guarantee. However, simply
combining ADMM and local randomization mechanisms would result in a
nonconvergent algorithm with bad performance even under moderate privacy
guarantees. Besides, this approach cannot be applied when the objective
functions of the learning problems are non-smooth. To address these concerns,
we propose an improved ADMM-based Differentially Private distributed learning
algorithm, DP-ADMM, where an approximate augmented Lagrangian function and
Gaussian mechanisms with time-varying variance are utilized. We also apply the
moment accountant method to bound the total privacy loss. Our theoretical
analysis shows that DP-ADMM can be applied to convex learning problems with
both smooth and non-smooth objectives, provides differential privacy guarantee,
and achieves a convergence rate of $O(1/\sqrt{t})$, where $t$ is the number of
iterations. Our evaluations demonstrate that our approach can achieve good
convergence and accuracy with strong privacy guarantee.
</summary>
    <author>
      <name>Zonghao Huang</name>
    </author>
    <author>
      <name>Rui Hu</name>
    </author>
    <author>
      <name>Eric Chan-Tin</name>
    </author>
    <author>
      <name>Yanmin Gong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 24 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10101v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10101v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00653v1</id>
    <updated>2018-09-03T16:52:19Z</updated>
    <published>2018-09-03T16:52:19Z</published>
    <title>Towards Dynamic Computation Graphs via Sparse Latent Structure</title>
    <summary>  Deep NLP models benefit from underlying structures in the data---e.g., parse
trees---typically extracted using off-the-shelf parsers. Recent attempts to
jointly learn the latent structure encounter a tradeoff: either make
factorization assumptions that limit expressiveness, or sacrifice end-to-end
differentiability. Using the recently proposed SparseMAP inference, which
retrieves a sparse distribution over latent structures, we propose a novel
approach for end-to-end learning of latent structure predictors jointly with a
downstream predictor. To the best of our knowledge, our method is the first to
enable unrestricted dynamic computation graph construction from the global
latent structure, while maintaining differentiability.
</summary>
    <author>
      <name>Vlad Niculae</name>
    </author>
    <author>
      <name>André F. T. Martins</name>
    </author>
    <author>
      <name>Claire Cardie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018; 9 pages (incl. appendix)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.10277v4</id>
    <updated>2018-09-03T15:23:40Z</updated>
    <published>2016-11-30T17:32:17Z</published>
    <title>Anchored Correlation Explanation: Topic Modeling with Minimal Domain
  Knowledge</title>
    <summary>  While generative models such as Latent Dirichlet Allocation (LDA) have proven
fruitful in topic modeling, they often require detailed assumptions and careful
specification of hyperparameters. Such model complexity issues only compound
when trying to generalize generative models to incorporate human input. We
introduce Correlation Explanation (CorEx), an alternative approach to topic
modeling that does not assume an underlying generative model, and instead
learns maximally informative topics through an information-theoretic framework.
This framework naturally generalizes to hierarchical and semi-supervised
extensions with no additional modeling assumptions. In particular, word-level
domain knowledge can be flexibly incorporated within CorEx through anchor
words, allowing topic separability and representation to be promoted with
minimal human intervention. Across a variety of datasets, metrics, and
experiments, we demonstrate that CorEx produces topics that are comparable in
quality to those produced by unsupervised and semi-supervised variants of LDA.
</summary>
    <author>
      <name>Ryan J. Gallagher</name>
    </author>
    <author>
      <name>Kyle Reing</name>
    </author>
    <author>
      <name>David Kale</name>
    </author>
    <author>
      <name>Greg Ver Steeg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 figures. 2018/09/03: Updated citation for HA/DR dataset</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Transactions of the Association for Computational Linguistics
  (TACL), Vol. 5, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.10277v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.10277v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03851v2</id>
    <updated>2018-09-03T13:50:26Z</updated>
    <published>2018-01-11T16:22:07Z</published>
    <title>Autoencoders and Probabilistic Inference with Missing Data: An Exact
  Solution for The Factor Analysis Case</title>
    <summary>  Latent variable models can be used to probabilistically "fill-in" missing
data entries. The variational autoencoder architecture (Kingma and Welling,
2014; Rezende et al., 2014) includes a "recognition" or "encoder" network that
infers the latent variables given the data variables. However, it is not clear
how to handle missing data variables in this network. The factor analysis (FA)
model is a basic autoencoder, using linear encoder and decoder networks. We
show how to calculate exactly the latent posterior distribution for the factor
analysis (FA) model in the presence of missing data, and note that this
solution exhibits a non-trivial dependence on the pattern of missingness. We
also discuss various approximations to the exact solution. Experiments compare
the effectiveness of various approaches to filling in the missing data.
</summary>
    <author>
      <name>Christopher K. I. Williams</name>
    </author>
    <author>
      <name>Charlie Nash</name>
    </author>
    <author>
      <name>Alfredo Nazábal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03851v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03851v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00593v1</id>
    <updated>2018-09-03T13:21:28Z</updated>
    <published>2018-09-03T13:21:28Z</published>
    <title>IoU is not submodular</title>
    <summary>  This short article aims at demonstrate that the Intersection over Union (or
Jaccard index) is not a submodular function. This mistake has been made in an
article which is cited and used as a foundation in another article. The
Intersection of Union is widely used in machine learning as a cost function
especially for imbalance data and semantic segmentation.
</summary>
    <author>
      <name>Tanguy Kerdoncuff</name>
    </author>
    <author>
      <name>Rémi Emonet</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00534v3</id>
    <updated>2018-09-03T11:46:46Z</updated>
    <published>2018-06-01T20:29:47Z</published>
    <title>Run Procrustes, Run! On the convergence of accelerated Procrustes Flow</title>
    <summary>  In this work, we present theoretical results on the convergence of non-convex
accelerated gradient descent in matrix factorization models. The technique is
applied to matrix sensing problems with squared loss, for the estimation of a
rank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show that
the acceleration leads to linear convergence rate, even under non-convex
settings where the variable $X$ is represented as $U U^\top$ for $U \in
\mathbb{R}^{n \times r}$. Our result has the same dependence on the condition
number of the objective --and the optimal solution-- as that of the recent
results on non-accelerated algorithms. However, acceleration is observed in
practice, both in synthetic examples and in two real applications: neuronal
multi-unit activities recovery from single electrode recordings, and quantum
state tomography on quantum computing simulators.
</summary>
    <author>
      <name>Anastasios Kyrillidis</name>
    </author>
    <author>
      <name>Shashanka Ubaru</name>
    </author>
    <author>
      <name>Georgios Kollias</name>
    </author>
    <author>
      <name>Kristofer Bouchard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, footnote 7 removed from previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00534v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00534v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00542v1</id>
    <updated>2018-09-03T10:43:02Z</updated>
    <published>2018-09-03T10:43:02Z</published>
    <title>Machine learning for predicting thermal power consumption of the Mars
  Express Spacecraft</title>
    <summary>  The thermal subsystem of the Mars Express (MEX) spacecraft keeps the on-board
equipment within its pre-defined operating temperatures range. To plan and
optimize the scientific operations of MEX, its operators need to estimate in
advance, as accurately as possible, the power consumption of the thermal
subsystem. The remaining power can then be allocated for scientific purposes.
We present a machine learning pipeline for efficiently constructing accurate
predictive models for predicting the power of the thermal subsystem on board
MEX. In particular, we employ state-of-the-art feature engineering approaches
for transforming raw telemetry data, in turn used for constructing accurate
models with different state-of-the-art machine learning methods. We show that
the proposed pipeline considerably improve our previous (competition-winning)
work in terms of time efficiency and predictive performance. Moreover, while
achieving superior predictive performance, the constructed models also provide
important insight into the spacecraft's behavior, allowing for further analyses
and optimal planning of MEX's operation.
</summary>
    <author>
      <name>Matej Petković</name>
    </author>
    <author>
      <name>Redouane Boumghar</name>
    </author>
    <author>
      <name>Martin Breskvar</name>
    </author>
    <author>
      <name>Sašo Džeroski</name>
    </author>
    <author>
      <name>Dragi Kocev</name>
    </author>
    <author>
      <name>Jurica Levatić</name>
    </author>
    <author>
      <name>Luke Lucas</name>
    </author>
    <author>
      <name>Aljaž Osojnik</name>
    </author>
    <author>
      <name>Bernard Ženko</name>
    </author>
    <author>
      <name>Nikola Simidjievski</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00510v1</id>
    <updated>2018-09-03T09:07:30Z</updated>
    <published>2018-09-03T09:07:30Z</published>
    <title>Flatland: a Lightweight First-Person 2-D Environment for Reinforcement
  Learning</title>
    <summary>  We propose Flatland, a simple, lightweight environment for fast prototyping
and testing of reinforcement learning agents. It is of lower complexity
compared to similar 3D platforms, e.g. DeepMind Lab or VizDoom. At the same
time it shares some properties with the real world, such as continuity,
multi-modal partially-observable states with first-person view and coherent
physics. We propose to use it as an intermediary benchmark for problems related
to Lifelong Learning. Flatland is highly customizable and offers a wide range
of task difficulty to extensively evaluate the properties of artificial agents.
We experiment with three reinforcement learning baseline agents and show that
they can rapidly solve a navigation task in Flatland. A video of an agent
acting in Flatland is available here: https://youtu.be/I5y6Y2ZypdA.
</summary>
    <author>
      <name>Hugo Caselles-Dupré</name>
    </author>
    <author>
      <name>Louis Annabi</name>
    </author>
    <author>
      <name>Oksana Hagen</name>
    </author>
    <author>
      <name>Michael Garcia-Ortiz</name>
    </author>
    <author>
      <name>David Filliat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Workshop on Continual Unsupervised Sensorimotor
  Learning (ICDL-EpiRob 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01305v2</id>
    <updated>2018-09-03T09:02:10Z</updated>
    <published>2017-05-03T08:44:32Z</published>
    <title>Mass Volume Curves and Anomaly Ranking</title>
    <summary>  This paper aims at formulating the issue of ranking multivariate unlabeled
observations depending on their degree of abnormality as an unsupervised
statistical learning task. In the 1-d situation, this problem is usually
tackled by means of tail estimation techniques: univariate observations are
viewed as all the more `abnormal' as they are located far in the tail(s) of the
underlying probability distribution. It would be desirable as well to dispose
of a scalar valued `scoring' function allowing for comparing the degree of
abnormality of multivariate observations. Here we formulate the issue of
scoring anomalies as a M-estimation problem by means of a novel functional
performance criterion, referred to as the Mass Volume curve (MV curve in
short), whose optimal elements are strictly increasing transforms of the
density almost everywhere on the support of the density. We first study the
statistical estimation of the MV curve of a given scoring function and we
provide a strategy to build confidence regions using a smoothed bootstrap
approach. Optimization of this functional criterion over the set of piecewise
constant scoring functions is next tackled. This boils down to estimating a
sequence of empirical minimum volume sets whose levels are chosen adaptively
from the data, so as to adjust to the variations of the optimal MV curve, while
controling the bias of its approximation by a stepwise curve. Generalization
bounds are then established for the difference in sup norm between the MV curve
of the empirical scoring function thus obtained and the optimal MV curve.
</summary>
    <author>
      <name>Stephan Clémençon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI, TSI</arxiv:affiliation>
    </author>
    <author>
      <name>Albert Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1705.01305v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01305v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00864v5</id>
    <updated>2018-09-03T08:55:55Z</updated>
    <published>2017-03-02T17:33:58Z</published>
    <title>The Unreasonable Effectiveness of Structured Random Orthogonal
  Embeddings</title>
    <summary>  We examine a class of embeddings based on structured random matrices with
orthogonal rows which can be applied in many machine learning applications
including dimensionality reduction and kernel approximation. For both the
Johnson-Lindenstrauss transform and the angular kernel, we show that we can
select matrices yielding guaranteed improved performance in accuracy and/or
speed compared to earlier methods. We introduce matrices with complex entries
which give significant further accuracy improvement. We provide geometric and
Markov chain-based perspectives to help understand the benefits, and empirical
results which suggest that the approach is helpful in a wider range of
applications.
</summary>
    <author>
      <name>Krzysztof Choromanski</name>
    </author>
    <author>
      <name>Mark Rowland</name>
    </author>
    <author>
      <name>Adrian Weller</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00864v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00864v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07287v3</id>
    <updated>2018-09-03T07:07:53Z</updated>
    <published>2017-07-23T12:07:58Z</published>
    <title>Pairing an arbitrary regressor with an artificial neural network
  estimating aleatoric uncertainty</title>
    <summary>  We suggest a general approach to quantification of different forms of
aleatoric uncertainty in regression tasks performed by artificial neural
networks. It is based on the simultaneous training of two neural networks with
a joint loss function and a specific hyperparameter $\lambda&gt;0$ that allows for
automatically detecting noisy and clean regions in the input space and
controlling their {\em relative contribution} to the loss and its gradients.
After the model has been trained, one of the networks performs predictions and
the other quantifies the uncertainty of these predictions by estimating the
locally averaged loss of the first one. Unlike in many classical uncertainty
quantification methods, we do not assume any a priori knowledge of the ground
truth probability distribution, neither do we, in general, maximize the
likelihood of a chosen parametric family of distributions. We analyze the
learning process and the influence of clean and noisy regions of the input
space on the loss surface, depending on $\lambda$. In particular, we show that
small values of $\lambda$ increase the relative contribution of clean regions
to the loss and its gradients. This explains why choosing small $\lambda$
allows for better predictions compared with neural networks without uncertainty
counterparts and those based on classical likelihood maximization. Finally, we
demonstrate that one can naturally form ensembles of pairs of our networks and
thus capture both aleatoric and epistemic uncertainty and avoid overfitting.
</summary>
    <author>
      <name>Pavel Gurevich</name>
    </author>
    <author>
      <name>Hannes Stuke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07287v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07287v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07215v2</id>
    <updated>2018-09-03T05:46:13Z</updated>
    <published>2018-07-19T02:01:36Z</published>
    <title>A Machine Learning Approach for Detecting Students at Risk of Low
  Academic Achievement</title>
    <summary>  We aim to predict whether a primary school student will perform in the `below
standard' band of a national standardized test. We exploit a data set
containing test performance on the National Assessment Program - Literacy and
Numeracy (NAPLAN); a test given annually to all Australian school students in
grades 3, 5, 7, and 9. We separate the analysis into students in grade 5 and
above, for which previous achievement may be used as a predictor; and students
in grade 3, which must rely on family- and school-level predictors only. We
train and compare a set of classifiers for reading and numeracy learning areas
respectively. The classifiers achieve good predictive power in terms of area
under the ROC curve, suggesting that it is feasible for schools to more
accurately screen a large number of students for academic risk.
</summary>
    <author>
      <name>Sarah Cornell-Farrow</name>
    </author>
    <author>
      <name>Robert Garrard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 tables, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01509v2</id>
    <updated>2018-09-03T04:17:46Z</updated>
    <published>2018-05-03T18:47:43Z</published>
    <title>SURREAL: SUbgraph Robust REpresentAtion Learning</title>
    <summary>  The success of graph embeddings or node representation learning in a variety
of downstream tasks, such as node classification, link prediction, and
recommendation systems, has led to their popularity in recent years.
Representation learning algorithms aim to preserve local and global network
structure by identifying node neighborhood notions. However, many existing
algorithms generate embeddings that fail to properly preserve the network
structure, or lead to unstable representations due to random processes (e.g.,
random walks to generate context) and, thus, cannot generate to multi-graph
problems. In this paper, we propose RECS, a novel, stable graph embedding
algorithmic framework. RECS learns graph representations using connection
subgraphs by employing the analogy of graphs with electrical circuits. It
preserves both local and global connectivity patterns, and addresses the issue
of high-degree nodes. Further, it exploits the strength of weak ties and
meta-data that have been neglected by baselines. The experiments show that RECS
outperforms state-of-the-art algorithms by up to 36.85% on multi-label
classification problem. Further, in contrast to baselines, RECS, being
deterministic, is completely stable.
</summary>
    <author>
      <name>Saba A. Al-Sayouri</name>
    </author>
    <author>
      <name>Danai Koutra</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <author>
      <name>Sarah S. Lam</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05810v2</id>
    <updated>2018-09-03T02:22:39Z</updated>
    <published>2018-04-16T17:29:43Z</published>
    <title>ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object
  Detector</title>
    <summary>  Given the ability to directly manipulate image pixels in the digital input
space, an adversary can easily generate imperceptible perturbations to fool a
Deep Neural Network (DNN) image classifier, as demonstrated in prior work. In
this work, we propose ShapeShifter, an attack that tackles the more challenging
problem of crafting physical adversarial perturbations to fool image-based
object detectors like Faster R-CNN. Attacking an object detector is more
difficult than attacking an image classifier, as it needs to mislead the
classification results in multiple bounding boxes with different scales.
Extending the digital attack to the physical world adds another layer of
difficulty, because it requires the perturbation to be robust enough to survive
real-world distortions due to different viewing distances and angles, lighting
conditions, and camera limitations. We show that the Expectation over
Transformation technique, which was originally proposed to enhance the
robustness of adversarial perturbations in image classification, can be
successfully adapted to the object detection setting. ShapeShifter can generate
adversarially perturbed stop signs that are consistently mis-detected by Faster
R-CNN as other objects, posing a potential threat to autonomous vehicles and
other safety-critical computer vision systems.
</summary>
    <author>
      <name>Shang-Tse Chen</name>
    </author>
    <author>
      <name>Cory Cornelius</name>
    </author>
    <author>
      <name>Jason Martin</name>
    </author>
    <author>
      <name>Duen Horng Chau</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05810v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05810v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06093v2</id>
    <updated>2018-09-03T01:35:59Z</updated>
    <published>2018-07-16T20:23:43Z</published>
    <title>Prognostics Estimations with Dynamic States</title>
    <summary>  The health state assessment and remaining useful life (RUL) estimation play
very important roles in prognostics and health management (PHM), owing to their
abilities to reduce the maintenance and improve the safety of machines or
equipment. However, they generally suffer from this problem of lacking prior
knowledge to pre-define the exact failure thresholds for a machinery operating
in a dynamic environment with a high level of uncertainty. In this case,
dynamic thresholds depicted by the discrete states is a very attractive way to
estimate the RUL of a dynamic machinery. Currently, there are only very few
works considering the dynamic thresholds, and these studies adopted different
algorithms to determine the discrete states and predict the continuous states
separately, which largely increases the complexity of the learning process. In
this paper, we propose a novel prognostics approach for RUL estimation of
aero-engines with self-joint prediction of continuous and discrete states,
wherein the prediction of continuous and discrete states are conducted
simultaneously and dynamically within one learning framework.
</summary>
    <author>
      <name>Rong-Jing Bao</name>
    </author>
    <author>
      <name>Hai-Jun Rong</name>
    </author>
    <author>
      <name>Zhi-Xin Yang</name>
    </author>
    <author>
      <name>Badong Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06093v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06093v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04427v2</id>
    <updated>2018-09-03T00:46:35Z</updated>
    <published>2018-07-12T05:14:45Z</published>
    <title>Simultaneous Coherent Structure Coloring facilitates interpretable
  clustering of scientific data by amplifying dissimilarity</title>
    <summary>  The clustering of data into physically meaningful subsets often requires
assumptions regarding the number, size, or shape of the subgroups. Here, we
present a new method, simultaneous Coherent Structure Coloring (sCSC), which
accomplishes the task of unsupervised clustering without a priori guidance
regarding the underlying structure of the data. sCSC performs a sequence of
binary splittings on the dataset such that the most dissimilar data points are
required to be in separate clusters. To achieve this, we obtain a set of
orthogonal coordinates along which dissimilarity in the dataset is maximized
from a generalized eigenvalue problem based on the pairwise dissimilarity
between the data points to be clustered. This sequence of bifurcations produces
a binary tree representation of the system, from which the number of clusters
in the data and their interrelationships naturally emerge. To illustrate the
effectiveness of the method in the absence of a priori assumptions we apply it
to two exemplary problems in fluid dynamics. Then, we illustrate its capacity
for interpretability using a high-dimensional protein folding simulation
dataset. While we restrict our examples to dynamical physical systems in this
work, we anticipate straightforward translation to other fields where existing
analysis tools require ad hoc assumptions on the data structure, lack the
interpretability of the present method, or in which the underlying processes
are less accessible, such as genomics and neuroscience.
</summary>
    <author>
      <name>Brooke E. Husic</name>
    </author>
    <author>
      <name>Kristy L. Schlueter-Kuck</name>
    </author>
    <author>
      <name>John O. Dabiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In revision</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04427v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04427v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00341v2</id>
    <updated>2018-09-02T22:41:07Z</updated>
    <published>2018-04-01T20:49:56Z</published>
    <title>Sparse Principal Component Analysis via Variable Projection</title>
    <summary>  Sparse principal component analysis (SPCA) has emerged as a powerful
technique for data analysis, providing improved interpretation of low-rank
structures by identifying localized spatial structures in the data and
disambiguating between distinct time scales. We demonstrate a robust and
scalable SPCA algorithm by formulating it as a value-function optimization
problem. This viewpoint leads to a flexible and computationally efficient
algorithm. It can further leverage randomized methods from linear algebra to
extend the approach to the large-scale (big data) setting. Our proposed
innovation also allows for a robust SPCA formulation which can obtain
meaningful sparse components in spite of grossly corrupted input data. The
proposed algorithms are demonstrated using both synthetic and real world data,
showing exceptional computational efficiency and diagnostic performance.
</summary>
    <author>
      <name>N. Benjamin Erichson</name>
    </author>
    <author>
      <name>Peng Zheng</name>
    </author>
    <author>
      <name>Krithika Manohar</name>
    </author>
    <author>
      <name>Steven L. Brunton</name>
    </author>
    <author>
      <name>J. Nathan Kutz</name>
    </author>
    <author>
      <name>Aleksandr Y. Aravkin</name>
    </author>
    <link href="http://arxiv.org/abs/1804.00341v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00341v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00381v1</id>
    <updated>2018-09-02T20:03:09Z</updated>
    <published>2018-09-02T20:03:09Z</published>
    <title>Multitask Learning for Fundamental Frequency Estimation in Music</title>
    <summary>  Fundamental frequency (f0) estimation from polyphonic music includes the
tasks of multiple-f0, melody, vocal, and bass line estimation. Historically
these problems have been approached separately, and only recently, using
learning-based approaches. We present a multitask deep learning architecture
that jointly estimates outputs for various tasks including multiple-f0, melody,
vocal and bass line estimation, and is trained using a large,
semi-automatically annotated dataset. We show that the multitask model
outperforms its single-task counterparts, and explore the effect of various
design decisions in our approach, and show that it performs better or at least
competitively when compared against strong baseline methods.
</summary>
    <author>
      <name>Rachel M. Bittner</name>
    </author>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Juan P. Bello</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07351v2</id>
    <updated>2018-09-02T20:00:32Z</updated>
    <published>2018-04-19T19:40:47Z</published>
    <title>Sampling-free Uncertainty Estimation in Gated Recurrent Units with
  Exponential Families</title>
    <summary>  There has recently been a concerted effort to derive mechanisms in vision and
machine learning systems to offer uncertainty estimates of the predictions they
make. Clearly, there are enormous benefits to a system that is not only
accurate but also has a sense for when it is not sure. Existing proposals
center around Bayesian interpretations of modern deep architectures -- these
are effective but can often be computationally demanding. We show how classical
ideas in the literature on exponential families on probabilistic networks
provide an excellent starting point to derive uncertainty estimates in Gated
Recurrent Units (GRU). Our proposal directly quantifies uncertainty
deterministically, without the need for costly sampling-based estimation. We
demonstrate how our model can be used to quantitatively and qualitatively
measure uncertainty in unsupervised image sequence prediction. To our
knowledge, this is the first result describing sampling-free uncertainty
estimation for powerful sequential models such as GRUs.
</summary>
    <author>
      <name>Seong Jae Hwang</name>
    </author>
    <author>
      <name>Ronak Mehta</name>
    </author>
    <author>
      <name>Hyunwoo J. Kim</name>
    </author>
    <author>
      <name>Vikas Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00366v1</id>
    <updated>2018-09-02T16:24:38Z</updated>
    <published>2018-09-02T16:24:38Z</published>
    <title>Cold-start recommendations in Collective Matrix Factorization</title>
    <summary>  This work explores the ability of collective matrix factorization models in
recommender systems to make predictions about users and items for which there
is side information available but no feedback or interactions data, and
proposes a new formulation with a faster cold-start prediction formula that can
be used in real-time systems. While these cold-start recommendations are not as
good as warm-start ones, they were found to be of better quality than
non-personalized recommendations, and predictions about new users were found to
be more reliable than those about new items. The formulation proposed here
resulted in improved cold-start recommendations in many scenarios, at the
expense of worse warm-start ones.
</summary>
    <author>
      <name>David Cortes</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06355v2</id>
    <updated>2018-09-02T16:04:16Z</updated>
    <published>2018-02-18T09:15:56Z</published>
    <title>Stochastic Chebyshev Gradient Descent for Spectral Optimization</title>
    <summary>  A large class of machine learning techniques requires the solution of
optimization problems involving spectral functions of parametric matrices, e.g.
log-determinant and nuclear norm. Unfortunately, computing the gradient of a
spectral function is generally of cubic complexity, as such gradient descent
methods are rather expensive for optimizing objectives involving the spectral
function. Thus, one naturally turns to stochastic gradient methods in hope that
they will provide a way to reduce or altogether avoid the computation of full
gradients. However, here a new challenge appears: there is no straightforward
way to compute unbiased stochastic gradients for spectral functions. In this
paper, we develop unbiased stochastic gradients for spectral-sums, an important
subclass of spectral functions. Our unbiased stochastic gradients are based on
combining randomized trace estimators with stochastic truncation of the
Chebyshev expansions. A careful design of the truncation distribution allows us
to offer distributions that are variance-optimal, which is crucial for fast and
stable convergence of stochastic gradient methods. We further leverage our
proposed stochastic gradients to devise stochastic methods for objective
functions involving spectral-sums, and rigorously analyze their convergence
rate. The utility of our methods is demonstrated in numerical experiments.
</summary>
    <author>
      <name>Insu Han</name>
    </author>
    <author>
      <name>Haim Avron</name>
    </author>
    <author>
      <name>Jinwoo Shin</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09730v3</id>
    <updated>2018-09-02T14:59:22Z</updated>
    <published>2018-03-26T17:41:05Z</published>
    <title>Resilient Active Information Gathering with Mobile Robots</title>
    <summary>  Applications of safety, security, and rescue in robotics, such as multi-robot
target tracking, involve the execution of information acquisition tasks by
teams of mobile robots. However, in failure-prone or adversarial environments,
robots get attacked, their communication channels get jammed, and their sensors
may fail, resulting in the withdrawal of robots from the collective task, and
consequently the inability of the remaining active robots to coordinate with
each other. As a result, traditional design paradigms become insufficient and,
in contrast, resilient designs against system-wide failures and attacks become
important. In general, resilient design problems are hard, and even though they
often involve objective functions that are monotone or submodular, scalable
approximation algorithms for their solution have been hitherto unknown. In this
paper, we provide the first algorithm, enabling the following capabilities:
minimal communication, i.e., the algorithm is executed by the robots based only
on minimal communication between them; system-wide resiliency, i.e., the
algorithm is valid for any number of denial-of-service attacks and failures;
and provable approximation performance, i.e., the algorithm ensures for all
monotone (and not necessarily submodular) objective functions a solution that
is finitely close to the optimal. We quantify our algorithm's approximation
performance using a notion of curvature for monotone set functions. We support
our theoretical analyses with simulated and real-world experiments, by
considering an active information gathering scenario, namely, multi-robot
target tracking.
</summary>
    <author>
      <name>Brent Schlotfeldt</name>
    </author>
    <author>
      <name>Vasileios Tzoumas</name>
    </author>
    <author>
      <name>Dinesh Thakur</name>
    </author>
    <author>
      <name>George J. Pappas</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09730v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09730v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00338v1</id>
    <updated>2018-09-02T13:58:37Z</updated>
    <published>2018-09-02T13:58:37Z</published>
    <title>Look Across Elapse: Disentangled Representation Learning and
  Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition</title>
    <summary>  Despite the remarkable progress in face recognition related technologies,
reliably recognizing faces across ages still remains a big challenge. The
appearance of a human face changes substantially over time, resulting in
significant intra-class variations. As opposed to current techniques for
age-invariant face recognition, which either directly extract age-invariant
features for recognition, or first synthesize a face that matches target age
before feature extraction, we argue that it is more desirable to perform both
tasks jointly so that they can leverage each other. To this end, we propose a
deep Age-Invariant Model (AIM) for face recognition in the wild with three
distinct novelties. First, AIM presents a novel unified deep architecture
jointly performing cross-age face synthesis and recognition in a mutual
boosting way. Second, AIM achieves continuous face rejuvenation/aging with
remarkable photorealistic and identity-preserving properties, avoiding the
requirement of paired data and the true age of testing samples. Third, we
develop effective and novel training strategies for end-to-end learning the
whole deep architecture, which generates powerful age-invariant face
representations explicitly disentangled from the age variation. Moreover, we
propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset
to facilitate existing efforts and push the frontiers of age-invariant face
recognition research. Extensive experiments on both our CAFR and several other
cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the
proposed AIM model over the state-of-the-arts. Benchmarking our model on one of
the most popular unconstrained face recognition datasets IJB-C additionally
verifies the promising generalizability of AIM in recognizing faces in the
wild.
</summary>
    <author>
      <name>Jian Zhao</name>
    </author>
    <author>
      <name>Yu Cheng</name>
    </author>
    <author>
      <name>Yi Cheng</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Haochong Lan</name>
    </author>
    <author>
      <name>Fang Zhao</name>
    </author>
    <author>
      <name>Lin Xiong</name>
    </author>
    <author>
      <name>Yan Xu</name>
    </author>
    <author>
      <name>Jianshu Li</name>
    </author>
    <author>
      <name>Sugiri Pranata</name>
    </author>
    <author>
      <name>Shengmei Shen</name>
    </author>
    <author>
      <name>Junliang Xing</name>
    </author>
    <author>
      <name>Hengzhu Liu</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06640v2</id>
    <updated>2018-09-02T10:29:44Z</updated>
    <published>2018-08-20T18:20:01Z</published>
    <title>Adversarial Removal of Demographic Attributes from Text Data</title>
    <summary>  Recent advances in Representation Learning and Adversarial Training seem to
succeed in removing unwanted features from the learned representation. We show
that demographic information of authors is encoded in -- and can be recovered
from -- the intermediate representations learned by text-based neural
classifiers. The implication is that decisions of classifiers trained on
textual data are not agnostic to -- and likely condition on -- demographic
attributes. When attempting to remove such demographic information using
adversarial training, we find that while the adversarial component achieves
chance-level development-set accuracy during training, a post-hoc classifier,
trained on the encoded sentences from the first part, still manages to reach
substantially higher classification accuracies on the same data. This behavior
is consistent across several tasks, demographic properties and datasets. We
explore several techniques to improve the effectiveness of the adversarial
component. Our main conclusion is a cautionary one: do not rely on the
adversarial training to achieve invariant representation to sensitive features.
</summary>
    <author>
      <name>Yanai Elazar</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06640v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06640v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09964v2</id>
    <updated>2018-09-02T06:17:56Z</updated>
    <published>2018-08-29T14:46:35Z</published>
    <title>Semi-Metrification of the Dynamic Time Warping Distance</title>
    <summary>  The dynamic time warping (dtw) distance fails to satisfy the triangle
inequality and the identity of indiscernibles. As a consequence, the
dtw-distance is not warping-invariant, which in turn results in peculiarities
in data mining applications. This article converts the dtw-distance to a
semi-metric and shows that its canonical extension is warping-invariant.
Empirical results indicate that the nearest-neighbor classifier in the proposed
semi-metric space performs comparably to the same classifier in the standard
dtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result
suggests to further explore the semi-metric space for data mining applications.
</summary>
    <author>
      <name>Brijnesh J. Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09964v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09964v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08306v5</id>
    <updated>2018-09-02T03:39:34Z</updated>
    <published>2016-08-30T02:36:41Z</published>
    <title>Machine Learning in Downlink Coordinated Multipoint in Heterogeneous
  Networks</title>
    <summary>  We propose a method for practical downlink coordinated multipoint (DL CoMP)
implementation in the fifth generation of wireless communications (5G) also
known as New Radio (NR). We base our method on supervised machine learning.
Contributions of this paper are to 1) demonstrate that a support vector machine
(SVM) classifier can learn improved conditions at which DL CoMP can be
dynamically triggered in a scalable realistic environment and 2) increase user
throughput in a heterogeneous network as a result of learning improved
triggering conditions of CoMP. Our simulation results show an improvement in
both the macro and pico base station peak throughputs due to the informed
triggering of the multiple DL CoMP radio streams as learned from the SVM
classifier.
</summary>
    <author>
      <name>Faris B. Mismar</name>
    </author>
    <author>
      <name>Brian L. Evans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, to be submitted to IEEE International Conference
  on Acoustics, Speech, and Signal Processing 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08306v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08306v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01478v1</id>
    <updated>2018-09-02T02:56:25Z</updated>
    <published>2018-09-02T02:56:25Z</published>
    <title>Weakly-Supervised Neural Text Classification</title>
    <summary>  Deep neural networks are gaining increasing popularity for the classic text
classification task, due to their strong expressive power and less requirement
for feature engineering. Despite such attractiveness, neural text
classification models suffer from the lack of training data in many real-world
applications. Although many semi-supervised and weakly-supervised text
classification models exist, they cannot be easily applied to deep neural
models and meanwhile support limited supervision types. In this paper, we
propose a weakly-supervised method that addresses the lack of training data in
neural text classification. Our method consists of two modules: (1) a
pseudo-document generator that leverages seed information to generate
pseudo-labeled documents for model pre-training, and (2) a self-training module
that bootstraps on real unlabeled data for model refinement. Our method has the
flexibility to handle different types of weak supervision and can be easily
integrated into existing deep neural models for text classification. We have
performed extensive experiments on three real-world datasets from different
domains. The results demonstrate that our proposed method achieves inspiring
performance without requiring excessive training data and outperforms baseline
methods significantly.
</summary>
    <author>
      <name>Yu Meng</name>
    </author>
    <author>
      <name>Jiaming Shen</name>
    </author>
    <author>
      <name>Chao Zhang</name>
    </author>
    <author>
      <name>Jiawei Han</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271737</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271737" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM 2018 Full Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07898v2</id>
    <updated>2018-09-01T20:44:05Z</updated>
    <published>2018-05-21T05:28:22Z</published>
    <title>SmoothOut: Smoothing Out Sharp Minima to Improve Generalization in Deep
  Learning</title>
    <summary>  In Deep Learning, Stochastic Gradient Descent (SGD) is usually selected as
the training method because of its efficiency and scalability; however,
recently, a problem in SGD gains research interest: sharp minima in Deep Neural
Networks (DNNs) have poor generalization [1][2]; especially, large-batch SGD
tends to converge to sharp minima. It becomes an open question whether escaping
sharp minima can improve the generalization. To answer this question, we
proposed SmoothOut to smooth out sharp minima in DNNs and thereby improve
generalization. In a nutshell, SmoothOut perturbs multiple copies of the DNN by
noise injection and averages these copies. Injecting noises to SGD is widely
for exploration, but SmoothOut differs in lots of ways: (1) de-noising process
is applied before parameter updating; (2) uniform noises are injected instead
of Gaussian noises; (3) the goal is to obtain an auxiliary function without
sharp minima for better generalization, instead of higher exploration. We prove
that SmoothOut can eliminate sharp minima. Training multiple DNN copies is
inefficient, we further propose a stochastic version of SmoothOut which only
introduces the overhead of noise injecting and de-noising per batch. We prove
that the Stochastic SmoothOut is an unbiased approximation of the original
SmoothOut. In experiments on a variety of DNNs and datasets, SmoothOut
consistently improve generalization in both small-batch and large-batch
training on the top of state-of-the-art solutions. Our source code is in
https://github.com/wenwei202/smoothout
</summary>
    <author>
      <name>Wei Wen</name>
    </author>
    <author>
      <name>Yandan Wang</name>
    </author>
    <author>
      <name>Feng Yan</name>
    </author>
    <author>
      <name>Cong Xu</name>
    </author>
    <author>
      <name>Chunpeng Wu</name>
    </author>
    <author>
      <name>Yiran Chen</name>
    </author>
    <author>
      <name>Hai Li</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07898v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07898v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07777v3</id>
    <updated>2018-09-01T20:07:42Z</updated>
    <published>2018-05-20T15:28:56Z</published>
    <title>DLBI: Deep learning guided Bayesian inference for structure
  reconstruction of super-resolution fluorescence microscopy</title>
    <summary>  Super-resolution fluorescence microscopy, with a resolution beyond the
diffraction limit of light, has become an indispensable tool to directly
visualize biological structures in living cells at a nanometer-scale
resolution. Despite advances in high-density super-resolution fluorescent
techniques, existing methods still have bottlenecks, including extremely long
execution time, artificial thinning and thickening of structures, and lack of
ability to capture latent structures. Here we propose a novel deep learning
guided Bayesian inference approach, DLBI, for the time-series analysis of
high-density fluorescent images. Our method combines the strength of deep
learning and statistical inference, where deep learning captures the underlying
distribution of the fluorophores that are consistent with the observed
time-series fluorescent images by exploring local features and correlation
along time-axis, and statistical inference further refines the ultrastructure
extracted by deep learning and endues physical meaning to the final image.
Comprehensive experimental results on both real and simulated datasets
demonstrate that our method provides more accurate and realistic local patch
and large-field reconstruction than the state-of-the-art method, the 3B
analysis, while our method is more than two orders of magnitude faster. The
main program is available at https://github.com/lykaust15/DLBI
</summary>
    <author>
      <name>Yu Li</name>
    </author>
    <author>
      <name>Fan Xu</name>
    </author>
    <author>
      <name>Fa Zhang</name>
    </author>
    <author>
      <name>Pingyong Xu</name>
    </author>
    <author>
      <name>Mingshu Zhang</name>
    </author>
    <author>
      <name>Ming Fan</name>
    </author>
    <author>
      <name>Lihua Li</name>
    </author>
    <author>
      <name>Xin Gao</name>
    </author>
    <author>
      <name>Renmin Han</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/bioinformatics/bty241</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/bioinformatics/bty241" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ISMB 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bioinformatics, Volume 34, Issue 13, 1 July 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.07777v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07777v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07706v2</id>
    <updated>2018-09-01T19:20:02Z</updated>
    <published>2018-07-20T04:24:51Z</published>
    <title>Efficient Probabilistic Inference in the Quest for Physics Beyond the
  Standard Model</title>
    <summary>  We present a novel framework that enables efficient probabilistic inference
in large-scale scientific models by allowing the execution of existing
domain-specific simulators as probabilistic programs, resulting in highly
interpretable posterior inference. Our framework is general purpose and
scalable, and is based on a cross-platform probabilistic execution protocol
through which an inference engine can control simulators in a language-agnostic
way. We demonstrate the technique in particle physics, on a scientifically
accurate simulation of the tau lepton decay, which is a key ingredient in
establishing the properties of the Higgs boson. High-energy physics has a rich
set of simulators based on quantum field theory and the interaction of
particles in matter. We show how to use probabilistic programming to perform
Bayesian inference in these existing simulator codebases directly, in
particular conditioning on observable outputs from a simulated particle
detector to directly produce an interpretable posterior distribution over decay
pathways. Inference efficiency is achieved via inference compilation where a
deep recurrent neural network is trained to parameterize proposal distributions
and control the stochastic simulator in a sequential importance sampling
scheme, at a fraction of the computational cost of Markov chain Monte Carlo
sampling.
</summary>
    <author>
      <name>Atilim Gunes Baydin</name>
    </author>
    <author>
      <name>Lukas Heinrich</name>
    </author>
    <author>
      <name>Wahid Bhimji</name>
    </author>
    <author>
      <name>Bradley Gram-Hansen</name>
    </author>
    <author>
      <name>Gilles Louppe</name>
    </author>
    <author>
      <name>Lei Shao</name>
    </author>
    <author>
      <name> Prabhat</name>
    </author>
    <author>
      <name>Kyle Cranmer</name>
    </author>
    <author>
      <name>Frank Wood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07706v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07706v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T37, 68T05, 62P35" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.2.6; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00238v1</id>
    <updated>2018-09-01T19:11:53Z</updated>
    <published>2018-09-01T19:11:53Z</published>
    <title>A Machine Learning Driven IoT Solution for Noise Classification in Smart
  Cities</title>
    <summary>  We present a machine learning based method for noise classification using a
low-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients
for audio feature extraction and supervised classification algorithms (that is,
support vector machine and k-nearest neighbors) for noise classification. We
evaluate our approach experimentally with a dataset of about 3000 sound samples
grouped in eight sound classes (such as, car horn, jackhammer, or street
music). We explore the parameter space of support vector machine and k-nearest
neighbors algorithms to estimate the optimal parameter values for
classification of sound samples in the dataset under study. We achieve a noise
classification accuracy in the range 85% -- 100%. Training and testing of our
k-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than
a second for a dataset with features of more than 3000 sound samples.
</summary>
    <author>
      <name>Yasser Alsouda</name>
    </author>
    <author>
      <name>Sabri Pllana</name>
    </author>
    <author>
      <name>Arianit Kurti</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02655v2</id>
    <updated>2018-09-01T19:06:27Z</updated>
    <published>2016-09-09T04:22:03Z</published>
    <title>Singularity structures and impacts on parameter estimation in finite
  mixtures of distributions</title>
    <summary>  Singularities of a statistical model are the elements of the model's
parameter space which make the corresponding Fisher information matrix
degenerate. These are the points for which estimation techniques such as the
maximum likelihood estimator and standard Bayesian procedures do not admit the
root-$n$ parametric rate of convergence. We propose a general framework for the
identification of singularity structures of the parameter space of finite
mixtures, and study the impacts of the singularity structures on minimax lower
bounds and rates of convergence for the maximum likelihood estimator over a
compact parameter space. Our study makes explicit the deep links between model
singularities, parameter estimation convergence rates and minimax lower bounds,
and the algebraic geometry of the parameter space for mixtures of continuous
distributions. The theory is applied to establish concrete convergence rates of
parameter estimation for finite mixture of skew-normal distributions. This rich
and increasingly popular mixture model is shown to exhibit a remarkably complex
range of asymptotic behaviors which have not been hitherto reported in the
literature.
</summary>
    <author>
      <name>Nhat Ho</name>
    </author>
    <author>
      <name>XuanLong Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">86 pages. This version includes new notions of singularities in
  finite mixtures based on several new types of optimal transportation
  distances</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02655v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02655v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01630v3</id>
    <updated>2018-09-01T16:29:56Z</updated>
    <published>2018-08-05T14:51:07Z</published>
    <title>A Review of Learning with Deep Generative Models from perspective of
  graphical modeling</title>
    <summary>  This document aims to provide a review on learning with deep generative
models (DGMs), which is an highly-active area in machine learning and more
generally, artificial intelligence. This review is not meant to be a tutorial,
but when necessary, we provide self-contained derivations for completeness.
This review has two features. First, though there are different perspectives to
classify DGMs, we choose to organize this review from the perspective of
graphical modeling, because the learning methods for directed DGMs and
undirected DGMs are fundamentally different. Second, we differentiate model
definitions from model learning algorithms, since different learning algorithms
can be applied to solve the learning problem on the same model, and an
algorithm can be applied to learn different models. We thus separate model
definition and model learning, with more emphasis on reviewing, differentiating
and connecting different learning algorithms. We also discuss promising future
research directions.
</summary>
    <author>
      <name>Zhijian Ou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">add SN-GANs, SA-GANs, conditional generation (cGANs, AC-GANs)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01630v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01630v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00175v1</id>
    <updated>2018-09-01T13:33:31Z</updated>
    <published>2018-09-01T13:33:31Z</published>
    <title>Hyperparameter Learning for Conditional Mean Embeddings with Rademacher
  Complexity Bounds</title>
    <summary>  Conditional mean embeddings are nonparametric models that encode conditional
expectations in a reproducing kernel Hilbert space. While they provide a
flexible and powerful framework for probabilistic inference, their performance
is highly dependent on the choice of kernel and regularization hyperparameters.
Nevertheless, current hyperparameter tuning methods predominantly rely on
expensive cross validation or heuristics that is not optimized for the
inference task. For conditional mean embeddings with categorical targets and
arbitrary inputs, we propose a hyperparameter learning framework based on
Rademacher complexity bounds to prevent overfitting by balancing data fit
against model complexity. Our approach only requires batch updates, allowing
scalable kernel hyperparameter tuning without invoking kernel approximations.
Experiments demonstrate that our learning framework outperforms competing
methods, and can be further extended to incorporate and learn deep neural
network weights to improve generalization.
</summary>
    <author>
      <name>Kelvin Hsu</name>
    </author>
    <author>
      <name>Richard Nock</name>
    </author>
    <author>
      <name>Fabio Ramos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the European Conference on Machine Learning (ECML-PKDD
  2018): Currently shortlisted for best student paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10938v4</id>
    <updated>2018-09-01T13:26:39Z</updated>
    <published>2018-04-29T14:18:07Z</published>
    <title>Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge,
  Deep Architectures, and Beyond</title>
    <summary>  Automatic understanding of human affect using visual signals is of great
importance in everyday human-machine interactions. Appraising human emotional
states, behaviors and reactions displayed in real-world settings, can be
accomplished using latent continuous dimensions (e.g., the circumplex model of
affect). Valence (i.e., how positive or negative is an emotion) and arousal
(i.e., power of the activation of the emotion) constitute the most popular and
effective affect representations. Nevertheless, the majority of collected
datasets this far, although containing naturalistic emotional states, have been
captured in highly controlled recording conditions. In this paper, we introduce
the Aff-Wild benchmark for training and evaluating affect recognition
algorithms. We also report on the results of the First Affect-in-the-wild
Challenge (Aff-Wild Challenge) that was recently organized on the Aff-Wild
database, and was the first ever challenge on the estimation of valence and
arousal in-the-wild. Furthermore, we design and extensively train an end-to-end
deep neural architecture which performs prediction of continuous emotion
dimensions based on visual cues. The proposed deep learning architecture,
AffWildNet, includes convolutional and recurrent neural network (CNN-RNN)
layers, exploiting the invariant properties of convolutional features, while
also modeling temporal dynamics that arise in human behavior via the recurrent
layers. The AffWildNet produced state-of-the-art results on the Aff-Wild
Challenge. We then exploit the AffWild database for learning features, which
can be used as priors for achieving best performances both for dimensional, as
well as categorical emotion recognition, using the RECOLA, AFEW-VA and EmotiW
2017 datasets, compared to all other methods designed for the same goal.
</summary>
    <author>
      <name>Dimitrios Kollias</name>
    </author>
    <author>
      <name>Panagiotis Tzirakis</name>
    </author>
    <author>
      <name>Mihalis A. Nicolaou</name>
    </author>
    <author>
      <name>Athanasios Papaioannou</name>
    </author>
    <author>
      <name>Guoying Zhao</name>
    </author>
    <author>
      <name>Björn Schuller</name>
    </author>
    <author>
      <name>Irene Kotsia</name>
    </author>
    <author>
      <name>Stefanos Zafeiriou</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10938v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10938v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02942v2</id>
    <updated>2018-09-01T12:37:58Z</updated>
    <published>2018-06-08T01:58:51Z</published>
    <title>SupportNet: solving catastrophic forgetting in class incremental
  learning with support data</title>
    <summary>  A plain well-trained deep learning model often does not have the ability to
learn new knowledge without forgetting the previously learned knowledge, which
is known as the catastrophic forgetting. Here we propose a novel method,
SupportNet, to solve the catastrophic forgetting problem in class incremental
learning scenario efficiently and effectively. SupportNet combines the strength
of deep learning and support vector machine (SVM), where SVM is used to
identify the support data from the old data, which are fed to the deep learning
model together with the new data for further training so that the model can
review the essential information of the old data when learning the new
information. Two powerful consolidation regularizers are applied to ensure the
robustness of the learned model. Comprehensive experiments on various tasks,
including enzyme function prediction, subcellular structure classification and
breast tumor classification, show that SupportNet drastically outperforms the
state-of-the-art incremental learning methods and even reaches similar
performance as the deep learning model trained from scratch on both old and new
data. Our program is accessible at: https://github.com/lykaust15/SupportNet
</summary>
    <author>
      <name>Yu Li</name>
    </author>
    <author>
      <name>Zhongxiao Li</name>
    </author>
    <author>
      <name>Lizhong Ding</name>
    </author>
    <author>
      <name>Yijie Pan</name>
    </author>
    <author>
      <name>Chao Huang</name>
    </author>
    <author>
      <name>Yuhui Hu</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Xin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02942v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02942v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.11451v2</id>
    <updated>2018-09-01T10:48:41Z</updated>
    <published>2018-03-30T13:41:42Z</published>
    <title>Minimax Estimation of Quadratic Fourier Functionals</title>
    <summary>  We study estimation of (semi-)inner products between two nonparametric
probability distributions, given IID samples from each distribution. These
products include relatively well-studied classical $\mathcal{L}^2$ and Sobolev
inner products, as well as those induced by translation-invariant reproducing
kernels, for which we believe our results are the first. We first propose
estimators for these quantities, and the induced (semi)norms and
(pseudo)metrics. We then prove non-asymptotic upper bounds on their mean
squared error, in terms of weights both of the inner product and of the two
distributions, in the Fourier basis. Finally, we prove minimax lower bounds
that imply rate-optimality of the proposed estimators over Fourier ellipsoids.
</summary>
    <author>
      <name>Shashank Singh</name>
    </author>
    <author>
      <name>Bharath K. Sriperumbudur</name>
    </author>
    <author>
      <name>Barnabás Póczos</name>
    </author>
    <link href="http://arxiv.org/abs/1803.11451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07169v2</id>
    <updated>2018-09-01T08:27:25Z</updated>
    <published>2018-04-19T13:51:45Z</published>
    <title>Large-scale Nonlinear Variable Selection via Kernel Random Features</title>
    <summary>  We propose a new method for input variable selection in nonlinear regression.
The method is embedded into a kernel regression machine that can model general
nonlinear functions, not being a priori limited to additive models. This is the
first kernel-based variable selection method applicable to large datasets. It
sidesteps the typical poor scaling properties of kernel methods by mapping the
inputs into a relatively low-dimensional space of random features. The
algorithm discovers the variables relevant for the regression task together
with learning the prediction model through learning the appropriate nonlinear
random feature maps. We demonstrate the outstanding performance of our method
on a set of large-scale synthetic and real datasets.
</summary>
    <author>
      <name>Magda Gregorová</name>
    </author>
    <author>
      <name>Jason Ramapuram</name>
    </author>
    <author>
      <name>Alexandros Kalousis</name>
    </author>
    <author>
      <name>Stéphane Marchand-Maillet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version for proceedings of ECML/PKDD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07169v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07169v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00101v1</id>
    <updated>2018-09-01T02:22:57Z</updated>
    <published>2018-09-01T02:22:57Z</published>
    <title>Attentive Crowd Flow Machines</title>
    <summary>  Traffic flow prediction is crucial for urban traffic management and public
safety. Its key challenges lie in how to adaptively integrate the various
factors that affect the flow changes. In this paper, we propose a unified
neural network module to address this problem, called Attentive Crowd Flow
Machine~(ACFM), which is able to infer the evolution of the crowd flow by
learning dynamic representations of temporally-varying data with an attention
mechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units
connected with a convolutional layer for spatial weight prediction. The first
LSTM takes the sequential flow density representation as input and generates a
hidden state at each time-step for attention map inference, while the second
LSTM aims at learning the effective spatial-temporal feature expression from
attentionally weighted crowd flow features. Based on the ACFM, we further build
a deep architecture with the application to citywide crowd flow prediction,
which naturally incorporates the sequential and periodic data as well as other
external influences. Extensive experiments on two standard benchmarks (i.e.,
crowd flow in Beijing and New York City) show that the proposed method achieves
significant improvements over the state-of-the-art methods.
</summary>
    <author>
      <name>Lingbo Liu</name>
    </author>
    <author>
      <name>Ruimao Zhang</name>
    </author>
    <author>
      <name>Jiefeng Peng</name>
    </author>
    <author>
      <name>Guanbin Li</name>
    </author>
    <author>
      <name>Bowen Du</name>
    </author>
    <author>
      <name>Liang Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM MM, full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01499v1</id>
    <updated>2018-09-01T00:15:30Z</updated>
    <published>2018-09-01T00:15:30Z</published>
    <title>Extractive Adversarial Networks: High-Recall Explanations for
  Identifying Personal Attacks in Social Media Posts</title>
    <summary>  We introduce an adversarial method for producing high-recall explanations of
neural text classifier decisions. Building on an existing architecture for
extractive explanations via hard attention, we add an adversarial layer which
scans the residual of the attention for remaining predictive signal. Motivated
by the important domain of detecting personal attacks in social media comments,
we additionally demonstrate the importance of manually setting a semantically
appropriate `default' behavior for the model by explicitly manipulating its
bias term. We develop a validation set of human-annotated personal attacks to
evaluate the impact of these changes.
</summary>
    <author>
      <name>Samuel Carton</name>
    </author>
    <author>
      <name>Qiaozhu Mei</name>
    </author>
    <author>
      <name>Paul Resnick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2018; code and data available soon</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00082v1</id>
    <updated>2018-08-31T23:38:00Z</updated>
    <published>2018-08-31T23:38:00Z</published>
    <title>The NEU Meta-Algorithm for Geometric Learning with Applications in
  Finance</title>
    <summary>  We introduce a meta-algorithm, called non-Euclidean upgrading (NEU), which
learns algorithm-specific geometries to improve the training and validation set
performance of a wide class of learning algorithms. Our approach is based on
iteratively performing local reconfigurations of the space in which the data
lie. These reconfigurations build universal approximation and universal
reconfiguration properties into the new algorithm being learned. This allows
any set of features to be learned by the new algorithm to arbitrary precision.
The training and validation set performance of NEU is investigated through
implementations predicting the relationship between select stock prices as well
as finding low-dimensional representations of the German Bond yield curve.
</summary>
    <author>
      <name>Anastasis Kratsios</name>
    </author>
    <author>
      <name>Cody B. Hyndman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60D05, 91G60, 62G08, 65D15, 62H25, 91G80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00068v1</id>
    <updated>2018-08-31T22:01:45Z</updated>
    <published>2018-08-31T22:01:45Z</published>
    <title>Denoising Neural Machine Translation Training with Trusted Data and
  Online Data Selection</title>
    <summary>  Measuring domain relevance of data and identifying or selecting well-fit
domain data for machine translation (MT) is a well-studied topic, but denoising
is not yet. Denoising is concerned with a different type of data quality and
tries to reduce the negative impact of data noise on MT training, in
particular, neural MT (NMT) training. This paper generalizes methods for
measuring and selecting data for domain MT and applies them to denoising NMT
training. The proposed approach uses trusted data and a denoising curriculum
realized by online data selection. Intrinsic and extrinsic evaluations of the
approach show its significant effectiveness for NMT to train on data with
severe noise.
</summary>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Taro Watanabe</name>
    </author>
    <author>
      <name>Macduff Hughes</name>
    </author>
    <author>
      <name>Tetsuji Nakagawa</name>
    </author>
    <author>
      <name>Ciprian Chelba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2018 Third Conference on Machine Translation (WMT18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00065v1</id>
    <updated>2018-08-31T21:22:52Z</updated>
    <published>2018-08-31T21:22:52Z</published>
    <title>MULDEF: Multi-model-based Defense Against Adversarial Examples for
  Neural Networks</title>
    <summary>  Despite being popularly used in many application domains such as image
recognition and classification, neural network models have been found to be
vulnerable to adversarial examples: given a model and an example correctly
classified by the model, an adversarial example is a new example formed by
applying small perturbation (imperceptible to human) on the given example so
that the model misclassifies the new example. Adversarial examples can pose
potential risks on safety or security in real-world applications. In recent
years, given a vulnerable model, defense approaches, such as adversarial
training and defensive distillation, improve the model to make it more robust
against adversarial examples. However, based on the improved model, attackers
can still generate adversarial examples to successfully attack the model. To
address such limitation, we propose a new defense approach, named MULDEF, based
on the design principle of diversity. Given a target model (as a seed model)
and an attack approach to be defended against, MULDEF constructs additional
models (from the seed model) together with the seed model to form a family of
models, such that the models are complementary to each other to accomplish
robustness diversity (i.e., one model's adversarial examples typically do not
become other models' adversarial examples), while maintaining about the same
accuracy for normal examples. At runtime, given an input example, MULDEF
randomly selects a model from the family to be applied on the given example.
The robustness diversity of the model family and the random selection of a
model from the family together lower the success rate of attacks. Our
evaluation results show that MULDEF substantially improves the target model's
accuracy on adversarial examples by 35-50% and 2-10% in the white-box and
black-box attack scenarios, respectively.
</summary>
    <author>
      <name>Siwakorn Srisakaokul</name>
    </author>
    <author>
      <name>Zexuan Zhong</name>
    </author>
    <author>
      <name>Yuhao Zhang</name>
    </author>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Tao Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00052v1</id>
    <updated>2018-08-31T20:39:11Z</updated>
    <published>2018-08-31T20:39:11Z</published>
    <title>Your Actions or Your Associates? Predicting Certification and Dropout in
  MOOCs with Behavioral and Social Features</title>
    <summary>  The high level of attrition and low rate of certification in Massive Open
Online Courses (MOOCs) has prompted a great deal of research. Prior researchers
have focused on predicting dropout based upon behavioral features such as
student confusion, click-stream patterns, and social interactions. However, few
studies have focused on combining student logs with forum data.
  In this work, we use data from two different offerings of the same MOOC. We
conduct a survival analysis to identify likely dropouts. We then examine two
classes of features, social and behavioral, and apply a combination of modeling
and feature-selection methods to identify the most relevant features to predict
both dropout and certification. We examine the utility of three different model
types and we consider the impact of different definitions of dropout on the
predictors. Finally, we assess the reliability of the models over time by
evaluating whether or not models from week 1 can predict dropout in week 2, and
so on. The outcomes of this study will help instructors identify students
likely to fail or dropout as soon as the first two weeks and provide them with
more support.
</summary>
    <author>
      <name>Niki Gitinabard</name>
    </author>
    <author>
      <name>Farzaneh Khoshnevisan</name>
    </author>
    <author>
      <name>Collin F. Lynch</name>
    </author>
    <author>
      <name>Elle Yuan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the 11th International Conference on Educational Data
  Mining (EDM 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08697v3</id>
    <updated>2018-08-31T19:48:51Z</updated>
    <published>2016-04-29T06:12:19Z</published>
    <title>Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via
  Truncated Rayleigh Flow</title>
    <summary>  Sparse generalized eigenvalue problem (GEP) plays a pivotal role in a large
family of high-dimensional statistical models, including sparse Fisher's
discriminant analysis, canonical correlation analysis, and sufficient dimension
reduction. Sparse GEP involves solving a non-convex optimization problem. Most
existing methods and theory in the context of specific statistical models that
are special cases of the sparse GEP require restrictive structural assumptions
on the input matrices. In this paper, we propose a two-stage computational
framework to solve the sparse GEP. At the first stage, we solve a convex
relaxation of the sparse GEP. Taking the solution as an initial value, we then
exploit a nonconvex optimization perspective and propose the truncated Rayleigh
flow method (Rifle) to estimate the leading generalized eigenvector. We show
that Rifle converges linearly to a solution with the optimal statistical rate
of convergence for many statistical models. Theoretically, our method
significantly improves upon the existing literature by eliminating structural
assumptions on the input matrices for both stages. To achieve this, our
analysis involves two key ingredients: (i) a new analysis of the gradient based
method on nonconvex objective functions, and (ii) a fine-grained
characterization of the evolution of sparsity patterns along the solution path.
Thorough numerical studies are provided to validate the theoretical results.
</summary>
    <author>
      <name>Kean Ming Tan</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in JRSSB</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08697v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08697v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01477v1</id>
    <updated>2018-08-31T19:31:05Z</updated>
    <published>2018-08-31T19:31:05Z</published>
    <title>A Supervised Learning Approach For Heading Detection</title>
    <summary>  As the Portable Document Format (PDF) file format increases in popularity,
research in analysing its structure for text extraction and analysis is
necessary. Detecting headings can be a crucial component of classifying and
extracting meaningful data. This research involves training a supervised
learning model to detect headings with features carefully selected through
recursive feature elimination. The best performing classifier had an accuracy
of 96.95%, sensitivity of 0.986 and a specificity of 0.953. This research into
heading detection contributes to the field of PDF based text extraction and can
be applied to the automation of large scale PDF text analysis in a variety of
professional and policy based contexts.
</summary>
    <author>
      <name>Sahib Singh Budhiraja</name>
    </author>
    <author>
      <name>Vijay Mago</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10867v1</id>
    <updated>2018-08-31T17:51:54Z</updated>
    <published>2018-08-31T17:51:54Z</published>
    <title>Tensor Embedding: A Supervised Framework for Human Behavioral Data
  Mining and Prediction</title>
    <summary>  Today's densely instrumented world offers tremendous opportunities for
continuous acquisition and analysis of multimodal sensor data providing
temporal characterization of an individual's behaviors. Is it possible to
efficiently couple such rich sensor data with predictive modeling techniques to
provide contextual, and insightful assessments of individual performance and
wellbeing? Prediction of different aspects of human behavior from these noisy,
incomplete, and heterogeneous bio-behavioral temporal data is a challenging
problem, beyond unsupervised discovery of latent structures. We propose a
Supervised Tensor Embedding (STE) algorithm for high dimension multimodal data
with join decomposition of input and target variable. Furthermore, we show that
features selection will help to reduce the contamination in the prediction and
increase the performance. The efficiently of the methods was tested via two
different real world datasets.
</summary>
    <author>
      <name>Homa Hosseinmardi</name>
    </author>
    <author>
      <name>Amir Ghasemian</name>
    </author>
    <author>
      <name>Shrikanth Narayanan</name>
    </author>
    <author>
      <name>Kristina Lerman</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10862v1</id>
    <updated>2018-08-31T17:43:21Z</updated>
    <published>2018-08-31T17:43:21Z</published>
    <title>Open Source Dataset and Machine Learning Techniques for Automatic
  Recognition of Historical Graffiti</title>
    <summary>  Machine learning techniques are presented for automatic recognition of the
historical letters (XI-XVIII centuries) carved on the stoned walls of St.Sophia
cathedral in Kyiv (Ukraine). A new image dataset of these carved Glagolitic and
Cyrillic letters (CGCL) was assembled and pre-processed for recognition and
prediction by machine learning methods. The dataset consists of more than 4000
images for 34 types of letters. The explanatory data analysis of CGCL and
notMNIST datasets shown that the carved letters can hardly be differentiated by
dimensionality reduction methods, for example, by t-distributed stochastic
neighbor embedding (tSNE) due to the worse letter representation by stone
carving in comparison to hand writing. The multinomial logistic regression
(MLR) and a 2D convolutional neural network (CNN) models were applied. The MLR
model demonstrated the area under curve (AUC) values for receiver operating
characteristic (ROC) are not lower than 0.92 and 0.60 for notMNIST and CGCL,
respectively. The CNN model gave AUC values close to 0.99 for both notMNIST and
CGCL (despite the much smaller size and quality of CGCL in comparison to
notMNIST) under condition of the high lossy data augmentation. CGCL dataset was
published to be available for the data science community as an open source
resource.
</summary>
    <author>
      <name>Nikita Gordienko</name>
    </author>
    <author>
      <name>Peng Gang</name>
    </author>
    <author>
      <name>Yuri Gordienko</name>
    </author>
    <author>
      <name>Wei Zeng</name>
    </author>
    <author>
      <name>Oleg Alienin</name>
    </author>
    <author>
      <name>Oleksandr Rokovyi</name>
    </author>
    <author>
      <name>Sergii Stirenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, accepted for 25th International Conference on
  Neural Information Processing (ICONIP 2018), 14-16 December, 2018 (Siem Reap,
  Cambodia)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01015v1</id>
    <updated>2018-08-31T17:07:31Z</updated>
    <published>2018-08-31T17:07:31Z</published>
    <title>Automated segmentation on the entire cardiac cycle using a deep learning
  work-flow</title>
    <summary>  The segmentation of the left ventricle (LV) from CINE MRI images is essential
to infer important clinical parameters. Typically, machine learning algorithms
for automated LV segmentation use annotated contours from only two cardiac
phases, diastole, and systole. In this work, we present an analysis work-flow
for fully-automated LV segmentation that learns from images acquired through
the cardiac cycle. The workflow consists of three components: first, for each
image in the sequence, we perform an automated localization and subsequent
cropping of the bounding box containing the cardiac silhouette. Second, we
identify the LV contours using a Temporal Fully Convolutional Neural Network
(T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a
recurrent mechanism enforcing temporal coherence across consecutive frames.
Finally, we further defined the boundaries using either one of two components:
fully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials
and Semantic Flow. Our initial experiments suggest that significant improvement
in performance can potentially be achieved by using a recurrent neural network
component that explicitly learns cardiac motion patterns whilst performing LV
segmentation.
</summary>
    <author>
      <name>Nicoló Savioli</name>
    </author>
    <author>
      <name>Miguel Silva Vieira</name>
    </author>
    <author>
      <name>Pablo Lamata</name>
    </author>
    <author>
      <name>Giovanni Montana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, published on IEEE Xplore</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00934v1</id>
    <updated>2018-08-31T14:45:33Z</updated>
    <published>2018-08-31T14:45:33Z</published>
    <title>A Deep Neural Network Sentence Level Classification Method with Context
  Information</title>
    <summary>  In the sentence classification task, context formed from sentences adjacent
to the sentence being classified can provide important information for
classification. This context is, however, often ignored. Where methods do make
use of context, only small amounts are considered, making it difficult to
scale. We present a new method for sentence classification, Context-LSTM-CNN,
that makes use of potentially large contexts. The method also utilizes
long-range dependencies within the sentence being classified, using an LSTM,
and short-span features, using a stacked CNN. Our experiments demonstrate that
this approach consistently improves over previous methods on two different
datasets.
</summary>
    <author>
      <name>Xingyi Song</name>
    </author>
    <author>
      <name>Johann Petrak</name>
    </author>
    <author>
      <name>Angus Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EMNLP2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10788v1</id>
    <updated>2018-08-31T14:40:03Z</updated>
    <published>2018-08-31T14:40:03Z</published>
    <title>Data-driven discovery of PDEs in complex datasets</title>
    <summary>  Many processes in science and engineering can be described by partial
differential equations (PDEs). Traditionally, PDEs are derived by considering
first principles of physics to derive the relations between the involved
physical quantities of interest. A different approach is to measure the
quantities of interest and use deep learning to reverse engineer the PDEs which
are describing the physical process.
  In this paper we use machine learning, and deep learning in particular, to
discover PDEs hidden in complex data sets from measurement data. We include
examples of data from a known model problem, and real data from weather station
measurements. We show how necessary transformations of the input data amounts
to coordinate transformations in the discovered PDE, and we elaborate on
feature and model selection. It is shown that the dynamics of a non-linear,
second order PDE can be accurately described by an ordinary differential
equation which is automatically discovered by our deep learning algorithm. Even
more interestingly, we show that similar results apply in the context of more
complex simulations of the Swedish temperature distribution.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nyström</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10724v1</id>
    <updated>2018-08-31T13:16:31Z</updated>
    <published>2018-08-31T13:16:31Z</published>
    <title>Learning Data-adaptive Nonparametric Kernels</title>
    <summary>  Traditional kernels or their combinations are often not sufficiently flexible
to fit the data in complicated practical tasks. In this paper, we present a
Data-Adaptive Nonparametric Kernel (DANK) learning framework by imposing an
adaptive matrix on the kernel/Gram matrix in an entry-wise strategy. Since we
do not specify the formulation of the adaptive matrix, each entry in it can be
directly and flexibly learned from the data. Therefore, the solution space of
the learned kernel is largely expanded, which makes DANK flexible to adapt to
the data. Specifically, the proposed kernel learning framework can be
seamlessly embedded to support vector machines (SVM) and support vector
regression (SVR), which has the capability of enlarging the margin between
classes and reducing the model generalization error. Theoretically, we
demonstrate that the objective function of our devised model is
gradient-Lipschitz continuous. Thereby, the training process for kernel and
parameter learning in SVM/SVR can be efficiently optimized in a unified
framework. Further, to address the scalability issue in DANK, a
decomposition-based scalable approach is developed, of which the effectiveness
is demonstrated by both empirical studies and theoretical guarantees.
Experimentally, our method outperforms other representative kernel learning
based algorithms on various classification and regression benchmark datasets.
</summary>
    <author>
      <name>Fanghui Liu</name>
    </author>
    <author>
      <name>Xiaolin Huang</name>
    </author>
    <author>
      <name>Chen Gong</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10705v1</id>
    <updated>2018-08-31T12:24:01Z</updated>
    <published>2018-08-31T12:24:01Z</published>
    <title>Bayesian Classifier for Route Prediction with Markov Chains</title>
    <summary>  We present here a general framework and a specific algorithm for predicting
the destination, route, or more generally a pattern, of an ongoing journey,
building on the recent work of [Y. Lassoued, J. Monteil, Y. Gu, G. Russo, R.
Shorten, and M. Mevissen, "Hidden Markov model for route and destination
prediction," in IEEE International Conference on Intelligent Transportation
Systems, 2017]. In the presented framework, known journey patterns are modelled
as stochastic processes, emitting the road segments visited during the journey,
and the ongoing journey is predicted by updating the posterior probability of
each journey pattern given the road segments visited so far. In this
contribution, we use Markov chains as models for the journey patterns, and
consider the prediction as final, once one of the posterior probabilities
crosses a predefined threshold. Despite the simplicity of both, examples run on
a synthetic dataset demonstrate high accuracy of the made predictions.
</summary>
    <author>
      <name>Jonathan P. Epperlein</name>
    </author>
    <author>
      <name>Julien Monteil</name>
    </author>
    <author>
      <name>Mingming Liu</name>
    </author>
    <author>
      <name>Yingqi Gu</name>
    </author>
    <author>
      <name>Sergiy Zhuk</name>
    </author>
    <author>
      <name>Robert Shorten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at The 21st IEEE International Conference on Intelligent
  Transportation Systems (ITSC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10692v1</id>
    <updated>2018-08-31T11:47:10Z</updated>
    <published>2018-08-31T11:47:10Z</published>
    <title>APES: a Python toolbox for simulating reinforcement learning
  environments</title>
    <summary>  Assisted by neural networks, reinforcement learning agents have been able to
solve increasingly complex tasks over the last years. The simulation
environment in which the agents interact is an essential component in any
reinforcement learning problem. The environment simulates the dynamics of the
agents' world and hence provides feedback to their actions in terms of state
observations and external rewards. To ease the design and simulation of such
environments this work introduces $\texttt{APES}$, a highly customizable and
open source package in Python to create 2D grid-world environments for
reinforcement learning problems. $\texttt{APES}$ equips agents with algorithms
to simulate any field of vision, it allows the creation and positioning of
items and rewards according to user-defined rules, and supports the interaction
of multiple agents.
</summary>
    <author>
      <name>Aqeel Labash</name>
    </author>
    <author>
      <name>Ardi Tampuu</name>
    </author>
    <author>
      <name>Tambet Matiisen</name>
    </author>
    <author>
      <name>Jaan Aru</name>
    </author>
    <author>
      <name>Raul Vicente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10664v1</id>
    <updated>2018-08-31T10:22:32Z</updated>
    <published>2018-08-31T10:22:32Z</published>
    <title>A novel graph-based model for hybrid recommendations in cold-start
  scenarios</title>
    <summary>  Cold-start is a very common and still open problem in the Recommender Systems
literature. Since cold start items do not have any interaction, collaborative
algorithms are not applicable. One of the main strategies is to use pure or
hybrid content-based approaches, which usually yield to lower recommendation
quality than collaborative ones. Some techniques to optimize performance of
this type of approaches have been studied in recent past. One of them is called
feature weighting, which assigns to every feature a real value, called weight,
that estimates its importance. Statistical techniques for feature weighting
commonly used in Information Retrieval, like TF-IDF, have been adapted for
Recommender Systems, but they often do not provide sufficient quality
improvements. More recent approaches, FBSM and LFW, estimate weights by
leveraging collaborative information via machine learning, in order to learn
the importance of a feature based on other users opinions. This type of models
have shown promising results compared to classic statistical analyzes cited
previously. We propose a novel graph, feature-based machine learning model to
face the cold-start item scenario, learning the relevance of features from
probabilities of item-based collaborative filtering algorithms.
</summary>
    <author>
      <name>Cesare Bernardis</name>
    </author>
    <author>
      <name>Maurizio Ferrari Dacrema</name>
    </author>
    <author>
      <name>Paolo Cremonesi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10663v1</id>
    <updated>2018-08-31T10:20:43Z</updated>
    <published>2018-08-31T10:20:43Z</published>
    <title>A Multi-layer Gaussian Process for Motor Symptom Estimation in People
  with Parkinson's Disease</title>
    <summary>  The assessment of Parkinson's disease (PD) poses a significant challenge as
it is influenced by various factors which lead to a complex and fluctuating
symptom manifestation. Thus, a frequent and objective PD assessment is highly
valuable for effective health management of people with Parkinson's disease
(PwP). Here, we propose a method for monitoring PwP by stochastically modeling
the relationships between their wrist movements during unscripted daily
activities and corresponding annotations about clinical displays of movement
abnormalities. We approach the estimation of PD motor signs by independently
modeling and hierarchically stacking Gaussian process models for three classes
of commonly observed movement abnormalities in PwP including tremor,
(non-tremulous) bradykinesia, and (non-tremulous) dyskinesia. We use clinically
adopted severity measures as annotations for training the models, thus allowing
our multi-layer Gaussian process prediction models to estimate not only their
presence but also their severities. The experimental validation of our approach
demonstrates strong agreement of the model predictions with these PD
annotations. Our results show the proposed method produces promising results in
objective monitoring of movement abnormalities of PD in the presence of
arbitrary and unknown voluntary motions, and makes an important step towards
continuous monitoring of PD in the home environment.
</summary>
    <author>
      <name>Muriel Lang</name>
    </author>
    <author>
      <name>Urban Fietzek</name>
    </author>
    <author>
      <name>Jakob Fröhner</name>
    </author>
    <author>
      <name>Franz M. J. Pfister</name>
    </author>
    <author>
      <name>Daniel Pichler</name>
    </author>
    <author>
      <name>Kian Abedinpour</name>
    </author>
    <author>
      <name>Terry T. Um</name>
    </author>
    <author>
      <name>Dana Kulić</name>
    </author>
    <author>
      <name>Satoshi Endo</name>
    </author>
    <author>
      <name>Sandra Hirche</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10650v1</id>
    <updated>2018-08-31T09:51:12Z</updated>
    <published>2018-08-31T09:51:12Z</published>
    <title>Graph reduction by local variation</title>
    <summary>  How can we reduce the size of a graph without significantly altering its
basic properties? We approach the graph reduction problem from the perspective
of restricted similarity, a modification of a well-known measure for graph
approximation. Our choice is motivated by the observation that restricted
similarity implies strong spectral guarantees and can be used to prove
statements about certain unsupervised learning problems. The paper then focuses
on coarsening, a popular type of graph reduction. We derive sufficient
conditions for a small graph to approximate a larger one in the sense of
restricted similarity. Our theoretical findings give rise to a novel
quasi-linear algorithm. Compared to both standard and advanced graph reduction
methods, the proposed algorithm finds coarse graphs of improved quality -often
by a large margin- without sacrificing speed.
</summary>
    <author>
      <name>Andreas Loukas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10648v1</id>
    <updated>2018-08-31T09:46:07Z</updated>
    <published>2018-08-31T09:46:07Z</published>
    <title>Adaptation and Robust Learning of Probabilistic Movement Primitives</title>
    <summary>  Probabilistic representations of movement primitives open important new
possibilities for machine learning in robotics. These representations are able
to capture the variability of the demonstrations from a teacher as a
probability distribution over trajectories, providing a sensible region of
exploration and the ability to adapt to changes in the robot environment.
However, to be able to capture variability and correlations between different
joints, a probabilistic movement primitive requires the estimation of a larger
number of parameters compared to their deterministic counterparts, that focus
on modeling only the mean behavior. In this paper, we make use of prior
distributions over the parameters of a probabilistic movement primitive to make
robust estimates of the parameters with few training instances. In addition, we
introduce general purpose operators to adapt movement primitives in joint and
task space. The proposed training method and adaptation operators are tested in
a coffee preparation and in robot table tennis task. In the coffee preparation
task we evaluate the generalization performance to changes in the location of
the coffee grinder and brewing chamber in a target area, achieving the desired
behavior after only two demonstrations. In the table tennis task we evaluate
the hit and return rates, outperforming previous approaches while using fewer
task specific heuristics.
</summary>
    <author>
      <name>Sebastian Gomez-Gonzalez</name>
    </author>
    <author>
      <name>Gerhard Neumann</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <author>
      <name>Jan Peters</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10632v1</id>
    <updated>2018-08-31T08:34:14Z</updated>
    <published>2018-08-31T08:34:14Z</published>
    <title>A novel extension of Generalized Low-Rank Approximation of Matrices
  based on multiple-pairs of transformations</title>
    <summary>  Dimension reduction is a main step in learning process which plays a
essential role in many applications. The most popular methods in this field
like SVD, PCA, and LDA, only can apply to vector data. This means that for
higher order data like matrices or more generally tensors, data should be fold
to a vector. By this folding, the probability of overfitting is increased and
also maybe some important spatial features are ignored. Then, to tackle these
issues, methods are proposed which work directly on data with their own format
like GLRAM, MPCA, and MLDA. In these methods the spatial relationship among
data are preserved and furthermore, the probability of overfitiing has fallen.
Also the time and space complexity are less than vector-based ones. Having said
that, because of the less parameters in multilinear methods, they have a much
smaller search space to find an optimal answer in comparison with vector-based
approach. To overcome this drawback of multilinear methods like GLRAM, we
proposed a new method which is a general form of GLRAM and by preserving the
merits of it have a larger search space. We have done plenty of experiments to
show that our proposed method works better than GLRAM. Also, applying this
approach to other multilinear dimension reduction methods like MPCA and MLDA is
straightforwar
</summary>
    <author>
      <name>Soheil Ahmadi</name>
    </author>
    <author>
      <name>Mansoor Rezghi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07174v3</id>
    <updated>2018-08-31T08:25:11Z</updated>
    <published>2018-06-19T12:01:57Z</published>
    <title>FRnet-DTI: Deep Convolutional Neural Networks with Evolutionary and
  Structural Features for Drug-Target Interaction</title>
    <summary>  The task of drug-target interaction prediction holds significant importance
in pharmacology and therapeutic drug design. In this paper, we present
FRnet-DTI, an auto encoder and a convolutional classifier for feature
manipulation and drug target interaction prediction. Two convolutional neural
neworks are proposed where one model is used for feature manipulation and the
other one for classification. Using the first method FRnet-1, we generate 4096
features for each of the instances in each of the datasets and use the second
method, FRnet-2, to identify interaction probability employing those features.
We have tested our method on four gold standard datasets exhaustively used by
other researchers. Experimental results shows that our method significantly
improves over the state-of-the-art method on three of the four drug-target
interaction gold standard datasets on both area under curve for Receiver
Operating Characteristic(auROC) and area under Precision Recall curve(auPR)
metric. We also introduce twenty new potential drug-target pairs for
interaction based on high prediction scores. Codes Available: https: // github.
com/ farshidrayhanuiu/ FRnet-DTI/ Web Implementation: http: // farshidrayhan.
pythonanywhere. com/ FRnet-DTI/
</summary>
    <author>
      <name>Farshid Rayhan</name>
    </author>
    <author>
      <name>Sajid Ahmed</name>
    </author>
    <author>
      <name>Zaynab Mousavian</name>
    </author>
    <author>
      <name>Dewan Md Farid</name>
    </author>
    <author>
      <name>Swakkhar Shatabda</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07174v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07174v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09856v2</id>
    <updated>2018-08-31T06:57:05Z</updated>
    <published>2018-01-30T05:47:01Z</published>
    <title>ReNN: Rule-embedded Neural Networks</title>
    <summary>  The artificial neural network shows powerful ability of inference, but it is
still criticized for lack of interpretability and prerequisite needs of big
dataset. This paper proposes the Rule-embedded Neural Network (ReNN) to
overcome the shortages. ReNN first makes local-based inferences to detect local
patterns, and then uses rules based on domain knowledge about the local
patterns to generate rule-modulated map. After that, ReNN makes global-based
inferences that synthesizes the local patterns and the rule-modulated map. To
solve the optimization problem caused by rules, we use a two-stage optimization
strategy to train the ReNN model. By introducing rules into ReNN, we can
strengthen traditional neural networks with long-term dependencies which are
difficult to learn with limited empirical dataset, thus improving inference
accuracy. The complexity of neural networks can be reduced since long-term
dependencies are not modeled with neural connections, and thus the amount of
data needed to optimize the neural networks can be reduced. Besides, inferences
from ReNN can be analyzed with both local patterns and rules, and thus have
better interpretability. In this paper, ReNN has been validated with a
time-series detection problem.
</summary>
    <author>
      <name>Hu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">poster paper in ICPR, 6 pages, 4 figures, and 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09856v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09856v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07276v3</id>
    <updated>2018-08-31T05:35:08Z</updated>
    <published>2018-03-20T07:24:40Z</published>
    <title>Removing Confounding Factors Associated Weights in Deep Neural Networks
  Improves the Prediction Accuracy for Healthcare Applications</title>
    <summary>  The proliferation of healthcare data has brought the opportunities of
applying data-driven approaches, such as machine learning methods, to assist
diagnosis. Recently, many deep learning methods have been shown with impressive
successes in predicting disease status with raw input data. However, the
"black-box" nature of deep learning and the high-reliability requirement of
biomedical applications have created new challenges regarding the existence of
confounding factors. In this paper, with a brief argument that inappropriate
handling of confounding factors will lead to models' sub-optimal performance in
real-world applications, we present an efficient method that can remove the
influences of confounding factors such as age or gender to improve the
across-cohort prediction accuracy of neural networks. One distinct advantage of
our method is that it only requires minimal changes of the baseline model's
architecture so that it can be plugged into most of the existing neural
networks. We conduct experiments across CT-scan, MRA, and EEG brain wave with
convolutional neural networks and LSTM to verify the efficiency of our method.
</summary>
    <author>
      <name>Haohan Wang</name>
    </author>
    <author>
      <name>Zhenglin Wu</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07276v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07276v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10594v1</id>
    <updated>2018-08-31T04:17:44Z</updated>
    <published>2018-08-31T04:17:44Z</published>
    <title>Proximity Forest: An effective and scalable distance-based classifier
  for time series</title>
    <summary>  Research into the classification of time series has made enormous progress in
the last decade. The UCR time series archive has played a significant role in
challenging and guiding the development of new learners for time series
classification. The largest dataset in the UCR archive holds 10 thousand time
series only; which may explain why the primary research focus has been in
creating algorithms that have high accuracy on relatively small datasets.
  This paper introduces Proximity Forest, an algorithm that learns accurate
models from datasets with millions of time series, and classifies a time series
in milliseconds. The models are ensembles of highly randomized Proximity Trees.
Whereas conventional decision trees branch on attribute values (and usually
perform poorly on time series), Proximity Trees branch on the proximity of time
series to one exemplar time series or another; allowing us to leverage the
decades of work into developing relevant measures for time series. Proximity
Forest gains both efficiency and accuracy by stochastic selection of both
exemplars and similarity measures.
  Our work is motivated by recent time series applications that provide orders
of magnitude more time series than the UCR benchmarks. Our experiments
demonstrate that Proximity Forest is highly competitive on the UCR archive: it
ranks among the most accurate classifiers while being significantly faster. We
demonstrate on a 1M time series Earth observation dataset that Proximity Forest
retains this accuracy on datasets that are many orders of magnitude greater
than those in the UCR repository, while learning its models at least 100,000
times faster than current state of the art models Elastic Ensemble and COTE.
</summary>
    <author>
      <name>Benjamin Lucas</name>
    </author>
    <author>
      <name>Ahmed Shifaz</name>
    </author>
    <author>
      <name>Charlotte Pelletier</name>
    </author>
    <author>
      <name>Lachlan O'Neill</name>
    </author>
    <author>
      <name>Nayyar Zaidi</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <author>
      <name>Francois Petitjean</name>
    </author>
    <author>
      <name>Geoffrey I. Webb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10585v1</id>
    <updated>2018-08-31T03:18:00Z</updated>
    <published>2018-08-31T03:18:00Z</published>
    <title>On the Minimal Supervision for Training Any Binary Classifier from Only
  Unlabeled Data</title>
    <summary>  Empirical risk minimization (ERM), with proper loss function and
regularization, is the common practice of supervised classification. In this
paper, we study training arbitrary (from linear to deep) binary classifier from
only unlabeled (U) data by ERM but not by clustering in the geometric space. A
two-step ERM is considered: first an unbiased risk estimator is designed, and
then the empirical training risk is minimized. This approach is advantageous in
that we can also evaluate the empirical validation risk, which is indispensable
for hyperparameter tuning when some validation data is split from U training
data instead of labeled test data. We prove that designing such an estimator is
impossible given a single set of U data, but it becomes possible given two sets
of U data with different class priors. This answers a fundamental question in
weakly-supervised learning, namely what the minimal supervision is for training
any binary classifier from only U data. Since the proposed learning method is
based on unbiased risk estimates, the asymptotic consistency of the learned
classifier is certainly guaranteed. Experiments demonstrate that the proposed
method could successfully train deep models like ResNet and outperform
state-of-the-art methods for learning from two sets of U data.
</summary>
    <author>
      <name>Nan Lu</name>
    </author>
    <author>
      <name>Gang Niu</name>
    </author>
    <author>
      <name>Aditya K. Menon</name>
    </author>
    <author>
      <name>Masashi Sugiyama</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10556v1</id>
    <updated>2018-08-31T00:15:53Z</updated>
    <published>2018-08-31T00:15:53Z</published>
    <title>Speaker Fluency Level Classification Using Machine Learning Techniques</title>
    <summary>  Level assessment for foreign language students is necessary for putting them
in the right level group, furthermore, interviewing students is a very
time-consuming task, so we propose to automate the evaluation of speaker
fluency level by implementing machine learning techniques. This work presents
an audio processing system capable of classifying the level of fluency of
non-native English speakers using five different machine learning models. As a
first step, we have built our own dataset, which consists of labeled audio
conversations in English between people ranging in different fluency
domains/classes (low, intermediate, high). We segment the audio conversations
into 5s non-overlapped audio clips to perform feature extraction on them. We
start by extracting Mel cepstral coefficients from the audios, selecting 20
coefficients is an appropriate quantity for our data. We thereafter extracted
zero-crossing rate, root mean square energy and spectral flux features, proving
that this improves model performance. Out of a total of 1424 audio segments,
with 70% training data and 30% test data, one of our trained models (support
vector machine) achieved a classification accuracy of 94.39%, whereas the other
four models passed an 89% classification accuracy threshold.
</summary>
    <author>
      <name>Alan Preciado-Grijalva</name>
    </author>
    <author>
      <name>Ramon F. Brena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10552v1</id>
    <updated>2018-08-31T00:00:22Z</updated>
    <published>2018-08-31T00:00:22Z</published>
    <title>Directed Exploration in PAC Model-Free Reinforcement Learning</title>
    <summary>  We study an exploration method for model-free RL that generalizes the
counter-based exploration bonus methods and takes into account long term
exploratory value of actions rather than a single step look-ahead. We propose a
model-free RL method that modifies Delayed Q-learning and utilizes the
long-term exploration bonus with provable efficiency. We show that our proposed
method finds a near-optimal policy in polynomial time (PAC-MDP), and also
provide experimental evidence that our proposed algorithm is an efficient
exploration method.
</summary>
    <author>
      <name>Min-hwan Oh</name>
    </author>
    <author>
      <name>Garud Iyengar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10551v1</id>
    <updated>2018-08-30T23:57:19Z</updated>
    <published>2018-08-30T23:57:19Z</published>
    <title>Dynamic mode decomposition in vector-valued reproducing kernel Hilbert
  spaces for extracting dynamical structure among observables</title>
    <summary>  Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety
of engineering and scientific fields. Dynamic mode decomposition (DMD), which
is a numerical algorithm for the spectral analysis of Koopman operators, has
been attracting attention as a way of obtaining global modal descriptions of
NLDSs without requiring explicit prior knowledge. However, since existing DMD
algorithms are in principle formulated based on the concatenation of scalar
observables, it is not directly applicable to data with dependent structures
among observables, which take, for example, the form of a sequence of graphs.
In this paper, we formulate Koopman spectral analysis for NLDSs with structures
among observables and propose an estimation algorithm for this problem. This
method can extract and visualize the underlying low-dimensional global dynamics
of NLDSs with structures among observables from data, which can be useful in
understanding the underlying dynamics of such NLDSs. To this end, we first
formulate the problem of estimating spectra of the Koopman operator defined in
vector-valued reproducing kernel Hilbert spaces, and then develop an estimation
procedure for this problem by reformulating tensor-based DMD. As a special case
of our method, we propose the method named as Graph DMD, which is a numerical
algorithm for Koopman spectral analysis of graph dynamical systems, using a
sequence of adjacency matrices. We investigate the empirical performance of our
method by using synthetic and real-world data.
</summary>
    <author>
      <name>Keisuke Fujii</name>
    </author>
    <author>
      <name>Yoshinobu Kawahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages with 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10549v1</id>
    <updated>2018-08-30T23:48:33Z</updated>
    <published>2018-08-30T23:48:33Z</published>
    <title>Fair Algorithms for Learning in Allocation Problems</title>
    <summary>  Settings such as lending and policing can be modeled by a centralized agent
allocating a resource (loans or police officers) amongst several groups, in
order to maximize some objective (loans given that are repaid or criminals that
are apprehended). Often in such problems fairness is also a concern. A natural
notion of fairness, based on general principles of equality of opportunity,
asks that conditional on an individual being a candidate for the resource, the
probability of actually receiving it is approximately independent of the
individual's group. In lending this means that equally creditworthy individuals
in different racial groups have roughly equal chances of receiving a loan. In
policing it means that two individuals committing the same crime in different
districts would have roughly equal chances of being arrested.
  We formalize this fairness notion for allocation problems and investigate its
algorithmic consequences. Our main technical results include an efficient
learning algorithm that converges to an optimal fair allocation even when the
frequency of candidates (creditworthy individuals or criminals) in each group
is unknown. The algorithm operates in a censored feedback model in which only
the number of candidates who received the resource in a given allocation can be
observed, rather than the true number of candidates. This models the fact that
we do not learn the creditworthiness of individuals we do not give loans to nor
learn about crimes committed if the police presence in a district is low.
  As an application of our framework, we consider the predictive policing
problem. The learning algorithm is trained on arrest data gathered from its own
deployments on previous days, resulting in a potential feedback loop that our
algorithm provably overcomes. We empirically investigate the performance of our
algorithm on the Philadelphia Crime Incidents dataset.
</summary>
    <author>
      <name>Hadi Elzayn</name>
    </author>
    <author>
      <name>Shahin Jabbari</name>
    </author>
    <author>
      <name>Christopher Jung</name>
    </author>
    <author>
      <name>Michael Kearns</name>
    </author>
    <author>
      <name>Seth Neel</name>
    </author>
    <author>
      <name>Aaron Roth</name>
    </author>
    <author>
      <name>Zachary Schutzman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10543v1</id>
    <updated>2018-08-30T22:56:46Z</updated>
    <published>2018-08-30T22:56:46Z</published>
    <title>A Self-Attention Network for Hierarchical Data Structures with an
  Application to Claims Management</title>
    <summary>  Insurance companies must manage millions of claims per year. While most of
these claims are non-fraudulent, fraud detection is core for insurance
companies. The ultimate goal is a predictive model to single out the fraudulent
claims and pay out the non-fraudulent ones immediately. Modern machine learning
methods are well suited for this kind of problem. Health care claims often have
a data structure that is hierarchical and of variable length. We propose one
model based on piecewise feed forward neural networks (deep learning) and
another model based on self-attention neural networks for the task of claim
management. We show that the proposed methods outperform bag-of-words based
models, hand designed features, and models based on convolutional neural
networks, on a data set of two million health care claims. The proposed
self-attention method performs the best.
</summary>
    <author>
      <name>Leander Löw</name>
    </author>
    <author>
      <name>Martin Spindler</name>
    </author>
    <author>
      <name>Eike Brechmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00999v1</id>
    <updated>2018-08-30T22:34:29Z</updated>
    <published>2018-08-30T22:34:29Z</published>
    <title>Towards Large Scale Training Of Autoencoders For Collaborative Filtering</title>
    <summary>  In this paper, we apply a mini-batch based negative sampling method to
efficiently train a latent factor autoencoder model on large scale and sparse
data for implicit feedback collaborative filtering. We compare our work against
a state-of-the-art baseline model on different experimental datasets and show
that this method can lead to a good and fast approximation of the baseline
model performance.
</summary>
    <author>
      <name>Abdallah Moussawi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, ACM RecSys 2018 Posters</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05006v3</id>
    <updated>2018-08-30T21:56:28Z</updated>
    <published>2017-09-14T23:06:19Z</published>
    <title>Two-sample Statistics Based on Anisotropic Kernels</title>
    <summary>  The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)
statistic for measuring the distance between two distributions given
finitely-many multivariate samples. When the distributions are locally
low-dimensional, the proposed test can be made more powerful to distinguish
certain alternatives by incorporating local covariance matrices and
constructing an anisotropic kernel. The kernel matrix is asymmetric; it
computes the affinity between $n$ data points and a set of $n_R$ reference
points, where $n_R$ can be drastically smaller than $n$. While the proposed
statistic can be viewed as a special class of Reproducing Kernel Hilbert Space
MMD, the consistency of the test is proved, under mild assumptions of the
kernel, as long as $\|p-q\| \sqrt{n} \to \infty $, and a finite-sample lower
bound of the testing power is obtained. Applications to flow cytometry and
diffusion MRI datasets are demonstrated, which motivate the proposed approach
to compare distributions.
</summary>
    <author>
      <name>Xiuyuan Cheng</name>
    </author>
    <author>
      <name>Alexander Cloninger</name>
    </author>
    <author>
      <name>Ronald R. Coifman</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05006v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05006v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10532v1</id>
    <updated>2018-08-30T21:53:06Z</updated>
    <published>2018-08-30T21:53:06Z</published>
    <title>Uniform Inference in High-Dimensional Gaussian Graphical Models</title>
    <summary>  Graphical models have become a very popular tool for representing
dependencies within a large set of variables and are key for representing
causal structures. We provide results for uniform inference on high-dimensional
graphical models with the number of target parameters being possible much
larger than sample size. This is in particular important when certain features
or structures of a causal model should be recovered. Our results highlight how
in high-dimensional settings graphical models can be estimated and recovered
with modern machine learning methods in complex data sets. We also demonstrate
in simulation study that our procedure has good small sample properties.
</summary>
    <author>
      <name>Sven Klaassen</name>
    </author>
    <author>
      <name>Jannis Kück</name>
    </author>
    <author>
      <name>Martin Spindler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 2 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H15, 62J07," scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07384v2</id>
    <updated>2018-08-30T21:33:26Z</updated>
    <published>2018-02-21T00:47:32Z</published>
    <title>Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic
  Corrections</title>
    <summary>  We present a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU activations
to change its output. We argue that such a correction is a useful way to
provide feedback to a user when the network's output is different from a
desired output. Our algorithm generates such a correction by solving a series
of linear constraint satisfaction problems. The technique is evaluated on three
neural network models: one predicting whether an applicant will pay a mortgage,
one predicting whether a first-order theorem can be proved efficiently by a
solver using certain heuristics, and the final one judging whether a drawing is
an accurate rendition of a canonical drawing of a cat.
</summary>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Armando Solar-Lezama</name>
    </author>
    <author>
      <name>Rishabh Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07384v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07384v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07450v2</id>
    <updated>2018-08-30T20:27:20Z</updated>
    <published>2017-06-22T18:18:58Z</published>
    <title>Revised Note on Learning Algorithms for Quadratic Assignment with Graph
  Neural Networks</title>
    <summary>  Inverse problems correspond to a certain type of optimization problems
formulated over appropriate input distributions. Recently, there has been a
growing interest in understanding the computational hardness of these
optimization problems, not only in the worst case, but in an average-complexity
sense under this same input distribution.
  In this revised note, we are interested in studying another aspect of
hardness, related to the ability to learn how to solve a problem by simply
observing a collection of previously solved instances. These 'planted
solutions' are used to supervise the training of an appropriate predictive
model that parametrizes a broad class of algorithms, with the hope that the
resulting model will provide good accuracy-complexity tradeoffs in the average
sense.
  We illustrate this setup on the Quadratic Assignment Problem, a fundamental
problem in Network Science. We observe that data-driven models based on Graph
Neural Networks offer intriguingly good performance, even in regimes where
standard relaxation based techniques appear to suffer.
</summary>
    <author>
      <name>Alex Nowak</name>
    </author>
    <author>
      <name>Soledad Villar</name>
    </author>
    <author>
      <name>Afonso S. Bandeira</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised note to arXiv:1706.07450v1 that appeared in IEEE Data Science
  Workshop 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07450v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07450v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09794v2</id>
    <updated>2018-08-30T19:40:56Z</updated>
    <published>2018-08-29T13:25:11Z</published>
    <title>Correlated Time Series Forecasting using Deep Neural Networks: A Summary
  of Results</title>
    <summary>  Cyber-physical systems often consist of entities that interact with each
other over time. Meanwhile, as part of the continued digitization of industrial
processes, various sensor technologies are deployed that enable us to record
time-varying attributes (a.k.a., time series) of such entities, thus producing
correlated time series. To enable accurate forecasting on such correlated time
series, this paper proposes two models that combine convolutional neural
networks (CNNs) and recurrent neural networks (RNNs). The first model employs a
CNN on each individual time series, combines the convoluted features, and then
applies an RNN on top of the convoluted features in the end to enable
forecasting. The second model adds additional auto-encoders into the individual
CNNs, making the second model a multi-task learning model, which provides
accurate and robust forecasting. Experiments on two real-world correlated time
series data set suggest that the proposed two models are effective and
outperform baselines in most settings.
  This report extends the paper "Correlated Time Series Forecasting using
Multi-Task Deep Neural Networks," to appear in ACM CIKM 2018, by providing
additional experimental results.
</summary>
    <author>
      <name>Razvan-Gabriel Cirstea</name>
    </author>
    <author>
      <name>Darius-Valer Micu</name>
    </author>
    <author>
      <name>Gabriel-Marcel Muresan</name>
    </author>
    <author>
      <name>Chenjuan Guo</name>
    </author>
    <author>
      <name>Bin Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09794v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09794v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10430v1</id>
    <updated>2018-08-30T17:48:23Z</updated>
    <published>2018-08-30T17:48:23Z</published>
    <title>Nested multi-instance classification</title>
    <summary>  There are classification tasks that take as inputs groups of images rather
than single images. In order to address such situations, we introduce a nested
multi-instance deep network. The approach is generic in that it is applicable
to general data instances, not just images. The network has several
convolutional neural networks grouped together at different stages. This
primarily differs from other previous works in that we organize instances into
relevant groups that are treated differently. We also introduce a method to
replace instances that are missing which successfully creates neutral input
instances and consistently outperforms standard fill-in methods in real world
use cases. In addition, we propose a method for manual dropout when a whole
group of instances is missing that allows us to use richer training data and
obtain higher accuracy at the end of training. With specific pretraining, we
find that the model works to great effect on our real world and pub-lic
datasets in comparison to baseline methods, justifying the different treatment
among groups of instances.
</summary>
    <author>
      <name>Alexander Stec</name>
    </author>
    <author>
      <name>Diego Klabjan</name>
    </author>
    <author>
      <name>Jean Utke</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05163v3</id>
    <updated>2018-08-30T17:48:04Z</updated>
    <published>2018-08-15T16:04:28Z</published>
    <title>A Simple but Hard-to-Beat Baseline for Session-based Recommendations</title>
    <summary>  Convolutional Neural Networks (CNNs) models have been recently introduced in
the domain of top-$N$ session-based recommendations. An ordered collection of
past items the user has interacted with in a session (or sequence) are embedded
into a 2-dimensional latent matrix, and treated as an image. The convolution
and pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we propose
a simple, but very effective generative model that is capable of learning
high-level representation from both short- and long-range dependencies. The
network architecture of the proposed model is formed of a stack of holed
convolutional layers, which can efficiently increase the receptive fields
without relying on the pooling operation. Another contribution is the effective
use of residual block structure in recommender systems, which can ease the
optimization for much deeper networks. The proposed generative model attains
state-of-the-art accuracy with less training time in the session-based
recommendation task. It accordingly can be used as a powerful session-based
recommendation baseline to beat in future, especially when there are long
sequences of user feedback.
</summary>
    <author>
      <name>Fajie Yuan</name>
    </author>
    <author>
      <name>Alexandros Karatzoglou</name>
    </author>
    <author>
      <name>Ioannis Arapakis</name>
    </author>
    <author>
      <name>Joemon M Jose</name>
    </author>
    <author>
      <name>Xiangnan He</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05163v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05163v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10406v1</id>
    <updated>2018-08-30T17:25:48Z</updated>
    <published>2018-08-30T17:25:48Z</published>
    <title>Towards Reproducible Empirical Research in Meta-Learning</title>
    <summary>  Meta-learning is increasingly used to support the recommendation of machine
learning algorithms and their configurations. Such recommendations are made
based on meta-data, consisting of performance evaluations of algorithms on
prior datasets, as well as characterizations of these datasets. These
characterizations, also called meta-features, describe properties of the data
which are predictive for the performance of machine learning algorithms trained
on them. Unfortunately, despite being used in a large number of studies,
meta-features are not uniformly described and computed, making many empirical
studies irreproducible and hard to compare. This paper aims to remedy this by
systematizing and standardizing data characterization measures used in
meta-learning, and performing an in-depth analysis of their utility. Moreover,
it presents MFE, a new tool for extracting meta-features from datasets and
identify more subtle reproducibility issues in the literature, proposing
guidelines for data characterization that strengthen reproducible empirical
research in meta-learning.
</summary>
    <author>
      <name>Adriano Rivolli</name>
    </author>
    <author>
      <name>Luís P. F. Garcia</name>
    </author>
    <author>
      <name>Carlos Soares</name>
    </author>
    <author>
      <name>Joaquin Vanschoren</name>
    </author>
    <author>
      <name>André C. P. L. F. de Carvalho</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10396v1</id>
    <updated>2018-08-30T17:00:03Z</updated>
    <published>2018-08-30T17:00:03Z</published>
    <title>A Unified Analysis of Stochastic Momentum Methods for Deep Learning</title>
    <summary>  Stochastic momentum methods have been widely adopted in training deep neural
networks. However, their theoretical analysis of convergence of the training
objective and the generalization error for prediction is still under-explored.
This paper aims to bridge the gap between practice and theory by analyzing the
stochastic gradient (SG) method, and the stochastic momentum methods including
two famous variants, i.e., the stochastic heavy-ball (SHB) method and the
stochastic variant of Nesterov's accelerated gradient (SNAG) method. We propose
a framework that unifies the three variants. We then derive the convergence
rates of the norm of gradient for the non-convex optimization problem, and
analyze the generalization performance through the uniform stability approach.
Particularly, the convergence analysis of the training objective exhibits that
SHB and SNAG have no advantage over SG. However, the stability analysis shows
that the momentum term can improve the stability of the learned model and hence
improve the generalization performance. These theoretical insights verify the
common wisdom and are also corroborated by our empirical analysis on deep
learning.
</summary>
    <author>
      <name>Yan Yan</name>
    </author>
    <author>
      <name>Tianbao Yang</name>
    </author>
    <author>
      <name>Zhe Li</name>
    </author>
    <author>
      <name>Qihang Lin</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Previous Technical Report: arXiv:1604.03257</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In IJCAI, pp. 2955-2961. 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.10396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10393v1</id>
    <updated>2018-08-30T16:46:22Z</updated>
    <published>2018-08-30T16:46:22Z</published>
    <title>Learning End-to-end Autonomous Driving using Guided Auxiliary
  Supervision</title>
    <summary>  Learning to drive faithfully in highly stochastic urban settings remains an
open problem. To that end, we propose a Multi-task Learning from Demonstration
(MT-LfD) framework which uses supervised auxiliary task prediction to guide the
main task of predicting the driving commands. Our framework involves an
end-to-end trainable network for imitating the expert demonstrator's driving
commands. The network intermediately predicts visual affordances and action
primitives through direct supervision which provide the aforementioned
auxiliary supervised guidance. We demonstrate that such joint learning and
supervised guidance facilitates hierarchical task decomposition, assisting the
agent to learn faster, achieve better driving performance and increases
transparency of the otherwise black-box end-to-end network. We run our
experiments to validate the MT-LfD framework in CARLA, an open-source urban
driving simulator. We introduce multiple non-player agents in CARLA and induce
temporal noise in them for realistic stochasticity.
</summary>
    <author>
      <name>Ashish Mehta</name>
    </author>
    <author>
      <name>Adithya Subramanian</name>
    </author>
    <author>
      <name>Anbumani Subramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06827v2</id>
    <updated>2018-08-30T16:44:24Z</updated>
    <published>2018-06-18T17:04:04Z</published>
    <title>PAC-Bayes bounds for stable algorithms with instance-dependent priors</title>
    <summary>  PAC-Bayes bounds have been proposed to get risk estimates based on a training
sample. In this paper the PAC-Bayes approach is combined with stability of the
hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting
is used with a Gaussian prior centered at the expected output. Thus a novelty
of our paper is using priors defined in terms of the data-generating
distribution. Our main result estimates the risk of the randomized algorithm in
terms of the hypothesis stability coefficients. We also provide a new bound for
the SVM classifier, which is compared to other known bounds experimentally.
Ours appears to be the first stability-based bound that evaluates to
non-trivial values.
</summary>
    <author>
      <name>Omar Rivasplata</name>
    </author>
    <author>
      <name>Emilio Parrado-Hernandez</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <author>
      <name>Shiliang Sun</name>
    </author>
    <author>
      <name>Csaba Szepesvari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, discussion of theory and experiments in the main body,
  detailed proofs and experimental details in the appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06827v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06827v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00977v1</id>
    <updated>2018-08-30T16:41:58Z</updated>
    <published>2018-08-30T16:41:58Z</published>
    <title>DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal
  Convolutional Autoencoders</title>
    <summary>  Human falls rarely occur; however, detecting falls is very important from the
health and safety perspective. Due to the rarity of falls, it is difficult to
employ supervised classification techniques to detect them. Moreover, in these
highly skewed situations it is also difficult to extract domain specific
features to identify falls. In this paper, we present a novel framework,
\textit{DeepFall}, which formulates the fall detection problem as an anomaly
detection problem. The \textit{DeepFall} framework presents the novel use of
deep spatio-temporal convolutional autoencoders to learn spatial and temporal
features from normal activities using non-invasive sensing modalities. We also
present a new anomaly scoring method that combines the reconstruction score of
frames across a video sequences to detect unseen falls. We tested the
\textit{DeepFall} framework on three publicly available datasets collected
through non-invasive sensing modalities, thermal camera and depth cameras and
show superior results in comparison to traditional autoencoder and
convolutional autoencoder methods to identify unseen falls.
</summary>
    <author>
      <name>Jacob Nogas</name>
    </author>
    <author>
      <name>Shehroz S. Khan</name>
    </author>
    <author>
      <name>Alex Mihailidis</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08294v3</id>
    <updated>2018-08-30T16:29:50Z</updated>
    <published>2017-09-25T02:29:26Z</published>
    <title>Learning Context-Sensitive Convolutional Filters for Text Processing</title>
    <summary>  Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.
</summary>
    <author>
      <name>Dinghan Shen</name>
    </author>
    <author>
      <name>Martin Renqiang Min</name>
    </author>
    <author>
      <name>Yitong Li</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP 2018 as a full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.08294v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08294v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10356v1</id>
    <updated>2018-08-30T15:32:40Z</updated>
    <published>2018-08-30T15:32:40Z</published>
    <title>Gaussian Mixture Generative Adversarial Networks for Diverse Datasets,
  and the Unsupervised Clustering of Images</title>
    <summary>  Generative Adversarial Networks (GANs) have been shown to produce
realistically looking synthetic images with remarkable success, yet their
performance seems less impressive when the training set is highly diverse. In
order to provide a better fit to the target data distribution when the dataset
includes many different classes, we propose a variant of the basic GAN model,
called Gaussian Mixture GAN (GM-GAN), where the probability distribution over
the latent space is a mixture of Gaussians. We also propose a supervised
variant which is capable of conditional sample synthesis. In order to evaluate
the model's performance, we propose a new scoring method which separately takes
into account two (typically conflicting) measures - diversity vs. quality of
the generated data. Through a series of empirical experiments, using both
synthetic and real-world datasets, we quantitatively show that GM-GANs
outperform baselines, both when evaluated using the commonly used Inception
Score, and when evaluated using our own alternative scoring method. In
addition, we qualitatively demonstrate how the \textit{unsupervised} variant of
GM-GAN tends to map latent vectors sampled from different Gaussians in the
latent space to samples of different classes in the data space. We show how
this phenomenon can be exploited for the task of unsupervised clustering, and
provide quantitative evaluation showing the superiority of our method for the
unsupervised clustering of image datasets. Finally, we demonstrate a feature
which further sets our model apart from other GAN models: the option to control
the quality-diversity trade-off by altering, post-training, the probability
distribution of the latent space. This allows one to sample higher quality and
lower diversity samples, or vice versa, according to one's needs.
</summary>
    <author>
      <name>Matan Ben-Yosef</name>
    </author>
    <author>
      <name>Daphna Weinshall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10350v1</id>
    <updated>2018-08-30T15:24:20Z</updated>
    <published>2018-08-30T15:24:20Z</published>
    <title>IEA: Inner Ensemble Average within a convolutional neural network</title>
    <summary>  Ensemble learning is a method of combining multiple trained models to improve
the model accuracy. We introduce the usage of such methods, specifically
ensemble average inside Convolutional Neural Networks (CNNs) architectures. By
Inner Average Ensemble (IEA) of multiple convolutional neural layers (CNLs)
replacing the single CNLs inside the CNN architecture, the accuracy of the CNN
increased. A visual and a similarity score analysis of the features generated
from IEA explains why it boosts the model performance. Empirical results using
different benchmarking datasets and well-known deep model architectures shows
that IEA outperforms the ordinary CNL used in CNNs.
</summary>
    <author>
      <name>Abduallah A. Mohamed</name>
    </author>
    <author>
      <name>Christian Claudel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10340v1</id>
    <updated>2018-08-30T15:06:03Z</updated>
    <published>2018-08-30T15:06:03Z</published>
    <title>A Coordinate-Free Construction of Scalable Natural Gradient</title>
    <summary>  Most neural networks are trained using first-order optimization methods,
which are sensitive to the parameterization of the model. Natural gradient
descent is invariant to smooth reparameterizations because it is defined in a
coordinate-free way, but tractable approximations are typically defined in
terms of coordinate systems, and hence may lose the invariance properties. We
analyze the invariance properties of the Kronecker-Factored Approximate
Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free
way. We explicitly construct a Riemannian metric under which the natural
gradient matches the K-FAC update; invariance to affine transformations of the
activations follows immediately. We extend our framework to analyze the
invariance properties of K-FAC applied to convolutional networks and recurrent
neural networks, as well as metrics other than the usual Fisher metric.
</summary>
    <author>
      <name>Kevin Luk</name>
    </author>
    <author>
      <name>Roger Grosse</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10307v1</id>
    <updated>2018-08-30T14:13:39Z</updated>
    <published>2018-08-30T14:13:39Z</published>
    <title>Backdoor Embedding in Convolutional Neural Network Models via Invisible
  Perturbation</title>
    <summary>  Deep learning models have consistently outperformed traditional machine
learning models in various classification tasks, including image
classification. As such, they have become increasingly prevalent in many real
world applications including those where security is of great concern. Such
popularity, however, may attract attackers to exploit the vulnerabilities of
the deployed deep learning models and launch attacks against security-sensitive
applications. In this paper, we focus on a specific type of data poisoning
attack, which we refer to as a {\em backdoor injection attack}. The main goal
of the adversary performing such attack is to generate and inject a backdoor
into a deep learning model that can be triggered to recognize certain embedded
patterns with a target label of the attacker's choice. Additionally, a backdoor
injection attack should occur in a stealthy manner, without undermining the
efficacy of the victim model. Specifically, we propose two approaches for
generating a backdoor that is hardly perceptible yet effective in poisoning the
model. We consider two attack settings, with backdoor injection carried out
either before model training or during model updating. We carry out extensive
experimental evaluations under various assumptions on the adversary model, and
demonstrate that such attacks can be effective and achieve a high attack
success rate (above $90\%$) at a small cost of model accuracy loss (below
$1\%$) with a small injection rate (around $1\%$), even under the weakest
assumption wherein the adversary has no knowledge either of the original
training data or the classifier model.
</summary>
    <author>
      <name>Cong Liao</name>
    </author>
    <author>
      <name>Haoti Zhong</name>
    </author>
    <author>
      <name>Anna Squicciarini</name>
    </author>
    <author>
      <name>Sencun Zhu</name>
    </author>
    <author>
      <name>David Miller</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01498v1</id>
    <updated>2018-08-30T13:54:45Z</updated>
    <published>2018-08-30T13:54:45Z</published>
    <title>Skip-gram word embeddings in hyperbolic space</title>
    <summary>  Embeddings of tree-like graphs in hyperbolic space were recently shown to
surpass their Euclidean counterparts in performance by a large margin. Inspired
by these results, we present an algorithm for learning word embeddings in
hyperbolic space from free text. An objective function based on the hyperbolic
distance is derived and included in the skip-gram architecture from word2vec.
The hyperbolic word embeddings are then evaluated on word similarity and
analogy benchmarks. The results demonstrate the potential of hyperbolic word
embeddings, particularly in low dimensions, though without clear superiority
over their Euclidean counterparts. We further discuss problems in the
formulation of the analogy task resulting from the curvature of hyperbolic
space.
</summary>
    <author>
      <name>Matthias Leimeister</name>
    </author>
    <author>
      <name>Benjamin J. Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01118v3</id>
    <updated>2018-08-30T13:13:49Z</updated>
    <published>2016-08-03T09:01:07Z</published>
    <title>A supermartingale approach to Gaussian process based sequential design
  of experiments</title>
    <summary>  Gaussian process (GP) models have become a well-established frameworkfor the
adaptive design of costly experiments, and notably of computerexperiments.
GP-based sequential designs have been found practicallyefficient for various
objectives, such as global optimization(estimating the global maximum or
maximizer(s) of a function),reliability analysis (estimating a probability of
failure) or theestimation of level sets and excursion sets. In this paper, we
studythe consistency of an important class of sequential designs, known
asstepwise uncertainty reduction (SUR) strategies. Our approach relieson the
key observation that the sequence of residual uncertaintymeasures, in SUR
strategies, is generally a supermartingale withrespect to the filtration
generated by the observations. Thisobservation enables us to establish generic
consistency results for abroad class of SUR strategies. The consistency of
several popularsequential design strategies is then obtained by means of this
generalresult. Notably, we establish the consistency of two SUR
strategiesproposed by Bect, Ginsbourger, Li, Picheny and Vazquez (Stat.
Comp.,2012)---to the best of our knowledge, these are the first proofs
ofconsistency for GP-based sequential design algorithms dedicated to
theestimation of excursion sets and their measure. We also establish anew, more
general proof of consistency for the expected improvementalgorithm for global
optimization which, unlike previous results inthe literature, applies to any GP
with continuous sample paths.
</summary>
    <author>
      <name>Julien Bect</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">L2S, GdR MASCOT-NUM</arxiv:affiliation>
    </author>
    <author>
      <name>François Bachoc</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT</arxiv:affiliation>
    </author>
    <author>
      <name>David Ginsbourger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMSV</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1608.01118v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01118v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09526v2</id>
    <updated>2018-08-30T13:03:16Z</updated>
    <published>2018-08-28T20:27:16Z</published>
    <title>Deep Lidar CNN to Understand the Dynamics of Moving Vehicles</title>
    <summary>  Perception technologies in Autonomous Driving are experiencing their golden
age due to the advances in Deep Learning. Yet, most of these systems rely on
the semantically rich information of RGB images. Deep Learning solutions
applied to the data of other sensors typically mounted on autonomous cars (e.g.
lidars or radars) are not explored much. In this paper we propose a novel
solution to understand the dynamics of moving vehicles of the scene from only
lidar information. The main challenge of this problem stems from the fact that
we need to disambiguate the proprio-motion of the 'observer' vehicle from that
of the external 'observed' vehicles. For this purpose, we devise a CNN
architecture which at testing time is fed with pairs of consecutive lidar
scans. However, in order to properly learn the parameters of this network,
during training we introduce a series of so-called pretext tasks which also
leverage on image data. These tasks include semantic information about
vehicleness and a novel lidar-flow feature which combines standard image-based
optical flow with lidar scans. We obtain very promising results and show that
including distilled image information only during training, allows improving
the inference results of the network at test time, even when image data is no
longer used.
</summary>
    <author>
      <name>Victor Vaquero</name>
    </author>
    <author>
      <name>Alberto Sanfeliu</name>
    </author>
    <author>
      <name>Francesc Moreno-Noguer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in IEEE ICRA 2018. IEEE Copyrights: Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses. (V2 just corrected comments on arxiv submission)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09526v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09526v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01000v1</id>
    <updated>2018-08-30T12:36:36Z</updated>
    <published>2018-08-30T12:36:36Z</published>
    <title>Bayesian Outdoor Defect Detection</title>
    <summary>  We introduce a Bayesian defect detector to facilitate the defect detection on
the motion blurred images on rough texture surfaces. To enhance the accuracy of
Bayesian detection on removing non-defect pixels, we develop a class of
reflected non-local prior distributions, which is constructed by using the mode
of a distribution to subtract its density. The reflected non-local priors
forces the Bayesian detector to approach 0 at the non-defect locations. We
conduct experiments studies to demonstrate the superior performance of the
Bayesian detector in eliminating the non-defect points. We implement the
Bayesian detector in the motion blurred drone images, in which the detector
successfully identifies the hail damages on the rough surface and substantially
enhances the accuracy of the entire defect detection pipeline.
</summary>
    <author>
      <name>Fei Jiang</name>
    </author>
    <author>
      <name>Guosheng Yin</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06791v2</id>
    <updated>2018-08-30T12:33:22Z</updated>
    <published>2018-08-21T07:45:10Z</published>
    <title>LRMM: Learning to Recommend with Missing Modalities</title>
    <summary>  Multimodal learning has shown promising performance in content-based
recommendation due to the auxiliary user and item information of multiple
modalities such as text and images. However, the problem of incomplete and
missing modality is rarely explored and most existing methods fail in learning
a recommendation model with missing or corrupted modalities. In this paper, we
propose LRMM, a novel framework that mitigates not only the problem of missing
modalities but also more generally the cold-start problem of recommender
systems. We propose modality dropout (m-drop) and a multimodal sequential
autoencoder (m-auto) to learn multimodal representations for complementing and
imputing missing modalities. Extensive experiments on real-world Amazon data
show that LRMM achieves state-of-the-art performance on rating prediction
tasks. More importantly, LRMM is more robust to previous methods in alleviating
data-sparsity and the cold-start problem.
</summary>
    <author>
      <name>Cheng Wang</name>
    </author>
    <author>
      <name>Mathias Niepert</name>
    </author>
    <author>
      <name>Hui Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06791v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06791v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00947v1</id>
    <updated>2018-08-30T11:30:19Z</updated>
    <published>2018-08-30T11:30:19Z</published>
    <title>Finding Dory in the Crowd: Detecting Social Interactions using
  Multi-Modal Mobile Sensing</title>
    <summary>  Remembering our day-to-day social interactions is challenging even if you
aren't a blue memory challenged fish. The ability to automatically detect and
remember these types of interactions is not only beneficial for individuals
interested in their behavior in crowded situations, but also of interest to
those who analyze crowd behavior. Currently, detecting social interactions is
often performed using a variety of methods including ethnographic studies,
computer vision techniques and manual annotation-based data analysis. However,
mobile phones offer easier means for data collection that is easy to analyze
and can preserve the user's privacy. In this work, we present a system for
detecting stationary social interactions inside crowds, leveraging multi-modal
mobile sensing data such as Bluetooth Smart (BLE), accelerometer and gyroscope.
To inform the development of such system, we conducted a study with 24
participants, where we asked them to socialize with each other for 45 minutes.
We built a machine learning system based on gradient-boosted trees that
predicts both 1:1 and group interactions with 77.8% precision and 86.5% recall,
a 30.2% performance increase compared to a proximity-based approach. By
utilizing a community detection based method, we further detected the various
group formation that exist within the crowd. Using mobile phone sensors already
carried by the majority of people in a crowd makes our approach particularly
well suited to real-life analysis of crowd behaviour and influence strategies.
</summary>
    <author>
      <name>Kleomenis Katevas</name>
    </author>
    <author>
      <name>Katrin Hänsel</name>
    </author>
    <author>
      <name>Richard Clegg</name>
    </author>
    <author>
      <name>Ilias Leontiadis</name>
    </author>
    <author>
      <name>Hamed Haddadi</name>
    </author>
    <author>
      <name>Laurissa Tokarchuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10442v1</id>
    <updated>2018-08-30T11:26:59Z</updated>
    <published>2018-08-30T11:26:59Z</published>
    <title>Application of Self-Play Reinforcement Learning to a Four-Player Game of
  Imperfect Information</title>
    <summary>  We introduce a new virtual environment for simulating a card game known as
"Big 2". This is a four-player game of imperfect information with a relatively
complicated action space (being allowed to play 1,2,3,4 or 5 card combinations
from an initial starting hand of 13 cards). As such it poses a challenge for
many current reinforcement learning methods. We then use the recently proposed
"Proximal Policy Optimization" algorithm to train a deep neural network to play
the game, purely learning via self-play, and find that it is able to reach a
level which outperforms amateur human players after only a relatively short
amount of training time and without needing to search a tree of future game
states.
</summary>
    <author>
      <name>Henry Charlesworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages + 7 pages SI, 5 figures in total</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09902v2</id>
    <updated>2018-08-30T07:51:57Z</updated>
    <published>2018-08-29T16:07:26Z</published>
    <title>Extreme Value Theory for Open Set Classification - GPD and GEV
  Classifiers</title>
    <summary>  Classification tasks usually assume that all possible classes are present
during the training phase. This is restrictive if the algorithm is used over a
long time and possibly encounters samples from unknown classes. The recently
introduced extreme value machine, a classifier motivated by extreme value
theory, addresses this problem and achieves competitive performance in specific
cases. We show that this algorithm can fail when the geometries of known and
unknown classes differ. To overcome this problem, we propose two new algorithms
relying on approximations from extreme value theory. We show the effectiveness
of our classifiers in simulations and on the LETTER and MNIST data sets.
</summary>
    <author>
      <name>Edoardo Vignotto</name>
    </author>
    <author>
      <name>Sebastian Engelke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10238v2</id>
    <updated>2018-08-30T02:04:16Z</updated>
    <published>2018-02-28T02:39:02Z</published>
    <title>DeepSOFA: A Continuous Acuity Score for Critically Ill Patients using
  Clinically Interpretable Deep Learning</title>
    <summary>  Traditional methods for assessing illness severity and predicting in-hospital
mortality among critically ill patients require time-consuming, error-prone
calculations using static variable thresholds. These methods do not capitalize
on the emerging availability of streaming electronic health record data or
capture time-sensitive individual physiological patterns, a critical task in
the intensive care unit. We propose a novel acuity score framework (DeepSOFA)
that leverages temporal measurements and interpretable deep learning models to
assess illness severity at any point during an ICU stay. We compare DeepSOFA
with SOFA (Sequential Organ Failure Assessment) baseline models using the same
model inputs and find that at any point during an ICU admission, DeepSOFA
yields significantly more accurate predictions of in-hospital mortality. A
DeepSOFA model developed in a public database and validated in a single
institutional cohort had a mean AUC for the entire ICU stay of 0.90 (95% CI
0.90-0.91) compared with baseline SOFA models with mean AUC 0.79 (95% CI
0.79-0.80) and 0.85 (95% CI 0.85-0.86). Deep models are well-suited to identify
ICU patients in need of life-saving interventions prior to the occurrence of an
unexpected adverse event and inform shared decision-making processes among
patients, providers, and families regarding goals of care and optimal resource
utilization.
</summary>
    <author>
      <name>Benjamin Shickel</name>
    </author>
    <author>
      <name>Tyler J. Loftus</name>
    </author>
    <author>
      <name>Lasith Adhikari</name>
    </author>
    <author>
      <name>Tezcan Ozrazgat-Baslanti</name>
    </author>
    <author>
      <name>Azra Bihorac</name>
    </author>
    <author>
      <name>Parisa Rashidi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Currently under review with Scientific Reports</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10238v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10238v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10078v1</id>
    <updated>2018-08-30T01:05:40Z</updated>
    <published>2018-08-30T01:05:40Z</published>
    <title>Discriminative Learning of Similarity and Group Equivariant
  Representations</title>
    <summary>  One of the most fundamental problems in machine learning is to compare
examples: Given a pair of objects we want to return a value which indicates
degree of (dis)similarity. Similarity is often task specific, and pre-defined
distances can perform poorly, leading to work in metric learning. However,
being able to learn a similarity-sensitive distance function also presupposes
access to a rich, discriminative representation for the objects at hand. In
this dissertation we present contributions towards both ends. In the first part
of the thesis, assuming good representations for the data, we present a
formulation for metric learning that makes a more direct attempt to optimize
for the k-NN accuracy as compared to prior work. We also present extensions of
this formulation to metric learning for kNN regression, asymmetric similarity
learning and discriminative learning of Hamming distance. In the second part,
we consider a situation where we are on a limited computational budget i.e.
optimizing over a space of possible metrics would be infeasible, but access to
a label aware distance metric is still desirable. We present a simple, and
computationally inexpensive approach for estimating a well motivated metric
that relies only on gradient estimates, discussing theoretical and experimental
results. In the final part, we address representational issues, considering
group equivariant convolutional neural networks (GCNNs). Equivariance to
symmetry transformations is explicitly encoded in GCNNs; a classical CNN being
the simplest example. In particular, we present a SO(3)-equivariant neural
network architecture for spherical data, that operates entirely in Fourier
space, while also providing a formalism for the design of fully Fourier neural
networks that are equivariant to the action of any continuous compact group.
</summary>
    <author>
      <name>Shubhendu Trivedi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08763v2</id>
    <updated>2018-08-30T00:51:38Z</updated>
    <published>2018-08-27T09:51:03Z</published>
    <title>On the convergence of optimistic policy iteration for stochastic
  shortest path problem</title>
    <summary>  In this paper, we prove some convergence results of a special case of
optimistic policy iteration algorithm for stochastic shortest path problem. We
consider both Monte Carlo and $TD(\lambda)$ methods for the policy evaluation
step under the condition that the termination state will eventually be reached
almost surely.
</summary>
    <author>
      <name>Yuanlong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09830v2</id>
    <updated>2018-08-30T00:35:52Z</updated>
    <published>2018-08-29T13:52:40Z</published>
    <title>Searching Toward Pareto-Optimal Device-Aware Neural Architectures</title>
    <summary>  Recent breakthroughs in Neural Architectural Search (NAS) have achieved
state-of-the-art performance in many tasks such as image classification and
language understanding. However, most existing works only optimize for model
accuracy and largely ignore other important factors imposed by the underlying
hardware and devices, such as latency and energy, when making inference. In
this paper, we first introduce the problem of NAS and provide a survey on
recent works. Then we deep dive into two recent advancements on extending NAS
into multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net
are capable of optimizing accuracy and other objectives imposed by devices,
searching for neural architectures that can be best deployed on a wide spectrum
of devices: from embedded systems and mobile devices to workstations.
Experimental results are poised to show that architectures found by MONAS and
DPP-Net achieves Pareto optimality w.r.t the given objectives for various
devices.
</summary>
    <author>
      <name>An-Chieh Cheng</name>
    </author>
    <author>
      <name>Jin-Dong Dong</name>
    </author>
    <author>
      <name>Chi-Hung Hsu</name>
    </author>
    <author>
      <name>Shu-Huan Chang</name>
    </author>
    <author>
      <name>Min Sun</name>
    </author>
    <author>
      <name>Shih-Chieh Chang</name>
    </author>
    <author>
      <name>Jia-Yu Pan</name>
    </author>
    <author>
      <name>Yu-Ting Chen</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Da-Cheng Juan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCAD'18 Invited Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09830v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09830v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10073v1</id>
    <updated>2018-08-30T00:33:28Z</updated>
    <published>2018-08-30T00:33:28Z</published>
    <title>Rational Neural Networks for Approximating Jump Discontinuities of Graph
  Convolution Operator</title>
    <summary>  For node level graph encoding, a recent important state-of-art method is the
graph convolutional networks (GCN), which nicely integrate local vertex
features and graph topology in the spectral domain. However, current studies
suffer from several drawbacks: (1) graph CNNs relies on Chebyshev polynomial
approximation which results in oscillatory approximation at jump
discontinuities; (2) Increasing the order of Chebyshev polynomial can reduce
the oscillations issue, but also incurs unaffordable computational cost; (3)
Chebyshev polynomials require degree $\Omega$(poly(1/$\epsilon$)) to
approximate a jump signal such as $|x|$, while rational function only needs
$\mathcal{O}$(poly log(1/$\epsilon$))\cite{liang2016deep,telgarsky2017neural}.
However, it's non-trivial to apply rational approximation without increasing
computational complexity due to the denominator. In this paper, the superiority
of rational approximation is exploited for graph signal recovering. RatioanlNet
is proposed to integrate rational function and neural networks. We show that
rational function of eigenvalues can be rewritten as a function of graph
Laplacian, which can avoid multiplication by the eigenvector matrix. Focusing
on the analysis of approximation on graph convolution operation, a graph signal
regression task is formulated. Under graph signal regression task, its time
complexity can be significantly reduced by graph Fourier transform. To overcome
the local minimum problem of neural networks model, a relaxed Remez algorithm
is utilized to initialize the weight parameters. Convergence rate of
RatioanlNet and polynomial based methods on jump signal is analyzed for a
theoretical guarantee. The extensive experimental results demonstrated that our
approach could effectively characterize the jump discontinuities, outperforming
competing methods by a substantial margin on both synthetic and real-world
graphs.
</summary>
    <author>
      <name>Zhiqian Chen</name>
    </author>
    <author>
      <name>Feng Chen</name>
    </author>
    <author>
      <name>Rongjie Lai</name>
    </author>
    <author>
      <name>Xuchao Zhang</name>
    </author>
    <author>
      <name>Chang-Tien Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICDM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07380v3</id>
    <updated>2018-08-30T00:28:59Z</updated>
    <published>2018-08-19T02:53:33Z</published>
    <title>On the Predictability of non-CGM Diabetes Data for Personalized
  Recommendation</title>
    <summary>  With continuous glucose monitoring (CGM), data-driven models on blood glucose
prediction have been shown to be effective in related work. However, such (CGM)
systems are not always available, e.g., for a patient at home. In this work, we
conduct a study on 9 patients and examine the predictability of data-driven
(aka. machine learning) based models on patient-level blood glucose prediction;
with measurements are taken only periodically (i.e., after several hours). To
this end, we propose several post-prediction methods to account for the noise
nature of these data, that marginally improves the performance of the end
system.
</summary>
    <author>
      <name>Tu Nguyen</name>
    </author>
    <author>
      <name>Markus Rokicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of ACM CIKM 2018 Workshops</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07380v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07380v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02129v2</id>
    <updated>2018-08-29T23:31:44Z</updated>
    <published>2018-08-06T21:44:06Z</published>
    <title>Probabilistic Causal Analysis of Social Influence</title>
    <summary>  Mastering the dynamics of social influence requires separating, in a database
of information propagation traces, the genuine causal processes from temporal
correlation, i.e., homophily and other spurious causes. However, most studies
to characterize social influence, and, in general, most data-science analyses
focus on correlations, statistical independence, or conditional independence.
Only recently, there has been a resurgence of interest in "causal data
science", e.g., grounded on causality theories. In this paper we adopt a
principled causal approach to the analysis of social influence from
information-propagation data, rooted in the theory of probabilistic causation.
  Our approach consists of two phases. In the first one, in order to avoid the
pitfalls of misinterpreting causation when the data spans a mixture of several
subtypes ("Simpson's paradox"), we partition the set of propagation traces into
groups, in such a way that each group is as less contradictory as possible in
terms of the hierarchical structure of information propagation. To achieve this
goal, we borrow the notion of "agony" and define the Agony-bounded Partitioning
problem, which we prove being hard, and for which we develop two efficient
algorithms with approximation guarantees. In the second phase, for each group
from the first phase, we apply a constrained MLE approach to ultimately learn a
minimal causal topology. Experiments on synthetic data show that our method is
able to retrieve the genuine causal arcs w.r.t. a ground-truth generative
model. Experiments on real data show that, by focusing only on the extracted
causal structures instead of the whole social graph, the effectiveness of
predicting influence spread is significantly improved.
</summary>
    <author>
      <name>Francesco Bonchi</name>
    </author>
    <author>
      <name>Francesco Gullo</name>
    </author>
    <author>
      <name>Bud Mishra</name>
    </author>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM 18, October 22-26, 2018, Torino, Italy</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.02129v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02129v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07821v4</id>
    <updated>2018-08-29T22:28:04Z</updated>
    <published>2017-07-25T06:05:27Z</published>
    <title>Concept Drift Detection and Adaptation with Hierarchical Hypothesis
  Testing</title>
    <summary>  When using statistical models (such as a classifier) in a streaming
environment, there is often a need to detect and adapt to concept drifts to
mitigate any deterioration in the model's predictive performance over time.
Unfortunately, the ability of popular concept drift approaches in detecting
these drifts in the relationship of the response and predictor variable is
often dependent on the distribution characteristics of the data streams, as
well as its sensitivity on parameter tuning. This paper presents Hierarchical
Linear Four Rates (HLFR), a framework that detects concept drifts for different
data stream distributions (including imbalanced data) by leveraging a
hierarchical set of hypothesis tests in an online setting. The performance of
HLFR is compared to benchmark approaches using both simulated and real-world
datasets spanning the breadth of concept drift types. HLFR significantly
outperforms benchmark approaches in terms of accuracy, G-mean, recall, delay in
detection and adaptability across the various datasets.
</summary>
    <author>
      <name>Shujian Yu</name>
    </author>
    <author>
      <name>Zubin Abraham</name>
    </author>
    <author>
      <name>Heng Wang</name>
    </author>
    <author>
      <name>Mohak Shah</name>
    </author>
    <author>
      <name>Xinge You</name>
    </author>
    <author>
      <name>José C. Príncipe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07821v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07821v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08316v2</id>
    <updated>2018-08-29T22:00:03Z</updated>
    <published>2018-08-24T21:29:53Z</published>
    <title>A Trio Neural Model for Dynamic Entity Relatedness Ranking</title>
    <summary>  Measuring entity relatedness is a fundamental task for many natural language
processing and information retrieval applications. Prior work often studies
entity relatedness in static settings and an unsupervised manner. However,
entities in real-world are often involved in many different relationships,
consequently entity-relations are very dynamic over time. In this work, we
propose a neural networkbased approach for dynamic entity relatedness,
leveraging the collective attention as supervision. Our model is capable of
learning rich and different entity representations in a joint framework.
Through extensive experiments on large-scale datasets, we demonstrate that our
method achieves better results than competitive baselines.
</summary>
    <author>
      <name>Tu Nguyen</name>
    </author>
    <author>
      <name>Tuan Tran</name>
    </author>
    <author>
      <name>Wolfgang Nejdl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of CoNLL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08316v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08316v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01496v1</id>
    <updated>2018-08-29T21:11:09Z</updated>
    <published>2018-08-29T21:11:09Z</published>
    <title>Learning Gender-Neutral Word Embeddings</title>
    <summary>  Word embedding models have become a fundamental component in a wide range of
Natural Language Processing (NLP) applications. However, embeddings trained on
human-generated corpora have been demonstrated to inherit strong gender
stereotypes that reflect social constructs. To address this concern, in this
paper, we propose a novel training procedure for learning gender-neutral word
embeddings. Our approach aims to preserve gender information in certain
dimensions of word vectors while compelling other dimensions to be free of
gender influence. Based on the proposed method, we generate a Gender-Neutral
variant of GloVe (GN-GloVe). Quantitative and qualitative experiments
demonstrate that GN-GloVe successfully isolates gender information without
sacrificing the functionality of the embedding model.
</summary>
    <author>
      <name>Jieyu Zhao</name>
    </author>
    <author>
      <name>Yichao Zhou</name>
    </author>
    <author>
      <name>Zeyu Li</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Kai-Wei Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10038v1</id>
    <updated>2018-08-29T20:45:54Z</updated>
    <published>2018-08-29T20:45:54Z</published>
    <title>Theoretical Linear Convergence of Unfolded ISTA and its Practical
  Weights and Thresholds</title>
    <summary>  In recent years, unfolding iterative algorithms as neural networks has become
an empirical success in solving sparse recovery problems. However, its
theoretical understanding is still immature, which prevents us from fully
utilizing the power of neural networks. In this work, we study unfolded ISTA
(Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We
introduce a weight structure that is necessary for asymptotic convergence to
the true sparse signal. With this structure, unfolded ISTA can attain a linear
convergence, which is better than the sublinear convergence of ISTA/FISTA in
general cases. Furthermore, we propose to incorporate thresholding in the
network to perform support selection, which is easy to implement and able to
boost the convergence rate both theoretically and empirically. Extensive
simulations, including sparse vector recovery and a compressive sensing
experiment on real image data, corroborate our theoretical results and
demonstrate their practical usefulness.
</summary>
    <author>
      <name>Xiaohan Chen</name>
    </author>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Zhangyang Wang</name>
    </author>
    <author>
      <name>Wotao Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 6 figures, 1 table. Under review for NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09034v2</id>
    <updated>2018-08-29T20:13:22Z</updated>
    <published>2018-08-27T21:12:47Z</published>
    <title>Importance Weighting and Variational Inference</title>
    <summary>  Recent work used importance sampling ideas for better variational bounds on
likelihoods. We clarify the applicability of these ideas to pure probabilistic
inference, by showing the resulting Importance Weighted Variational Inference
(IWVI) technique is an instance of augmented variational inference, thus
identifying the looseness in previous work. Experiments confirm IWVI's
practicality for probabilistic inference. As a second contribution, we
investigate inference with elliptical distributions, which improves accuracy in
low dimensions, and convergence in high dimensions.
</summary>
    <author>
      <name>Justin Domke</name>
    </author>
    <author>
      <name>Daniel Sheldon</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10026v1</id>
    <updated>2018-08-29T20:02:41Z</updated>
    <published>2018-08-29T20:02:41Z</published>
    <title>Physically-inspired Gaussian processes for transcriptional regulation in
  Drosophila melanogaster</title>
    <summary>  The regulatory process in Drosophila melanogaster is thoroughly studied for
understanding several principles in systems biology. Since transcriptional
regulation of the Drosophila depends on spatiotemporal interactions between
mRNA expressions and gap-gene proteins, proper physically-inspired stochastic
models are required to describe the existing link between both biological
quantities. Many studies have shown that the use of Gaussian processes (GPs)
and differential equations yields promising inference results when modelling
regulatory processes. In order to exploit the benefits of GPs, two types of
physically-inspired GPs based on the reaction-diffusion equation are further
investigated in this paper. The main difference between both approaches lies on
whether the GP prior is placed: either over mRNA expressions or protein
concentrations. Contrarily to other stochastic frameworks, discretising the
spatial space is not required here. Both GP models are tested under different
conditions depending on the availability of biological data. Finally, their
performances are assessed using a high-resolution dataset describing the
blastoderm stage of the early embryo of Drosophila.
</summary>
    <author>
      <name>Andrés F. López-Lopera</name>
    </author>
    <author>
      <name>Nicolas Durrande</name>
    </author>
    <author>
      <name>Mauricio A. Alvarez</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02586v3</id>
    <updated>2018-08-29T19:51:04Z</updated>
    <published>2017-06-08T13:52:23Z</published>
    <title>DSOS and SDSOS Optimization: More Tractable Alternatives to Sum of
  Squares and Semidefinite Optimization</title>
    <summary>  In recent years, optimization theory has been greatly impacted by the advent
of sum of squares (SOS) optimization. The reliance of this technique on
large-scale semidefinite programs however, has limited the scale of problems to
which it can be applied. In this paper, we introduce DSOS and SDSOS
optimization as linear programming and second-order cone programming-based
alternatives to sum of squares optimization that allow one to trade off
computation time with solution quality. These are optimization problems over
certain subsets of sum of squares polynomials (or equivalently subsets of
positive semidefinite matrices), which can be of interest in general
applications of semidefinite programming where scalability is a limitation. We
show that some basic theorems from SOS optimization which rely on results from
real algebraic geometry are still valid for DSOS and SDSOS optimization.
Furthermore, we show with numerical experiments from diverse application
areas---polynomial optimization, statistics and machine learning, derivative
pricing, and control theory---that with reasonable tradeoffs in accuracy, we
can handle problems at scales that are currently significantly beyond the reach
of traditional sum of squares approaches. Finally, we provide a review of
recent techniques that bridge the gap between our DSOS/SDSOS approach and the
SOS approach at the expense of additional running time. The Supplementary
Material of the paper introduces an accompanying MATLAB package for DSOS and
SDSOS optimization.
</summary>
    <author>
      <name>Amir Ali Ahmadi</name>
    </author>
    <author>
      <name>Anirudha Majumdar</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02586v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02586v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05484v3</id>
    <updated>2018-08-29T19:10:54Z</updated>
    <published>2018-04-16T02:52:45Z</published>
    <title>Block Mean Approximation for Efficient Second Order Optimization</title>
    <summary>  Advanced optimization algorithms such as Newton method and AdaGrad benefit
from second order derivative or second order statistics to achieve better
descent directions and faster convergence rates. At their heart, such
algorithms need to compute the inverse or inverse square root of a matrix whose
size is quadratic of the dimensionality of the search space. For high
dimensional search spaces, the matrix inversion or inversion of square root
becomes overwhelming which in turn demands for approximate methods. In this
work, we propose a new matrix approximation method which divides a matrix into
blocks and represents each block by one or two numbers. The method allows
efficient computation of matrix inverse and inverse square root. We apply our
method to AdaGrad in training deep neural networks. Experiments show
encouraging results compared to the diagonal approximation.
</summary>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Mehrtash Harandi</name>
    </author>
    <author>
      <name>Richard Hartley</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05484v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05484v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00047v4</id>
    <updated>2018-08-29T19:03:53Z</updated>
    <published>2018-01-31T20:03:07Z</published>
    <title>Matrix completion with deterministic pattern - a geometric perspective</title>
    <summary>  We consider the matrix completion problem with a deterministic pattern of
observed entries. In this setting, we aim to answer the question: under what
condition there will be (at least locally) unique solution to the matrix
completion problem, i.e., the underlying true matrix is identifiable. We answer
the question from a certain point of view and outline a geometric perspective.
We give an algebraically verifiable sufficient condition, which we call the
well-posedness condition, for the local uniqueness of MRMC solutions. We argue
that this condition is necessary for local stability of MRMC solutions, and we
show that the condition is generic using the characteristic rank. We also argue
that the low-rank approximation approaches are more stable than MRMC and
further propose a sequential statistical testing procedure to determine the
"true" rank from observed entries. Finally, we provide numerical examples aimed
at verifying validity of the presented theory.
</summary>
    <author>
      <name>Alexander Shapiro</name>
    </author>
    <author>
      <name>Yao Xie</name>
    </author>
    <author>
      <name>Rui Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00047v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00047v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10013v1</id>
    <updated>2018-08-29T18:55:22Z</updated>
    <published>2018-08-29T18:55:22Z</published>
    <title>Group calibration is a byproduct of unconstrained learning</title>
    <summary>  Much recent work on fairness in machine learning has focused on how well a
score function is calibrated in different groups within a given population,
where each group is defined by restricting one or more sensitive attributes.
  We investigate to which extent group calibration follows from unconstrained
empirical risk minimization on its own, without the need for any explicit
intervention. We show that under reasonable conditions, the deviation from
satisfying group calibration is bounded by the excess loss of the empirical
risk minimizer relative to the Bayes optimal score function. As a corollary, it
follows that empirical risk minimization can simultaneously achieve calibration
for many groups, a task that prior work deferred to highly complex algorithms.
We complement our results with a lower bound, and a range of experimental
findings.
  Our results challenge the view that group calibration necessitates an active
intervention, suggesting that often we ought to think of it as a byproduct of
unconstrained machine learning.
</summary>
    <author>
      <name>Lydia T. Liu</name>
    </author>
    <author>
      <name>Max Simchowitz</name>
    </author>
    <author>
      <name>Moritz Hardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02933v2</id>
    <updated>2018-08-29T18:52:44Z</updated>
    <published>2018-08-08T20:40:42Z</published>
    <title>(Sequential) Importance Sampling Bandits</title>
    <summary>  The multi-armed bandit (MAB) problem is a sequential allocation task where
the goal is to learn a policy that maximizes long term payoff, where only the
reward of the executed action is observed; i.e., sequential optimal decisions
are made, while simultaneously learning how the world operates. In the
stochastic setting, the reward for each action is generated from an unknown
distribution. To decide the next optimal action to take, one must compute
sufficient statistics of this unknown reward distribution, e.g.
upper-confidence bounds (UCB), or expectations in Thompson sampling.
Closed-form expressions for these statistics of interest are analytically
intractable except for simple cases. We here propose to leverage Monte Carlo
estimation and, in particular, the flexibility of (sequential) importance
sampling (IS) to allow for accurate estimation of the statistics of interest
within the MAB problem. IS methods estimate posterior densities or expectations
in probabilistic models that are analytically intractable. We first show how IS
can be combined with state-of-the-art MAB algorithms (Thompson sampling and
Bayes-UCB) for classic (Bernoulli and contextual linear-Gaussian) bandit
problems. Furthermore, we leverage the power of sequential IS to extend the
applicability of these algorithms beyond the classic settings, and tackle
additional useful cases. Specifically, we study the dynamic linear-Gaussian
bandit, and both the static and dynamic logistic cases too. The flexibility of
(sequential) importance sampling is shown to be fundamental for obtaining
efficient estimates of the key sufficient statistics in these challenging
scenarios.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02933v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02933v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09940v1</id>
    <updated>2018-08-29T17:39:08Z</updated>
    <published>2018-08-29T17:39:08Z</published>
    <title>Deep Reinforcement Learning in Portfolio Management</title>
    <summary>  In this paper, we implement two state-of-art continuous reinforcement
learning algorithms, Deep Deterministic Policy Gradient (DDPG) and Proximal
Policy Optimization (PPO) in portfolio management. Both of them are widely-used
in game playing and robot control. What's more, PPO has appealing theoretical
propeties which is hopefully potential in portfolio management. We present the
performances of them under different settings, including different learning
rate, objective function, markets, feature combinations, in order to provide
insights for parameter tuning, features selection and data preparation.
</summary>
    <author>
      <name>Zhipeng Liang</name>
    </author>
    <author>
      <name>Kangkang Jiang</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Junhao Zhu</name>
    </author>
    <author>
      <name>Yanran Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09935v1</id>
    <updated>2018-08-29T17:29:12Z</updated>
    <published>2018-08-29T17:29:12Z</published>
    <title>Attention-based Neural Text Segmentation</title>
    <summary>  Text segmentation plays an important role in various Natural Language
Processing (NLP) tasks like summarization, context understanding, document
indexing and document noise removal. Previous methods for this task require
manual feature engineering, huge memory requirements and large execution times.
To the best of our knowledge, this paper is the first one to present a novel
supervised neural approach for text segmentation. Specifically, we propose an
attention-based bidirectional LSTM model where sentence embeddings are learned
using CNNs and the segments are predicted based on contextual information. This
model can automatically handle variable sized context information. Compared to
the existing competitive baselines, the proposed model shows a performance
improvement of ~7% in WinDiff score on three benchmark datasets.
</summary>
    <author>
      <name>Pinkesh Badjatiya</name>
    </author>
    <author>
      <name>Litton J Kurisinkel</name>
    </author>
    <author>
      <name>Manish Gupta</name>
    </author>
    <author>
      <name>Vasudeva Varma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12317v2</id>
    <updated>2018-08-29T16:53:44Z</updated>
    <published>2018-05-31T04:51:08Z</published>
    <title>Multiaccuracy: Black-Box Post-Processing for Fairness in Classification</title>
    <summary>  Prediction systems are successfully deployed in applications ranging from
disease diagnosis, to predicting credit worthiness, to image recognition. Even
when the overall accuracy is high, these systems may exhibit systematic biases
that harm specific subpopulations; such biases may arise inadvertently due to
underrepresentation in the data used to train a machine-learning model, or as
the result of intentional malicious discrimination. We develop a rigorous
framework of *multiaccuracy* auditing and post-processing to ensure accurate
predictions across *identifiable subgroups*.
  Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have
black-box access to a predictor and a relatively small set of labeled data for
auditing; importantly, this black-box framework allows for improved fairness
and accountability of predictions, even when the predictor is minimally
transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show
that if the initial model is accurate on an identifiable subgroup, then the
post-processed model will be also. We experimentally demonstrate the
effectiveness of the approach to improve the accuracy among minority subgroups
in diverse applications (image classification, finance, population health).
Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for
"black women") even when the sensitive features (e.g. "race", "gender") are not
given to the algorithm explicitly.
</summary>
    <author>
      <name>Michael P. Kim</name>
    </author>
    <author>
      <name>Amirata Ghorbani</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09920v1</id>
    <updated>2018-08-29T16:44:51Z</updated>
    <published>2018-08-29T16:44:51Z</published>
    <title>Question Answering by Reasoning Across Documents with Graph
  Convolutional Networks</title>
    <summary>  Most research in reading comprehension has focused on answering questions
based on individual documents or even single paragraphs. We introduce a method
which integrates and reasons relying on information spread within documents and
across multiple documents. We frame it as an inference problem on a graph.
Mentions of entities are nodes of this graph where edges encode relations
between different mentions (e.g., within- and cross-document co-references).
Graph convolutional networks (GCNs) are applied to these graphs and trained to
perform multi-step reasoning. Our Entity-GCN method is scalable and compact,
and it achieves state-of-the-art results on the WikiHop dataset (Welbl et al.
2017).
</summary>
    <author>
      <name>Nicola De Cao</name>
    </author>
    <author>
      <name>Wilker Aziz</name>
    </author>
    <author>
      <name>Ivan Titov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01046v1</id>
    <updated>2018-08-29T16:32:22Z</updated>
    <published>2018-08-29T16:32:22Z</published>
    <title>Group-Representative Functional Network Estimation from Multi-Subject
  fMRI Data via MRF-based Image Segmentation</title>
    <summary>  We propose a novel two-phase approach to functional network estimation of
multi-subject functional Magnetic Resonance Imaging (fMRI) data, which applies
model-based image segmentation to determine a group-representative connectivity
map. In our approach, we first improve clustering-based Independent Component
Analysis (ICA) to generate maps of components occurring consistently across
subjects, and then estimate the group-representative map through MAP-MRF
(Maximum a priori - Markov random field) labeling. For the latter, we provide a
novel and efficient variational Bayes algorithm. We study the performance of
the proposed method using synthesized data following a theoretical model, and
demonstrate its viability in blind extraction of group-representative
functional networks using simulated fMRI data. We anticipate the proposed
method will be applied in identifying common neuronal characteristics in a
population, and could be further extended to real-world clinical diagnosis.
</summary>
    <author>
      <name>Aditi Iyer</name>
    </author>
    <author>
      <name>Bingjing Tang</name>
    </author>
    <author>
      <name>Vinayak Rao</name>
    </author>
    <author>
      <name>Nan Kong</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09907v1</id>
    <updated>2018-08-29T16:18:02Z</updated>
    <published>2018-08-29T16:18:02Z</published>
    <title>Dropout with Tabu Strategy for Regularizing Deep Neural Networks</title>
    <summary>  Dropout has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons in deep neural networks (DNN). It
randomly drops units with a probability $p$ during the training stage of DNN.
Dropout also provides a way of approximately combining exponentially many
different neural network architectures efficiently. In this work, we add a
diversification strategy into dropout, which aims at generating more different
neural network architectures in a proper times of iterations. The dropped units
in last forward propagation will be marked. Then the selected units for
dropping in the current FP will be kept if they have been marked in the last
forward propagation. We only mark the units from the last forward propagation.
We call this new technique Tabu Dropout. Tabu Dropout has no extra parameters
compared with the standard Dropout and also it is computationally cheap. The
experiments conducted on MNIST, Fashion-MNIST datasets show that Tabu Dropout
improves the performance of the standard dropout.
</summary>
    <author>
      <name>Zongjie Ma</name>
    </author>
    <author>
      <name>Abdul Sattar</name>
    </author>
    <author>
      <name>Jun Zhou</name>
    </author>
    <author>
      <name>Qingliang Chen</name>
    </author>
    <author>
      <name>Kaile Su</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09897v1</id>
    <updated>2018-08-29T15:57:27Z</updated>
    <published>2018-08-29T15:57:27Z</published>
    <title>Towards security defect prediction with AI</title>
    <summary>  In this study, we investigate the limits of the current state of the art AI
system for detecting buffer overflows and compare it with current static
analysis tools. To do so, we developed a code generator, s-bAbI, capable of
producing an arbitrarily large number of code samples of controlled complexity.
We found that the static analysis engines we examined have good precision, but
poor recall on this dataset, except for a sound static analyzer that has good
precision and recall. We found that the state of the art AI system, a memory
network modeled after Choi et al. [1], can achieve similar performance to the
static analysis engines, but requires an exhaustive amount of training data in
order to do so. Our work points towards future approaches that may solve these
problems; namely, using representations of code that can capture appropriate
scope information and using deep learning methods that are able to perform
arithmetic operations.
</summary>
    <author>
      <name>Carson D. Sestili</name>
    </author>
    <author>
      <name>William S. Snavely</name>
    </author>
    <author>
      <name>Nathan M. VanHoudnos</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10261v1</id>
    <updated>2018-08-29T15:24:33Z</updated>
    <published>2018-08-29T15:24:33Z</published>
    <title>Centroid estimation based on symmetric KL divergence for Multinomial
  text classification problem</title>
    <summary>  We define a new method to estimate centroid for text classification based on
the symmetric KL-divergence between the distribution of words in training
documents and their class centroids. Experiments on several standard data sets
indicate that the new method achieves substantial improvements over the
traditional classifiers.
</summary>
    <author>
      <name>Jiangning Chen</name>
    </author>
    <author>
      <name>Heinrich Matzinger</name>
    </author>
    <author>
      <name>Haoyan Zhai</name>
    </author>
    <author>
      <name>Mi Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04212v3</id>
    <updated>2018-08-29T15:16:08Z</updated>
    <published>2018-04-11T20:37:35Z</published>
    <title>Word2Vec applied to Recommendation: Hyperparameters Matter</title>
    <summary>  Skip-gram with negative sampling, a popular variant of Word2vec originally
designed and tuned to create word embeddings for Natural Language Processing,
has been used to create item embeddings with successful applications in
recommendation. While these fields do not share the same type of data, neither
evaluate on the same tasks, recommendation applications tend to use the same
already tuned hyperparameters values, even if optimal hyperparameters values
are often known to be data and task dependent. We thus investigate the marginal
importance of each hyperparameter in a recommendation setting through large
hyperparameter grid searches on various datasets. Results reveal that
optimizing neglected hyperparameters, namely negative sampling distribution,
number of epochs, subsampling parameter and window-size, significantly improves
performance on a recommendation task, and can increase it by an order of
magnitude. Importantly, we find that optimal hyperparameters configurations for
Natural Language Processing tasks and Recommendation tasks are noticeably
different.
</summary>
    <author>
      <name>Hugo Caselles-Dupré</name>
    </author>
    <author>
      <name>Florian Lesaint</name>
    </author>
    <author>
      <name>Jimena Royo-Letelier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published on the 12th ACM Conference on Recommender
  Systems, Vancouver, Canada, 2nd-7th October 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04212v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04212v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01495v1</id>
    <updated>2018-08-29T15:11:49Z</updated>
    <published>2018-08-29T15:11:49Z</published>
    <title>A Reinforcement Learning-driven Translation Model for Search-Oriented
  Conversational Systems</title>
    <summary>  Search-oriented conversational systems rely on information needs expressed in
natural language (NL). We focus here on the understanding of NL expressions for
building keyword-based queries. We propose a reinforcement-learning-driven
translation model framework able to 1) learn the translation from NL
expressions to queries in a supervised way, and, 2) to overcome the lack of
large-scale dataset by framing the translation model as a word selection
approach and injecting relevance feedback in the learning process. Experiments
are carried out on two TREC datasets and outline the effectiveness of our
approach.
</summary>
    <author>
      <name>Wafa Aissa</name>
    </author>
    <author>
      <name>Laure Soulier</name>
    </author>
    <author>
      <name>Ludovic Denoyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the author's pre-print version of the work. It is posted here
  for your personal use, not for redistribution. Please cite the definitive
  version which will be published in Proceedings of the 2018 EMNLP Workshop
  SCAI: The 2nd International Workshop on Search-Oriented Conversational AI -
  ISBN: 978-1-948087-75-9</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09856v1</id>
    <updated>2018-08-29T14:43:12Z</updated>
    <published>2018-08-29T14:43:12Z</published>
    <title>Application of Machine Learning in Rock Facies Classification with
  Physics-Motivated Feature Augmentation</title>
    <summary>  With recent progress in algorithms and the availability of massive amounts of
computation power, application of machine learning techniques is becoming a hot
topic in the oil and gas industry. One of the most promising aspects to apply
machine learning to the upstream field is the rock facies classification in
reservoir characterization, which is crucial in determining the net pay
thickness of reservoirs, thus a definitive factor in drilling decision making
process. For complex machine learning tasks like facies classification, feature
engineering is often critical. This paper shows the inclusion of
physics-motivated feature interaction in feature augmentation can further
improve the capability of machine learning in rock facies classification. We
demonstrate this approach with the SEG 2016 machine learning contest dataset
and the top winning algorithms. The improvement is roboust and can be $\sim5\%$
better than current existing best F-1 score, where F-1 is an evaluation metric
used to quantify average prediction accuracy.
</summary>
    <author>
      <name>Jie Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Corresponding author</arxiv:affiliation>
    </author>
    <author>
      <name>Yu Zeng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Corresponding author</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10259v1</id>
    <updated>2018-08-29T14:06:31Z</updated>
    <published>2018-08-29T14:06:31Z</published>
    <title>Analyze Unstructured Data Patterns for Conceptual Representation</title>
    <summary>  Online news media provides aggregated news and stories from different sources
all over the world and up-to-date news coverage. The main goal of this study is
to have a solution that considered as a homogeneous source for the news and to
represent the news in a new conceptual framework. Furthermore, the user can
easily find different updated news in a fast way through the designed
interface. The Mobile App implementation is based on modeling the multi-level
conceptual analysis discipline. Discovering main concepts of any domain is
captured from the hidden unstructured data that are analyzed by the proposed
solution. Concepts are discovered through analyzing data patterns to be
structured into a tree-based interface for easy navigation for the end user,
through the discovered news concepts. Our final experiment results showing that
analyzing the news before displaying to the end-user and restructuring the
final output in a conceptual multilevel structure, that producing new display
frame for the end user to find the related information to his interest.
</summary>
    <author>
      <name>Aboubakr Aqle</name>
    </author>
    <author>
      <name>Dena Al-Thani</name>
    </author>
    <author>
      <name>Ali Jaoua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 Figures, 4th Annual Conference on Computational Science &amp;
  Computational Intelligence (CSCI'17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04566v2</id>
    <updated>2018-08-29T13:55:56Z</updated>
    <published>2018-04-12T15:23:39Z</published>
    <title>Latent Geometry Inspired Graph Dissimilarities Enhance Affinity
  Propagation Community Detection in Complex Networks</title>
    <summary>  Affinity propagation is one of the most effective unsupervised pattern
recognition algorithms for data clustering in high-dimensional feature space.
However, the numerous attempts to test its performance for community detection
in complex networks have been attaining results very far from the state of the
art methods such as Infomap and Louvain. Yet, all these studies agreed that the
crucial problem is to convert the unweighted network topology in a
'smart-enough' node dissimilarity matrix that is able to properly address the
message passing procedure behind affinity propagation clustering. Here we
introduce a conceptual innovation and we discuss how to leverage network latent
geometry notions in order to design dissimilarity matrices for affinity
propagation community detection. Our results demonstrate that the latent
geometry inspired dissimilarity measures we design bring affinity propagation
to equal or outperform current state of the art methods for community
detection. These findings are solidly proven considering both synthetic
'realistic' networks (with known ground-truth communities) and real networks
(with community metadata), even when the data structure is corrupted by noise
artificially induced by missing or spurious connectivity.
</summary>
    <author>
      <name>Carlo Vittorio Cannistraci</name>
    </author>
    <author>
      <name>Alessandro Muscoloni</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04566v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04566v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09819v1</id>
    <updated>2018-08-29T13:41:33Z</updated>
    <published>2018-08-29T13:41:33Z</published>
    <title>Approximate Exploration through State Abstraction</title>
    <summary>  Although exploration in reinforcement learning is well understood from a
theoretical point of view, provably correct methods remain impractical. In this
paper we study the interplay between exploration and approximation, what we
call \emph{approximate exploration}. We first provide results when the
approximation is explicit, quantifying the performance of an exploration
algorithm, MBIE-EB \citep{strehl2008analysis}, when combined with state
aggregation. In particular, we show that this allows the agent to trade off
between learning speed and quality of the policy learned. We then turn to a
successful exploration scheme in practical, pseudo-count based exploration
bonuses \citep{bellemare2016unifying}. We show that choosing a density model
implicitly defines an abstraction and that the pseudo-count bonus incentivizes
the agent to explore using this abstraction. We find, however, that implicit
exploration may result in a mismatch between the approximated value function
and exploration bonus, leading to either under- or over-exploration.
</summary>
    <author>
      <name>Adrien Ali Taïga</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Marc G. Bellemare</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09744v1</id>
    <updated>2018-08-29T12:02:11Z</updated>
    <published>2018-08-29T12:02:11Z</published>
    <title>Rule induction for global explanation of trained models</title>
    <summary>  Understanding the behavior of a trained network and finding explanations for
its outputs is important for improving the network's performance and
generalization ability, and for ensuring trust in automated systems. Several
approaches have previously been proposed to identify and visualize the most
important features by analyzing a trained network. However, the relations
between different features and classes are lost in most cases. We propose a
technique to induce sets of if-then-else rules that capture these relations to
globally explain the predictions of a network. We first calculate the
importance of the features in the trained network. We then weigh the original
inputs with these feature importance scores, simplify the transformed input
space, and finally fit a rule induction model to explain the model predictions.
We find that the output rule-sets can explain the predictions of a neural
network trained for 4-class text classification from the 20 newsgroups dataset
to a macro-averaged F-score of 0.80. We make the code available at
https://github.com/clips/interpret_with_rules.
</summary>
    <author>
      <name>Madhumita Sushil</name>
    </author>
    <author>
      <name>Simon Šuster</name>
    </author>
    <author>
      <name>Walter Daelemans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the Workshop on 'Analyzing and interpreting neural
  networks for NLP' (BlackboxNLP), EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10260v1</id>
    <updated>2018-08-29T11:04:07Z</updated>
    <published>2018-08-29T11:04:07Z</published>
    <title>Understanding Latent Factors Using a GWAP</title>
    <summary>  Recommender systems relying on latent factor models often appear as black
boxes to their users. Semantic descriptions for the factors might help to
mitigate this problem. Achieving this automatically is, however, a
non-straightforward task due to the models' statistical nature. We present an
output-agreement game that represents factors by means of sample items and
motivates players to create such descriptions. A user study shows that the
collected output actually reflects real-world characteristics of the factors.
</summary>
    <author>
      <name>Johannes Kunkel</name>
    </author>
    <author>
      <name>Benedikt Loepp</name>
    </author>
    <author>
      <name>Jürgen Ziegler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Late-Breaking Results track part of the Twelfth
  ACM Conference on Recommender Systems (RecSys '18), Vancouver, BC, Canada,
  October 2-7, 2018, 2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01471v1</id>
    <updated>2018-08-29T09:21:22Z</updated>
    <published>2018-08-29T09:21:22Z</published>
    <title>Chest X-ray Inpainting with Deep Generative Models</title>
    <summary>  Generative adversarial networks have been successfully applied to inpainting
in natural images. However, the current state-of-the-art models have not yet
been widely adopted in the medical imaging domain. In this paper, we
investigate the performance of three recently published deep learning based
inpainting models: context encoders, semantic image inpainting, and the
contextual attention model, applied to chest x-rays, as the chest exam is the
most commonly performed radiological procedure. We train these generative
models on 1.2M 128 $\times$ 128 patches from 60K healthy x-rays, and learn to
predict the center 64 $\times$ 64 region in each patch. We test the models on
both the healthy and abnormal radiographs. We evaluate the results by visual
inspection and comparing the PSNR scores. The outputs of the models are in most
cases highly realistic. We show that the methods have potential to enhance and
detect abnormalities. In addition, we perform a 2AFC observer study and show
that an experienced human observer performs poorly in detecting inpainted
regions, particularly those generated by the contextual attention model.
</summary>
    <author>
      <name>Ecem Sogancioglu</name>
    </author>
    <author>
      <name>Shi Hu</name>
    </author>
    <author>
      <name>Davide Belli</name>
    </author>
    <author>
      <name>Bram van Ginneken</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09670v1</id>
    <updated>2018-08-29T07:58:28Z</updated>
    <published>2018-08-29T07:58:28Z</published>
    <title>Accelerated proximal boosting</title>
    <summary>  Gradient boosting is a prediction method that iteratively combines weak
learners to produce a complex and accurate model. From an optimization point of
view, the learning procedure of gradient boosting mimics a gradient descent on
a functional variable. This paper proposes to build upon the proximal point
algorithm when the empirical risk to minimize is not differentiable. In
addition, the novel boosting approach, called accelerated proximal boosting,
benefits from Nesterov's acceleration in the same way as gradient boosting
[Biau et al., 2018]. Advantages of leveraging proximal methods for boosting are
illustrated by numerical experiments on simulated and real-world data. In
particular, we exhibit a favorable comparison over gradient boosting regarding
convergence rate and prediction accuracy.
</summary>
    <author>
      <name>Erwan Fouillen</name>
    </author>
    <author>
      <name>Claire Boyer</name>
    </author>
    <author>
      <name>Maxime Sangnier</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09663v1</id>
    <updated>2018-08-29T07:18:29Z</updated>
    <published>2018-08-29T07:18:29Z</published>
    <title>Wasserstein is all you need</title>
    <summary>  We propose a unified framework for building unsupervised representations of
individual objects or entities (and their compositions), by associating with
each object both a distributional as well as a point estimate (vector
embedding). This is made possible by the use of optimal transport, which allows
us to build these associated estimates while harnessing the underlying geometry
of the ground space. Our method gives a novel perspective for building rich and
powerful feature representations that simultaneously capture uncertainty (via a
distributional estimate) and interpretability (with the optimal transport map).
As a guiding example, we formulate unsupervised representations for text, in
particular for sentence representation and entailment detection. Empirical
results show strong advantages gained through the proposed framework. This
approach can be used for any unsupervised or supervised problem (on text or
other modalities) with a co-occurrence structure, such as any sequence data.
The key tools underlying the framework are Wasserstein distances and
Wasserstein barycenters (and, hence the title!).
</summary>
    <author>
      <name>Sidak Pal Singh</name>
    </author>
    <author>
      <name>Andreas Hug</name>
    </author>
    <author>
      <name>Aymeric Dieuleveut</name>
    </author>
    <author>
      <name>Martin Jaggi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05240v3</id>
    <updated>2018-08-29T07:04:22Z</updated>
    <published>2018-08-15T18:13:12Z</published>
    <title>Blended Coarse Gradient Descent for Full Quantization of Deep Neural
  Networks</title>
    <summary>  Quantized deep neural networks (QDNNs) are attractive due to their much lower
memory storage and faster inference speed than their regular full precision
counterparts. To maintain the same performance level especially at low
bit-widths, QDNNs must be retrained. Their training involves piecewise constant
activation functions and discrete weights, hence mathematical challenges arise.
We introduce the notion of coarse derivative and propose the blended coarse
gradient descent (BCGD) algorithm, for training fully quantized neural
networks. Coarse gradient is generally not a gradient of any function but an
artificial ascent direction. The weight update of BCGD goes by coarse gradient
correction of a weighted average of the full precision weights and their
quantization (the so-called blending), which yields sufficient descent in the
objective value and thus accelerates the training. Our experiments demonstrate
that this simple blending technique is very effective for quantization at
extremely low bit-width such as binarization. In full quantization of ResNet-18
for ImageNet classification task, BCGD gives 64.36% top-1 accuracy with binary
weights across all layers and 4-bit adaptive activation. If the weights in the
first and last layers are kept in full precision, this number increases to
65.46%. As theoretical justification, we provide the convergence analysis of
coarse gradient descent for a two-layer neural network model with Gaussian
input data, and prove that the expected coarse gradient correlates positively
with the underlying true gradient.
</summary>
    <author>
      <name>Penghang Yin</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Jiancheng Lyu</name>
    </author>
    <author>
      <name>Stanley Osher</name>
    </author>
    <author>
      <name>Yingyong Qi</name>
    </author>
    <author>
      <name>Jack Xin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05240v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05240v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09645v1</id>
    <updated>2018-08-29T05:36:07Z</updated>
    <published>2018-08-29T05:36:07Z</published>
    <title>Diffusion Approximations for Online Principal Component Estimation and
  Global Convergence</title>
    <summary>  In this paper, we propose to adopt the diffusion approximation tools to study
the dynamics of Oja's iteration which is an online stochastic gradient descent
method for the principal component analysis. Oja's iteration maintains a
running estimate of the true principal component from streaming data and enjoys
less temporal and spatial complexities. We show that the Oja's iteration for
the top eigenvector generates a continuous-state discrete-time Markov chain
over the unit sphere. We characterize the Oja's iteration in three phases using
diffusion approximation and weak convergence tools. Our three-phase analysis
further provides a finite-sample error bound for the running estimate, which
matches the minimax information lower bound for principal component analysis
under the additional assumption of bounded samples.
</summary>
    <author>
      <name>Chris Junchi Li</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09642v1</id>
    <updated>2018-08-29T05:30:21Z</updated>
    <published>2018-08-29T05:30:21Z</published>
    <title>Online ICA: Understanding Global Dynamics of Nonconvex Optimization via
  Diffusion Processes</title>
    <summary>  Solving statistical learning problems often involves nonconvex optimization.
Despite the empirical success of nonconvex statistical optimization methods,
their global dynamics, especially convergence to the desirable local minima,
remain less well understood in theory. In this paper, we propose a new analytic
paradigm based on diffusion processes to characterize the global dynamics of
nonconvex statistical optimization. As a concrete example, we study stochastic
gradient descent (SGD) for the tensor decomposition formulation of independent
component analysis. In particular, we cast different phases of SGD into
diffusion processes, i.e., solutions to stochastic differential equations.
Initialized from an unstable equilibrium, the global dynamics of SGD transit
over three consecutive phases: (i) an unstable Ornstein-Uhlenbeck process
slowly departing from the initialization, (ii) the solution to an ordinary
differential equation, which quickly evolves towards the desirable local
minimum, and (iii) a stable Ornstein-Uhlenbeck process oscillating around the
desirable local minimum. Our proof techniques are based upon Stroock and
Varadhan's weak convergence of Markov chains to diffusion processes, which are
of independent interest.
</summary>
    <author>
      <name>Chris Junchi Li</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08270v2</id>
    <updated>2018-08-29T04:29:26Z</updated>
    <published>2018-08-24T19:22:12Z</published>
    <title>Building a Robust Text Classifier on a Test-Time Budget</title>
    <summary>  We propose a generic and interpretable learning framework for building robust
text classification model that achieves accuracy comparable to full models
under test-time budget constraints. Our approach learns a selector to identify
words that are relevant to the prediction tasks and passes them to the
classifier for processing. The selector is trained jointly with the classifier
and directly learns to incorporate with the classifier. We further propose a
data aggregation scheme to improve the robustness of the classifier. Our
learning framework is general and can be incorporated with any type of text
classification model. On real-world data, we show that the proposed approach
improves the performance of a given classifier and speeds up the model with a
mere loss in accuracy performance.
</summary>
    <author>
      <name>Md Rizwan Parvez</name>
    </author>
    <author>
      <name>Tolga Bolukbasi</name>
    </author>
    <author>
      <name>kai-Wei Chang</name>
    </author>
    <author>
      <name>Venkatesh Saligrama</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08270v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08270v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09484v2</id>
    <updated>2018-08-29T04:14:30Z</updated>
    <published>2018-05-24T02:22:08Z</published>
    <title>Multi-Level Deep Cascade Trees for Conversion Rate Prediction</title>
    <summary>  Developing effective and efficient recommendation methods is very challenging
for modern e-commerce platforms (e.g., Taobao). In this paper, we tackle this
problem by proposing multi-Level Deep Cascade Trees (ldcTree), which is a novel
decision tree ensemble approach. It leverages deep cascade structures by
stacking Gradient Boosting Decision Trees (GBDT) to effectively learn feature
representation. In addition, we propose to utilize the cross-entropy in each
tree of the preceding GBDT as the input feature representation for next level
GBDT, which has a clear explanation, i.e., a traversal from root to leaf nodes
in the next level GBDT corresponds to the combination of certain traversals in
the preceding GBDT. The deep cascade structure and the combination rule enable
the proposed ldcTree to have a stronger distributed feature representation
ability. Moreover, we propose an ensemble ldcTree to take full use of weak and
strong correlation features. Experimental results on off-line dataset and
online deployment demonstrate the effectiveness of the proposed methods.
</summary>
    <author>
      <name>Hong Wen</name>
    </author>
    <author>
      <name>Jing Zhang</name>
    </author>
    <author>
      <name>Quan Lin</name>
    </author>
    <author>
      <name>Keping Yang</name>
    </author>
    <author>
      <name>Taiwei Jin</name>
    </author>
    <author>
      <name>Fuyu Lv</name>
    </author>
    <author>
      <name>Xiaofeng Pan</name>
    </author>
    <author>
      <name>Pipei Huang</name>
    </author>
    <author>
      <name>Zheng-Jun Zha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, Submitted to CIKM'2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09484v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09484v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09617v1</id>
    <updated>2018-08-29T03:22:47Z</updated>
    <published>2018-08-29T03:22:47Z</published>
    <title>Elastic bands across the path: A new framework and methods to lower
  bound DTW</title>
    <summary>  There has been renewed recent interest in developing effective lower bounds
for Dynamic Time Warping (DTW) distance between time series. These have many
applications in time series indexing, clustering, forecasting, regression and
classification. One of the key time series classification algorithms, the
nearest neighbor algorithm with DTW distance (NN-DTW) is very expensive to
compute, due to the quadratic complexity of DTW. Lower bound search can speed
up NN-DTW substantially. An effective and tight lower bound quickly prunes off
unpromising nearest neighbor candidates from the search space and minimises the
number of the costly DTW computations. The speed up provided by lower bound
search becomes increasingly critical as training set size increases. Different
lower bounds provide different trade-offs between computation time and
tightness. Most existing lower bounds interact with DTW warping window sizes.
They are very tight and effective at smaller warping window sizes, but become
looser as the warping window increases, thus reducing the pruning effectiveness
for NN-DTW. In this work, we present a new class of lower bounds that are
tighter than the popular Keogh lower bound, while requiring similar computation
time. Our new lower bounds take advantage of the DTW boundary condition,
monotonicity and continuity constraints to create a tighter lower bound. Of
particular significance, they remain relatively tight even for large windows. A
single parameter to these new lower bounds controls the speed-tightness
trade-off. We demonstrate that these new lower bounds provide an exceptional
balance between computation time and tightness for the NN-DTW time series
classification task, resulting in greatly improved efficiency for NN-DTW lower
bound search.
</summary>
    <author>
      <name>Chang Wei Tan</name>
    </author>
    <author>
      <name>Francois Petitjean</name>
    </author>
    <author>
      <name>Geoffrey I. Webb</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07746v2</id>
    <updated>2018-08-29T02:57:36Z</updated>
    <published>2018-05-20T09:37:19Z</published>
    <title>Network Reconstruction and Controlling Based on Structural Regularity
  Analysis</title>
    <summary>  From the perspective of network analysis, the ubiquitous networks are
comprised of regular and irregular components, which makes uncovering the
complexity of network structures to be a fundamental challenge. Exploring the
regular information and identifying the roles of microscopic elements in
network data can help us recognize the principle of network organization and
contribute to network data utilization. However, the intrinsic structural
properties of networks remain so far inadequately explored and theorised. With
the realistic assumption that there are consistent features across the local
structures of networks, we propose a low-rank pursuit based self-representation
network model, in which the principle of network organization can be uncovered
by a representation matrix. According to this model, original true networks can
be reconstructed based on the observed unreliable network topology. In
particular, the proposed model enables us to estimate the extent to which the
networks are regulable, i.e., measuring the reconstructability of networks. In
addition, the model is capable of measuring the importance of microscopic
network elements, i.e., nodes and links, in terms of network regularity thereby
allowing us to regulate the reconstructability of networks based on them.
Extensive experiments on disparate real-world networks demonstrate the
effectiveness of the proposed network reconstruction and regulation algorithm.
Specifically, the network regularity metric can reflect the reconstructability
of networks, and the reconstruction accuracy can be improved by removing
irregular network links. Lastly, our approach provides an unique and novel
insight into the organization exploring of complex networks.
</summary>
    <author>
      <name>Tao Wu</name>
    </author>
    <author>
      <name>Shaojie Qiao</name>
    </author>
    <author>
      <name>Xingping Xian</name>
    </author>
    <author>
      <name>Xi-Zhao Wang</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Yanbing Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07226v2</id>
    <updated>2018-08-29T02:06:06Z</updated>
    <published>2018-08-22T05:43:16Z</published>
    <title>Mean-field approximation, convex hierarchies, and the optimality of
  correlation rounding: a unified perspective</title>
    <summary>  The free energy is a key quantity of interest in Ising models, but
unfortunately, computing it in general is computationally intractable. Two
popular (variational) approximation schemes for estimating the free energy of
general Ising models (in particular, even in regimes where correlation decay
does not hold) are: (i) the mean-field approximation with roots in statistical
physics, which estimates the free energy from below, and (ii) hierarchies of
convex relaxations with roots in theoretical computer science, which estimate
the free energy from above. We show, surprisingly, that the tight regime for
both methods to compute the free energy to leading order is identical.
  More precisely, we show that the mean-field approximation is within
$O((n\|J\|_{F})^{2/3})$ of the free energy, where $\|J\|_F$ denotes the
Frobenius norm of the interaction matrix of the Ising model. This
simultaneously subsumes both the breakthrough work of Basak and Mukherjee, who
showed the tight result that the mean-field approximation is within $o(n)$
whenever $\|J\|_{F} = o(\sqrt{n})$, as well as the work of Jain, Koehler, and
Mossel, who gave the previously best known non-asymptotic bound of
$O((n\|J\|_{F})^{2/3}\log^{1/3}(n\|J\|_{F}))$. We give a simple, algorithmic
proof of this result using a convex relaxation proposed by Risteski based on
the Sherali-Adams hierarchy, automatically giving sub-exponential time
approximation schemes for the free energy in this entire regime. Our
algorithmic result is tight under Gap-ETH.
  We furthermore combine our techniques with spin glass theory to prove (in a
strong sense) the optimality of correlation rounding, refuting a recent
conjecture of Allen, O'Donnell, and Zhou. Finally, we give the tight
generalization of all of these results to $k$-MRFs, capturing as a special case
previous work on approximating MAX-$k$-CSP.
</summary>
    <author>
      <name>Vishesh Jain</name>
    </author>
    <author>
      <name>Frederic Koehler</name>
    </author>
    <author>
      <name>Andrej Risteski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version: minor formatting changes, added grant acknowledgements</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07226v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07226v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09574v1</id>
    <updated>2018-08-28T23:03:55Z</updated>
    <published>2018-08-28T23:03:55Z</published>
    <title>Probabilistic Sparse Subspace Clustering Using Delayed Association</title>
    <summary>  Discovering and clustering subspaces in high-dimensional data is a
fundamental problem of machine learning with a wide range of applications in
data mining, computer vision, and pattern recognition. Earlier methods divided
the problem into two separate stages of finding the similarity matrix and
finding clusters. Similar to some recent works, we integrate these two steps
using a joint optimization approach. We make the following contributions: (i)
we estimate the reliability of the cluster assignment for each point before
assigning a point to a subspace. We group the data points into two groups of
"certain" and "uncertain", with the assignment of latter group delayed until
their subspace association certainty improves. (ii) We demonstrate that delayed
association is better suited for clustering subspaces that have ambiguities,
i.e. when subspaces intersect or data are contaminated with outliers/noise.
(iii) We demonstrate experimentally that such delayed probabilistic association
leads to a more accurate self-representation and final clusters. The proposed
method has higher accuracy both for points that exclusively lie in one
subspace, and those that are on the intersection of subspaces. (iv) We show
that delayed association leads to huge reduction of computational cost, since
it allows for incremental spectral clustering.
</summary>
    <author>
      <name>Maryam Jaberi</name>
    </author>
    <author>
      <name>Marianna Pensky</name>
    </author>
    <author>
      <name>Hassan Foroosh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICPR 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.09574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08012v4</id>
    <updated>2018-08-28T21:52:57Z</updated>
    <published>2018-02-22T12:39:59Z</published>
    <title>Learning Topic Models by Neighborhood Aggregation</title>
    <summary>  Topic models are frequently used in machine learning owing to their high
interpretability and modular structure. However, extending a topic model to
include a supervisory signal, to incorporate pre-trained word embedding vectors
and to include a nonlinear output function is not an easy task because one has
to resort to a highly intricate approximate inference procedure. The present
paper shows that topic modeling with pre-trained word embedding vectors can be
viewed as implementing a neighborhood aggregation algorithm where messages are
passed through a network defined over words. From the network view of topic
models, nodes correspond to words in a document and edges correspond to either
a relationship describing co-occurring words in a document or a relationship
describing the same word in the corpus. The network view allows us to extend
the model to include supervisory signals, incorporate pre-trained word
embedding vectors and include a nonlinear output function in a simple manner.
In experiments, we show that our approach outperforms the state-of-the-art
supervised Latent Dirichlet Allocation implementation in terms of both held-out
document classification tasks and topic coherence.
</summary>
    <author>
      <name>Ryohei Hisano</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08012v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08012v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08156v4</id>
    <updated>2018-08-28T20:33:50Z</updated>
    <published>2018-06-21T10:23:23Z</published>
    <title>Identifiability of Gaussian Structural Equation Models with Dependent
  Errors Having Equal Variances</title>
    <summary>  In this paper, we prove that some Gaussian structural equation models with
dependent errors having equal variances are identifiable from their
corresponding Gaussian distributions. Specifically, we prove identifiability
for the Gaussian structural equation models that can be represented as
Andersson-Madigan-Perlman chain graphs (Andersson et al., 2001). These chain
graphs were originally developed to represent independence models. However,
they are also suitable for representing causal models with additive noise
(Pe\~na, 2016. Our result implies then that these causal models can be
identified from observational data alone. Our result generalizes the result by
Peters and B\"uhlmann (2014), who considered independent errors having equal
variances. The suitability of the equal error variances assumption should be
assessed on a per domain basis.
</summary>
    <author>
      <name>Jose M. Peña</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7th Causal Inference Workshop at UAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08156v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08156v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09517v1</id>
    <updated>2018-08-28T20:01:11Z</updated>
    <published>2018-08-28T20:01:11Z</published>
    <title>Extracting Epistatic Interactions in Type 2 Diabetes Genome-Wide Data
  Using Stacked Autoencoder</title>
    <summary>  2 Diabetes is a leading worldwide public health concern, and its increasing
prevalence has significant health and economic importance in all nations. The
condition is a multifactorial disorder with a complex aetiology. The genetic
determinants remain largely elusive, with only a handful of identified
candidate genes. Genome wide association studies (GWAS) promised to
significantly enhance our understanding of genetic based determinants of common
complex diseases. To date, 83 single nucleotide polymorphisms (SNPs) for type 2
diabetes have been identified using GWAS. Standard statistical tests for single
and multi-locus analysis such as logistic regression, have demonstrated little
effect in understanding the genetic architecture of complex human diseases.
Logistic regression is modelled to capture linear interactions but neglects the
non-linear epistatic interactions present within genetic data. There is an
urgent need to detect epistatic interactions in complex diseases as this may
explain the remaining missing heritability in such diseases. In this paper, we
present a novel framework based on deep learning algorithms that deal with
non-linear epistatic interactions that exist in genome wide association data.
Logistic association analysis under an additive genetic model, adjusted for
genomic control inflation factor, is conducted to remove statistically
improbable SNPs to minimize computational overheads.
</summary>
    <author>
      <name>Basma Abdulaimma</name>
    </author>
    <author>
      <name>Paul Fergus</name>
    </author>
    <author>
      <name>Carl Chalmers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01494v1</id>
    <updated>2018-08-28T19:44:51Z</updated>
    <published>2018-08-28T19:44:51Z</published>
    <title>Interpretation of Natural Language Rules in Conversational Machine
  Reading</title>
    <summary>  Most work in machine reading focuses on question answering problems where the
answer is directly expressed in the text to read. However, many real-world
question answering problems require the reading of text not because it contains
the literal answer, but because it contains a recipe to derive an answer
together with the reader's background knowledge. One example is the task of
interpreting regulations to answer "Can I...?" or "Do I have to...?" questions
such as "I am working in Canada. Do I have to carry on paying UK National
Insurance?" after reading a UK government website about this topic. This task
requires both the interpretation of rules and the application of background
knowledge. It is further complicated due to the fact that, in practice, most
questions are underspecified, and a human assistant will regularly have to ask
clarification questions such as "How long have you been working abroad?" when
the answer cannot be directly derived from the question and text. In this
paper, we formalise this task and develop a crowd-sourcing strategy to collect
32k task instances based on real-world rules and crowd-generated questions and
scenarios. We analyse the challenges of this task and assess its difficulty by
evaluating the performance of rule-based and machine-learning baselines. We
observe promising results when no background knowledge is necessary, and
substantial room for improvement whenever background knowledge is needed.
</summary>
    <author>
      <name>Marzieh Saeidi</name>
    </author>
    <author>
      <name>Max Bartolo</name>
    </author>
    <author>
      <name>Patrick Lewis</name>
    </author>
    <author>
      <name>Sameer Singh</name>
    </author>
    <author>
      <name>Tim Rocktäschel</name>
    </author>
    <author>
      <name>Mike Sheldon</name>
    </author>
    <author>
      <name>Guillaume Bouchard</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.01494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09501v1</id>
    <updated>2018-08-28T19:15:12Z</updated>
    <published>2018-08-28T19:15:12Z</published>
    <title>Concentrated Differentially Private Gradient Descent with Adaptive
  per-Iteration Privacy Budget</title>
    <summary>  Iterative algorithms, like gradient descent, are common tools for solving a
variety of problems, such as model fitting. For this reason, there is interest
in creating differentially private versions of them. However, their conversion
to differentially private algorithms is often naive. For instance, a fixed
number of iterations are chosen, the privacy budget is split evenly among them,
and at each iteration, parameters are updated with a noisy gradient. In this
paper, we show that gradient-based algorithms can be improved by a more careful
allocation of privacy budget per iteration. Intuitively, at the beginning of
the optimization, gradients are expected to be large, so that they do not need
to be measured as accurately. However, as the parameters approach their optimal
values, the gradients decrease and hence need to be measured more accurately.
We add a basic line-search capability that helps the algorithm decide when more
accurate gradient measurements are necessary. Our gradient descent algorithm
works with the recently introduced zCDP version of differential privacy. It
outperforms prior algorithms for model fitting and is competitive with the
state-of-the-art for $(\epsilon,\delta)$-differential privacy, a strictly
weaker definition than zCDP.
</summary>
    <author>
      <name>Jaewoo Lee</name>
    </author>
    <author>
      <name>Daniel Kifer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09489v1</id>
    <updated>2018-08-28T18:47:20Z</updated>
    <published>2018-08-28T18:47:20Z</published>
    <title>Convergence of Krasulina Scheme</title>
    <summary>  Principal component analysis (PCA) is one of the most commonly used
statistical procedures with a wide range of applications. Consider the points
$X_1, X_2,..., X_n$ are vectors drawn i.i.d. from a distribution with mean zero
and covariance $\Sigma$, where $\Sigma$ is unknown. Let $A_n = X_nX_n^T$, then
$E[A_n] = \Sigma$. This paper consider the problem of finding the least
eigenvalue and eigenvector of matrix $\Sigma$. A classical such estimator are
due to Krasulina\cite{krasulina_method_1969}. We are going to state the
convergence proof of Krasulina for the least eigenvalue and corresponding
eigenvector, and then find their convergence rate.
</summary>
    <author>
      <name>Jiangning Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05689v2</id>
    <updated>2018-08-28T18:19:23Z</updated>
    <published>2018-08-16T22:02:15Z</published>
    <title>Graph Edit Distance Computation via Graph Neural Networks</title>
    <summary>  Graph similarity search is among the most important graph-based applications,
e.g. finding the chemical compounds that are most similar to a query compound.
Graph similarity/distance computation, such as Graph Edit Distance (GED) and
Maximum Common Subgraph (MCS), is the core operation of graph similarity search
and many other applications, which is usually very costly to compute. Inspired
by the recent success of neural network approaches to several graph
applications, such as node classification and graph classification, we propose
a novel neural network-based approach to address this challenging while
classical graph problem, with the hope to alleviate the computational burden
while preserving a good performance. Our model generalizes to unseen graphs,
and in the worst case runs in linear time with respect to the number of nodes
in two graphs. Taking GED computation as an example, experimental results on
three real graph datasets demonstrate the effectiveness and efficiency of our
approach. Specifically, our model achieves smaller error and great time
reduction compared against several approximate algorithms on GED computation.
To the best of our knowledge, we are among the first to adopt neural networks
to model the similarity between two graphs, and provide a new direction for
future research on graph similarity computation and graph similarity search.
</summary>
    <author>
      <name>Yunsheng Bai</name>
    </author>
    <author>
      <name>Hao Ding</name>
    </author>
    <author>
      <name>Song Bian</name>
    </author>
    <author>
      <name>Ting Chen</name>
    </author>
    <author>
      <name>Yizhou Sun</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05689v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05689v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07018v2</id>
    <updated>2018-08-28T17:50:05Z</updated>
    <published>2018-08-21T17:05:28Z</published>
    <title>Hypernetwork Knowledge Graph Embeddings</title>
    <summary>  Knowledge graphs are large graph-structured databases of facts, which
typically suffer from incompleteness. Link prediction is the task of inferring
missing relations (links) between entities (nodes) in a knowledge graph. We
propose to solve this task by using a hypernetwork architecture to generate
convolutional layer filters specific to each relation and apply those filters
to the subject entity embeddings. This architecture enables a trade-off between
non-linear expressiveness and the number of parameters to learn. Our model
simplifies the entity and relation embedding interactions introduced by the
predecessor convolutional model, while outperforming all previous approaches to
link prediction across all standard link prediction datasets.
</summary>
    <author>
      <name>Ivana Balazevic</name>
    </author>
    <author>
      <name>Carl Allen</name>
    </author>
    <author>
      <name>Timothy M. Hospedales</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08773v2</id>
    <updated>2018-08-28T17:30:39Z</updated>
    <published>2018-08-27T10:37:16Z</published>
    <title>Learning Multilingual Word Embeddings in Latent Metric Space: A
  Geometric Approach</title>
    <summary>  We propose a novel geometric approach for learning bilingual mappings given
monolingual embeddings and a bilingual dictionary. Our approach decouples
learning the transformation from the source language to the target language
into (a) learning rotations for language-specific embeddings to align them to a
common space, and (b) learning a similarity metric in the common space to model
similarities between the embeddings. We model the bilingual mapping problem as
an optimization problem on smooth Riemannian manifolds. We show that our
approach outperforms previous approaches on the bilingual lexicon induction and
cross-lingual word similarity tasks. We also generalize our framework to
represent multiple languages in a common latent space. In particular, the
latent space representations for several languages are learned jointly, given
bilingual dictionaries for multiple language pairs. We illustrate the
effectiveness of joint learning for multiple languages in zero-shot word
translation setting.
</summary>
    <author>
      <name>Pratik Jawanpuria</name>
    </author>
    <author>
      <name>Arjun Balgovind</name>
    </author>
    <author>
      <name>Anoop Kunchukuttan</name>
    </author>
    <author>
      <name>Bamdev Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08773v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08773v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03846v4</id>
    <updated>2018-08-28T17:07:30Z</updated>
    <published>2017-11-08T19:00:29Z</published>
    <title>"Dave...I can assure you...that it's going to be all right..." -- A
  definition, case for, and survey of algorithmic assurances in human-autonomy
  trust relationships</title>
    <summary>  People who design, use, and are affected by autonomous artificially
intelligent agents want to be able to \emph{trust} such agents -- that is, to
know that these agents will perform correctly, to understand the reasoning
behind their actions, and to know how to use them appropriately. Many
techniques have been devised to assess and influence human trust in
artificially intelligent agents. However, these approaches are typically ad
hoc, and have not been formally related to each other or to formal trust
models. This paper presents a survey of \emph{algorithmic assurances}, i.e.
programmed components of agent operation that are expressly designed to
calibrate user trust in artificially intelligent agents. Algorithmic assurances
are first formally defined and classified from the perspective of formally
modeled human-artificially intelligent agent trust relationships. Building on
these definitions, a synthesis of research across communities such as machine
learning, human-computer interaction, robotics, e-commerce, and others reveals
that assurance algorithms naturally fall along a spectrum in terms of their
impact on an agent's core functionality, with seven notable classes ranging
from integral assurances (which impact an agent's core functionality) to
supplemental assurances (which have no direct effect on agent performance).
Common approaches within each of these classes are identified and discussed;
benefits and drawbacks of different approaches are also investigated.
</summary>
    <author>
      <name>Brett W Israelsen</name>
    </author>
    <author>
      <name>Nisar R Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">final version of accepted manuscript</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.03846v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03846v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09785v1</id>
    <updated>2018-08-28T15:53:14Z</updated>
    <published>2018-08-28T15:53:14Z</published>
    <title>Using Taste Groups for Collaborative Filtering</title>
    <summary>  Implicit feedback is the simplest form of user feedback that can be used for
item recommendation. It is easy to collect and domain independent. However,
there is a lack of negative examples. Existing works circumvent this problem by
making various assumptions regarding the unconsumed items, which fail to hold
when the user did not consume an item because she was unaware of it. In this
paper, we propose as a novel method for addressing the lack of negative
examples in implicit feedback. The motivation is that if there is a large group
of users who share the same taste and none of them consumed an item, then it is
highly likely that the item is irrelevant to this taste. We use Hierarchical
Latent Tree Analysis(HLTA) to identify taste-based user groups and make
recommendations for a user based on her memberships in the groups.
</summary>
    <author>
      <name>Farhan Khawar</name>
    </author>
    <author>
      <name>Nevin L. Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RecSys 2018 LBRS. arXiv admin note: substantial text overlap with
  arXiv:1704.01889</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09372v1</id>
    <updated>2018-08-28T15:43:20Z</updated>
    <published>2018-08-28T15:43:20Z</published>
    <title>Mean Field Analysis of Neural Networks: A Central Limit Theorem</title>
    <summary>  Machine learning has revolutionized fields such as image, text, and speech
recognition. There's also growing interest in applying machine and deep
learning methods in science, engineering, medicine, and finance. Despite their
immense success in practice, there is limited mathematical understanding of
neural networks. We mathematically study neural networks in the asymptotic
regime of simultaneously (A) large network sizes and (B) large numbers of
stochastic gradient descent training iterations. We rigorously prove that the
neural network satisfies a central limit theorem. Our result describes the
neural network's fluctuations around its mean-field limit. The fluctuations
have a Gaussian distribution and satisfy a stochastic partial differential
equation.
</summary>
    <author>
      <name>Justin Sirignano</name>
    </author>
    <author>
      <name>Konstantinos Spiliopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60F05, 60G57, 62M45" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09371v1</id>
    <updated>2018-08-28T15:40:47Z</updated>
    <published>2018-08-28T15:40:47Z</published>
    <title>Matrix Factorization Equals Efficient Co-occurrence Representation</title>
    <summary>  Matrix factorization is a simple and effective solution to the recommendation
problem. It has been extensively employed in the industry and has attracted
much attention from the academia. However, it is unclear what the
low-dimensional matrices represent. We show that matrix factorization can
actually be seen as simultaneously calculating the eigenvectors of the
user-user and item-item sample co-occurrence matrices. We then use insights
from random matrix theory (RMT) to show that picking the top eigenvectors
corresponds to removing sampling noise from user/item co-occurrence matrices.
Therefore, the low-dimension matrices represent a reduced noise user and item
co-occurrence space. We also analyze the structure of the top eigenvector and
show that it corresponds to global effects and removing it results in less
popular items being recommended. This increases the diversity of the items
recommended without affecting the accuracy.
</summary>
    <author>
      <name>Farhan Khawar</name>
    </author>
    <author>
      <name>Nevin L. Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RecSys 2018 LBRS</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01653v2</id>
    <updated>2018-08-28T15:34:03Z</updated>
    <published>2018-04-05T02:23:59Z</published>
    <title>Review of Deep Learning</title>
    <summary>  In recent years, China, the United States and other countries, Google and
other high-tech companies have increased investment in artificial intelligence.
Deep learning is one of the current artificial intelligence research's key
areas. This paper analyzes and summarizes the latest progress and future
research directions of deep learning. Firstly, three basic models of deep
learning are outlined, including multilayer perceptrons, convolutional neural
networks, and recurrent neural networks. On this basis, we further analyze the
emerging new models of convolution neural networks and recurrent neural
networks. This paper then summarizes deep learning's applications in many areas
of artificial intelligence, including speech processing, computer vision,
natural language processing and so on. Finally, this paper discusses the
existing problems of deep learning and gives the corresponding possible
solutions.
</summary>
    <author>
      <name>Rong Zhang</name>
    </author>
    <author>
      <name>Weiping Li</name>
    </author>
    <author>
      <name>Tong Mo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Chinese. Have been published in the journal "Information and
  Control"</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01653v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01653v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09347v1</id>
    <updated>2018-08-28T15:04:32Z</updated>
    <published>2018-08-28T15:04:32Z</published>
    <title>Joint Domain Alignment and Discriminative Feature Learning for
  Unsupervised Deep Domain Adaptation</title>
    <summary>  Recently, considerable effort has been devoted to deep domain adaptation in
computer vision and machine learning communities. However, most of existing
work only concentrates on learning shared feature representation by minimizing
the distribution discrepancy across different domains. Due to the fact that all
the domain alignment approaches can only reduce, but not remove the domain
shift. Target domain samples distributed near the edge of the clusters, or far
from their corresponding class centers are easily to be misclassified by the
hyperplane learned from the source domain. To alleviate this issue, we propose
to joint domain alignment and discriminative feature learning, which could
benefit both domain alignment and final classification. Specifically, an
instance-based discriminative feature learning method and a center-based
discriminative feature learning method are proposed, both of which guarantee
the domain invariant features with better intra-class compactness and
inter-class separability. Extensive experiments show that learning the
discriminative features in the shared feature space can significantly boost the
performance of deep domain adaptation methods.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Zhihong Chen</name>
    </author>
    <author>
      <name>Boyuan Jiang</name>
    </author>
    <author>
      <name>Xinyu Jin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09334v1</id>
    <updated>2018-08-28T14:47:33Z</updated>
    <published>2018-08-28T14:47:33Z</published>
    <title>A Discriminative Latent-Variable Model for Bilingual Lexicon Induction</title>
    <summary>  We introduce a novel discriminative latent-variable model for the task of
bilingual lexicon induction. Our model combines the bipartite matching
dictionary prior of Haghighi et al. (2008) with a state-of-the-art
embedding-based approach. To train the model, we derive an efficient Viterbi EM
algorithm. We provide empirical improvements on six language pairs under two
metrics and show that the prior theoretically and empirically helps to mitigate
the hubness problem. We also demonstrate how previous work may be viewed as a
similarly fashioned latent-variable model, albeit with a different prior.
</summary>
    <author>
      <name>Sebastian Ruder</name>
    </author>
    <author>
      <name>Ryan Cotterell</name>
    </author>
    <author>
      <name>Yova Kementchedjhieva</name>
    </author>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01216v3</id>
    <updated>2018-08-28T14:38:40Z</updated>
    <published>2016-12-05T01:40:36Z</published>
    <title>Decentralized Frank-Wolfe Algorithm for Convex and Non-convex Problems</title>
    <summary>  Decentralized optimization algorithms have received much attention due to the
recent advances in network information processing. However, conventional
decentralized algorithms based on projected gradient descent are incapable of
handling high dimensional constrained problems, as the projection step becomes
computationally prohibitive to compute. To address this problem, this paper
adopts a projection-free optimization approach, a.k.a.~the Frank-Wolfe (FW) or
conditional gradient algorithm. We first develop a decentralized FW (DeFW)
algorithm from the classical FW algorithm. The convergence of the proposed
algorithm is studied by viewing the decentralized algorithm as an inexact FW
algorithm. Using a diminishing step size rule and letting $t$ be the iteration
number, we show that the DeFW algorithm's convergence rate is ${\cal O}(1/t)$
for convex objectives; is ${\cal O}(1/t^2)$ for strongly convex objectives with
the optimal solution in the interior of the constraint set; and is ${\cal
O}(1/\sqrt{t})$ towards a stationary point for smooth but non-convex
objectives. We then show that a consensus-based DeFW algorithm meets the above
guarantees with two communication rounds per iteration. Furthermore, we
demonstrate the advantages of the proposed DeFW algorithm on low-complexity
robust matrix completion and communication efficient sparse learning. Numerical
results on synthetic and real data are presented to support our findings.
</summary>
    <author>
      <name>Hoi-To Wai</name>
    </author>
    <author>
      <name>Jean Lafond</name>
    </author>
    <author>
      <name>Anna Scaglione</name>
    </author>
    <author>
      <name>Eric Moulines</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TAC.2017.2685559</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TAC.2017.2685559" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE Transactions on Automatic Control. 33 pages, 7
  figures, include an improved constant in Lemma 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01216v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01216v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08646v2</id>
    <updated>2018-08-28T13:30:10Z</updated>
    <published>2018-08-27T00:54:02Z</published>
    <title>The Disparate Effects of Strategic Manipulation</title>
    <summary>  When consequential decisions are informed by algorithmic input, individuals
may feel compelled to alter their behavior in order to gain a system's
approval. Previous models of agent responsiveness, termed "strategic
manipulation," have analyzed the interaction between a learner and agents in a
world where all agents are equally able to manipulate their features in an
attempt to "trick" a published classifier. In cases of real world
classification, however, an agent's ability to adapt to an algorithm, is not
simply a function of her personal interest in receiving a positive
classification, but is bound up in a complex web of social factors that affect
her ability to pursue certain action responses. In this paper, we adapt models
of strategic manipulation to better capture dynamics that may arise in a
setting of social inequality wherein candidate groups face different costs to
manipulation. We find that whenever one group's costs are higher than the
other's, the learner's equilibrium strategy exhibits an inequality-reinforcing
phenomenon wherein the learner erroneously admits some members of the
advantaged group, while erroneously excluding some members of the disadvantaged
group. We also consider the effects of potential interventions in which a
learner can subsidize members of the disadvantaged group, lowering their costs
in order to improve her own classification performance. Here we encounter a
paradoxical result: there exist cases in which providing a subsidy improves
only the learner's utility while actually making both candidate groups
worse-off--even the group receiving the subsidy. Our results reveal the
potentially adverse social ramifications of deploying tools that attempt to
evaluate an individual's "quality" when agents' capacities to adaptively
respond differ.
</summary>
    <author>
      <name>Lily Hu</name>
    </author>
    <author>
      <name>Nicole Immorlica</name>
    </author>
    <author>
      <name>Jennifer Wortman Vaughan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08646v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08646v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09271v1</id>
    <updated>2018-08-28T13:14:33Z</updated>
    <published>2018-08-28T13:14:33Z</published>
    <title>Distance Based Source Domain Selection for Sentiment Classification</title>
    <summary>  Automated sentiment classification (SC) on short text fragments has received
increasing attention in recent years. Performing SC on unseen domains with few
or no labeled samples can significantly affect the classification performance
due to different expression of sentiment in source and target domain. In this
study, we aim to mitigate this undesired impact by proposing a methodology
based on a predictive measure, which allows us to select an optimal source
domain from a set of candidates. The proposed measure is a linear combination
of well-known distance functions between probability distributions supported on
the source and target domains (e.g. Earth Mover's distance and Kullback-Leibler
divergence). The performance of the proposed methodology is validated through
an SC case study in which our numerical experiments suggest a significant
improvement in the cross domain classification error in comparison with a
random selected source domain for both a naive and adaptive learning setting.
In the case of more heterogeneous datasets, the predictability feature of the
proposed model can be utilized to further select a subset of candidate domains,
where the corresponding classifier outperforms the one trained on all available
source domains. This observation reinforces a hypothesis that our proposed
model may also be deployed as a means to filter out redundant information
during a training phase of SC.
</summary>
    <author>
      <name>Lex Razoux Schultz</name>
    </author>
    <author>
      <name>Marco Loog</name>
    </author>
    <author>
      <name>Peyman Mohajerin Esfahani</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09784v1</id>
    <updated>2018-08-28T13:07:34Z</updated>
    <published>2018-08-28T13:07:34Z</published>
    <title>Superhighway: Bypass Data Sparsity in Cross-Domain CF</title>
    <summary>  Cross-domain collaborative filtering (CF) aims to alleviate data sparsity in
single-domain CF by leveraging knowledge transferred from related domains. Many
traditional methods focus on enriching compared neighborhood relations in CF
directly to address the sparsity problem. In this paper, we propose
superhighway construction, an alternative explicit relation-enrichment
procedure, to improve recommendations by enhancing cross-domain connectivity.
Specifically, assuming partially overlapped items (users), superhighway
bypasses multi-hop inter-domain paths between cross-domain users (items,
respectively) with direct paths to enrich the cross-domain connectivity. The
experiments conducted on a real-world cross-region music dataset and a
cross-platform movie dataset show that the proposed superhighway construction
significantly improves recommendation performance in both target and source
domains.
</summary>
    <author>
      <name>Kwei-Herng Lai</name>
    </author>
    <author>
      <name>Ting-Hsiang Wang</name>
    </author>
    <author>
      <name>Heng-Yu Chi</name>
    </author>
    <author>
      <name>Yian Chen</name>
    </author>
    <author>
      <name>Ming-Feng Tsai</name>
    </author>
    <author>
      <name>Chuan-Ju Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08568v2</id>
    <updated>2018-08-28T11:13:40Z</updated>
    <published>2018-06-22T09:22:42Z</published>
    <title>Continuous Learning in Single-Incremental-Task Scenarios</title>
    <summary>  It was recently shown that architectural, regularization and rehearsal
strategies can be used to train deep models sequentially on a number of
disjoint tasks without forgetting previously acquired knowledge. However, these
strategies are still unsatisfactory if the tasks are not disjoint but
constitute a single incremental task (e.g., class-incremental learning). In
this paper we point out the differences between multi-task and
single-incremental-task scenarios and show that well-known approaches such as
LWF, EWC and SI are not ideal for incremental task scenarios. A new approach,
denoted as AR1, combining architectural and regularization strategies is then
specifically proposed. AR1 overhead (in term of memory and computation) is very
small thus making it suitable for online learning. When tested on CORe50 and
iCIFAR-100, AR1 outperformed existing regularization strategies by a good
margin.
</summary>
    <author>
      <name>Davide Maltoni</name>
    </author>
    <author>
      <name>Vincenzo Lomonaco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 14 figures; v2: several typos and minor mistakes corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08568v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08568v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09222v1</id>
    <updated>2018-08-28T11:09:02Z</updated>
    <published>2018-08-28T11:09:02Z</published>
    <title>Making \emph{ordinary least squares} linear classfiers more robust</title>
    <summary>  In the field of statistics and machine learning, the sums-of-squares,
commonly referred to as \emph{ordinary least squares}, can be used as a
convenient choice of cost function because of its many nice analytical
properties, though not always the best choice. However, it has been long known
that \emph{ordinary least squares} is not robust to outliers. Several attempts
to resolve this problem led to the creation of alternative methods that, either
did not fully resolved the \emph{outlier problem} or were computationally
difficult. In this paper, we provide a very simple solution that can make
\emph{ordinary least squares} less sensitive to outliers in data
classification, by \emph{scaling the augmented input vector by its length}. We
show some mathematical expositions of the \emph{outlier problem} using some
approximations and geometrical techniques. We present numerical results to
support the efficacy of our method.
</summary>
    <author>
      <name>Babatunde M. Ayeni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages with 6 figures. Comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00177v2</id>
    <updated>2018-08-28T09:08:32Z</updated>
    <published>2018-08-01T06:02:36Z</published>
    <title>Learning Dexterous In-Hand Manipulation</title>
    <summary>  We use reinforcement learning (RL) to learn dexterous in-hand manipulation
policies which can perform vision-based object reorientation on a physical
Shadow Dexterous Hand. The training is performed in a simulated environment in
which we randomize many of the physical properties of the system like friction
coefficients and an object's appearance. Our policies transfer to the physical
robot despite being trained entirely in simulation. Our method does not rely on
any human demonstrations, but many behaviors found in human manipulation emerge
naturally, including finger gaiting, multi-finger coordination, and the
controlled use of gravity. Our results were obtained using the same distributed
RL system that was used to train OpenAI Five. We also include a video of our
results: https://youtu.be/jwSbzNHGflM
</summary>
    <author>
      <name> OpenAI</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Marcin Andrychowicz</name>
    </author>
    <author>
      <name>Bowen Baker</name>
    </author>
    <author>
      <name>Maciek Chociej</name>
    </author>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Bob McGrew</name>
    </author>
    <author>
      <name>Jakub Pachocki</name>
    </author>
    <author>
      <name>Arthur Petron</name>
    </author>
    <author>
      <name>Matthias Plappert</name>
    </author>
    <author>
      <name>Glenn Powell</name>
    </author>
    <author>
      <name>Alex Ray</name>
    </author>
    <author>
      <name>Jonas Schneider</name>
    </author>
    <author>
      <name>Szymon Sidor</name>
    </author>
    <author>
      <name>Josh Tobin</name>
    </author>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Lilian Weng</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures, minor typos fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00177v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00177v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09446v1</id>
    <updated>2018-08-28T08:30:12Z</updated>
    <published>2018-08-28T08:30:12Z</published>
    <title>A Particle Filter based Multi-Objective Optimization Algorithm: PFOPS</title>
    <summary>  This letter is concerned with a recently developed paradigm of
population-based optimization, termed particle filter optimization (PFO). In
contrast with the commonly used meta-heuristics based methods, the PFO paradigm
is attractive in terms of coherence in theory and easiness in mathematical
analysis and interpretation. However, current PFO algorithms only work for
single-objective optimization cases, while many real-life problems involve
multiple objectives to be optimized simultaneously. To this end, we make an
effort to extend the scope of application of the PFO paradigm to
multi-objective optimization (MOO) cases. An idea called path sampling is
adopted within the PFO scheme to balance the different objectives to be
optimized. The resulting algorithm is thus termed PFO with Path Sampling
(PFOPS). Experimental results show that the proposed algorithm works
consistently well for three different types of MOO problems, which are
characterized by an associated convex, concave and discontinuous Pareto front,
respectively.
</summary>
    <author>
      <name>Bin Liu</name>
    </author>
    <author>
      <name>Yaochu Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09144v1</id>
    <updated>2018-08-28T07:19:43Z</updated>
    <published>2018-08-28T07:19:43Z</published>
    <title>Weighted total variation based convex clustering</title>
    <summary>  Data clustering is a fundamental problem with a wide range of applications.
Standard methods, eg the $k$-means method, usually require solving a non-convex
optimization problem. Recently, total variation based convex relaxation to the
$k$-means model has emerged as an attractive alternative for data clustering.
However, the existing results on its exact clustering property, ie, the
condition imposed on data so that the method can provably give correct
identification of all cluster memberships, is only applicable to very specific
data and is also much more restrictive than that of some other methods. This
paper aims at the revisit of total variation based convex clustering, by
proposing a weighted sum-of-$\ell_1$-norm relating convex model. Its exact
clustering property established in this paper, in both deterministic and
probabilistic context, is applicable to general data and is much sharper than
the existing results. These results provided good insights to advance the
research on convex clustering. Moreover, the experiments also demonstrated that
the proposed convex model has better empirical performance when be compared to
standard clustering methods, and thus it can see its potential in practice.
</summary>
    <author>
      <name>Guodong Xu</name>
    </author>
    <author>
      <name>Yu Xia</name>
    </author>
    <author>
      <name>Hui Ji</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06561v2</id>
    <updated>2018-08-28T06:21:23Z</updated>
    <published>2018-04-18T05:31:45Z</published>
    <title>A Mean Field View of the Landscape of Two-Layers Neural Networks</title>
    <summary>  Multi-layer neural networks are among the most powerful models in machine
learning, yet the fundamental reasons for this success defy mathematical
understanding. Learning a neural network requires to optimize a non-convex
high-dimensional objective (risk function), a problem which is usually attacked
using stochastic gradient descent (SGD). Does SGD converge to a global optimum
of the risk or only to a local optimum? In the first case, does this happen
because local minima are absent, or because SGD somehow avoids them? In the
second, why do local minima reached by SGD have good generalization properties?
  In this paper we consider a simple case, namely two-layers neural networks,
and prove that -in a suitable scaling limit- SGD dynamics is captured by a
certain non-linear partial differential equation (PDE) that we call
distributional dynamics (DD). We then consider several specific examples, and
show how DD can be used to prove convergence of SGD to networks with nearly
ideal generalization error. This description allows to 'average-out' some of
the complexities of the landscape of neural networks, and can be used to prove
a general convergence result for noisy SGD.
</summary>
    <author>
      <name>Song Mei</name>
    </author>
    <author>
      <name>Andrea Montanari</name>
    </author>
    <author>
      <name>Phan-Minh Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">103 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09127v1</id>
    <updated>2018-08-28T05:39:48Z</updated>
    <published>2018-08-28T05:39:48Z</published>
    <title>High-confidence error estimates for learned value functions</title>
    <summary>  Estimating the value function for a fixed policy is a fundamental problem in
reinforcement learning. Policy evaluation algorithms---to estimate value
functions---continue to be developed, to improve convergence rates, improve
stability and handle variability, particularly for off-policy learning. To
understand the properties of these algorithms, the experimenter needs
high-confidence estimates of the accuracy of the learned value functions. For
environments with small, finite state-spaces, like chains, the true value
function can be easily computed, to compute accuracy. For large, or continuous
state-spaces, however, this is no longer feasible. In this paper, we address
the largely open problem of how to obtain these high-confidence estimates, for
general state-spaces. We provide a high-confidence bound on an empirical
estimate of the value error to the true value error. We use this bound to
design an offline sampling algorithm, which stores the required quantities to
repeatedly compute value error estimates for any learned value function. We
provide experiments investigating the number of samples required by this
offline algorithm in simple benchmark reinforcement learning domains, and
highlight that there are still many open questions to be solved for this
important problem.
</summary>
    <author>
      <name>Touqir Sajed</name>
    </author>
    <author>
      <name>Wesley Chung</name>
    </author>
    <author>
      <name>Martha White</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at (UAI) Uncertainty in Artificial Intelligence 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09123v1</id>
    <updated>2018-08-28T05:28:35Z</updated>
    <published>2018-08-28T05:28:35Z</published>
    <title>Investigating Human + Machine Complementarity for Recidivism Predictions</title>
    <summary>  When might human input help (or not) when assessing risk in fairness-related
domains? Dressel and Farid asked Mechanical Turk workers to evaluate a subset
of individuals in the ProPublica COMPAS data set for risk of recidivism, and
concluded that COMPAS predictions were no more accurate or fair than
predictions made by humans. We delve deeper into this claim in this paper. We
construct a Human Risk Score based on the predictions made by multiple
Mechanical Turk workers on the same individual, study the agreement and
disagreement between COMPAS and Human Scores on subgroups of individuals, and
construct hybrid Human+AI models to predict recidivism. Our key finding is that
on this data set, human and COMPAS decision making differed, but not in ways
that could be leveraged to significantly improve ground truth prediction. We
present the results of our analyses and suggestions for how machine and human
input may have complementary strengths to address challenges in the fairness
domain.
</summary>
    <author>
      <name>Sarah Tan</name>
    </author>
    <author>
      <name>Julius Adebayo</name>
    </author>
    <author>
      <name>Kori Inkpen</name>
    </author>
    <author>
      <name>Ece Kamar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09105v1</id>
    <updated>2018-08-28T03:48:25Z</updated>
    <published>2018-08-28T03:48:25Z</published>
    <title>SOLAR: Deep Structured Latent Representations for Model-Based
  Reinforcement Learning</title>
    <summary>  Model-based reinforcement learning (RL) methods can be broadly categorized as
global model methods, which depend on learning models that provide sensible
predictions in a wide range of states, or local model methods, which
iteratively refit simple models that are used for policy improvement. While
predicting future states that will result from the current actions is
difficult, local model methods only attempt to understand system dynamics in
the neighborhood of the current policy, making it possible to produce local
improvements without ever learning to predict accurately far into the future.
The main idea in this paper is that we can learn representations that make it
easy to retrospectively infer simple dynamics given the data from the current
policy, thus enabling local models to be used for policy learning in complex
systems. To that end, we focus on learning representations with probabilistic
graphical model (PGM) structure, which allows us to devise an efficient local
model method that infers dynamics from real-world rollouts with the PGM as a
global prior. We compare our method to other model-based and model-free RL
methods on a suite of robotics tasks, including manipulation tasks on a real
Sawyer robotic arm directly from camera images. Videos of our results are
available at https://sites.google.com/view/solar-iclips
</summary>
    <author>
      <name>Marvin Zhang</name>
    </author>
    <author>
      <name>Sharad Vikram</name>
    </author>
    <author>
      <name>Laura Smith</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Matthew J. Johnson</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02920v2</id>
    <updated>2018-08-28T03:34:14Z</updated>
    <published>2018-02-08T15:28:46Z</published>
    <title>Spectral State Compression of Markov Processes</title>
    <summary>  Model reduction of the Markov process is a basic problem in modeling
state-transition systems. Motivated by the state aggregation approach rooted in
control theory, we study the statistical state compression of a finite-state
Markov chain from empirical trajectories. Through the lens of spectral
decomposition, we study the rank and features of Markov processes, as well as
properties like representability, aggregatability, and lumpability. We develop
a class of spectral state compression methods for three tasks: (1) estimate the
transition matrix of a low-rank Markov model, (2) estimate the leading subspace
spanned by Markov features, and (3) recover latent structures of the state
space like state aggregation and lumpable partition. The proposed methods
provide an unsupervised learning framework for identifying Markov features and
clustering states. We provide upper bounds for the estimation errors and nearly
matching minimax lower bounds. Numerical studies are performed on synthetic
data and a dataset of New York City taxi trips.
</summary>
    <author>
      <name>Anru Zhang</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02920v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02920v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05697v2</id>
    <updated>2018-08-28T01:26:33Z</updated>
    <published>2018-08-16T22:46:40Z</published>
    <title>Deep Bayesian Active Learning for Natural Language Processing: Results
  of a Large-Scale Empirical Study</title>
    <summary>  Several recent papers investigate Active Learning (AL) for mitigating the
data dependence of deep learning for natural language processing. However, the
applicability of AL to real-world problems remains an open question. While in
supervised learning, practitioners can try many different methods, evaluating
each against a validation set before selecting a model, AL affords no such
luxury. Over the course of one AL run, an agent annotates its dataset
exhausting its labeling budget. Thus, given a new task, an active learner has
no opportunity to compare models and acquisition functions. This paper provides
a large scale empirical study of deep active learning, addressing multiple
tasks and, for each, multiple datasets, multiple models, and a full suite of
acquisition functions. We find that across all settings, Bayesian active
learning by disagreement, using uncertainty estimates provided either by
Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and
usually outperforms classic uncertainty sampling.
</summary>
    <author>
      <name>Aditya Siddhant</name>
    </author>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03740v2</id>
    <updated>2018-08-28T00:30:44Z</updated>
    <published>2018-04-10T22:27:21Z</published>
    <title>Multimodal Sparse Bayesian Dictionary Learning</title>
    <summary>  The purpose of this paper is to address the problem of learning dictionaries
for multimodal datasets, i.e. datasets collected from multiple data sources. We
present an algorithm called multimodal sparse Bayesian dictionary learning
(MSBDL). MSBDL leverages information from all available data modalities through
a joint sparsity constraint. The underlying framework offers a considerable
amount of flexibility to practitioners and addresses many of the shortcomings
of existing multimodal dictionary learning approaches. In particular, the
procedure includes the automatic tuning of hyperparameters and is unique in
that it allows the dictionaries for each data modality to have different
cardinality, a significant feature in cases when the dimensionality of data
differs across modalities. MSBDL is scalable and can be used in supervised
learning settings. Theoretical results relating to the convergence of MSBDL are
presented and the numerical results provide evidence of the superior
performance on synthetic and real datasets compared to existing methods.
</summary>
    <author>
      <name>Igor Fedorov</name>
    </author>
    <author>
      <name>Bhaskar D. Rao</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03740v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03740v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09050v1</id>
    <updated>2018-08-27T22:26:00Z</updated>
    <published>2018-08-27T22:26:00Z</published>
    <title>Adversarial Feature Learning of Online Monitoring Data for Operation
  Reliability Assessment in Distribution Network</title>
    <summary>  With deployments of online monitoring systems in distribution networks,
massive amounts of data collected through them contain rich information on the
operating status of distribution networks. By leveraging the data, based on
bidirectional generative adversarial networks (BiGANs), we propose an
unsupervised approach for online distribution reliability assessment. It is
capable of discovering the latent structure and automatically learning the most
representative features of the spatio-temporal data in distribution networks in
an adversarial way and it does not rely on any assumptions of the input data.
Based on the extracted features, a statistical magnitude for them is calculated
to indicate the data behavior. Furthermore, distribution reliability states are
divided into different levels and we combine them with the calculated
confidence level $1-\alpha$, during which clear criteria is defined
empirically. Case studies on both synthetic data and real-world online
monitoring data show that our proposed approach is feasible for the assessment
of distribution operation reliability and outperforms other existed techniques.
</summary>
    <author>
      <name>Xin Shi</name>
    </author>
    <author>
      <name>Robert Qiu</name>
    </author>
    <author>
      <name>Tiebin Mi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02658v3</id>
    <updated>2018-08-27T22:25:54Z</updated>
    <published>2017-04-09T20:43:55Z</published>
    <title>Distributed Statistical Estimation and Rates of Convergence in Normal
  Approximation</title>
    <summary>  This paper presents a class of new algorithms for distributed statistical
estimation that exploit divide-and-conquer approach. We show that one of the
key benefits of the divide-and-conquer strategy is robustness, an important
characteristic for large distributed systems. We establish connections between
performance of these distributed algorithms and the rates of convergence in
normal approximation, and prove non-asymptotic deviations guarantees, as well
as limit theorems, for the resulting estimators. Our techniques are illustrated
through several examples: in particular, we obtain new results for the
median-of-means estimator, as well as provide performance guarantees for
distributed maximum likelihood estimation.
</summary>
    <author>
      <name>Stanislav Minsker</name>
    </author>
    <author>
      <name>Nate Strawn</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02658v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02658v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W15, 62G35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10173v2</id>
    <updated>2018-08-27T21:15:33Z</updated>
    <published>2018-07-26T14:47:52Z</published>
    <title>Differential Analysis of Directed Networks</title>
    <summary>  We developed a novel statistical method to identify structural differences
between networks characterized by structural equation models. We propose to
reparameterize the model to separate the differential structures from common
structures, and then design an algorithm with calibration and construction
stages to identify these differential structures. The calibration stage serves
to obtain consistent prediction by building the L2 regularized regression of
each endogenous variables against pre-screened exogenous variables, correcting
for potential endogeneity issue. The construction stage consistently selects
and estimates both common and differential effects by undertaking L1
regularized regression of each endogenous variable against the predicts of
other endogenous variables as well as its anchoring exogenous variables. Our
method allows easy parallel computation at each stage. Theoretical results are
obtained to establish nonasymptotic error bounds of predictions and estimates
at both stages, as well as the consistency of identified common and
differential effects. Our studies on synthetic data demonstrated that our
proposed method performed much better than independently constructing the
networks. A real data set is analyzed to illustrate the applicability of our
method.
</summary>
    <author>
      <name>Min Ren</name>
    </author>
    <author>
      <name>Dabao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, Proceedings of the 34th Conference on Uncertainty in
  Artificial Intelligence (UAI), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10173v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10173v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04572v2</id>
    <updated>2018-08-27T19:52:28Z</updated>
    <published>2018-03-12T23:27:06Z</published>
    <title>COPA: Constrained PARAFAC2 for Sparse &amp; Large Datasets</title>
    <summary>  PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.
</summary>
    <author>
      <name>Ardavan Afshar</name>
    </author>
    <author>
      <name>Ioakeim Perros</name>
    </author>
    <author>
      <name>Evangelos E. Papalexakis</name>
    </author>
    <author>
      <name>Elizabeth Searles</name>
    </author>
    <author>
      <name>Joyce Ho</name>
    </author>
    <author>
      <name>Jimeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08994v1</id>
    <updated>2018-08-27T18:59:43Z</updated>
    <published>2018-08-27T18:59:43Z</published>
    <title>Data Poisoning Attacks against Online Learning</title>
    <summary>  We consider data poisoning attacks, a class of adversarial attacks on machine
learning where an adversary has the power to alter a small fraction of the
training data in order to make the trained classifier satisfy certain
objectives. While there has been much prior work on data poisoning, most of it
is in the offline setting, and attacks for online learning, where training data
arrives in a streaming manner, are not well understood.
  In this work, we initiate a systematic investigation of data poisoning
attacks for online learning. We formalize the problem into two settings, and we
propose a general attack strategy, formulated as an optimization problem, that
applies to both with some modifications. We propose three solution strategies,
and perform extensive experimental evaluation. Finally, we discuss the
implications of our findings for building successful defenses.
</summary>
    <author>
      <name>Yizhen Wang</name>
    </author>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00209v3</id>
    <updated>2018-08-27T18:45:10Z</updated>
    <published>2017-03-01T10:13:52Z</published>
    <title>Online Natural Gradient as a Kalman Filter</title>
    <summary>  We cast Amari's natural gradient in statistical learning as a specific case
of Kalman filtering. Namely, applying an extended Kalman filter to estimate a
fixed unknown parameter of a probabilistic model from a series of observations,
is rigorously equivalent to estimating this parameter via an online stochastic
natural gradient descent on the log-likelihood of the observations.
  In the i.i.d. case, this relation is a consequence of the "information
filter" phrasing of the extended Kalman filter. In the recurrent (state space,
non-i.i.d.) case, we prove that the joint Kalman filter over states and
parameters is a natural gradient on top of real-time recurrent learning (RTRL),
a classical algorithm to train recurrent models.
  This exact algebraic correspondence provides relevant interpretations for
natural gradient hyperparameters such as learning rates or initialization and
regularization of the Fisher information matrix.
</summary>
    <author>
      <name>Yann Ollivier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd version: expanded intro</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00209v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00209v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06901v3</id>
    <updated>2018-08-27T18:00:08Z</updated>
    <published>2018-02-19T22:57:54Z</published>
    <title>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative
  Refinement</title>
    <summary>  We propose a conditional non-autoregressive neural sequence model based on
iterative refinement. The proposed model is designed based on the principles of
latent variable models and denoising autoencoders, and is generally applicable
to any sequence generation task. We extensively evaluate the proposed model on
machine translation (En-De and En-Ro) and image caption generation, and observe
that it significantly speeds up decoding while maintaining the generation
quality comparable to the autoregressive counterpart.
</summary>
    <author>
      <name>Jason Lee</name>
    </author>
    <author>
      <name>Elman Mansimov</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06901v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06901v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02963v4</id>
    <updated>2018-08-27T17:42:56Z</updated>
    <published>2017-07-10T17:48:28Z</published>
    <title>An Interactive Greedy Approach to Group Sparsity in High Dimensions</title>
    <summary>  Sparsity learning with known grouping structure has received considerable
attention due to wide modern applications in high-dimensional data analysis.
Although advantages of using group information have been well-studied by
shrinkage-based approaches, benefits of group sparsity have not been
well-documented for greedy-type methods, which much limits our understanding
and use of this important class of methods. In this paper, generalizing from a
popular forward-backward greedy approach, we propose a new interactive greedy
algorithm for group sparsity learning and prove that the proposed greedy-type
algorithm attains the desired benefits of group sparsity under high dimensional
settings. An estimation error bound refining other existing methods and a
guarantee for group support recovery are also established simultaneously. In
addition, we incorporate a general M-estimation framework and introduce an
interactive feature to allow extra algorithm flexibility without compromise in
theoretical properties. The promising use of our proposal is demonstrated
through numerical evaluations including a real industrial application in human
activity recognition at home. Supplementary materials for this article are
available online.
</summary>
    <author>
      <name>Wei Qian</name>
    </author>
    <author>
      <name>Wending Li</name>
    </author>
    <author>
      <name>Yasuhiro Sogawa</name>
    </author>
    <author>
      <name>Ryohei Fujimaki</name>
    </author>
    <author>
      <name>Xitong Yang</name>
    </author>
    <author>
      <name>Ji Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02963v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02963v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09270v1</id>
    <updated>2018-08-27T17:42:14Z</updated>
    <published>2018-08-27T17:42:14Z</published>
    <title>Models for Predicting Community-Specific Interest in News Articles</title>
    <summary>  In this work, we ask two questions: 1. Can we predict the type of community
interested in a news article using only features from the article content? and
2. How well do these models generalize over time? To answer these questions, we
compute well-studied content-based features on over 60K news articles from 4
communities on reddit.com. We train and test models over three different time
periods between 2015 and 2017 to demonstrate which features degrade in
performance the most due to concept drift. Our models can classify news
articles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0
ROC AUC. However, while we can predict the community-specific popularity of
news articles with high accuracy, practitioners should approach these models
carefully. Predictions are both community-pair dependent and feature group
dependent. Moreover, these feature groups generalize over time differently,
with some only degrading slightly over time, but others degrading greatly.
Therefore, we recommend that community-interest predictions are done in a
hierarchical structure, where multiple binary classifiers can be used to
separate community pairs, rather than a traditional multi-class model. Second,
these models should be retrained over time based on accuracy goals and the
availability of training data.
</summary>
    <author>
      <name>Benjamin D. Horne</name>
    </author>
    <author>
      <name>William Dron</name>
    </author>
    <author>
      <name>Sibel Adali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at IEEE MILCOM 2018 in Los Angeles, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08914v1</id>
    <updated>2018-08-27T16:34:51Z</updated>
    <published>2018-08-27T16:34:51Z</published>
    <title>Deep Learning for Stress Field Prediction Using Convolutional Neural
  Networks</title>
    <summary>  This research presents a deep learning based approach to predict stress
fields in the solid material elastic deformation using convolutional neural
networks (CNN). Two different architectures are proposed to solve the problem.
One is Feature Representation embedded Convolutional Neural Network (FR-CNN)
with a single input channel, and the other is Squeeze-and-Excitation Residual
network modules embedded Fully Convolutional Neural network (SE-Res-FCN) with
multiple input channels. Both the tow architectures are stable and converged
reliably in training and testing on GPUs. Accuracy analysis shows that
SE-Res-FCN has a significantly smaller mean squared error (MSE) and mean
absolute error (MAE) than FR-CNN. Mean relative error (MRE) of the SE-Res-FCN
model is about 0.25% with respect to the average ground truth. The validation
results indicate that the SE-Res-FCN model can accurately predict the stress
field. For stress field prediction, the hierarchical architecture becomes
deeper within certain limits, and then its prediction becomes more accurate.
Fully trained deep learning models have higher computational efficiency over
conventional FEM models, so they have great foreground and potential in
structural design and topology optimization.
</summary>
    <author>
      <name>Zhenguo Nie</name>
    </author>
    <author>
      <name>Haoliang Jiang</name>
    </author>
    <author>
      <name>Levent Burak Kara</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09889v1</id>
    <updated>2018-08-27T16:12:36Z</updated>
    <published>2018-08-27T16:12:36Z</published>
    <title>Zero-shot Transfer Learning for Semantic Parsing</title>
    <summary>  While neural networks have shown impressive performance on large datasets,
applying these models to tasks where little data is available remains a
challenging problem.
  In this paper we propose to use feature transfer in a zero-shot experimental
setting on the task of semantic parsing.
  We first introduce a new method for learning the shared space between
multiple domains based on the prediction of the domain label for each example.
  Our experiments support the superiority of this method in a zero-shot
experimental setting in terms of accuracy metrics compared to state-of-the-art
techniques.
  In the second part of this paper we study the impact of individual domains
and examples on semantic parsing performance.
  We use influence functions to this aim and investigate the sensitivity of
domain-label classification loss on each example.
  Our findings reveal that cross-domain adversarial attacks identify useful
examples for training even from the domains the least similar to the target
domain. Augmenting our training data with these influential examples further
boosts our accuracy at both the token and the sequence level.
</summary>
    <author>
      <name>Javid Dadashkarimi</name>
    </author>
    <author>
      <name>Alexander Fabbri</name>
    </author>
    <author>
      <name>Sekhar Tatikonda</name>
    </author>
    <author>
      <name>Dragomir R. Radev</name>
    </author>
    <link href="http://arxiv.org/abs/1808.09889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08871v1</id>
    <updated>2018-08-27T14:57:17Z</updated>
    <published>2018-08-27T14:57:17Z</published>
    <title>BézierGAN: Automatic Generation of Smooth Curves from Interpretable
  Low-Dimensional Parameters</title>
    <summary>  Many real-world objects are designed by smooth curves, especially in the
domain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and
hydrodynamic shapes (e.g., hulls) are designed. To facilitate the design
process of those objects, we propose a deep learning based generative model
that can synthesize smooth curves. The model maps a low-dimensional latent
representation to a sequence of discrete points sampled from a rational
B\'ezier curve. We demonstrate the performance of our method in completing both
synthetic and real-world generative tasks. Results show that our method can
generate diverse and realistic curves, while preserving consistent shape
variation in the latent space, which is favorable for latent space design
optimization or design space exploration.
</summary>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Mark Fuge</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08866v1</id>
    <updated>2018-08-27T14:43:38Z</updated>
    <published>2018-08-27T14:43:38Z</published>
    <title>A Study of Reinforcement Learning for Neural Machine Translation</title>
    <summary>  Recent studies have shown that reinforcement learning (RL) is an effective
approach for improving the performance of neural machine translation (NMT)
system. However, due to its instability, successfully RL training is
challenging, especially in real-world systems where deep models and large
datasets are leveraged. In this paper, taking several large-scale translation
tasks as testbeds, we conduct a systematic study on how to train better NMT
models using reinforcement learning. We provide a comprehensive comparison of
several important factors (e.g., baseline reward, reward shaping) in RL
training. Furthermore, to fill in the gap that it remains unclear whether RL is
still beneficial when monolingual data is used, we propose a new method to
leverage RL to further boost the performance of NMT systems trained with
source/target monolingual data. By integrating all our findings, we obtain
competitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17
Chinese-English translation tasks, especially setting a state-of-the-art
performance on WMT17 Chinese-English translation task.
</summary>
    <author>
      <name>Lijun Wu</name>
    </author>
    <author>
      <name>Fei Tian</name>
    </author>
    <author>
      <name>Tao Qin</name>
    </author>
    <author>
      <name>Jianhuang Lai</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07954v3</id>
    <updated>2018-08-27T13:53:56Z</updated>
    <published>2017-10-22T14:59:08Z</published>
    <title>Bayesian Cluster Enumeration Criterion for Unsupervised Learning</title>
    <summary>  We derive a new Bayesian Information Criterion (BIC) by formulating the
problem of estimating the number of clusters in an observed data set as
maximization of the posterior probability of the candidate models. Given that
some mild assumptions are satisfied, we provide a general BIC expression for a
broad class of data distributions. This serves as a starting point when
deriving the BIC for specific distributions. Along this line, we provide a
closed-form BIC expression for multivariate Gaussian distributed variables. We
show that incorporating the data structure of the clustering problem into the
derivation of the BIC results in an expression whose penalty term is different
from that of the original BIC. We propose a two-step cluster enumeration
algorithm. First, a model-based unsupervised learning algorithm partitions the
data according to a given set of candidate models. Subsequently, the number of
clusters is determined as the one associated with the model for which the
proposed BIC is maximal. The performance of the proposed two-step algorithm is
tested using synthetic and real data sets.
</summary>
    <author>
      <name>Freweyni K. Teklehaymanot</name>
    </author>
    <author>
      <name>Michael Muma</name>
    </author>
    <author>
      <name>Abdelhak M. Zoubir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2018.2866385</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2018.2866385" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.07954v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07954v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08833v1</id>
    <updated>2018-08-27T13:08:43Z</updated>
    <published>2018-08-27T13:08:43Z</published>
    <title>Gradient-based Training of Slow Feature Analysis by Differentiable
  Approximate Whitening</title>
    <summary>  This paper proposes Power Slow Feature Analysis, a gradient-based method to
extract temporally-slow features from a high-dimensional input stream that
varies on a faster time-scale, and a variant of Slow Feature Analysis (SFA).
While displaying performance comparable to hierarchical extensions to the SFA
algorithm, such as Hierarchical Slow Feature Analysis, for a small number of
output-features, our algorithm allows end-to-end training of arbitrary
differentiable approximators (e.g., deep neural networks). We provide
experimental evidence that PowerSFA is able to extract meaningful and
informative low-dimensional features in the case of a) synthetic
low-dimensional data, b) visual data, and also for c) a general dataset for
which symmetric non-temporal relations between points can be defined.
</summary>
    <author>
      <name>Merlin Schüler</name>
    </author>
    <author>
      <name>Hlynur Davíð Hlynsson</name>
    </author>
    <author>
      <name>Laurenz Wiskott</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05784v2</id>
    <updated>2018-08-27T12:59:53Z</updated>
    <published>2018-08-17T07:54:35Z</published>
    <title>Multiview Boosting by Controlling the Diversity and the Accuracy of
  View-specific Voters</title>
    <summary>  In this paper we propose a boosting based multiview learning algorithm,
referred to as PB-MVBoost, which iteratively learns i) weights over
view-specific voters capturing view-specific information; and ii) weights over
views by optimizing a PAC-Bayes multiview C-Bound that takes into account the
accuracy of view-specific classifiers and the diversity between the views. We
derive a generalization bound for this strategy following the PAC-Bayes theory
which is a suitable tool to deal with models expressed as weighted combination
over a set of voters. Different experiments on three publicly available
datasets show the efficiency of the proposed approach with respect to
state-of-art models.
</summary>
    <author>
      <name>Anil Goyal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AMA, LHC</arxiv:affiliation>
    </author>
    <author>
      <name>Emilie Morvant</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LHC</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Germain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MODAL</arxiv:affiliation>
    </author>
    <author>
      <name>Massih-Reza Amini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AMA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1808.05784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04476v4</id>
    <updated>2018-08-27T12:59:00Z</updated>
    <published>2017-07-14T12:07:22Z</published>
    <title>Collaborative Nested Sampling: Big Data vs. complex physical models</title>
    <summary>  The data torrent unleashed by current and upcoming instruments requires
scalable analysis methods. Machine Learning approaches scale well. However,
separating the instrument measurement from the physical effects of interest,
dealing with variable errors, and deriving parameter uncertainties is usually
an afterthought. Classic forward-folding analyses with Markov Chain Monte Carlo
or Nested Sampling enable parameter estimation and model comparison, even for
complex and slow-to-evaluate physical models. However, these approaches require
independent runs for each data set, implying an unfeasible number of model
evaluations in the Big Data regime. Here we present a new algorithm,
collaborative nested sampling, for deriving parameter probability distributions
for each observation. Importantly, in our method the number of physical model
evaluations scales sub-linearly with the number of data sets, and we make no
assumptions about homogeneous errors, Gaussianity, the form of the model or
heterogeneity/completeness of the observations. Collaborative nested sampling
has immediate application in speeding up analyses of large surveys,
integral-field-unit observations, and Monte Carlo simulations.
</summary>
    <author>
      <name>Johannes Buchner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to PASP Focus on Machine Intelligence in Astronomy and
  Astrophysics. Figure 6 demonstrates the scaling, Figure 10 application to IFU
  data. Implementation at https://github.com/JohannesBuchner/massivedatans</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04476v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04476v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08811v1</id>
    <updated>2018-08-27T12:15:14Z</updated>
    <published>2018-08-27T12:15:14Z</published>
    <title>Exponential inequalities for nonstationary Markov Chains</title>
    <summary>  Exponential inequalities are main tools in machine learning theory. To prove
exponential inequalities for non i.i.d random variables allows to extend many
learning techniques to these variables. Indeed, much work has been done both on
inequalities and learning theory for time series, in the past 15 years.
However, for the non independent case, almost all the results concern
stationary time series. This excludes many important applications: for example
any series with a periodic behaviour is non-stationary. In this paper, we
extend the basic tools of Dedecker and Fan (2015) to nonstationary Markov
chains. As an application, we provide a Bernstein-type inequality, and we
deduce risk bounds for the prediction of periodic autoregressive processes with
an unknown period.
</summary>
    <author>
      <name>Pierre Alquier</name>
    </author>
    <author>
      <name>Paul Doukhan</name>
    </author>
    <author>
      <name>Xiequan Fan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08798v1</id>
    <updated>2018-08-27T11:49:44Z</updated>
    <published>2018-08-27T11:49:44Z</published>
    <title>Beyond expectation: Deep joint mean and quantile regression for
  spatio-temporal problems</title>
    <summary>  Spatio-temporal problems are ubiquitous and of vital importance in many
research fields. Despite the potential already demonstrated by deep learning
methods in modeling spatio-temporal data, typical approaches tend to focus
solely on conditional expectations of the output variables being modeled. In
this paper, we propose a multi-output multi-quantile deep learning approach for
jointly modeling several conditional quantiles together with the conditional
expectation as a way to provide a more complete "picture" of the predictive
density in spatio-temporal problems. Using two large-scale datasets from the
transportation domain, we empirically demonstrate that, by approaching the
quantile regression problem from a multi-task learning perspective, it is
possible to solve the embarrassing quantile crossings problem, while
simultaneously significantly outperforming state-of-the-art quantile regression
methods. Moreover, we show that jointly modeling the mean and several
conditional quantiles not only provides a rich description about the predictive
density that can capture heteroscedastic properties at a neglectable
computational overhead, but also leads to improved predictions of the
conditional expectation due to the extra information and a regularization
effect induced by the added quantiles.
</summary>
    <author>
      <name>Filipe Rodrigues</name>
    </author>
    <author>
      <name>Francisco C. Pereira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08784v1</id>
    <updated>2018-08-27T11:12:14Z</updated>
    <published>2018-08-27T11:12:14Z</published>
    <title>Sparsity in Deep Neural Networks - An Empirical Investigation with
  TensorQuant</title>
    <summary>  Deep learning is finding its way into the embedded world with applications
such as autonomous driving, smart sensors and aug- mented reality. However, the
computation of deep neural networks is demanding in energy, compute power and
memory. Various approaches have been investigated to reduce the necessary
resources, one of which is to leverage the sparsity occurring in deep neural
networks due to the high levels of redundancy in the network parameters. It has
been shown that sparsity can be promoted specifically and the achieved sparsity
can be very high. But in many cases the methods are evaluated on rather small
topologies. It is not clear if the results transfer onto deeper topologies. In
this paper, the TensorQuant toolbox has been extended to offer a platform to
investigate sparsity, especially in deeper models. Several practical relevant
topologies for varying classification problem sizes are investigated to show
the differences in sparsity for activations, weights and gradients.
</summary>
    <author>
      <name>Dominik Marek Loroch</name>
    </author>
    <author>
      <name>Franz-Josef Pfreundt</name>
    </author>
    <author>
      <name>Norbert Wehn</name>
    </author>
    <author>
      <name>Janis Keuper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECML18, Decentralized Machine Learning at the Edge workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08782v1</id>
    <updated>2018-08-27T11:04:55Z</updated>
    <published>2018-08-27T11:04:55Z</published>
    <title>Amobee at IEST 2018: Transfer Learning from Language Models</title>
    <summary>  This paper describes the system developed at Amobee for the WASSA 2018
implicit emotions shared task (IEST). The goal of this task was to predict the
emotion expressed by missing words in tweets without an explicit mention of
those words. We developed an ensemble system consisting of language models
together with LSTM-based networks containing a CNN attention mechanism. Our
approach represents a novel use of language models (specifically trained on a
large Twitter dataset) to predict and classify emotions. Our system reached 1st
place with a macro $\text{F}_1$ score of 0.7145.
</summary>
    <author>
      <name>Alon Rozental</name>
    </author>
    <author>
      <name>Daniel Fleischer</name>
    </author>
    <author>
      <name>Zohar Kelrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, accepted to the 9th Workshop on Computational Approaches to
  Subjectivity, Sentiment &amp; Social Media Analysis, part of the EMNLP 2018
  Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07607v2</id>
    <updated>2018-08-27T10:06:23Z</updated>
    <published>2016-07-26T09:26:20Z</published>
    <title>Adaptive Nonnegative Matrix Factorization and Measure Comparisons for
  Recommender Systems</title>
    <summary>  The Nonnegative Matrix Factorization (NMF) of the rating matrix has shown to
be an effective method to tackle the recommendation problem. In this paper we
propose new methods based on the NMF of the rating matrix and we compare them
with some classical algorithms such as the SVD and the regularized and
unregularized non-negative matrix factorization approach. In particular a new
algorithm is obtained changing adaptively the function to be minimized at each
step, realizing a sort of dynamic prior strategy. Another algorithm is obtained
modifying the function to be minimized in the NMF formulation by enforcing the
reconstruction of the unknown ratings toward a prior term. We then combine
different methods obtaining two mixed strategies which turn out to be very
effective in the reconstruction of missing observations. We perform a
thoughtful comparison of different methods on the basis of several evaluation
measures. We consider in particular rating, classification and ranking measures
showing that the algorithm obtaining the best score for a given measure is in
general the best also when different measures are considered, lowering the
interest in designing specific evaluation measures. The algorithms have been
tested on different datasets, in particular the 1M, and 10M MovieLens datasets
containing ratings on movies, the Jester dataset with ranting on jokes and
Amazon Fine Foods dataset with ratings on foods. The comparison of the
different algorithms, shows the good performance of methods employing both an
explicit and an implicit regularization scheme. Moreover we can get a boost by
mixed strategies combining a fast method with a more accurate one.
</summary>
    <author>
      <name>Gianna M. Del Corso</name>
    </author>
    <author>
      <name>Francesco Romani</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65F99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08766v1</id>
    <updated>2018-08-27T10:06:01Z</updated>
    <published>2018-08-27T10:06:01Z</published>
    <title>Learning behavioral context recognition with multi-stream temporal
  convolutional networks</title>
    <summary>  Smart devices of everyday use (such as smartphones and wearables) are
increasingly integrated with sensors that provide immense amounts of
information about a person's daily life such as behavior and context. The
automatic and unobtrusive sensing of behavioral context can help develop
solutions for assisted living, fitness tracking, sleep monitoring, and several
other fields. Towards addressing this issue, we raise the question: can a
machine learn to recognize a diverse set of contexts and activities in a
real-life through joint learning from raw multi-modal signals (e.g.
accelerometer, gyroscope and audio etc.)? In this paper, we propose a
multi-stream temporal convolutional network to address the problem of
multi-label behavioral context recognition. A four-stream network architecture
handles learning from each modality with a contextualization module which
incorporates extracted representations to infer a user's context. Our empirical
evaluation suggests that a deep convolutional network trained end-to-end
achieves an optimal recognition rate. Furthermore, the presented architecture
can be extended to include similar sensors for performance improvements and
handles missing modalities through multi-task learning without any manual
feature engineering on highly imbalanced and sparsely labeled dataset.
</summary>
    <author>
      <name>Aaqib Saeed</name>
    </author>
    <author>
      <name>Tanir Ozcelebi</name>
    </author>
    <author>
      <name>Stojan Trajanovski</name>
    </author>
    <author>
      <name>Johan Lukkien</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08765v1</id>
    <updated>2018-08-27T10:04:36Z</updated>
    <published>2018-08-27T10:04:36Z</published>
    <title>Identifiability of Low-Rank Sparse Component Analysis</title>
    <summary>  Sparse component analysis (SCA) is the following problem: Given an input
matrix $M$ and an integer $r$, find a dictionary $D$ with $r$ columns and a
sparse matrix $B$ with $r$ rows such that $M \approx DB$. A key issue in SCA is
identifiability, that is, characterizing the conditions under which $D$ and $B$
are essentially unique (that is, they are unique up to permutation and scaling
of the columns of $D$ and rows of $B$). Although SCA has been vastly
investigated in the last two decades, only a few works have tackled this issue
in the deterministic scenario, and no work provides reasonable bounds in the
minimum number of data points (that is, columns of $M$) that leads to
identifiability. In this work, we provide new results in the deterministic
scenario when the data has a low-rank structure, that is, when $D$ has rank
$r$, drastically improving with respect to previous results. In particular, we
show that if each column of $B$ contains at least $s$ zeros then
$\mathcal{O}(r^3/s^2)$ data points are sufficient to obtain an essentially
unique decomposition, as long as these data points are well spread among the
subspaces spanned by $r-1$ columns of $D$. This implies for example that for a
fixed proportion of zeros (constant and independent of $r$, e.g., 10\% of zero
entries in $B$), one only requires $O(r)$ data points to guarantee
identifiability.
</summary>
    <author>
      <name>Jérémy E. Cohen</name>
    </author>
    <author>
      <name>Nicolas Gillis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08755v1</id>
    <updated>2018-08-27T09:39:52Z</updated>
    <published>2018-08-27T09:39:52Z</published>
    <title>Learning from Positive and Unlabeled Data under the Selected At Random
  Assumption</title>
    <summary>  For many interesting tasks, such as medical diagnosis and web page
classification, a learner only has access to some positively labeled examples
and many unlabeled examples. Learning from this type of data requires making
assumptions about the true distribution of the classes and/or the mechanism
that was used to select the positive examples to be labeled. The commonly made
assumptions, separability of the classes and positive examples being selected
completely at random, are very strong. This paper proposes a weaker assumption
that assumes the positive examples to be selected at random, conditioned on
some of the attributes. To learn under this assumption, an EM method is
proposed. Experiments show that our method is not only very capable of learning
under this assumption, but it also outperforms the state of the art for
learning under the selected completely at random assumption.
</summary>
    <author>
      <name>Jessa Bekker</name>
    </author>
    <author>
      <name>Jesse Davis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08750v1</id>
    <updated>2018-08-27T09:17:57Z</updated>
    <published>2018-08-27T09:17:57Z</published>
    <title>Generalisation in humans and deep neural networks</title>
    <summary>  We compare the robustness of humans and current convolutional deep neural
networks (DNNs) on object recognition under twelve different types of image
degradations. First, using three well known DNNs (ResNet-152, VGG-19,
GoogLeNet) we find the human visual system to be more robust to nearly all of
the tested image manipulations, and we observe progressively diverging
classification error-patterns between humans and DNNs when the signal gets
weaker. Secondly, we show that DNNs trained directly on distorted images
consistently surpass human performance on the exact distortion types they were
trained on, yet they display extremely poor generalisation abilities when
tested on other distortion types. For example, training on salt-and-pepper
noise does not imply robustness on uniform white noise and vice versa. Thus,
changes in the noise distribution between training and testing constitutes a
crucial challenge to deep learning vision systems that can be systematically
addressed in a lifelong machine learning approach. Our new dataset consisting
of 83K carefully measured human psychophysical trials provide a useful
reference for lifelong robustness against image degradations set by the human
visual system.
</summary>
    <author>
      <name>Robert Geirhos</name>
    </author>
    <author>
      <name>Carlos R. Medina Temme</name>
    </author>
    <author>
      <name>Jonas Rauber</name>
    </author>
    <author>
      <name>Heiko H. Schuett</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <author>
      <name>Felix A. Wichmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to NIPS 2018. 26 pages, 14 figures, 1 table. Supersedes and
  greatly extends arXiv:1706.06969</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08720v1</id>
    <updated>2018-08-27T07:55:41Z</updated>
    <published>2018-08-27T07:55:41Z</published>
    <title>Predefined Sparseness in Recurrent Sequence Models</title>
    <summary>  Inducing sparseness while training neural networks has been shown to yield
models with a lower memory footprint but similar effectiveness to dense models.
However, sparseness is typically induced starting from a dense model, and thus
this advantage does not hold during training. We propose techniques to enforce
sparseness upfront in recurrent sequence models for NLP applications, to also
benefit training. First, in language modeling, we show how to increase hidden
state sizes in recurrent layers without increasing the number of parameters,
leading to more expressive models. Second, for sequence labeling, we show that
word embeddings with predefined sparseness lead to similar performance as dense
embeddings, at a fraction of the number of trainable parameters.
</summary>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <author>
      <name>Johannes Deleu</name>
    </author>
    <author>
      <name>Fréderic Godin</name>
    </author>
    <author>
      <name>Chris Develder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">the SIGNLL Conference on Computational Natural Language Learning
  (CoNLL, 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12547v3</id>
    <updated>2018-08-27T05:35:36Z</updated>
    <published>2018-05-31T16:36:26Z</published>
    <title>Long-time predictive modeling of nonlinear dynamical systems using
  neural networks</title>
    <summary>  We study the use of feedforward neural networks (FNN) to develop models of
nonlinear dynamical systems from data. Emphasis is placed on predictions at
long times, with limited data availability. Inspired by global stability
analysis, and the observation of the strong correlation between the local error
and the maximum singular value of the Jacobian of the ANN, we introduce
Jacobian regularization in the loss function. This regularization suppresses
the sensitivity of the prediction to the local error and is shown to improve
accuracy and robustness. Comparison between the proposed approach and sparse
polynomial regression is presented in numerical examples ranging from simple
ODE systems to nonlinear PDE systems including vortex shedding behind a
cylinder, and instability-driven buoyant mixing flow. Furthermore, limitations
of feedforward neural networks are highlighted, especially when the training
data does not include a low dimensional attractor. Strategies of data
augmentation are presented as remedies to address these issues to a certain
extent.
</summary>
    <author>
      <name>Shaowu Pan</name>
    </author>
    <author>
      <name>Karthik Duraisamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.12547v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12547v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="37M99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08716v2</id>
    <updated>2018-08-27T05:22:38Z</updated>
    <published>2018-07-23T16:50:31Z</published>
    <title>NullaNet: Training Deep Neural Networks for Reduced-Memory-Access
  Inference</title>
    <summary>  Deep neural networks have been successfully deployed in a wide variety of
applications including computer vision and speech recognition. However,
computational and storage complexity of these models has forced the majority of
computations to be performed on high-end computing platforms or on the cloud.
To cope with computational and storage complexity of these models, this paper
presents a training method that enables a radically different approach for
realization of deep neural networks through Boolean logic minimization. The
aforementioned realization completely removes the energy-hungry step of
accessing memory for obtaining model parameters, consumes about two orders of
magnitude fewer computing resources compared to realizations that use
floatingpoint operations, and has a substantially lower latency.
</summary>
    <author>
      <name>Mahdi Nazemi</name>
    </author>
    <author>
      <name>Ghasem Pasandi</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08716v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08716v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08149v2</id>
    <updated>2018-08-27T02:04:51Z</updated>
    <published>2018-08-24T14:17:01Z</published>
    <title>From Random to Supervised: A Novel Dropout Mechanism Integrated with
  Global Information</title>
    <summary>  Dropout is used to avoid overfitting by randomly dropping units from the
neural networks during training. Inspired by dropout, this paper presents
GI-Dropout, a novel dropout method integrating with global information to
improve neural networks for text classification. Unlike the traditional dropout
method in which the units are dropped randomly according to the same
probability, we aim to use explicit instructions based on global information of
the dataset to guide the training process. With GI-Dropout, the model is
supposed to pay more attention to inapparent features or patterns. Experiments
demonstrate the effectiveness of the dropout with global information on seven
text classification tasks, including sentiment analysis and topic
classification.
</summary>
    <author>
      <name>Hengru Xu</name>
    </author>
    <author>
      <name>Shen Li</name>
    </author>
    <author>
      <name>Renfen Hu</name>
    </author>
    <author>
      <name>Si Li</name>
    </author>
    <author>
      <name>Sheng Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08149v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08149v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07318v3</id>
    <updated>2018-08-27T00:36:45Z</updated>
    <published>2018-01-22T20:57:39Z</published>
    <title>Variable Prioritization in Nonlinear Black Box Methods: A Genetic
  Association Case Study</title>
    <summary>  The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the "RelATive cEntrality" (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other "black box" methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
</summary>
    <author>
      <name>Lorin Crawford</name>
    </author>
    <author>
      <name>Seth R. Flaxman</name>
    </author>
    <author>
      <name>Daniel E. Runcie</name>
    </author>
    <author>
      <name>Mike West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures, 1 tables; Supplementary Material</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07318v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07318v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00946v1</id>
    <updated>2018-08-26T23:09:03Z</updated>
    <published>2018-08-26T23:09:03Z</published>
    <title>Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing
  GANs</title>
    <summary>  We present a framework for translating unlabeled images from one domain into
analog images in another domain. We employ a progressively growing
skip-connected encoder-generator structure and train it with a GAN loss for
realistic output, a cycle consistency loss for maintaining same-domain
translation identity, and a semantic consistency loss that encourages the
network to keep the input semantic features in the output. We apply our
framework on the task of translating face images, and show that it is capable
of learning semantic mappings for face images with no supervised one-to-one
image mapping.
</summary>
    <author>
      <name>Jerry Li</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08640v1</id>
    <updated>2018-08-26T23:02:54Z</updated>
    <published>2018-08-26T23:02:54Z</published>
    <title>Detecting Outliers in Data with Correlated Measures</title>
    <summary>  Advances in sensor technology have enabled the collection of large-scale
datasets. Such datasets can be extremely noisy and often contain a significant
amount of outliers that result from sensor malfunction or human operation
faults. In order to utilize such data for real-world applications, it is
critical to detect outliers so that models built from these datasets will not
be skewed by outliers.
  In this paper, we propose a new outlier detection method that utilizes the
correlations in the data (e.g., taxi trip distance vs. trip time). Different
from existing outlier detection methods, we build a robust regression model
that explicitly models the outliers and detects outliers simultaneously with
the model fitting.
  We validate our approach on real-world datasets against methods specifically
designed for each dataset as well as the state of the art outlier detectors.
Our outlier detection method achieves better performances, demonstrating the
robustness and generality of our method. Last, we report interesting case
studies on some outliers that result from atypical events.
</summary>
    <author>
      <name>Yu-Hsuan Kuo</name>
    </author>
    <author>
      <name>Zhenhui Li</name>
    </author>
    <author>
      <name>Daniel Kifer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271798</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271798" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04523v3</id>
    <updated>2018-08-26T22:45:16Z</updated>
    <published>2018-08-14T05:01:55Z</published>
    <title>Adaptive Sampling for Convex Regression</title>
    <summary>  In this paper, we introduce the first principled adaptive-sampling procedure
for learning a convex function in the $L_\infty$ norm, a problem that arises
often in the behavioral and social sciences. We present a function-specific
measure of complexity and use it to prove that, for each convex function
$f_{\star}$, our algorithm nearly attains the information-theoretically
optimal, function-specific error rate. We also corroborate our theoretical
contributions with numerical experiments, finding that our method substantially
outperforms passive, uniform sampling for favorable synthetic and data-derived
functions in low-noise settings with large sampling budgets. Our results also
suggest an idealized "oracle strategy", which we use to gauge the potential
advance of any adaptive-sampling strategy over passive sampling, for any given
convex function.
</summary>
    <author>
      <name>Max Simchowitz</name>
    </author>
    <author>
      <name>Kevin Jamieson</name>
    </author>
    <author>
      <name>Jordan W. Suchow</name>
    </author>
    <author>
      <name>Thomas L. Griffiths</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04523v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04523v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08627v1</id>
    <updated>2018-08-26T21:34:36Z</updated>
    <published>2018-08-26T21:34:36Z</published>
    <title>Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation</title>
    <summary>  As opposed to manual feature engineering which is tedious and difficult to
scale, network representation learning has attracted a surge of research
interests as it automates the process of feature learning on graphs. The
learned low-dimensional node vector representation is generalizable and eases
the knowledge discovery process on graphs by enabling various off-the-shelf
machine learning tools to be directly applied. Recent research has shown that
the past decade of network embedding approaches either explicitly factorize a
carefully designed matrix to obtain the low-dimensional node vector
representation or are closely related to implicit matrix factorization, with
the fundamental assumption that the factorized node connectivity matrix is
low-rank. Nonetheless, the global low-rank assumption does not necessarily hold
especially when the factorized matrix encodes complex node interactions, and
the resultant single low-rank embedding matrix is insufficient to capture all
the observed connectivity patterns. In this regard, we propose a novel
multi-level network embedding framework BoostNE, which can learn multiple
network embedding representations of different granularity from coarse to fine
without imposing the prevalent global low-rank assumption. The proposed BoostNE
method is also in line with the successful gradient boosting method in ensemble
learning as multiple weak embeddings lead to a stronger and more effective one.
We assess the effectiveness of the proposed BoostNE framework by comparing it
with existing state-of-the-art network embedding methods on various datasets,
and the experimental results corroborate the superiority of the proposed
BoostNE network embedding framework.
</summary>
    <author>
      <name>Jundong Li</name>
    </author>
    <author>
      <name>Liang Wu</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08952v1</id>
    <updated>2018-08-26T20:54:33Z</updated>
    <published>2018-08-26T20:54:33Z</published>
    <title>Deep Learning of Vortex Induced Vibrations</title>
    <summary>  Vortex induced vibrations of bluff bodies occur when the vortex shedding
frequency is close to the natural frequency of the structure. Of interest is
the prediction of the lift and drag forces on the structure given some limited
and scattered information on the velocity field. This is an inverse problem
that is not straightforward to solve using standard computational fluid
dynamics (CFD) methods, especially since no information is provided for the
pressure. An even greater challenge is to infer the lift and drag forces given
some dye or smoke visualizations of the flow field. Here we employ deep neural
networks that are extended to encode the incompressible Navier-Stokes equations
coupled with the structure's dynamic motion equation. In the first case, given
scattered data in space-time on the velocity field and the structure's motion,
we use four coupled deep neural networks to infer very accurately the
structural parameters, the entire time-dependent pressure field (with no prior
training data), and reconstruct the velocity vector field and the structure's
dynamic motion. In the second case, given scattered data in space-time on a
concentration field only, we use five coupled deep neural networks to infer
very accurately the vector velocity field and all other quantities of interest
as before. This new paradigm of inference in fluid mechanics for coupled
multi-physics problems enables velocity and pressure quantification from flow
snapshots in small subdomains and can be exploited for flow control
applications and also for system identification.
</summary>
    <author>
      <name>Maziar Raissi</name>
    </author>
    <author>
      <name>Zhicheng Wang</name>
    </author>
    <author>
      <name>Michael S. Triantafyllou</name>
    </author>
    <author>
      <name>George Em Karniadakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1808.04327</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01860v2</id>
    <updated>2018-08-26T20:50:58Z</updated>
    <published>2017-06-06T17:18:38Z</published>
    <title>Attributed Network Embedding for Learning in a Dynamic Environment</title>
    <summary>  Network embedding leverages the node proximity manifested to learn a
low-dimensional node vector representation for each node in the network. The
learned embeddings could advance various learning tasks such as node
classification, network clustering, and link prediction. Most, if not all, of
the existing works, are overwhelmingly performed in the context of plain and
static networks. Nonetheless, in reality, network structure often evolves over
time with addition/deletion of links and nodes. Also, a vast majority of
real-world networks are associated with a rich set of node attributes, and
their attribute values are also naturally changing, with the emerging of new
content patterns and the fading of old content patterns. These changing
characteristics motivate us to seek an effective embedding representation to
capture network and attribute evolving patterns, which is of fundamental
importance for learning in a dynamic environment. To our best knowledge, we are
the first to tackle this problem with the following two challenges: (1) the
inherently correlated network and node attributes could be noisy and
incomplete, it necessitates a robust consensus representation to capture their
individual properties and correlations; (2) the embedding learning needs to be
performed in an online fashion to adapt to the changes accordingly. In this
paper, we tackle this problem by proposing a novel dynamic attributed network
embedding framework - DANE. In particular, DANE first provides an offline
method for a consensus embedding and then leverages matrix perturbation theory
to maintain the freshness of the end embedding results in an online manner. We
perform extensive experiments on both synthetic and real attributed networks to
corroborate the effectiveness and efficiency of the proposed framework.
</summary>
    <author>
      <name>Jundong Li</name>
    </author>
    <author>
      <name>Harsh Dani</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <author>
      <name>Yi Chang</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3132847.3132919</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3132847.3132919" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.01860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07753v2</id>
    <updated>2018-08-26T20:42:24Z</updated>
    <published>2018-03-21T05:55:11Z</published>
    <title>Sample Complexity of Sparse System Identification Problem</title>
    <summary>  In this paper, we study the system identification problem for sparse linear
time-invariant systems. We propose a sparsity promoting block-regularized
estimator to identify the dynamics of the system with only a limited number of
input-state data samples. We characterize the properties of this estimator
under high-dimensional scaling, where the growth rate of the system dimension
is comparable to or even faster than that of the number of available sample
trajectories. In particular, using contemporary results on high-dimensional
statistics, we show that the proposed estimator results in a small element-wise
error, provided that the number of sample trajectories is above a threshold.
This threshold depends polynomially on the size of each block and the number of
nonzero elements at different rows of input and state matrices, but only
logarithmically on the system dimension. A by-product of this result is that
the number of sample trajectories required for sparse system identification is
significantly smaller than the dimension of the system. Furthermore, we show
that, unlike the recently celebrated least-squares estimators for system
identification problems, the method developed in this work is capable of
\textit{exact recovery} of the underlying sparsity structure of the system with
the aforementioned number of data samples. Extensive case studies on
synthetically generated systems, physical mass-spring networks, and multi-agent
systems are offered to demonstrate the effectiveness of the proposed method.
</summary>
    <author>
      <name>Salar Fattahi</name>
    </author>
    <author>
      <name>Somayeh Sojoudi</name>
    </author>
    <link href="http://arxiv.org/abs/1803.07753v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07753v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08619v1</id>
    <updated>2018-08-26T20:36:58Z</updated>
    <published>2018-08-26T20:36:58Z</published>
    <title>Discriminative but Not Discriminatory: A Comparison of Fairness
  Definitions under Different Worldviews</title>
    <summary>  We mathematically compare three competing definitions of group-level
nondiscrimination: demographic parity, equalized odds, and calibration. Using
the theoretical framework of Friedler et al., we study the properties of each
definition under various worldviews, which are assumptions about how, if at
all, the observed data is biased. We prove that different worldviews call for
different definitions of fairness, and we specify when it is appropriate to use
demographic parity and equalized odds. In addition, we argue that calibration
is unsuitable for the purpose of ensuring nondiscrimination. Finally, we define
a worldview that is more realistic than the previously considered ones, and we
introduce a new notion of fairness that is suitable for this worldview.
</summary>
    <author>
      <name>Samuel Yeom</name>
    </author>
    <author>
      <name>Michael Carl Tschantz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08618v1</id>
    <updated>2018-08-26T20:26:11Z</updated>
    <published>2018-08-26T20:26:11Z</published>
    <title>Deep Learning: Computational Aspects</title>
    <summary>  In this article we review computational aspects of Deep Learning (DL). Deep
learning uses network architectures consisting of hierarchical layers of latent
variables to construct predictors for high-dimensional input-output models.
Training a deep learning architecture is computationally intensive, and
efficient linear algebra libraries is the key for training and inference.
Stochastic gradient descent (SGD) optimization and batch sampling are used to
learn from massive data sets.
</summary>
    <author>
      <name>Nicholas Polson</name>
    </author>
    <author>
      <name>Vadim Sokolov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08473v2</id>
    <updated>2018-08-26T19:48:07Z</updated>
    <published>2017-10-23T19:37:00Z</published>
    <title>A Unified Framework for Long Range and Cold Start Forecasting of
  Seasonal Profiles in Time Series</title>
    <summary>  Providing long-range forecasts is a fundamental challenge in time series
modeling, which is only compounded by the challenge of having to form such
forecasts when a time series has never previously been observed. The latter
challenge is the time series version of the cold-start problem seen in
recommender systems which, to our knowledge, has not been addressed in previous
work. A similar problem occurs when a long range forecast is required after
only observing a small number of time points --- a warm start forecast. With
these aims in mind, we focus on forecasting seasonal profiles---or baseline
demand---for periods on the order of a year in three cases: the long range case
with multiple previously observed seasonal profiles, the cold start case with
no previous observed seasonal profiles, and the warm start case with only a
single partially observed profile. Classical time series approaches that
perform iterated step-ahead forecasts based on previous observations struggle
to provide accurate long range predictions; in settings with little to no
observed data, such approaches are simply not applicable. Instead, we present a
straightforward framework which combines ideas from high-dimensional regression
and matrix factorization on a carefully constructed data matrix. Key to our
formulation and resulting performance is leveraging (1) repeated patterns over
fixed periods of time and across series, and (2) metadata associated with the
individual series; without this additional data, the cold-start/warm-start
problems are nearly impossible to solve. We demonstrate that our framework can
accurately forecast an array of seasonal profiles on multiple large scale
datasets.
</summary>
    <author>
      <name>Christopher Xie</name>
    </author>
    <author>
      <name>Alex Tank</name>
    </author>
    <author>
      <name>Alec Greaves-Tunnell</name>
    </author>
    <author>
      <name>Emily Fox</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08473v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08473v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02365v4</id>
    <updated>2018-08-26T19:23:06Z</updated>
    <published>2016-11-08T02:20:46Z</published>
    <title>NonSTOP: A NonSTationary Online Prediction Method for Time Series</title>
    <summary>  We present online prediction methods for time series that let us explicitly
handle nonstationary artifacts (e.g. trend and seasonality) present in most
real time series. Specifically, we show that applying appropriate
transformations to such time series before prediction can lead to improved
theoretical and empirical prediction performance. Moreover, since these
transformations are usually unknown, we employ the learning with experts
setting to develop a fully online method (NonSTOP-NonSTationary Online
Prediction) for predicting nonstationary time series. This framework allows for
seasonality and/or other trends in univariate time series and cointegration in
multivariate time series. Our algorithms and regret analysis subsume recent
related work while significantly expanding the applicability of such methods.
For all the methods, we provide sub-linear regret bounds using relaxed
assumptions. The theoretical guarantees do not fully capture the benefits of
the transformations, thus we provide a data-dependent analysis of the
follow-the-leader algorithm that provides insight into the success of using
such transformations. We support all of our results with experiments on
simulated and real data.
</summary>
    <author>
      <name>Christopher Xie</name>
    </author>
    <author>
      <name>Avleen Bijral</name>
    </author>
    <author>
      <name>Juan Lavista Ferres</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02365v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02365v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08613v1</id>
    <updated>2018-08-26T19:19:24Z</updated>
    <published>2018-08-26T19:19:24Z</published>
    <title>Ensemble Learning Applied to Classify GPS Trajectories of Birds into
  Male or Female</title>
    <summary>  We describe our first-place solution to the Animal Behavior Challenge (ABC
2018) on predicting gender of bird from its GPS trajectory. The task consisted
in predicting the gender of shearwater based on how they navigate themselves
across a big ocean. The trajectories are collected from GPS loggers attached on
shearwaters' body, and represented as a variable-length sequence of GPS points
(latitude and longitude), and associated meta-information, such as the sun
azimuth, the sun elevation, the daytime, the elapsed time on each GPS location
after starting the trip, the local time (date is trimmed), and the indicator of
the day starting the from the trip. We used ensemble of several variants of
Gradient Boosting Classifier along with Gaussian Process Classifier and Support
Vector Classifier after extensive feature engineering and we ranked first out
of 74 registered teams. The variants of Gradient Boosting Classifier we tried
are CatBoost (Developed by Yandex), LightGBM (Developed by Microsoft), XGBoost
(Developed by Distributed Machine Learning Community). Our approach could
easily be adapted to other applications in which the goal is to predict a
classification output from a variable-length sequence.
</summary>
    <author>
      <name>Dewan Fayzur</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08609v1</id>
    <updated>2018-08-26T18:36:20Z</updated>
    <published>2018-08-26T18:36:20Z</published>
    <title>Adversarially Regularising Neural NLI Models to Integrate Logical
  Background Knowledge</title>
    <summary>  Adversarial examples are inputs to machine learning models designed to cause
the model to make a mistake. They are useful for understanding the shortcomings
of machine learning models, interpreting their results, and for regularisation.
In NLP, however, most example generation strategies produce input text by using
known, pre-specified semantic transformations, requiring significant manual
effort and in-depth understanding of the problem and domain. In this paper, we
investigate the problem of automatically generating adversarial examples that
violate a set of given First-Order Logic constraints in Natural Language
Inference (NLI). We reduce the problem of identifying such adversarial examples
to a combinatorial optimisation problem, by maximising a quantity measuring the
degree of violation of such constraints and by using a language model for
generating linguistically-plausible examples. Furthermore, we propose a method
for adversarially regularising neural NLI models for incorporating background
knowledge. Our results show that, while the proposed method does not always
improve results on the SNLI and MultiNLI datasets, it significantly and
consistently increases the predictive accuracy on adversarially-crafted
datasets -- up to a 79.6% relative improvement -- while drastically reducing
the number of background knowledge violations. Furthermore, we show that
adversarial examples transfer among model architectures, and that the proposed
adversarial training procedure improves the robustness of NLI models to
adversarial examples.
</summary>
    <author>
      <name>Pasquale Minervini</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the SIGNLL Conference on Computational Natural Language
  Learning (CoNLL 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08680v3</id>
    <updated>2018-08-26T16:43:36Z</updated>
    <published>2018-03-23T07:57:04Z</published>
    <title>Improving DNN Robustness to Adversarial Attacks using Jacobian
  Regularization</title>
    <summary>  Deep neural networks have lately shown tremendous performance in various
applications including vision and speech processing tasks. However, alongside
their ability to perform these tasks with such high accuracy, it has been shown
that they are highly susceptible to adversarial attacks: a small change in the
input would cause the network to err with high confidence. This phenomenon
exposes an inherent fault in these networks and their ability to generalize
well. For this reason, providing robustness to adversarial attacks is an
important challenge in networks training, which has led to extensive research.
In this work, we suggest a theoretically inspired novel approach to improve the
networks' robustness. Our method applies regularization using the Frobenius
norm of the Jacobian of the network, which is applied as post-processing, after
regular training has finished. We demonstrate empirically that it leads to
enhanced robustness results with a minimal change in the original network's
accuracy.
</summary>
    <author>
      <name>Daniel Jakubovitz</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 Conference Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.08680v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08680v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08951v1</id>
    <updated>2018-08-26T16:01:11Z</updated>
    <published>2018-08-26T16:01:11Z</published>
    <title>Water Disaggregation via Shape Features based Bayesian Discriminative
  Sparse Coding</title>
    <summary>  As the issue of freshwater shortage is increasing daily, it is critical to
take effective measures for water conservation. According to previous studies,
device level consumption could lead to significant freshwater conservation.
Existing water disaggregation methods focus on learning the signatures for
appliances; however, they are lack of the mechanism to accurately discriminate
parallel appliances' consumption. In this paper, we propose a Bayesian
Discriminative Sparse Coding model using Laplace Prior (BDSC-LP) to extensively
enhance the disaggregation performance. To derive discriminative basis
functions, shape features are presented to describe the low-sampling-rate water
consumption patterns. A Gibbs sampling based inference method is designed to
extend the discriminative capability of the disaggregation dictionaries.
Extensive experiments were performed to validate the effectiveness of the
proposed model using both real-world and synthetic datasets.
</summary>
    <author>
      <name>Bingsheng Wang</name>
    </author>
    <author>
      <name>Xuchao Zhang</name>
    </author>
    <author>
      <name>Chang-Tien Lu</name>
    </author>
    <author>
      <name>Feng Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08558v1</id>
    <updated>2018-08-26T14:25:52Z</updated>
    <published>2018-08-26T14:25:52Z</published>
    <title>Spectral-Pruning: Compressing deep neural network via spectral analysis</title>
    <summary>  The model size of deep neural network is getting larger and larger to realize
superior performance in complicated tasks. This makes it difficult to implement
deep neural network in small edge-computing devices. To overcome this problem,
model compression methods have been gathering much attention. However, there
have been only few theoretical back-grounds that explain what kind of quantity
determines the compression ability. To resolve this issue, we develop a new
theoretical frame-work for model compression, and propose a new method called
{\it Spectral-Pruning} based on the theory. Our theoretical analysis is based
on the observation such that the eigenvalues of the covariance matrix of the
output from nodes in the internal layers often shows rapid decay. We define
"degree of freedom" to quantify an intrinsic dimensionality of the model by
using the eigenvalue distribution and show that the compression ability is
essentially controlled by this quantity. Along with this, we give a
generalization error bound of the compressed model. Our proposed method is
applicable to wide range of models, unlike the existing methods, e.g., ones
possess complicated branches as implemented in SegNet and ResNet. Our method
makes use of both "input" and "output" in each layer and is easy to implement.
We apply our method to several datasets to justify our theoretical analyses and
show that the proposed method achieves the state-of-the-art performance.
</summary>
    <author>
      <name>Taiji Suzuki</name>
    </author>
    <author>
      <name>Hiroshi Abe</name>
    </author>
    <author>
      <name>Tomoya Murata</name>
    </author>
    <author>
      <name>Shingo Horiuchi</name>
    </author>
    <author>
      <name>Kotaro Ito</name>
    </author>
    <author>
      <name>Tokuma Wachi</name>
    </author>
    <author>
      <name>So Hirai</name>
    </author>
    <author>
      <name>Masatoshi Yukishima</name>
    </author>
    <author>
      <name>Tomoaki Nishimura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05092v2</id>
    <updated>2018-08-26T07:34:07Z</updated>
    <published>2018-08-13T23:31:01Z</published>
    <title>ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary
  classifier variational autoencoder</title>
    <summary>  This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
</summary>
    <author>
      <name>Hirokazu Kameoka</name>
    </author>
    <author>
      <name>Takuhiro Kaneko</name>
    </author>
    <author>
      <name>Kou Tanaka</name>
    </author>
    <author>
      <name>Nobukatsu Hojo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1806.02169.
  arXiv admin note: substantial text overlap with arXiv:1806.02169</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05092v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05092v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00892v3</id>
    <updated>2018-08-26T07:29:03Z</updated>
    <published>2018-08-02T16:30:51Z</published>
    <title>Semi-blind source separation with multichannel variational autoencoder</title>
    <summary>  This paper proposes a multichannel source separation technique called the
multichannel variational autoencoder (MVAE) method, which uses a conditional
VAE (CVAE) to model and estimate the power spectrograms of the sources in a
mixture. By training the CVAE using the spectrograms of training examples with
source-class labels, we can use the trained decoder distribution as a universal
generative model capable of generating spectrograms conditioned on a specified
class label. By treating the latent space variables and the class label as the
unknown parameters of this generative model, we can develop a
convergence-guaranteed semi-blind source separation algorithm that consists of
iteratively estimating the power spectrograms of the underlying sources as well
as the separation matrices. In experimental evaluations, our MVAE produced
better separation performance than a baseline method.
</summary>
    <author>
      <name>Hirokazu Kameoka</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Shota Inoue</name>
    </author>
    <author>
      <name>Shoji Makino</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00892v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00892v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05512v2</id>
    <updated>2018-08-26T01:33:02Z</updated>
    <published>2018-06-14T12:55:35Z</published>
    <title>NetScore: Towards Universal Metrics for Large-scale Performance Analysis
  of Deep Neural Networks for Practical On-Device Edge Usage</title>
    <summary>  Much of the focus in the design of deep neural networks has been on improving
accuracy, leading to more powerful yet highly complex network architectures
that are difficult to deploy in practical scenarios, particularly on edge
devices such as mobile and other consumer devices given their high
computational and memory requirements. As a result, there has been a recent
interest in the design of quantitative metrics for evaluating deep neural
networks that accounts for more than just model accuracy as the sole indicator
of network performance. In this study, we continue the conversation towards
universal metrics for evaluating the performance of deep neural networks for
practical on-device edge usage. In particular, we propose a new balanced metric
called NetScore, which is designed specifically to provide a quantitative
assessment of the balance between accuracy, computational complexity, and
network architecture complexity of a deep neural network, which is important
for on-device edge operation. In what is one of the largest comparative
analysis between deep neural networks in literature, the NetScore metric, the
top-1 accuracy metric, and the popular information density metric were compared
across a diverse set of 60 different deep convolutional neural networks for
image classification on the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC 2012) dataset. The evaluation results across these three metrics for
this diverse set of networks are presented in this study to act as a reference
guide for practitioners in the field. The proposed NetScore metric, along with
the other tested metrics, are by no means perfect, but the hope is to push the
conversation towards better universal metrics for evaluating deep neural
networks for use in practical on-device edge scenarios to help guide
practitioners in model design for such scenarios.
</summary>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05512v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05512v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08493v1</id>
    <updated>2018-08-26T01:17:50Z</updated>
    <published>2018-08-26T01:17:50Z</published>
    <title>Contextual Parameter Generation for Universal Neural Machine Translation</title>
    <summary>  We propose a simple modification to existing neural machine translation (NMT)
models that enables using a single universal model to translate between
multiple languages while allowing for language specific parameterization, and
that can also be used for domain adaptation. Our approach requires no changes
to the model architecture of a standard NMT system, but instead introduces a
new component, the contextual parameter generator (CPG), that generates the
parameters of the system (e.g., weights in a neural network). This parameter
generator accepts source and target language embeddings as input, and generates
the parameters for the encoder and the decoder, respectively. The rest of the
model remains unchanged and is shared across all languages. We show how this
simple modification enables the system to use monolingual data for training and
also perform zero-shot translation. We further show it is able to surpass
state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and
that the learned language embeddings are able to uncover interesting
relationships between languages.
</summary>
    <author>
      <name>Emmanouil Antonios Platanios</name>
    </author>
    <author>
      <name>Mrinmaya Sachan</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <author>
      <name>Tom Mitchell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the proceedings of Empirical Methods in Natural Language
  Processing (EMNLP), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08090v4</id>
    <updated>2018-08-26T00:13:38Z</updated>
    <published>2018-05-21T14:38:31Z</published>
    <title>Graph Capsule Convolutional Neural Networks</title>
    <summary>  Graph Convolutional Neural Networks (GCNNs) are the most recent exciting
advancement in deep learning field and their applications are quickly spreading
in multi-cross-domains including bioinformatics, chemoinformatics, social
networks, natural language processing and computer vision. In this paper, we
expose and tackle some of the basic weaknesses of a GCNN model with a capsule
idea presented in \cite{hinton2011transforming} and propose our Graph Capsule
Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve
especially graph classification problem which current GCNN models find
challenging. Through extensive experiments, we show that our proposed Graph
Capsule Network can significantly outperforms both the existing state-of-art
deep learning methods and graph kernels on graph classification benchmark
datasets.
</summary>
    <author>
      <name>Saurabh Verma</name>
    </author>
    <author>
      <name>Zhi-Li Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Joint ICML and IJCAI Workshop on Computational Biology,
  Stockholm, Sweden, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08090v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08090v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00957v1</id>
    <updated>2018-08-25T23:18:52Z</updated>
    <published>2018-08-25T23:18:52Z</published>
    <title>Road User Abnormal Trajectory Detection using a Deep Autoencoder</title>
    <summary>  In this paper, we focus on the development of a method that detects abnormal
trajectories of road users at traffic intersections. The main difficulty with
this is the fact that there are very few abnormal data and the normal ones are
insufficient for the training of any kinds of machine learning model. To tackle
these problems, we proposed the solution of using a deep autoencoder network
trained solely through augmented data considered as normal. By generating
artificial abnormal trajectories, our method is tested on four different
outdoor urban users scenes and performs better compared to some classical
outlier detection methods.
</summary>
    <author>
      <name>Pankaj Raj Roy</name>
    </author>
    <author>
      <name>Guillaume-Alexandre Bilodeau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted for oral presentation at ISVC'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08362v2</id>
    <updated>2018-08-25T22:25:00Z</updated>
    <published>2018-07-22T20:37:45Z</published>
    <title>An Intersectional Definition of Fairness</title>
    <summary>  We introduce a measure of fairness for algorithms and data with regard to
multiple protected attributes. Our proposed definition, differential fairness,
is informed by the framework of intersectionality, which analyzes how
interlocking systems of power and oppression affect individuals along
overlapping dimensions including race, gender, sexual orientation, class, and
disability. We show that our criterion behaves sensibly for any subset of the
set of protected attributes, and we illustrate links to differential privacy. A
case study on census data demonstrates the utility of our approach.
</summary>
    <author>
      <name>James Foulds</name>
    </author>
    <author>
      <name>Shimei Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08362v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08362v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00961v1</id>
    <updated>2018-08-25T22:00:10Z</updated>
    <published>2018-08-25T22:00:10Z</published>
    <title>MSCE: An edge preserving robust loss function for improving
  super-resolution algorithms</title>
    <summary>  With the recent advancement in the deep learning technologies such as CNNs
and GANs, there is significant improvement in the quality of the images
reconstructed by deep learning based super-resolution (SR) techniques. In this
work, we propose a robust loss function based on the preservation of edges
obtained by the Canny operator. This loss function, when combined with the
existing loss function such as mean square error (MSE), gives better SR
reconstruction measured in terms of PSNR and SSIM. Our proposed loss function
guarantees improved performance on any existing algorithm using MSE loss
function, without any increase in the computational complexity during testing.
</summary>
    <author>
      <name>Ram Krishna Pandey</name>
    </author>
    <author>
      <name>Nabagata Saha</name>
    </author>
    <author>
      <name>Samarjit Karmakar</name>
    </author>
    <author>
      <name>A G Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICONIP-2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08469v1</id>
    <updated>2018-08-25T21:02:46Z</updated>
    <published>2018-08-25T21:02:46Z</published>
    <title>DNN: A Two-Scale Distributional Tale of Heterogeneous Treatment Effect
  Inference</title>
    <summary>  Heterogeneous treatment effects are the center of gravity in many modern
causal inference applications. In this paper, we investigate the estimation and
inference of heterogeneous treatment effects with precision in a general
nonparametric setting. To this end, we enhance the classical $k$-nearest
neighbor method with a simple algorithm, extend it to a distributional setting,
and suggest the two-scale distributional nearest neighbors (DNN) estimator with
reduced finite-sample bias. Our recipe is first to subsample the data and
average the 1-nearest neighbor estimators from each subsample. With
appropriately chosen subsampling scale, the resulting DNN estimator is proved
to be asymptotically unbiased and normal under mild regularity conditions. We
then proceed with combining DNN estimators with different subsampling scales to
further reduce bias. Our theoretical results on the advantages of the new
two-scale DNN framework are well supported by several Monte Carlo simulations.
The newly suggested method is also applied to a real-life data set to study the
heterogeneity of treatment effects of smoking on children's birth weights
across mothers' ages.
</summary>
    <author>
      <name>Yingying Fan</name>
    </author>
    <author>
      <name>Jinchi Lv</name>
    </author>
    <author>
      <name>Jingbo Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10253v2</id>
    <updated>2018-08-25T20:23:27Z</updated>
    <published>2018-04-26T19:28:02Z</published>
    <title>From Principal Subspaces to Principal Components with Linear
  Autoencoders</title>
    <summary>  The autoencoder is an effective unsupervised learning model which is widely
used in deep learning. It is well known that an autoencoder with a single
fully-connected hidden layer, a linear activation function and a squared error
cost function trains weights that span the same subspace as the one spanned by
the principal component loading vectors, but that they are not identical to the
loading vectors. In this paper, we show how to recover the loading vectors from
the autoencoder weights.
</summary>
    <author>
      <name>Elad Plaut</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03432v3</id>
    <updated>2018-08-25T19:32:18Z</updated>
    <published>2018-06-09T07:42:01Z</published>
    <title>Hierarchical Clustering with Prior Knowledge</title>
    <summary>  Hierarchical clustering is a class of algorithms that seeks to build a
hierarchy of clusters. It has been the dominant approach to constructing
embedded classification schemes since it outputs dendrograms, which capture the
hierarchical relationship among members at all levels of granularity,
simultaneously. Being greedy in the algorithmic sense, a hierarchical
clustering partitions data at every step solely based on a similarity /
dissimilarity measure. The clustering results oftentimes depend on not only the
distribution of the underlying data, but also the choice of dissimilarity
measure and the clustering algorithm. In this paper, we propose a method to
incorporate prior domain knowledge about entity relationship into the
hierarchical clustering. Specifically, we use a distance function in
ultrametric space to encode the external ontological information. We show that
popular linkage-based algorithms can faithfully recover the encoded structure.
Similar to some regularized machine learning techniques, we add this distance
as a penalty term to the original pairwise distance to regulate the final
structure of the dendrogram. As a case study, we applied this method on real
data in the building of a customer behavior based product taxonomy for an
Amazon service, leveraging the information from a larger Amazon-wide browse
structure. The method is useful when one wants to leverage the relational
information from external sources, or the data used to generate the distance
matrix is noisy and sparse. Our work falls in the category of semi-supervised
or constrained clustering.
</summary>
    <author>
      <name>Xiaofei Ma</name>
    </author>
    <author>
      <name>Satya Dhavala</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03432v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03432v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08460v1</id>
    <updated>2018-08-25T18:31:52Z</updated>
    <published>2018-08-25T18:31:52Z</published>
    <title>The Social Cost of Strategic Classification</title>
    <summary>  Consequential decision-making typically incentivizes individuals to behave
strategically, tailoring their behavior to the specifics of the decision rule.
A long line of work has therefore sought to counteract strategic behavior by
designing more conservative decision boundaries in an effort to increase
robustness to the effects of strategic covariate shift.
  We show that these efforts benefit the institutional decision maker at the
expense of the individuals being classified. Introducing a notion of social
burden, we prove that any increase in institutional utility necessarily leads
to a corresponding increase in social burden. Moreover, we show that the
negative externalities of strategic classification can disproportionately harm
disadvantaged groups in the population.
  Our results highlight that strategy-robustness must be weighed against
considerations of social welfare and fairness.
</summary>
    <author>
      <name>Smitha Milli</name>
    </author>
    <author>
      <name>John Miller</name>
    </author>
    <author>
      <name>Anca D. Dragan</name>
    </author>
    <author>
      <name>Moritz Hardt</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08416v1</id>
    <updated>2018-08-25T12:06:17Z</updated>
    <published>2018-08-25T12:06:17Z</published>
    <title>Multiplayer bandits without observing collision information</title>
    <summary>  We study multiplayer stochastic multi-armed bandit problems in which the
players cannot communicate, and if two or more players pull the same arm, a
collision occurs and the involved players receive zero reward. We consider two
feedback models: a model in which the players can observe whether a collision
has occurred, and a more difficult setup when no collision information is
available. We give the first theoretical guarantees for the second model: an
algorithm with a logarithmic regret, and an algorithm with a square-root regret
type that does not depend on the gaps between the means. For the first model,
we give the first square-root regret bounds that do not depend on the gaps.
Building on these ideas, we also give an algorithm for reaching approximate
Nash equilibria quickly in stochastic anti-coordination games.
</summary>
    <author>
      <name>Gabor Lugosi</name>
    </author>
    <author>
      <name>Abbas Mehrabian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08414v1</id>
    <updated>2018-08-25T12:02:41Z</updated>
    <published>2018-08-25T12:02:41Z</published>
    <title>Unsupervised Hypergraph Feature Selection via a Novel Point-Weighting
  Framework and Low-Rank Representation</title>
    <summary>  Feature selection methods are widely used in order to solve the 'curse of
dimensionality' problem. Many proposed feature selection frameworks, treat all
data points equally; neglecting their different representation power and
importance. In this paper, we propose an unsupervised hypergraph feature
selection method via a novel point-weighting framework and low-rank
representation that captures the importance of different data points. We
introduce a novel soft hypergraph with low complexity to model data. Then, we
formulate the feature selection as an optimization problem to preserve local
relationships and also global structure of data. Our approach for global
structure preservation helps the framework overcome the problem of
unavailability of data labels in unsupervised learning. The proposed feature
selection method treats with different data points based on their importance in
defining data structure and representation power. Moreover, since the
robustness of feature selection methods against noise and outlier is of great
importance, we adopt low-rank representation in our model. Also, we provide an
efficient algorithm to solve the proposed optimization problem. The
computational cost of the proposed algorithm is lower than many
state-of-the-art methods which is of high importance in feature selection
tasks. We conducted comprehensive experiments with various evaluation methods
on different benchmark data sets. These experiments indicate significant
improvement, compared with state-of-the-art feature selection methods.
</summary>
    <author>
      <name>Ammar Gilani</name>
    </author>
    <author>
      <name>Maryam Amirmazlaghani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08366v1</id>
    <updated>2018-08-25T07:07:42Z</updated>
    <published>2018-08-25T07:07:42Z</published>
    <title>Relaxing the Identically Distributed Assumption in Gaussian
  Co-Clustering for High Dimensional Data</title>
    <summary>  A co-clustering model for continuous data that relaxes the identically
distributed assumption within blocks of traditional co-clustering is presented.
The proposed model, although allowing more flexibility, still maintains the
very high degree of parsimony achieved by traditional co-clustering. A
stochastic EM algorithm along with a Gibbs sampler is used for parameter
estimation and an ICL criterion is used for model selection. Simulated and real
datasets are used for illustration and comparison with traditional
co-clustering.
</summary>
    <author>
      <name>M. P. B. Gallaugher</name>
    </author>
    <author>
      <name>C. Biernacki</name>
    </author>
    <author>
      <name>P. D. McNicholas</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08361v1</id>
    <updated>2018-08-25T06:00:44Z</updated>
    <published>2018-08-25T06:00:44Z</published>
    <title>Data-dependent Learning of Symmetric/Antisymmetric Relations for
  Knowledge Base Completion</title>
    <summary>  Embedding-based methods for knowledge base completion (KBC) learn
representations of entities and relations in a vector space, along with the
scoring function to estimate the likelihood of relations between entities. The
learnable class of scoring functions is designed to be expressive enough to
cover a variety of real-world relations, but this expressive comes at the cost
of an increased number of parameters. In particular, parameters in these
methods are superfluous for relations that are either symmetric or
antisymmetric. To mitigate this problem, we propose a new L1 regularizer for
Complex Embeddings, which is one of the state-of-the-art embedding-based
methods for KBC. This regularizer promotes symmetry or antisymmetry of the
scoring function on a relation-by-relation basis, in accordance with the
observed data. Our empirical evaluation shows that the proposed method
outperforms the original Complex Embeddings and other baseline methods on the
FB15k dataset.
</summary>
    <author>
      <name>Hitoshi Manabe</name>
    </author>
    <author>
      <name>Katsuhiko Hayashi</name>
    </author>
    <author>
      <name>Masashi Shimbo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03467v4</id>
    <updated>2018-08-25T05:52:08Z</updated>
    <published>2018-03-09T11:12:01Z</published>
    <title>RippleNet: Propagating User Preferences on the Knowledge Graph for
  Recommender Systems</title>
    <summary>  To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple "ripples"
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.
</summary>
    <author>
      <name>Hongwei Wang</name>
    </author>
    <author>
      <name>Fuzheng Zhang</name>
    </author>
    <author>
      <name>Jialin Wang</name>
    </author>
    <author>
      <name>Miao Zhao</name>
    </author>
    <author>
      <name>Wenjie Li</name>
    </author>
    <author>
      <name>Xing Xie</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3269206.3271739</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3269206.3271739" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03467v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03467v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09184v3</id>
    <updated>2018-08-25T05:22:45Z</updated>
    <published>2018-02-26T07:01:24Z</published>
    <title>Variance Reduction Methods for Sublinear Reinforcement Learning</title>
    <summary>  This work considers the problem of provably optimal reinforcement learning
for episodic finite horizon MDPs, i.e. how an agent learns to maximize his/her
long term reward in an uncertain environment. The main contribution is in
providing a novel algorithm --- Variance-reduced Upper Confidence Q-learning
(vUCQ) --- which enjoys a regret bound of $\widetilde{O}(\sqrt{HSAT} + H^5SA)$,
where the $T$ is the number of time steps the agent acts in the MDP, $S$ is the
number of states, $A$ is the number of actions, and $H$ is the (episodic)
horizon time.
  This is the first regret bound that is both sub-linear in the model size and
asymptotically optimal. The algorithm is sub-linear in that the time to achieve
$\epsilon$-average regret for any constant $\epsilon$ is $O(SA)$, which is a
number of samples that is far less than that required to learn any non-trivial
estimate of the transition model (the transition model is specified by
$O(S^2A)$ parameters). The importance of sub-linear algorithms is largely the
motivation for algorithms such as $Q$-learning and other "model free"
approaches. vUCQ algorithm also enjoys minimax optimal regret in the long run,
matching the $\Omega(\sqrt{HSAT})$ lower bound.
  Variance-reduced Upper Confidence Q-learning (vUCQ) is a successive
refinement method in which the algorithm reduces the variance in $Q$-value
estimates and couples this estimation scheme with an upper confidence based
algorithm. Technically, the coupling of both of these techniques is what leads
to the algorithm enjoying both the sub-linear regret property and the
asymptotically optimal regret.
</summary>
    <author>
      <name>Sham Kakade</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <author>
      <name>Lin F. Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed a bug of a previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09184v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09184v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02957v2</id>
    <updated>2018-08-24T23:59:31Z</updated>
    <published>2018-06-08T03:24:50Z</published>
    <title>A Deep Neural Network Surrogate for High-Dimensional Random Partial
  Differential Equations</title>
    <summary>  Developing efficient numerical algorithms for the solution of high
dimensional random Partial Differential Equations (PDEs) has been a challenging
task due to the well-known curse of dimensionality. We present a new solution
framework for these problems based on a deep learning approach. Specifically,
the random PDE is approximated by a feed-forward fully-connected deep residual
network, with either strong or weak enforcement of initial and boundary
constraints. The framework is mesh-free, and can handle irregular computational
domains. Parameters of the approximating deep neural network are determined
iteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.
The satisfactory accuracy of the proposed frameworks is numerically
demonstrated on diffusion and heat conduction problems, in comparison with the
converged Monte Carlo-based finite element results.
</summary>
    <author>
      <name>Mohammad Amin Nabian</name>
    </author>
    <author>
      <name>Hadi Meidani</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02957v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02957v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08317v1</id>
    <updated>2018-08-24T21:36:05Z</updated>
    <published>2018-08-24T21:36:05Z</published>
    <title>To Cluster, or Not to Cluster: An Analysis of Clusterability Methods</title>
    <summary>  Clustering is an essential data mining tool that aims to discover inherent
cluster structure in data. For most applications, applying clustering is only
appropriate when cluster structure is present. As such, the study of
clusterability, which evaluates whether data possesses such structure, is an
integral part of cluster analysis. However, methods for evaluating
clusterability vary radically, making it challenging to select a suitable
measure. In this paper, we perform an extensive comparison of measures of
clusterability and provide guidelines that clustering users can reference to
select suitable measures for their applications.
</summary>
    <author>
      <name>A. Adolfsson</name>
    </author>
    <author>
      <name>M. Ackerman</name>
    </author>
    <author>
      <name>N. C. Brownstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 3 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08315v1</id>
    <updated>2018-08-24T21:28:36Z</updated>
    <published>2018-08-24T21:28:36Z</published>
    <title>A Deterministic Self-Organizing Map Approach and its Application on
  Satellite Data based Cloud Type Classification</title>
    <summary>  A self-organizing map (SOM) is a type of competitive artificial neural
network, which projects the high-dimensional input space of the training
samples into a low-dimensional space with the topology relations preserved.
This makes SOMs supportive of organizing and visualizing complex data sets and
have been pervasively used among numerous disciplines with different
applications. Notwithstanding its wide applications, the self-organizing map is
perplexed by its inherent randomness, which produces dissimilar SOM patterns
even when being trained on identical training samples with the same parameters
every time, and thus causes usability concerns for other domain practitioners
and precludes more potential users from exploring SOM based applications in a
broader spectrum. Motivated by this practical concern, we propose a
deterministic approach as a supplement to the standard self-organizing map. In
accordance with the theoretical design, the experimental results with satellite
cloud data demonstrate the effective and efficient organization as well as
simplification capabilities of the proposed approach.
</summary>
    <author>
      <name>Wenbin Zhang</name>
    </author>
    <author>
      <name>Jianwu Wang</name>
    </author>
    <author>
      <name>Daeho Jin</name>
    </author>
    <author>
      <name>Lazaros Oreopoulos</name>
    </author>
    <author>
      <name>Zhibo Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08294v1</id>
    <updated>2018-08-24T20:13:49Z</updated>
    <published>2018-08-24T20:13:49Z</published>
    <title>Unknown Examples &amp; Machine Learning Model Generalization</title>
    <summary>  Over the past decades, researchers and ML practitioners have come up with
better and better ways to build, understand and improve the quality of ML
models, but mostly under the key assumption that the training data is
distributed identically to the testing data. In many real-world applications,
however, some potential training examples are unknown to the modeler, due to
sample selection bias or, more generally, covariate shift, i.e., a distribution
shift between the training and deployment stage. The resulting discrepancy
between training and testing distributions leads to poor generalization
performance of the ML model and hence biased predictions. We provide novel
algorithms that estimate the number and properties of these unknown training
examples---unknown unknowns. This information can then be used to correct the
training set, prior to seeing any test data. The key idea is to combine
species-estimation techniques with data-driven methods for estimating the
feature values for the unknown unknowns. Experiments on a variety of ML models
and datasets indicate that taking the unknown examples into account can yield a
more robust ML model that generalizes better.
</summary>
    <author>
      <name>Yeounoh Chung</name>
    </author>
    <author>
      <name>Peter J. Haas</name>
    </author>
    <author>
      <name>Eli Upfal</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08286v1</id>
    <updated>2018-08-24T19:44:30Z</updated>
    <published>2018-08-24T19:44:30Z</published>
    <title>Probabilistic Graphical Modeling approach to dynamic PET direct
  parametric map estimation and image reconstruction</title>
    <summary>  In the context of dynamic emission tomography, the conventional processing
pipeline consists of independent image reconstruction of single time frames,
followed by the application of a suitable kinetic model to time activity curves
(TACs) at the voxel or region-of-interest level. The relatively new field of 4D
PET direct reconstruction, by contrast, seeks to move beyond this scheme and
incorporate information from multiple time frames within the reconstruction
task. Existing 4D direct models are based on a deterministic description of
voxels' TACs, captured by the chosen kinetic model, considering the photon
counting process the only source of uncertainty. In this work, we introduce a
new probabilistic modeling strategy based on the key assumption that activity
time course would be subject to uncertainty even if the parameters of the
underlying dynamic process were known. This leads to a hierarchical Bayesian
model, which we formulate using the formalism of Probabilistic Graphical
Modeling (PGM). The inference of the joint probability density function arising
from PGM is addressed using a new gradient-based iterative algorithm, which
presents several advantages compared to existing direct methods: it is flexible
to an arbitrary choice of linear and nonlinear kinetic model; it enables the
inclusion of arbitrary (sub)differentiable priors for parametric maps; it is
simpler to implement and suitable to integration in computing frameworks for
machine learning. Computer simulations and an application to real patient scan
showed how the proposed approach allows us to weight the importance of the
kinetic model, providing a bridge between indirect and deterministic direct
methods.
</summary>
    <author>
      <name>Michele Scipioni</name>
    </author>
    <author>
      <name>Stefano Pedemonte</name>
    </author>
    <author>
      <name>Maria Filomena Santarelli</name>
    </author>
    <author>
      <name>Luigi Landini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages (main manuscript + supplementary material); submitted for
  peer-review to IEEE Transactions of Medical Imaging (IEEE-TMI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05559v2</id>
    <updated>2018-08-24T18:16:03Z</updated>
    <published>2018-06-13T13:57:13Z</published>
    <title>Extracting Parallel Sentences with Bidirectional Recurrent Neural
  Networks to Improve Machine Translation</title>
    <summary>  Parallel sentence extraction is a task addressing the data sparsity problem
found in multilingual natural language processing applications. We propose a
bidirectional recurrent neural network based approach to extract parallel
sentences from collections of multilingual texts. Our experiments with noisy
parallel corpora show that we can achieve promising results against a
competitive baseline by removing the need of specific feature engineering or
additional external resources. To justify the utility of our approach, we
extract sentence pairs from Wikipedia articles to train machine translation
systems and show significant improvements in translation performance.
</summary>
    <author>
      <name>Francis Grégoire</name>
    </author>
    <author>
      <name>Philippe Langlais</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, COLING 2018. arXiv admin note: text overlap with
  arXiv:1709.09783</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05559v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05559v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00552v3</id>
    <updated>2018-08-24T18:14:12Z</updated>
    <published>2018-06-01T22:07:13Z</published>
    <title>Bayesian approach to model-based extrapolation of nuclear observables</title>
    <summary>  The mass, or binding energy, is the basis property of the atomic nucleus. It
determines its stability, and reaction and decay rates. Quantifying the nuclear
binding is important for understanding the origin of elements in the universe.
The astrophysical processes responsible for the nucleosynthesis in stars often
take place far from the valley of stability, where experimental masses are not
known. In such cases, missing nuclear information must be provided by
theoretical predictions using extreme extrapolations. Bayesian machine learning
techniques can be applied to improve predictions by taking full advantage of
the information contained in the deviations between experimental and calculated
masses. We consider 10 global models based on nuclear Density Functional Theory
as well as two more phenomenological mass models. The emulators of S2n
residuals and credibility intervals defining theoretical error bars are
constructed using Bayesian Gaussian processes and Bayesian neural networks. We
consider a large training dataset pertaining to nuclei whose masses were
measured before 2003. For the testing datasets, we considered those exotic
nuclei whose masses have been determined after 2003. We then carried out
extrapolations towards the 2n dripline. While both Gaussian processes and
Bayesian neural networks reduce the rms deviation from experiment
significantly, GP offers a better and much more stable performance. The
increase in the predictive power is quite astonishing: the resulting rms
deviations from experiment on the testing dataset are similar to those of more
phenomenological models. The empirical coverage probability curves we obtain
match very well the reference values which is highly desirable to ensure
honesty of uncertainty quantification, and the estimated credibility intervals
on predictions make it possible to evaluate predictive power of individual
models.
</summary>
    <author>
      <name>Léo Neufcourt</name>
    </author>
    <author>
      <name>Yuchen Cao</name>
    </author>
    <author>
      <name>Witold Nazarewicz</name>
    </author>
    <author>
      <name>Frederi Viens</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00552v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00552v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15, 62P35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09064v3</id>
    <updated>2018-08-24T17:45:32Z</updated>
    <published>2018-02-25T19:06:06Z</published>
    <title>Time Series Analysis via Matrix Estimation</title>
    <summary>  We propose an algorithm to impute and forecast a time series by transforming
the observed time series into a matrix, utilizing matrix estimation to recover
missing values and de-noise observed entries, and performing linear regression
to make predictions. At the core of our analysis is a representation result,
which states that for a large model class, the transformed matrix obtained from
the time series via our algorithm is (approximately) low-rank. This, in effect,
generalizes the widely used Singular Spectrum Analysis (SSA) in literature, and
allows us to establish a rigorous link between time series analysis and matrix
estimation. The key is to construct a matrix with non-overlapping entries
rather than with the Hankel matrix as done in the literature, including in SSA.
We provide finite sample analysis for imputation and prediction leading to the
asymptotic consistency of our method. A salient feature of our algorithm is
that it is model agnostic both with respect to the underlying time dynamics as
well as the noise model in the observations. Being noise agnostic makes our
algorithm applicable to the setting where the state is hidden and we only have
access to its noisy observations a la a Hidden Markov Model, e.g., observing a
Poisson process with a time-varying parameter without knowing that the process
is Poisson, but still recovering the time-varying parameter accurately. As part
of the forecasting algorithm, an important task is to perform regression with
noisy observations of the features a la an error- in-variable regression. In
essence, our approach suggests a matrix estimation based method for such a
setting, which could be of interest in its own right. Through synthetic and
real-world datasets, we demonstrate that our algorithm outperforms standard
software packages (including R libraries) in the presence of missing data as
well as high levels of noise.
</summary>
    <author>
      <name>Anish Agarwal</name>
    </author>
    <author>
      <name>Muhammad Jehangir Amjad</name>
    </author>
    <author>
      <name>Devavrat Shah</name>
    </author>
    <author>
      <name>Dennis Shen</name>
    </author>
    <link href="http://arxiv.org/abs/1802.09064v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09064v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03963v3</id>
    <updated>2018-08-24T17:09:49Z</updated>
    <published>2018-05-10T13:24:34Z</published>
    <title>Monotone Learning with Rectified Wire Networks</title>
    <summary>  We introduce a new neural network model, together with a tractable and
monotone online learning algorithm. Our model describes feed-forward networks
for classification, with one output node for each class. The only nonlinear
operation is rectification using a ReLU function with a bias. However, there is
a rectifier on every edge rather than at the nodes of the network. There are
also weights, but these are positive, static, and associated with the nodes.
Our "rectified wire" networks are able to represent arbitrary Boolean
functions. Only the bias parameters, on the edges of the network, are learned.
Another departure in our approach, from standard neural networks, is that the
loss function is replaced by a constraint. This constraint is simply that the
value of the output node associated with the correct class should be zero. Our
model has the property that the exact norm-minimizing parameter update,
required to correctly classify a training item, is the solution to a quadratic
program that can be computed with a few passes through the network. We
demonstrate a training algorithm using this update, called sequential
deactivation (SDA), on MNIST and some synthetic datasets. Upon adopting a
natural choice for the nodal weights, SDA has no hyperparameters other than
those describing the network structure. Our experiments explore behavior with
respect to network size and depth in a family of sparse expander networks.
</summary>
    <author>
      <name>Veit Elser</name>
    </author>
    <author>
      <name>Dan Schmidt</name>
    </author>
    <author>
      <name>Jonathan Yedidia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 21 figures, new experimental results, various improvements</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03963v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03963v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08266v2</id>
    <updated>2018-08-24T16:27:28Z</updated>
    <published>2016-08-29T22:10:17Z</published>
    <title>Visualizing and Understanding Sum-Product Networks</title>
    <summary>  Sum-Product Networks (SPNs) are recently introduced deep tractable
probabilistic models by which several kinds of inference queries can be
answered exactly and in a tractable time. Up to now, they have been largely
used as black box density estimators, assessed only by comparing their
likelihood scores only. In this paper we explore and exploit the inner
representations learned by SPNs. We do this with a threefold aim: first we want
to get a better understanding of the inner workings of SPNs; secondly, we seek
additional ways to evaluate one SPN model and compare it against other
probabilistic models, providing diagnostic tools to practitioners; lastly, we
want to empirically evaluate how good and meaningful the extracted
representations are, as in a classic Representation Learning framework. In
order to do so we revise their interpretation as deep neural networks and we
propose to exploit several visualization techniques on their node activations
and network outputs under different types of inference queries. To investigate
these models as feature extractors, we plug some SPNs, learned in a greedy
unsupervised fashion on image datasets, in supervised classification learning
tasks. We extract several embedding types from node activations by filtering
nodes by their type, by their associated feature abstraction level and by their
scope. In a thorough empirical comparison we prove them to be competitive
against those generated from popular feature extractors as Restricted Boltzmann
Machines. Finally, we investigate embeddings generated from random
probabilistic marginal queries as means to compare other tractable
probabilistic models on a common ground, extending our experiments to Mixtures
of Trees.
</summary>
    <author>
      <name>Antonio Vergari</name>
    </author>
    <author>
      <name>Nicola Di Mauro</name>
    </author>
    <author>
      <name>Floriana Esposito</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10994-018-5760-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10994-018-5760-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning Journal paper (First Online), 24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08266v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08266v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08195v1</id>
    <updated>2018-08-24T16:12:21Z</updated>
    <published>2018-08-24T16:12:21Z</published>
    <title>GoT-WAVE: Temporal network alignment using graphlet-orbit transitions</title>
    <summary>  Global pairwise network alignment (GPNA) aims to find a one-to-one node
mapping between two networks that identifies conserved network regions. GPNA
algorithms optimize node conservation (NC) and edge conservation (EC). NC
quantifies topological similarity between nodes. Graphlet-based degree vectors
(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were
used as a dynamic NC measure within the first-ever algorithms for GPNA of
temporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger
networks. We recently developed a different graphlet-based measure of temporal
node similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead
of DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new
approach, GoT-WAVE.
  On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed
by 64%. On real networks, when optimizing only dynamic NC, each method is
superior ~50% of the time. While DynaWAVE benefits more from also optimizing
dynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a
promising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.
Future work on better incorporating dynamic EC may yield further improvements.
</summary>
    <author>
      <name>David Aparício</name>
    </author>
    <author>
      <name>Pedro Ribeiro</name>
    </author>
    <author>
      <name>Tijana Milenković</name>
    </author>
    <author>
      <name>Fernando Silva</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02162v3</id>
    <updated>2018-08-24T15:12:46Z</updated>
    <published>2017-12-06T12:44:27Z</published>
    <title>A trans-disciplinary review of deep learning research for water
  resources scientists</title>
    <summary>  Deep learning (DL), a new-generation of artificial neural network research,
has transformed industries, daily lives and various scientific disciplines in
recent years. DL represents significant progress in the ability of neural
networks to automatically engineer problem-relevant features and capture highly
complex data distributions. I argue that DL can help address several major new
and old challenges facing research in water sciences such as
inter-disciplinarity, data discoverability, hydrologic scaling, equifinality,
and needs for parameter regionalization. This review paper is intended to
provide water resources scientists and hydrologists in particular with a simple
technical overview, trans-disciplinary progress update, and a source of
inspiration about the relevance of DL to water. The review reveals that various
physical and geoscientific disciplines have utilized DL to address data
challenges, improve efficiency, and gain scientific insights. DL is especially
suited for information extraction from image-like data and sequential data.
Techniques and experiences presented in other disciplines are of high relevance
to water research. Meanwhile, less noticed is that DL may also serve as a
scientific exploratory tool. A new area termed 'AI neuroscience,' where
scientists interpret the decision process of deep networks and derive insights,
has been born. This budding sub-discipline has demonstrated methods including
correlation-based analysis, inversion of network-extracted features,
reduced-order approximations by interpretable models, and attribution of
network decisions to inputs. Moreover, DL can also use data to condition
neurons that mimic problem-specific fundamental organizing units, thus
revealing emergent behaviors of these units. Vast opportunities exist for DL to
propel advances in water sciences.
</summary>
    <author>
      <name>Chaopeng Shen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1029/2018WR022643</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1029/2018WR022643" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Water Resources Research, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.02162v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02162v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08166v1</id>
    <updated>2018-08-24T15:08:33Z</updated>
    <published>2018-08-24T15:08:33Z</published>
    <title>An Empirical Study of Rich Subgroup Fairness for Machine Learning</title>
    <summary>  Kearns et al. [2018] recently proposed a notion of rich subgroup fairness
intended to bridge the gap between statistical and individual notions of
fairness. Rich subgroup fairness picks a statistical fairness constraint (say,
equalizing false positive rates across protected groups), but then asks that
this constraint hold over an exponentially or infinitely large collection of
subgroups defined by a class of functions with bounded VC dimension. They give
an algorithm guaranteed to learn subject to this constraint, under the
condition that it has access to oracles for perfectly learning absent a
fairness constraint. In this paper, we undertake an extensive empirical
evaluation of the algorithm of Kearns et al. On four real datasets for which
fairness is a concern, we investigate the basic convergence of the algorithm
when instantiated with fast heuristics in place of learning oracles, measure
the tradeoffs between fairness and accuracy, and compare this approach with the
recent algorithm of Agarwal et al. [2018], which implements weaker and more
traditional marginal fairness constraints defined by individual protected
attributes. We find that in general, the Kearns et al. algorithm converges
quickly, large gains in fairness can be obtained with mild costs to accuracy,
and that optimizing accuracy subject only to marginal fairness leads to
classifiers with substantial subgroup unfairness. We also provide a number of
analyses and visualizations of the dynamics and behavior of the Kearns et al.
algorithm. Overall we find this algorithm to be effective on real data, and
rich subgroup fairness to be a viable notion in practice.
</summary>
    <author>
      <name>Michael Kearns</name>
    </author>
    <author>
      <name>Seth Neel</name>
    </author>
    <author>
      <name>Aaron Roth</name>
    </author>
    <author>
      <name>Zhiwei Steven Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08230v1</id>
    <updated>2018-08-24T13:57:40Z</updated>
    <published>2018-08-24T13:57:40Z</published>
    <title>Using Apple Machine Learning Algorithms to Detect and Subclassify
  Non-Small Cell Lung Cancer</title>
    <summary>  Lung cancer continues to be a major healthcare challenge with high morbidity
and mortality rates among both men and women worldwide. The majority of lung
cancer cases are of non-small cell lung cancer type. With the advent of
targeted cancer therapy, it is imperative not only to properly diagnose but
also sub-classify non-small cell lung cancer. In our study, we evaluated the
utility of using Apple Create ML module to detect and sub-classify non-small
cell carcinomas based on histopathological images. After module optimization,
the program detected 100% of non-small cell lung cancer images and successfully
subclassified the majority of the images. Trained modules, such as ours, can be
utilized in diagnostic smartphone-based applications, augmenting diagnostic
services in understaffed areas of the world.
</summary>
    <author>
      <name>Andrew A. Borkowski MD</name>
    </author>
    <author>
      <name>Catherine P. Wilson MT</name>
    </author>
    <author>
      <name>Steven A. Borkowski</name>
    </author>
    <author>
      <name>Lauren A. Deland RN</name>
    </author>
    <author>
      <name>Stephen M. Mastorides MD</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08097v1</id>
    <updated>2018-08-24T11:36:15Z</updated>
    <published>2018-08-24T11:36:15Z</published>
    <title>Memory Time Span in LSTMs for Multi-Speaker Source Separation</title>
    <summary>  With deep learning approaches becoming state-of-the-art in many speech (as
well as non-speech) related machine learning tasks, efforts are being taken to
delve into the neural networks which are often considered as a black box. In
this paper it is analyzed how recurrent neural network (RNNs) cope with
temporal dependencies by determining the relevant memory time span in a long
short-term memory (LSTM) cell. This is done by leaking the state variable with
a controlled lifetime and evaluating the task performance. This technique can
be used for any task to estimate the time span the LSTM exploits in that
specific scenario. The focus in this paper is on the task of separating
speakers from overlapping speech. We discern two effects: A long term effect,
probably due to speaker characterization and a short term effect, probably
exploiting phone-size formant tracks.
</summary>
    <author>
      <name>Jeroen Zegers</name>
    </author>
    <author>
      <name>Hugo Van hamme</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.08097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08095v1</id>
    <updated>2018-08-24T11:29:07Z</updated>
    <published>2018-08-24T11:29:07Z</published>
    <title>Multi-scenario deep learning for multi-speaker source separation</title>
    <summary>  Research in deep learning for multi-speaker source separation has received a
boost in the last years. However, most studies are restricted to mixtures of a
specific number of speakers, called a specific scenario. While some works
included experiments for different scenarios, research towards combining data
of different scenarios or creating a single model for multiple scenarios have
been very rare. In this work it is shown that data of a specific scenario is
relevant for solving another scenario. Furthermore, it is concluded that a
single model, trained on different scenarios is capable of matching performance
of scenario specific models.
</summary>
    <author>
      <name>Jeroen Zegers</name>
    </author>
    <author>
      <name>Hugo Van hamme</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.08095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08068v1</id>
    <updated>2018-08-24T09:55:41Z</updated>
    <published>2018-08-24T09:55:41Z</published>
    <title>Self-Paced Multi-Task Clustering</title>
    <summary>  Multi-task clustering (MTC) has attracted a lot of research attentions in
machine learning due to its ability in utilizing the relationship among
different tasks. Despite the success of traditional MTC models, they are either
easy to stuck into local optima, or sensitive to outliers and noisy data. To
alleviate these problems, we propose a novel self-paced multi-task clustering
(SPMTC) paradigm. In detail, SPMTC progressively selects data examples to train
a series of MTC models with increasing complexity, thus highly decreases the
risk of trapping into poor local optima. Furthermore, to reduce the negative
influence of outliers and noisy data, we design a soft version of SPMTC to
further improve the clustering performance. The corresponding SPMTC framework
can be easily solved by an alternating optimization method. The proposed model
is guaranteed to converge and experiments on real data sets have demonstrated
its promising results compared with state-of-the-art multi-task clustering
methods.
</summary>
    <author>
      <name>Yazhou Ren</name>
    </author>
    <author>
      <name>Xiaofan Que</name>
    </author>
    <author>
      <name>Dezhong Yao</name>
    </author>
    <author>
      <name>Zenglin Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05833v2</id>
    <updated>2018-08-24T07:12:13Z</updated>
    <published>2018-06-15T07:24:36Z</published>
    <title>On the exact minimization of saturated loss functions for robust
  regression and subspace estimation</title>
    <summary>  This paper deals with robust regression and subspace estimation and more
precisely with the problem of minimizing a saturated loss function. In
particular, we focus on computational complexity issues and show that an exact
algorithm with polynomial time-complexity with respect to the number of data
can be devised for robust regression and subspace estimation. This result is
obtained by adopting a classification point of view and relating the problems
to the search for a linear model that can approximate the maximal number of
points with a given error. Approximate variants of the algorithms based on
ramdom sampling are also discussed and experiments show that it offers an
accuracy gain over the traditional RANSAC for a similar algorithmic simplicity.
</summary>
    <author>
      <name>Fabien Lauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ABC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters, Elsevier, 2018,
  \&amp;\#x3008;10.1016/j.patrec.2018.08.004\&amp;\#x3009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.05833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08023v1</id>
    <updated>2018-08-24T06:52:04Z</updated>
    <published>2018-08-24T06:52:04Z</published>
    <title>A Jointly Learned Context-Aware Place of Interest Embedding for Trip
  Recommendations</title>
    <summary>  Trip recommendation is an important location-based service that helps relieve
users from the time and efforts for trip planning. It aims to recommend a
sequence of places of interest (POIs) for a user to visit that maximizes the
user's satisfaction. When adding a POI to a recommended trip, it is essential
to understand the context of the recommendation, including the POI popularity,
other POIs co-occurring in the trip, and the preferences of the user. These
contextual factors are learned separately in existing studies, while in
reality, they impact jointly on a user's choice of a POI to visit. In this
study, we propose a POI embedding model to jointly learn the impact of these
contextual factors. We call the learned POI embedding a context-aware POI
embedding. To showcase the effectiveness of this embedding, we apply it to
generate trip recommendations given a user and a time budget. We propose two
trip recommendation algorithms based on our context-aware POI embedding. The
first algorithm finds the exact optimal trip by transforming and solving the
trip recommendation problem as an integer linear programming problem. To
achieve a high computation efficiency, the second algorithm finds a
heuristically optimal trip based on adaptive large neighborhood search. We
perform extensive experiments on real datasets. The results show that our
proposed algorithms consistently outperform state-of-the-art algorithms in trip
recommendation quality, with an advantage of up to 43% in F1-score.
</summary>
    <author>
      <name>Jiayuan He</name>
    </author>
    <author>
      <name>Jianzhong Qi</name>
    </author>
    <author>
      <name>Kotagiri Ramamohanarao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08013v1</id>
    <updated>2018-08-24T06:08:56Z</updated>
    <published>2018-08-24T06:08:56Z</published>
    <title>Reinforcement Learning for Relation Classification from Noisy Data</title>
    <summary>  Existing relation classification methods that rely on distant supervision
assume that a bag of sentences mentioning an entity pair are all describing a
relation for the entity pair. Such methods, performing classification at the
bag level, cannot identify the mapping between a relation and a sentence, and
largely suffers from the noisy labeling problem. In this paper, we propose a
novel model for relation classification at the sentence level from noisy data.
The model has two modules: an instance selector and a relation classifier. The
instance selector chooses high-quality sentences with reinforcement learning
and feeds the selected sentences into the relation classifier, and the relation
classifier makes sentence level prediction and provides rewards to the instance
selector. The two modules are trained jointly to optimize the instance
selection and relation classification processes. Experiment results show that
our model can deal with the noise of data effectively and obtains better
performance for relation classification at the sentence level.
</summary>
    <author>
      <name>Jun Feng</name>
    </author>
    <author>
      <name>Minlie Huang</name>
    </author>
    <author>
      <name>Li Zhao</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Xiaoyan Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01876v2</id>
    <updated>2018-08-24T06:02:32Z</updated>
    <published>2018-08-06T13:16:52Z</published>
    <title>An Efficient Deep Reinforcement Learning Model for Urban Traffic Control</title>
    <summary>  Urban Traffic Control (UTC) plays an essential role in Intelligent
Transportation System (ITS) but remains difficult. Since model-based UTC
methods may not accurately describe the complex nature of traffic dynamics in
all situations, model-free data-driven UTC methods, especially reinforcement
learning (RL) based UTC methods, received increasing interests in the last
decade. However, existing DL approaches did not propose an efficient algorithm
to solve the complicated multiple intersections control problems whose
state-action spaces are vast. To solve this problem, we propose a Deep
Reinforcement Learning (DRL) algorithm that combines several tricks to master
an appropriate control strategy within an acceptable time. This new algorithm
relaxes the fixed traffic demand pattern assumption and reduces human invention
in parameter tuning. Simulation experiments have shown that our method
outperforms traditional rule-based approaches and has the potential to handle
more complex traffic problems in the real world.
</summary>
    <author>
      <name>Yilun Lin</name>
    </author>
    <author>
      <name>Xingyuan Dai</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Fei-Yue Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01876v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01876v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01771v3</id>
    <updated>2018-08-24T04:22:19Z</updated>
    <published>2018-06-05T16:09:45Z</published>
    <title>Cycle-Consistent Adversarial Learning as Approximate Bayesian Inference</title>
    <summary>  We formalize the problem of learning interdomain correspondences in the
absence of paired data as Bayesian inference in a latent variable model (LVM),
where one seeks the underlying hidden representations of entities from one
domain as entities from the other domain. First, we introduce implicit latent
variable models, where the prior over hidden representations can be specified
flexibly as an implicit distribution. Next, we develop a new variational
inference (VI) algorithm for this model based on minimization of the symmetric
Kullback-Leibler (KL) divergence between a variational joint and the exact
joint distribution. Lastly, we demonstrate that the state-of-the-art
cycle-consistent adversarial learning (CYCLEGAN) models can be derived as a
special case within our proposed VI framework, thus establishing its connection
to approximate Bayesian inference methods.
</summary>
    <author>
      <name>Louis C. Tiao</name>
    </author>
    <author>
      <name>Edwin V. Bonilla</name>
    </author>
    <author>
      <name>Fabio Ramos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the ICML 2018 Workshop on Theoretical Foundations and
  Applications of Deep Generative Models. Stockholm, Sweden, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01771v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01771v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06670v2</id>
    <updated>2018-08-24T04:02:03Z</updated>
    <published>2018-08-20T19:52:51Z</published>
    <title>Learning deep representations by mutual information estimation and
  maximization</title>
    <summary>  Many popular representation-learning algorithms use training objectives
defined on the observed data space, which we call pixel-level. This may be
detrimental when only a small fraction of the bits of signal actually matter at
a semantic level. We hypothesize that representations should be learned and
evaluated more directly in terms of their information content and statistical
or structural constraints. To address the first quality, we consider learning
unsupervised representations by maximizing mutual information between part or
all of the input and a high-level feature vector. To address the second, we
control characteristics of the representation by matching to a prior
adversarially. Our method, which we call Deep INFOMAX (DIM), can be used to
learn representations with desired characteristics and which empirically
outperform a number of popular unsupervised learning methods on classification
tasks. DIM opens new avenues for unsupervised learn-ing of representations and
is an important step towards flexible formulations of representation learning
objectives catered towards specific end-goals.
</summary>
    <author>
      <name>R Devon Hjelm</name>
    </author>
    <author>
      <name>Alex Fedorov</name>
    </author>
    <author>
      <name>Samuel Lavoie-Marchildon</name>
    </author>
    <author>
      <name>Karan Grewal</name>
    </author>
    <author>
      <name>Adam Trischler</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07997v1</id>
    <updated>2018-08-24T03:42:44Z</updated>
    <published>2018-08-24T03:42:44Z</published>
    <title>Non-asymptotic bounds for percentiles of independent non-identical
  random variables</title>
    <summary>  This note displays an interesting phenomenon for percentiles of independent
but non-identical random variables. Let $X_1,\cdots,X_n$ be independent random
variables obeying non-identical continuous distributions and $X^{(1)}\geq
\cdots\geq X^{(n)}$ be the corresponding order statistics. For any $p\in(0,1)$,
we investigate the $100(1-p)$%-th percentile $X^{(pn)}$ and prove
non-asymptotic bounds for $X^{(pn)}$. In particular, for a wide class of
distributions, we discover an intriguing connection between their median and
the harmonic mean of the associated standard deviations. For example, if
$X_k\sim\mathcal{N}(0,\sigma_k^2)$ for $k=1,\cdots,n$ and $p=\frac{1}{2}$, we
show that its median $\big|{\rm Med}\big(X_1,\cdots,X_n\big)\big|=
O_P\Big(n^{1/2}\cdot\big(\sum_{k=1}^n\sigma_k^{-1}\big)^{-1}\Big)$ as long as
$\{\sigma_k\}_{k=1}^n$ satisfy certain mild non-dispersion property.
</summary>
    <author>
      <name>Dong Xia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05760v2</id>
    <updated>2018-08-24T03:26:42Z</updated>
    <published>2018-08-17T05:25:29Z</published>
    <title>Data Poisoning Attacks in Contextual Bandits</title>
    <summary>  We study offline data poisoning attacks in contextual bandits, a class of
reinforcement learning problems with important applications in online
recommendation and adaptive medical treatment, among others. We provide a
general attack framework based on convex optimization and show that by slightly
manipulating rewards in the data, an attacker can force the bandit algorithm to
pull a target arm for a target contextual vector. The target arm and target
contextual vector are both chosen by the attacker. That is, the attacker can
hijack the behavior of a contextual bandit. We also investigate the feasibility
and the side effects of such attacks, and identify future directions for
defense. Experiments on both synthetic and real-world data demonstrate the
efficiency of the attack algorithm.
</summary>
    <author>
      <name>Yuzhe Ma</name>
    </author>
    <author>
      <name>Kwang-Sung Jun</name>
    </author>
    <author>
      <name>Lihong Li</name>
    </author>
    <author>
      <name>Xiaojin Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GameSec 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05760v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05760v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07992v1</id>
    <updated>2018-08-24T03:15:15Z</updated>
    <published>2018-08-24T03:15:15Z</published>
    <title>Undersampling and Bagging of Decision Trees in the Analysis of
  Cardiorespiratory Behavior for the Prediction of Extubation Readiness in
  Extremely Preterm Infants</title>
    <summary>  Extremely preterm infants often require endotracheal intubation and
mechanical ventilation during the first days of life. Due to the detrimental
effects of prolonged invasive mechanical ventilation (IMV), clinicians aim to
extubate infants as soon as they deem them ready. Unfortunately, existing
strategies for prediction of extubation readiness vary across clinicians and
institutions, and lead to high reintubation rates. We present an approach using
Random Forest classifiers for the analysis of cardiorespiratory variability to
predict extubation readiness. We address the issue of data imbalance by
employing random undersampling of examples from the majority class before
training each Decision Tree in a bag. By incorporating clinical domain
knowledge, we further demonstrate that our classifier could have identified 71%
of infants who failed extubation, while maintaining a success detection rate of
78%.
</summary>
    <author>
      <name>Lara J. Kanbar</name>
    </author>
    <author>
      <name>Charles C. Onu</name>
    </author>
    <author>
      <name>Wissam Shalish</name>
    </author>
    <author>
      <name>Karen A. Brown</name>
    </author>
    <author>
      <name>Guilherme M. Sant'Anna</name>
    </author>
    <author>
      <name>Robert E. Kearney</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: 2018 40th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07991v1</id>
    <updated>2018-08-24T03:10:48Z</updated>
    <published>2018-08-24T03:10:48Z</published>
    <title>Predicting Extubation Readiness in Extreme Preterm Infants based on
  Patterns of Breathing</title>
    <summary>  Extremely preterm infants commonly require intubation and invasive mechanical
ventilation after birth. While the duration of mechanical ventilation should be
minimized in order to avoid complications, extubation failure is associated
with increases in morbidities and mortality. As part of a prospective
observational study aimed at developing an accurate predictor of extubation
readiness, Markov and semi-Markov chain models were applied to gain insight
into the respiratory patterns of these infants, with more robust time-series
modeling using semi-Markov models. This model revealed interesting similarities
and differences between newborns who succeeded extubation and those who failed.
The parameters of the model were further applied to predict extubation
readiness via generative (joint likelihood) and discriminative (support vector
machine) approaches. Results showed that up to 84\% of infants who failed
extubation could have been accurately identified prior to extubation.
</summary>
    <author>
      <name>Charles C. Onu</name>
    </author>
    <author>
      <name>Lara J. Kanbar</name>
    </author>
    <author>
      <name>Wissam Shalish</name>
    </author>
    <author>
      <name>Karen A. Brown</name>
    </author>
    <author>
      <name>Guilherme M. Sant'Anna</name>
    </author>
    <author>
      <name>Robert E. Kearney</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: 2017 IEEE Symposium Series on Computational
  Intelligence (SSCI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07989v1</id>
    <updated>2018-08-24T03:04:11Z</updated>
    <published>2018-08-24T03:04:11Z</published>
    <title>A Semi-Markov Chain Approach to Modeling Respiratory Patterns Prior to
  Extubation in Preterm Infants</title>
    <summary>  After birth, extremely preterm infants often require specialized respiratory
management in the form of invasive mechanical ventilation (IMV). Protracted IMV
is associated with detrimental outcomes and morbidities. Premature extubation,
on the other hand, would necessitate reintubation which is risky, technically
challenging and could further lead to lung injury or disease. We present an
approach to modeling respiratory patterns of infants who succeeded extubation
and those who required reintubation which relies on Markov models. We compare
the use of traditional Markov chains to semi-Markov models which emphasize
cross-pattern transitions and timing information, and to multi-chain Markov
models which can concisely represent non-stationarity in respiratory behavior
over time. The models we developed expose specific, unique similarities as well
as vital differences between the two populations.
</summary>
    <author>
      <name>Charles C. Onu</name>
    </author>
    <author>
      <name>Lara J. Kanbar</name>
    </author>
    <author>
      <name>Wissam Shalish</name>
    </author>
    <author>
      <name>Karen A. Brown</name>
    </author>
    <author>
      <name>Guilherme M. Sant'Anna</name>
    </author>
    <author>
      <name>Robert E. Kearney</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: 2017 39th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07632v2</id>
    <updated>2018-08-24T02:26:23Z</updated>
    <published>2018-08-23T04:44:25Z</published>
    <title>DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection
  with GAN</title>
    <summary>  Recently, the introduction of the generative adversarial network (GAN) and
its variants has enabled the generation of realistic synthetic samples, which
has been used for enlarging training sets. Previous work primarily focused on
data augmentation for semi-supervised and supervised tasks. In this paper, we
instead focus on unsupervised anomaly detection and propose a novel generative
data augmentation framework optimized for this task. In particular, we propose
to oversample infrequent normal samples - normal samples that occur with small
probability, e.g., rare normal events. We show that these samples are
responsible for false positives in anomaly detection. However, oversampling of
infrequent normal samples is challenging for real-world high-dimensional data
with multimodal distributions. To address this challenge, we propose to use a
GAN variant known as the adversarial autoencoder (AAE) to transform the
high-dimensional multimodal data distributions into low-dimensional unimodal
latent distributions with well-defined tail probability. Then, we
systematically oversample at the `edge' of the latent distributions to increase
the density of infrequent normal samples. We show that our oversampling
pipeline is a unified one: it is generally applicable to datasets with
different complex data distributions. To the best of our knowledge, our method
is the first data augmentation technique focused on improving performance in
unsupervised anomaly detection. We validate our method by demonstrating
consistent improvements across several real-world datasets.
</summary>
    <author>
      <name>Swee Kiat Lim</name>
    </author>
    <author>
      <name>Yi Loo</name>
    </author>
    <author>
      <name>Ngoc-Trung Tran</name>
    </author>
    <author>
      <name>Ngai-Man Cheung</name>
    </author>
    <author>
      <name>Gemma Roig</name>
    </author>
    <author>
      <name>Yuval Elovici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICDM 2018 (IEEE International
  Conference on Data Mining)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07632v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07632v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07983v1</id>
    <updated>2018-08-24T02:14:45Z</updated>
    <published>2018-08-24T02:14:45Z</published>
    <title>Analysis of Noise Contrastive Estimation from the Perspective of
  Asymptotic Variance</title>
    <summary>  There are many models, often called unnormalized models, whose normalizing
constants are not calculated in closed form. Maximum likelihood estimation is
not directly applicable to unnormalized models. Score matching, contrastive
divergence method, pseudo-likelihood, Monte Carlo maximum likelihood, and noise
contrastive estimation (NCE) are popular methods for estimating parameters of
such models. In this paper, we focus on NCE. The estimator derived from NCE is
consistent and asymptotically normal because it is an M-estimator. NCE
characteristically uses an auxiliary distribution to calculate the normalizing
constant in the same spirit of the importance sampling. In addition, there are
several candidates as objective functions of NCE.
  We focus on how to reduce asymptotic variance. First, we propose a method for
reducing asymptotic variance by estimating the parameters of the auxiliary
distribution. Then, we determine the form of the objective functions, where the
asymptotic variance takes the smallest values in the original estimator class
and the proposed estimator classes. We further analyze the robustness of the
estimator.
</summary>
    <author>
      <name>Masatoshi Uehara</name>
    </author>
    <author>
      <name>Takeru Matsuda</name>
    </author>
    <author>
      <name>Fumiyasu Komaki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07982v1</id>
    <updated>2018-08-24T02:14:43Z</updated>
    <published>2018-08-24T02:14:43Z</published>
    <title>Proximal Policy Optimization and its Dynamic Version for Sequence
  Generation</title>
    <summary>  In sequence generation task, many works use policy gradient for model
optimization to tackle the intractable backpropagation issue when maximizing
the non-differentiable evaluation metrics or fooling the discriminator in
adversarial learning. In this paper, we replace policy gradient with proximal
policy optimization (PPO), which is a proved more efficient reinforcement
learning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We
demonstrate the efficacy of PPO and PPO-dynamic on conditional sequence
generation tasks including synthetic experiment and chit-chat chatbot. The
results show that PPO and PPO-dynamic can beat policy gradient by stability and
performance.
</summary>
    <author>
      <name>Yi-Lin Tuan</name>
    </author>
    <author>
      <name>Jinzhi Zhang</name>
    </author>
    <author>
      <name>Yujia Li</name>
    </author>
    <author>
      <name>Hung-yi Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00057v2</id>
    <updated>2018-08-23T21:14:20Z</updated>
    <published>2018-03-30T21:13:34Z</published>
    <title>Understanding Autoencoders with Information Theoretic Concepts</title>
    <summary>  Despite their great success in practical applications, there is still a lack
of theoretical and systematic methods to analyze deep neural networks. In this
paper, we illustrate an advanced information theoretic methodology to
understand the dynamics of learning and the design of autoencoders, a special
type of deep learning architectures that resembles a communication channel. By
generalizing the information plane to any cost function, and inspecting the
roles and dynamics of different layers using layer-wise information quantities,
we emphasize the role that mutual information plays in quantifying learning
from data. We further suggest and also experimentally validate, for mean square
error training, three fundamental properties regarding the layer-wise flow of
information and intrinsic dimensionality of the bottleneck layer, using
respectively the data processing inequality and the identification of a
bifurcation point in the information plane that is controlled by the given
data. Our observations have direct impact on the optimal design of
autoencoders, the design of alternative feedforward training methods, and even
in the problem of generalization.
</summary>
    <author>
      <name>Shujian Yu</name>
    </author>
    <author>
      <name>Jose C. Principe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">64 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07945v1</id>
    <updated>2018-08-23T20:58:08Z</updated>
    <published>2018-08-23T20:58:08Z</published>
    <title>Maximal Jacobian-based Saliency Map Attack</title>
    <summary>  The Jacobian-based Saliency Map Attack is a family of adversarial attack
methods for fooling classification models, such as deep neural networks for
image classification tasks. By saturating a few pixels in a given image to
their maximum or minimum values, JSMA can cause the model to misclassify the
resulting adversarial image as a specified erroneous target class. We propose
two variants of JSMA, one which removes the requirement to specify a target
class, and another that additionally does not need to specify whether to only
increase or decrease pixel intensities. Our experiments highlight the
competitive speeds and qualities of these variants when applied to datasets of
hand-written digits and natural scenes.
</summary>
    <author>
      <name>Rey Wiyatno</name>
    </author>
    <author>
      <name>Anqi Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of extended abstract for MAIS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07912v1</id>
    <updated>2018-08-23T19:19:16Z</updated>
    <published>2018-08-23T19:19:16Z</published>
    <title>Multivariate Extension of Matrix-based Renyi's α-order Entropy
  Functional</title>
    <summary>  The matrix-based Renyi's {\alpha}-order entropy functional was recently
introduced using the normalized eigenspectrum of an Hermitian matrix of the
projected data in the reproducing kernel Hilbert space (RKHS). However, the
current theory in the matrix-based Renyi's {\alpha}-order entropy functional
only defines the entropy of a single variable or mutual information between two
random variables. In information theory and machine learning communities, one
is also frequently interested in multivariate information quantities, such as
the multivariate joint entropy and different interactive quantities among
multiple variables. In this paper, we first define the matrix-based Renyi's
{\alpha}-order joint entropy among multiple variables. We then show how this
definition can ease the estimation of various information quantities that
measure the interactions among multiple variables, such as interactive
information and total correlation. We finally present an application to feature
selection to show how our definition provides a simple yet powerful way to
estimate a widely-acknowledged intractable quantity from data. A real example
on hyperspectral image (HSI) band selection is also provided.
</summary>
    <author>
      <name>Shujian Yu</name>
    </author>
    <author>
      <name>Luis Gonzalo Sanchez Giraldo</name>
    </author>
    <author>
      <name>Robert Jenssen</name>
    </author>
    <author>
      <name>Jose C. Principe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07910v1</id>
    <updated>2018-08-23T19:17:24Z</updated>
    <published>2018-08-23T19:17:24Z</published>
    <title>The Importance of Generation Order in Language Modeling</title>
    <summary>  Neural language models are a critical component of state-of-the-art systems
for machine translation, summarization, audio transcription, and other tasks.
These language models are almost universally autoregressive in nature,
generating sentences one token at a time from left to right. This paper studies
the influence of token generation order on model quality via a novel two-pass
language model that produces partially-filled sentence "templates" and then
fills in missing tokens. We compare various strategies for structuring these
two passes and observe a surprisingly large variation in model quality. We find
the most effective strategy generates function words in the first pass followed
by content words in the second. We believe these experimental results justify a
more extensive investigation of generation order for neural language models.
</summary>
    <author>
      <name>Nicolas Ford</name>
    </author>
    <author>
      <name>Daniel Duckworth</name>
    </author>
    <author>
      <name>Mohammad Norouzi</name>
    </author>
    <author>
      <name>George E. Dahl</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07903v1</id>
    <updated>2018-08-23T18:50:49Z</updated>
    <published>2018-08-23T18:50:49Z</published>
    <title>LIFT: Reinforcement Learning in Computer Systems by Learning From
  Demonstrations</title>
    <summary>  Reinforcement learning approaches have long appealed to the data management
community due to their ability to learn to control dynamic behavior from raw
system performance. Recent successes in combining deep neural networks with
reinforcement learning have sparked significant new interest in this domain.
However, practical solutions remain elusive due to large training data
requirements, algorithmic instability, and lack of standard tools. In this
work, we introduce LIFT, an end-to-end software stack for applying deep
reinforcement learning to data management tasks. While prior work has
frequently explored applications in simulations, LIFT centers on utilizing
human expertise to learn from demonstrations, thus lowering online training
times. We further introduce TensorForce, a TensorFlow library for applied deep
reinforcement learning exposing a unified declarative interface to common RL
algorithms, thus providing a backend to LIFT. We demonstrate the utility of
LIFT in two case studies in database compound indexing and resource management
in stream processing. Results show LIFT controllers initialized from
demonstrations can outperform human baselines and heuristics across latency
metrics and space usage by up to 70%.
</summary>
    <author>
      <name>Michael Schaarschmidt</name>
    </author>
    <author>
      <name>Alexander Kuhnle</name>
    </author>
    <author>
      <name>Ben Ellis</name>
    </author>
    <author>
      <name>Kai Fricke</name>
    </author>
    <author>
      <name>Felix Gessert</name>
    </author>
    <author>
      <name>Eiko Yoneki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07840v1</id>
    <updated>2018-08-23T16:55:53Z</updated>
    <published>2018-08-23T16:55:53Z</published>
    <title>Learning to Importance Sample in Primary Sample Space</title>
    <summary>  Importance sampling is one of the most widely used variance reduction
strategies in Monte Carlo rendering. In this paper, we propose a novel
importance sampling technique that uses a neural network to learn how to sample
from a desired density represented by a set of samples. Our approach considers
an existing Monte Carlo rendering algorithm as a black box. During a
scene-dependent training phase, we learn to generate samples with a desired
density in the primary sample space of the rendering algorithm using maximum
likelihood estimation. We leverage a recent neural network architecture that
was designed to represent real-valued non-volume preserving ('Real NVP')
transformations in high dimensional spaces. We use Real NVP to non-linearly
warp primary sample space and obtain desired densities. In addition, Real NVP
efficiently computes the determinant of the Jacobian of the warp, which is
required to implement the change of integration variables implied by the warp.
A main advantage of our approach is that it is agnostic of underlying light
transport effects, and can be combined with many existing rendering techniques
by treating them as a black box. We show that our approach leads to effective
variance reduction in several practical scenarios.
</summary>
    <author>
      <name>Quan Zheng</name>
    </author>
    <author>
      <name>Matthias Zwicker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to SIGGRAPH ASIA'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07804v1</id>
    <updated>2018-08-23T15:27:14Z</updated>
    <published>2018-08-23T15:27:14Z</published>
    <title>Transfer Learning for Estimating Causal Effects using Neural Networks</title>
    <summary>  We develop new algorithms for estimating heterogeneous treatment effects,
combining recent developments in transfer learning for neural networks with
insights from the causal inference literature. By taking advantage of transfer
learning, we are able to efficiently use different data sources that are
related to the same underlying causal mechanisms. We compare our algorithms
with those in the extant literature using extensive simulation studies based on
large-scale voter persuasion experiments and the MNIST database. Our methods
can perform an order of magnitude better than existing benchmarks while using a
fraction of the data.
</summary>
    <author>
      <name>Sören R. Künzel</name>
    </author>
    <author>
      <name>Bradly C. Stadie</name>
    </author>
    <author>
      <name>Nikita Vemuri</name>
    </author>
    <author>
      <name>Varsha Ramakrishnan</name>
    </author>
    <author>
      <name>Jasjeet S. Sekhon</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07801v1</id>
    <updated>2018-08-23T15:22:57Z</updated>
    <published>2018-08-23T15:22:57Z</published>
    <title>On a 'Two Truths' Phenomenon in Spectral Graph Clustering</title>
    <summary>  Clustering is concerned with coherently grouping observations without any
explicit concept of true groupings. Spectral graph clustering - clustering the
vertices of a graph based on their spectral embedding - is commonly approached
via K-means (or, more generally, Gaussian mixture model) clustering composed
with either Laplacian or Adjacency spectral embedding (LSE or ASE). Recent
theoretical results provide new understanding of the problem and solutions, and
lead us to a 'Two Truths' LSE vs. ASE spectral graph clustering phenomenon
convincingly illustrated here via a diffusion MRI connectome data set: the
different embedding methods yield different clustering results, with LSE
capturing left hemisphere/right hemisphere affinity structure and ASE capturing
gray matter/white matter core-periphery structure.
</summary>
    <author>
      <name>Carey E. Priebe</name>
    </author>
    <author>
      <name>Youngser Park</name>
    </author>
    <author>
      <name>Joshua T. Vogelstein</name>
    </author>
    <author>
      <name>John M. Conroy</name>
    </author>
    <author>
      <name>Vince Lyzinskic</name>
    </author>
    <author>
      <name>Minh Tang</name>
    </author>
    <author>
      <name>Avanti Athreya</name>
    </author>
    <author>
      <name>Joshua Cape</name>
    </author>
    <author>
      <name>Eric Bridgeford</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05756v2</id>
    <updated>2018-08-23T15:06:13Z</updated>
    <published>2016-10-18T19:39:44Z</published>
    <title>Modeling community structure and topics in dynamic text networks</title>
    <summary>  The last decade has seen great progress in both dynamic network modeling and
topic modeling. This paper draws upon both areas to create a Bayesian method
that allows topic discovery to inform the latent network model and the network
structure to facilitate topic identification. We apply this method to the 467
top political blogs of 2012. Our results find complex community structure
within this set of blogs, where community membership depends strongly upon the
set of topics in which the blogger is interested.
</summary>
    <author>
      <name>Teague Henry</name>
    </author>
    <author>
      <name>David Banks</name>
    </author>
    <author>
      <name>Christine Chai</name>
    </author>
    <author>
      <name>Derek Owens-Oas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Journal of Classification</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.05756v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05756v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07784v1</id>
    <updated>2018-08-23T14:52:40Z</updated>
    <published>2018-08-23T14:52:40Z</published>
    <title>Time-Agnostic Prediction: Predicting Predictable Video Frames</title>
    <summary>  Prediction is arguably one of the most basic functions of an intelligent
system. In general, the problem of predicting events in the future or between
two waypoints is exceedingly difficult. However, most phenomena naturally pass
through relatively predictable bottlenecks---while we cannot predict the
precise trajectory of a robot arm between being at rest and holding an object
up, we can be certain that it must have picked the object up. To exploit this,
we decouple visual prediction from a rigid notion of time. While conventional
approaches predict frames at regularly spaced temporal intervals, our
time-agnostic predictors (TAP) are not tied to specific times so that they may
instead discover predictable "bottleneck" frames no matter when they occur. We
evaluate our approach for future and intermediate frame prediction across three
robotic manipulation tasks. Our predictions are not only of higher visual
quality, but also correspond to coherent semantic subgoals in temporally
extended tasks. Project website: goo.gl/tL6Jgr.
</summary>
    <author>
      <name>Dinesh Jayaraman</name>
    </author>
    <author>
      <name>Frederik Ebert</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, plus appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11139v2</id>
    <updated>2018-08-23T14:27:53Z</updated>
    <published>2017-11-29T22:43:20Z</published>
    <title>Easy High-Dimensional Likelihood-Free Inference</title>
    <summary>  We introduce a framework using Generative Adversarial Networks (GANs) for
likelihood--free inference (LFI) and Approximate Bayesian Computation (ABC)
where we replace the black-box simulator model with an approximator network and
generate a rich set of summary features in a data driven fashion. On benchmark
data sets, our approach improves on others with respect to scalability, ability
to handle high dimensional data and complex probability distributions.
</summary>
    <author>
      <name>Vinay Jethava</name>
    </author>
    <author>
      <name>Devdatt Dubhashi</name>
    </author>
    <link href="http://arxiv.org/abs/1711.11139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00953v1</id>
    <updated>2018-08-23T14:05:31Z</updated>
    <published>2018-08-23T14:05:31Z</published>
    <title>Deep Learning Based Vehicle Make-Model Classification</title>
    <summary>  This paper studies the problems of vehicle make &amp; model classification. Some
of the main challenges are reaching high classification accuracy and reducing
the annotation time of the images. To address these problems, we have created a
fine-grained database using online vehicle marketplaces of Turkey. A pipeline
is proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN
(Convolutional Neural Network) model to train on the database. In the pipeline,
we first detect the vehicles by following an algorithm which reduces the time
for annotation. Then, we feed them into the CNN model. It is reached
approximately 4% better classification accuracy result than using a
conventional CNN model. Next, we propose to use the detected vehicles as ground
truth bounding box (GTBB) of the images and feed them into an SSD model in
another pipeline. At this stage, it is reached reasonable classification
accuracy result without using perfectly shaped GTBB. Lastly, an application is
implemented in a use case by using our proposed pipelines. It detects the
unauthorized vehicles by comparing their license plate numbers and make &amp;
models. It is assumed that license plates are readable.
</summary>
    <author>
      <name>Burak Satar</name>
    </author>
    <author>
      <name>Ahmet Emir Dirik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. It is accepted by 27th International Conference on
  Artificial Neural Networks 2018. It hasn't been presented yet. Conference
  proceedings are published by Springer in Lecture Notes in Computer Science</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07769v1</id>
    <updated>2018-08-23T14:01:05Z</updated>
    <published>2018-08-23T14:01:05Z</published>
    <title>Topology and Prediction Focused Research on Graph Convolutional Neural
  Networks</title>
    <summary>  Important advances have been made using convolutional neural network (CNN)
approaches to solve complicated problems in areas that rely on grid structured
data such as image processing and object classification. Recently, research on
graph convolutional neural networks (GCNN) has increased dramatically as
researchers try to replicate the success of CNN for graph structured data.
Unfortunately, traditional CNN methods are not readily transferable to GCNN,
given the irregularity and geometric complexity of graphs. The emerging field
of GCNN is further complicated by research papers that differ greatly in their
scope, detail, and level of academic sophistication needed by the reader.
  The present paper provides a review of some basic properties of GCNN. As a
guide to the interested reader, recent examples of GCNN research are then
grouped according to techniques that attempt to uncover the underlying topology
of the graph model and those that seek to generalize traditional CNN methods on
graph data to improve prediction of class membership. Discrete Signal
Processing on Graphs (DSPg) is used as a theoretical framework to better
understand some of the performance gains and limitations of these recent GCNN
approaches. A brief discussion of Topology Adaptive Graph Convolutional
Networks (TAGCN) is presented as an approach motivated by DSPg and future
research directions using this approach are briefly discussed.
</summary>
    <author>
      <name>Matthew Baron</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07739v1</id>
    <updated>2018-08-23T13:24:17Z</updated>
    <published>2018-08-23T13:24:17Z</published>
    <title>Diversity-Driven Selection of Exploration Strategies in Multi-Armed
  Bandits</title>
    <summary>  We consider a scenario where an agent has multiple available strategies to
explore an unknown environment. For each new interaction with the environment,
the agent must select which exploration strategy to use. We provide a new
strategy-agnostic method that treat the situation as a Multi-Armed Bandits
problem where the reward signal is the diversity of effects that each strategy
produces. We test the method empirically on a simulated planar robotic arm, and
establish that the method is both able discriminate between strategies of
dissimilar quality, even when the differences are tenuous, and that the
resulting performance is competitive with the best fixed mixture of strategies.
</summary>
    <author>
      <name>Fabien C. Y. Benureau</name>
    </author>
    <author>
      <name>Pierre-Yves Oudeyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/devlrn.2015.7346130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/devlrn.2015.7346130" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 Joint IEEE International Conference on Development and
  Learning and Epigenetic Robotics (ICDL-EpiRob), pp. 135-142</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.07739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07181v2</id>
    <updated>2018-08-23T12:27:47Z</updated>
    <published>2018-08-22T01:47:53Z</published>
    <title>Efficient sparse Hessian based algorithms for the clustered lasso
  problem</title>
    <summary>  We focus on solving the clustered lasso problem, which is a least squares
problem with the $\ell_1$-type penalties imposed on both the coefficients and
their pairwise differences to learn the group structure of the regression
parameters. Here we first reformulate the clustered lasso regularizer as a
weighted ordered-lasso regularizer, which is essential in reducing the
computational cost from $O(n^2)$ to $O(n\log (n))$. We then propose an inexact
semismooth Newton augmented Lagrangian (SSNAL) algorithm to solve the clustered
lasso problem or its dual via this equivalent formulation, depending on whether
the sample size is larger than the dimension of the features. An essential
component of the SSNAL algorithm is the computation of the generalized Jacobian
of the proximal mapping of the clustered lasso regularizer. Based on the new
formulation, we derive an efficient procedure for its computation.
Comprehensive results on the global convergence and local linear convergence of
the SSNAL algorithm are established. For the purpose of exposition and
comparison, we also summarize/design several first-order methods that can be
used to solve the problem under consideration, but with the key improvement
from the new formulation of the clustered lasso regularizer. As a demonstration
of the applicability of our algorithms, numerical experiments on the clustered
lasso problem are performed. The experiments show that the SSNAL algorithm
substantially outperforms the best alternative algorithm for the clustered
lasso problem.
</summary>
    <author>
      <name>Meixia Lin</name>
    </author>
    <author>
      <name>Yong-Jin Liu</name>
    </author>
    <author>
      <name>Defeng Sun</name>
    </author>
    <author>
      <name>Kim-Chuan Toh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07713v1</id>
    <updated>2018-08-23T12:12:10Z</updated>
    <published>2018-08-23T12:12:10Z</published>
    <title>Adversarial Attacks on Deep-Learning Based Radio Signal Classification</title>
    <summary>  Deep learning (DL), despite its enormous success in many computer vision and
language processing applications, is exceedingly vulnerable to adversarial
attacks. We consider the use of DL for radio signal (modulation) classification
tasks, and present practical methods for the crafting of white-box and
universal black-box adversarial attacks in that application. We show that these
attacks can considerably reduce the classification performance, with extremely
small perturbations of the input. In particular, these attacks are
significantly more powerful than classical jamming attacks, which raises
significant security and robustness concerns in the use of DL-based algorithms
for the wireless physical layer.
</summary>
    <author>
      <name>Meysam Sadeghi</name>
    </author>
    <author>
      <name>Erik G. Larsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01610v4</id>
    <updated>2018-08-23T12:07:40Z</updated>
    <published>2018-06-05T11:16:42Z</published>
    <title>Training Generative Reversible Networks</title>
    <summary>  Generative models with an encoding component such as autoencoders currently
receive great interest. However, training of autoencoders is typically
complicated by the need to train a separate encoder and decoder model that have
to be enforced to be reciprocal to each other. To overcome this problem,
by-design reversible neural networks (RevNets) had been previously used as
generative models either directly optimizing the likelihood of the data under
the model or using an adversarial approach on the generated data. Here, we
instead investigate their performance using an adversary on the latent space in
the adversarial autoencoder framework. We investigate the generative
performance of RevNets on the CelebA dataset, showing that generative RevNets
can generate coherent faces with similar quality as Variational Autoencoders.
This first attempt to use RevNets inside the adversarial autoencoder framework
slightly underperformed relative to recent advanced generative models using an
autoencoder component on CelebA, but this gap may diminish with further
optimization of the training setup of generative RevNets. In addition to the
experiments on CelebA, we show a proof-of-principle experiment on the MNIST
dataset suggesting that adversary-free trained RevNets can discover meaningful
latent dimensions without pre-specifying the number of dimensions of the latent
sampling distribution. In summary, this study shows that RevNets can be
employed in different generative training settings.
  Source code for this study is at
https://github.com/robintibor/generative-reversible
</summary>
    <author>
      <name>Robin Tibor Schirrmeister</name>
    </author>
    <author>
      <name>Patryk Chrabąszcz</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <author>
      <name>Tonio Ball</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source code for this study is at
  https://github.com/robintibor/generative-reversible</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01610v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01610v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07560v2</id>
    <updated>2018-08-23T09:12:37Z</updated>
    <published>2018-07-19T17:57:16Z</published>
    <title>Compositional GAN: Learning Conditional Image Composition</title>
    <summary>  Generative Adversarial Networks (GANs) can produce images of surprising
complexity and realism, but are generally modeled to sample from a single
latent source ignoring the explicit spatial interaction between multiple
entities that could be present in a scene. Capturing such complex interactions
between different objects in the world, including their relative scaling,
spatial layout, occlusion, or viewpoint transformation is a challenging
problem. In this work, we propose to model object composition in a GAN
framework as a self-consistent composition-decomposition network. Our model is
conditioned on the object images from their marginal distributions to generate
a realistic image from their joint distribution by explicitly learning the
possible interactions. We evaluate our model through qualitative experiments
and user evaluations in both the scenarios when either paired or unpaired
examples for the individual object images and the joint scenes are given during
training. Our results reveal that the learned model captures potential
interactions between the two object domains given as input to output new
instances of composed scene at test time in a reasonable fashion.
</summary>
    <author>
      <name>Samaneh Azadi</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <author>
      <name>Sayna Ebrahimi</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1807.07560v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07560v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08111v1</id>
    <updated>2018-08-23T06:51:09Z</updated>
    <published>2018-08-23T06:51:09Z</published>
    <title>Multiclass Universum SVM</title>
    <summary>  We introduce Universum learning for multiclass problems and propose a novel
formulation for multiclass universum SVM (MU-SVM). We also propose an analytic
span bound for model selection with almost 2-4x faster computation times than
standard resampling techniques. We empirically demonstrate the efficacy of the
proposed MUSVM formulation on several real world datasets achieving &gt; 20%
improvement in test accuracies compared to multi-class SVM.
</summary>
    <author>
      <name>Sauptik Dhar</name>
    </author>
    <author>
      <name>Vladimir Cherkassky</name>
    </author>
    <author>
      <name>Mohak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages. arXiv admin note: text overlap with arXiv:1609.09162</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08124v1</id>
    <updated>2018-08-23T05:01:05Z</updated>
    <published>2018-08-23T05:01:05Z</published>
    <title>Insect cyborgs: Biological feature generators improve machine learning
  accuracy on limited data</title>
    <summary>  Despite many successes, machine learning (ML) methods such as neural nets
often struggle to learn given small training sets. In contrast, biological
neural nets (BNNs) excel at fast learning. We can thus look to BNNs for tools
to improve performance of ML methods in this low-data regime.
  The insect olfactory network, though simple, can learn new odors very
rapidly. Its two key structures are a layer with competitive inhibition (the
Antennal Lobe, AL), followed by a high dimensional sparse plastic layer (the
Mushroom Body, MB). This AL-MB network can rapidly learn not only odors but
also handwritten digits, better in fact than standard ML methods in the
few-shot regime.
  In this work, we deploy the AL-MB network as an automatic feature generator,
using its Readout Neurons as additional features for standard ML classifiers.
We hypothesize that the AL-MB structure has a strong intrinsic clustering
ability; and that its Readout Neurons, used as input features, will boost the
performance of ML methods.
  We find that these "insect cyborgs", ie classifiers that are part-moth and
part-ML method, deliver significantly better performance than baseline ML
methods alone on a generic (non-spatial) 85-feature, 10-class task derived from
the MNIST dataset. Accuracy improves by an average of 6% to 33% for N &lt; 15
training samples per class, and by 6% to 10% for N &gt; 15. Remarkably, these
moth-generated features increase ML accuracy even when the ML method's baseline
accuracy already exceeds the AL-MB's own limited capacity.
  The two structures in the AL-MB, a competitive inhibition layer and a
high-dimensional sparse layer with Hebbian plasticity, are novel in the context
of artificial NNs but endemic in BNNs. We believe they can be deployed either
prepended as feature generators or inserted as layers into deep NNs, to
potentially improve ML performance.
</summary>
    <author>
      <name>Charles B Delahunt</name>
    </author>
    <author>
      <name>J Nathan Kutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07220v2</id>
    <updated>2018-08-23T02:21:56Z</updated>
    <published>2018-08-22T05:14:41Z</published>
    <title>Approximating Poker Probabilities with Deep Learning</title>
    <summary>  Many poker systems, whether created with heuristics or machine learning, rely
on the probability of winning as a key input. However calculating the precise
probability using combinatorics is an intractable problem, so instead we
approximate it. Monte Carlo simulation is an effective technique that can be
used to approximate the probability that a player will win and/or tie a hand.
However, without the use of a memory-intensive lookup table or a supercomputer,
it becomes infeasible to run millions of times when training an agent with
self-play. To combat the space-time tradeoff, we use deep learning to
approximate the probabilities obtained from the Monte Carlo simulation with
high accuracy. The learned model proves to be a lightweight alternative to
Monte Carlo simulation, which ultimately allows us to use the probabilities as
inputs during self-play efficiently. The source code and optimized neural
network can be found at
https://github.com/brandinho/Poker-Probability-Approximation
</summary>
    <author>
      <name>Brandon Da Silva</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06638v3</id>
    <updated>2018-08-23T02:20:15Z</updated>
    <published>2018-08-20T18:18:52Z</published>
    <title>Supervised Kernel PCA For Longitudinal Data</title>
    <summary>  In statistical learning, high covariate dimensionality poses challenges for
robust prediction and inference. To address this challenge, supervised
dimension reduction is often performed, where dependence on the outcome is
maximized for a selected covariate subspace with smaller dimensionality.
Prevalent dimension reduction techniques assume data are $i.i.d.$, which is not
appropriate for longitudinal data comprising multiple subjects with repeated
measurements over time. In this paper, we derive a decomposition of the
Hilbert-Schmidt Independence Criterion as a supervised loss function for
longitudinal data, enabling dimension reduction between and within clusters
separately, and propose a dimensionality-reduction technique, $sklPCA$, that
performs this decomposed dimension reduction. We also show that this technique
yields superior model accuracy compared to the model it extends.
</summary>
    <author>
      <name>Patrick Staples</name>
    </author>
    <author>
      <name>Min Ouyang</name>
    </author>
    <author>
      <name>Robert F. Dougherty</name>
    </author>
    <author>
      <name>Gregory A. Ryslik</name>
    </author>
    <author>
      <name>Paul Dagum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06638v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06638v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00373v2</id>
    <updated>2018-08-23T01:39:44Z</updated>
    <published>2018-07-01T19:09:15Z</published>
    <title>New Heuristics for Parallel and Scalable Bayesian Optimization</title>
    <summary>  Bayesian optimization has emerged as a strong candidate tool for global
optimization of functions with expensive evaluation costs. However, due to the
dynamic nature of research in Bayesian approaches, and the evolution of
computing technology, using Bayesian optimization in a parallel computing
environment remains a challenge for the non-expert. In this report, I review
the state-of-the-art in parallel and scalable Bayesian optimization methods. In
addition, I propose practical ways to avoid a few of the pitfalls of Bayesian
optimization, such as oversampling of edge parameters and over-exploitation of
high performance parameters. Finally, I provide relatively simple, heuristic
algorithms, along with their open source software implementations, that can be
immediately and easily deployed in any computing environment.
</summary>
    <author>
      <name>Ran Rubin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00373v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00373v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08121v1</id>
    <updated>2018-08-23T00:38:14Z</updated>
    <published>2018-08-23T00:38:14Z</published>
    <title>An Improvement of Data Classification Using Random Multimodel Deep
  Learning (RMDL)</title>
    <summary>  The exponential growth in the number of complex datasets every year requires
more enhancement in machine learning methods to provide robust and accurate
data classification. Lately, deep learning approaches have achieved surpassing
results in comparison to previous machine learning algorithms. However, finding
the suitable structure for these models has been a challenge for researchers.
This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,
deep learning approach for classification. RMDL solves the problem of finding
the best deep learning structure and architecture while simultaneously
improving robustness and accuracy through ensembles of deep learning
architectures. In short, RMDL trains multiple randomly generated models of Deep
Neural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural
Network (RNN) in parallel and combines their results to produce better result
of any of those models individually. In this paper, we describe RMDL model and
compare the results for image and text classification as well as face
recognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for
image classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text
classification. Lastly, we used ORL dataset to compare the model performance on
face recognition task.
</summary>
    <author>
      <name>Mojtaba Heidarysafa</name>
    </author>
    <author>
      <name>Kamran Kowsari</name>
    </author>
    <author>
      <name>Donald E. Brown</name>
    </author>
    <author>
      <name>Kiana Jafari Meimandi</name>
    </author>
    <author>
      <name>Laura E. Barnes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18178/ijmlc.2018.8.4.703</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18178/ijmlc.2018.8.4.703" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in International Journal of Machine Learning and Computing
  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07593v1</id>
    <updated>2018-08-23T00:13:18Z</updated>
    <published>2018-08-23T00:13:18Z</published>
    <title>Pathologies in information bottleneck for deterministic supervised
  learning</title>
    <summary>  Information bottleneck (IB) is a method for extracting information from one
random variable $X$ that is relevant for predicting another random variable
$Y$. To do so, IB identifies an intermediate "bottleneck" variable $T$ that has
low mutual information $I(X;T)$ and high mutual information $I(Y;T)$. The "IB
curve" characterizes the set of bottleneck variables that achieve maximal
$I(Y;T)$ for a given $I(X;T)$, and is typically explored by optimizing the "IB
Lagrangian", $I(Y;T) - \beta I(X;T)$. Recently, there has been interest in
applying IB to supervised learning, particularly for classification problems
that use neural networks. In most classification problems, the output class $Y$
is a deterministic function of the input $X$, which we refer to as
"deterministic supervised learning". We demonstrate three pathologies that
arise when IB is used in any scenario where $Y$ is a deterministic function of
$X$: (1) the IB curve cannot be recovered by optimizing the IB Lagrangian for
different values of $\beta$; (2) there are "uninteresting" solutions at all
points of the IB curve; and (3) for classifiers that achieve low error rates,
the activity of different hidden layers will not exhibit a strict trade-off
between compression and prediction, contrary to a recent proposal. To address
problem (1), we propose a functional that, unlike the IB Lagrangian, can
recover the IB curve in all cases. We finish by demonstrating these issues on
the MNIST dataset.
</summary>
    <author>
      <name>Artemy Kolchinsky</name>
    </author>
    <author>
      <name>Brendan D. Tracey</name>
    </author>
    <author>
      <name>Steven Van Kuyk</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06234v3</id>
    <updated>2018-08-22T23:44:34Z</updated>
    <published>2018-04-13T23:09:12Z</published>
    <title>Clustering Analysis on Locally Asymptotically Self-similar Processes
  with Known Number of Clusters</title>
    <summary>  We study the problems of clustering locally asymptotically self-similar
stochastic processes, when the true number of clusters is priorly known. A new
covariance-based dissimilarity measure is introduced, from which the so-called
approximately asymptotically consistent clustering algorithms are obtained. In
a simulation study, clustering data sampled from multifractional Brownian
motions is performed to illustrate the approximated asymptotic consistency of
the proposed algorithms.
</summary>
    <author>
      <name>Qidi Peng</name>
    </author>
    <author>
      <name>Nan Rao</name>
    </author>
    <author>
      <name>Ran Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1804.06234v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06234v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07, 60G10, 62M10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07576v1</id>
    <updated>2018-08-22T22:06:26Z</updated>
    <published>2018-08-22T22:06:26Z</published>
    <title>Cooperative SGD: A unified Framework for the Design and Analysis of
  Communication-Efficient SGD Algorithms</title>
    <summary>  State-of-the-art distributed machine learning suffers from significant delays
due to frequent communication and synchronizing between worker nodes. Emerging
communication-efficient SGD algorithms that limit synchronization between
locally trained models have been shown to be effective in speeding-up
distributed SGD. However, a rigorous convergence analysis and comparative study
of different communication-reduction strategies remains a largely open problem.
This paper presents a new framework called Coooperative SGD that subsumes
existing communication-efficient SGD algorithms such as federated-averaging,
elastic-averaging and decentralized SGD. By analyzing Cooperative SGD, we
provide novel convergence guarantees for existing algorithms. Moreover this
framework enables us to design new communication-efficient SGD algorithms that
strike the best balance between reducing communication overhead and achieving
fast error convergence.
</summary>
    <author>
      <name>Jianyu Wang</name>
    </author>
    <author>
      <name>Gauri Joshi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07573v1</id>
    <updated>2018-08-22T21:50:56Z</updated>
    <published>2018-08-22T21:50:56Z</published>
    <title>Approximation Trees: Statistical Stability in Model Distillation</title>
    <summary>  This paper examines the stability of learned explanations for black-box
predictions via model distillation with decision trees. One approach to
intelligibility in machine learning is to use an understandable `student' model
to mimic the output of an accurate `teacher'. Here, we consider the use of
regression trees as a student model, in which nodes of the tree can be used as
`explanations' for particular predictions, and the whole structure of the tree
can be used as a global representation of the resulting function. However,
individual trees are sensitive to the particular data sets used to train them,
and an interpretation of a student model may be suspect if small changes in the
training data have a large effect on it. In this context, access to outcomes
from a teacher helps to stabilize the greedy splitting strategy by generating a
much larger corpus of training examples than was originally available. We
develop tests to ensure that enough examples are generated at each split so
that the same splitting rule would be chosen with high probability were the
tree to be re trained. Further, we develop a stopping rule to indicate how deep
the tree should be built based on recent results on the variability of Random
Forests when these are used as the teacher. We provide concrete examples of
these procedures on the CAD-MDD and COMPAS data sets.
</summary>
    <author>
      <name>Yichen Zhou</name>
    </author>
    <author>
      <name>Zhengze Zhou</name>
    </author>
    <author>
      <name>Giles Hooker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper supercedes arXiv:1610.09036</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07569v1</id>
    <updated>2018-08-22T21:26:06Z</updated>
    <published>2018-08-22T21:26:06Z</published>
    <title>Robust Counterfactual Inferences using Feature Learning and their
  Applications</title>
    <summary>  In a wide variety of applications, including personalization, we want to
measure the difference in outcome due to an intervention and thus have to deal
with counterfactual inference. The feedback from a customer in any of these
situations is only 'bandit feedback' - that is, a partial feedback based on
whether we chose to intervene or not. Typically randomized experiments are
carried out to understand whether an intervention is overall better than no
intervention. Here we present a feature learning algorithm to learn from a
randomized experiment where the intervention in consideration is most effective
and where it is least effective rather than only focusing on the overall
impact, thus adding a context to our learning mechanism and extract more
information. From the randomized experiment, we learn the feature
representations which divide the population into subpopulations where we
observe statistically significant difference in average customer feedback
between those who were subjected to the intervention and those who were not,
with a level of significance l, where l is a configurable parameter in our
model. We use this information to derive the value of the intervention in
consideration for each instance in the population. With experiments, we show
that using this additional learning, in future interventions, the context for
each instance could be leveraged to decide whether to intervene or not.
</summary>
    <author>
      <name>Abhimanyu Mitra</name>
    </author>
    <author>
      <name>Kannan Achan</name>
    </author>
    <author>
      <name>Sushant Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09952v4</id>
    <updated>2018-08-22T18:52:38Z</updated>
    <published>2017-05-28T15:41:12Z</published>
    <title>Optimal sequential treatment allocation</title>
    <summary>  In treatment allocation problems the individuals to be treated often arrive
sequentially. We study a problem in which the policy maker is not only
interested in the expected cumulative welfare but is also concerned about the
uncertainty/risk of the treatment outcomes. At the outset, the total number of
treatment assignments to be made may even be unknown. A sequential treatment
policy which attains the minimax optimal regret is proposed. We also
demonstrate that the expected number of suboptimal treatments only grows slowly
in the number of treatments. Finally, we study a setting where outcomes are
only observed with delay.
</summary>
    <author>
      <name>Anders Bredahl Kock</name>
    </author>
    <author>
      <name>Martin Thyrsgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09952v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09952v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07510v1</id>
    <updated>2018-08-22T18:25:53Z</updated>
    <published>2018-08-22T18:25:53Z</published>
    <title>XPCA: Extending PCA for a Combination of Discrete and Continuous
  Variables</title>
    <summary>  Principal component analysis (PCA) is arguably the most popular tool in
multivariate exploratory data analysis. In this paper, we consider the question
of how to handle heterogeneous variables that include continuous, binary, and
ordinal. In the probabilistic interpretation of low-rank PCA, the data has a
normal multivariate distribution and, therefore, normal marginal distributions
for each column. If some marginals are continuous but not normal, the
semiparametric copula-based principal component analysis (COCA) method is an
alternative to PCA that combines a Gaussian copula with nonparametric
marginals. If some marginals are discrete or semi-continuous, we propose a new
extended PCA (XPCA) method that also uses a Gaussian copula and nonparametric
marginals and accounts for discrete variables in the likelihood calculation by
integrating over appropriate intervals. Like PCA, the factors produced by XPCA
can be used to find latent structure in data, build predictive models, and
perform dimensionality reduction. We present the new model, its induced
likelihood function, and a fitting algorithm which can be applied in the
presence of missing data. We demonstrate how to use XPCA to produce an
estimated full conditional distribution for each data point, and use this to
produce to provide estimates for missing data that are automatically range
respecting. We compare the methods as applied to simulated and real-world data
sets that have a mixture of discrete and continuous variables.
</summary>
    <author>
      <name>Clifford Anderson-Bergman</name>
    </author>
    <author>
      <name>Tamara G. Kolda</name>
    </author>
    <author>
      <name>Kina Kincher-Winoto</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07440v1</id>
    <updated>2018-08-22T17:03:10Z</updated>
    <published>2018-08-22T17:03:10Z</published>
    <title>3D Topology Optimization using Convolutional Neural Networks</title>
    <summary>  Topology optimization is computationally demanding that requires the assembly
and solution to a finite element problem for each material distribution
hypothesis. As a complementary alternative to the traditional physics-based
topology optimization, we explore a data-driven approach that can quickly
generate accurate solutions. To this end, we propose a deep learning approach
based on a 3D encoder-decoder Convolutional Neural Network architecture for
accelerating 3D topology optimization and to determine the optimal
computational strategy for its deployment. Analysis of iteration-wise progress
of the Solid Isotropic Material with Penalization process is used as a
guideline to study how the earlier steps of the conventional topology
optimization can be used as input for our approach to predict the final
optimized output structure directly from this input. We conduct a comparative
study between multiple strategies for training the neural network and assess
the effect of using various input combinations for the CNN to finalize the
strategy with the highest accuracy in predictions for practical deployment. For
the best performing network, we achieved about 40% reduction in overall
computation time while also attaining structural accuracies in the order of
96%.
</summary>
    <author>
      <name>Saurabh Banga</name>
    </author>
    <author>
      <name>Harsh Gehani</name>
    </author>
    <author>
      <name>Sanket Bhilare</name>
    </author>
    <author>
      <name>Sagar Patel</name>
    </author>
    <author>
      <name>Levent Kara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is under review in 'Special issue on Computer-Aided Design
  on Advances in Generative Design', 16 Pages, 7 tables, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02703v4</id>
    <updated>2018-08-22T16:53:03Z</updated>
    <published>2016-10-09T18:07:07Z</published>
    <title>Nonparametric Bayesian inference of the microcanonical stochastic block
  model</title>
    <summary>  A principled approach to characterize the hidden structure of networks is to
formulate generative models, and then infer their parameters from data. When
the desired structure is composed of modules or "communities", a suitable
choice for this task is the stochastic block model (SBM), where nodes are
divided into groups, and the placement of edges is conditioned on the group
memberships. Here, we present a nonparametric Bayesian method to infer the
modular structure of empirical networks, including the number of modules and
their hierarchical organization. We focus on a microcanonical variant of the
SBM, where the structure is imposed via hard constraints, i.e. the generated
networks are not allowed to violate the patterns imposed by the model. We show
how this simple model variation allows simultaneously for two important
improvements over more traditional inference approaches: 1. Deeper Bayesian
hierarchies, with noninformative priors replaced by sequences of priors and
hyperpriors, that not only remove limitations that seriously degrade the
inference on large networks, but also reveal structures at multiple scales; 2.
A very efficient inference algorithm that scales well not only for networks
with a large number of nodes and edges, but also with an unlimited number of
modules. We show also how this approach can be used to sample modular
hierarchies from the posterior distribution, as well as to perform model
selection. We discuss and analyze the differences between sampling from the
posterior and simply finding the single parameter estimate that maximizes it.
Furthermore, we expose a direct equivalence between our microcanonical approach
and alternative derivations based on the canonical SBM.
</summary>
    <author>
      <name>Tiago P. Peixoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.95.012317</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.95.012317" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 9 figures, 1 table. Code is freely available as part of
  graph-tool at https://graph-tool.skewed.de . See also the HOWTO at
  https://graph-tool.skewed.de/static/doc/demos/inference/inference.html .
  Minor typos fixed in most recent version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 95, 012317 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.02703v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02703v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07431v1</id>
    <updated>2018-08-22T16:40:22Z</updated>
    <published>2018-08-22T16:40:22Z</published>
    <title>Deep Boosted Regression for MR to CT Synthesis</title>
    <summary>  Attenuation correction is an essential requirement of positron emission
tomography (PET) image reconstruction to allow for accurate quantification.
However, attenuation correction is particularly challenging for PET-MRI as
neither PET nor magnetic resonance imaging (MRI) can directly image tissue
attenuation properties. MRI-based computed tomography (CT) synthesis has been
proposed as an alternative to physics based and segmentation-based approaches
that assign a population-based tissue density value in order to generate an
attenuation map. We propose a novel deep fully convolutional neural network
that generates synthetic CTs in a recursive manner by gradually reducing the
residuals of the previous network, increasing the overall accuracy and
generalisability, while keeping the number of trainable parameters within
reasonable limits. The model is trained on a database of 20 pre-acquired MRI/CT
pairs and a four-fold random bootstrapped validation with a 80:20 split is
performed. Quantitative results show that the proposed framework outperforms a
state-of-the-art atlas-based approach decreasing the Mean Absolute Error (MAE)
from 131HU to 68HU for the synthetic CTs and reducing the PET reconstruction
error from 14.3% to 7.2%.
</summary>
    <author>
      <name>Kerstin Kläser</name>
    </author>
    <author>
      <name>Pawel Markiewicz</name>
    </author>
    <author>
      <name>Marta Ranzini</name>
    </author>
    <author>
      <name>Wenqi Li</name>
    </author>
    <author>
      <name>Marc Modat</name>
    </author>
    <author>
      <name>Brian F Hutton</name>
    </author>
    <author>
      <name>David Atkinson</name>
    </author>
    <author>
      <name>Kris Thielemans</name>
    </author>
    <author>
      <name>M Jorge Cardoso</name>
    </author>
    <author>
      <name>Sebastien Ourselin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SASHIMI2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04572v3</id>
    <updated>2018-08-22T14:48:43Z</updated>
    <published>2018-08-14T08:01:07Z</published>
    <title>Small Sample Learning in Big Data Era</title>
    <summary>  As a promising area in artificial intelligence, a new learning paradigm,
called Small Sample Learning (SSL), has been attracting prominent research
attention in the recent years. In this paper, we aim to present a survey to
comprehensively introduce the current techniques proposed on this topic.
Specifically, current SSL techniques can be mainly divided into two categories.
The first category of SSL approaches can be called "concept learning", which
emphasizes learning new concepts from only few related observations. The
purpose is mainly to simulate human learning behaviors like recognition,
generation, imagination, synthesis and analysis. The second category is called
"experience learning", which usually co-exists with the large sample learning
manner of conventional machine learning. This category mainly focuses on
learning with insufficient samples, and can also be called small data learning
in some literatures. More extensive surveys on both categories of SSL
techniques are introduced and some neuroscience evidences are provided to
clarify the rationality of the entire SSL regime, and the relationship with
human learning process. Some discussions on the main challenges and possible
future research directions along this line are also presented.
</summary>
    <author>
      <name>Jun Shu</name>
    </author>
    <author>
      <name>Zongben Xu</name>
    </author>
    <author>
      <name>Deyu Meng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">76 pages, 15 figures, survey of small sample learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04572v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04572v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07383v1</id>
    <updated>2018-08-22T14:30:03Z</updated>
    <published>2018-08-22T14:30:03Z</published>
    <title>Dynamic Self-Attention : Computing Attention over Words Dynamically for
  Sentence Embedding</title>
    <summary>  In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention
mechanism for sentence embedding. We design DSA by modifying dynamic routing in
capsule network (Sabouretal.,2017) for natural language processing. DSA attends
to informative words with a dynamic weight vector. We achieve new
state-of-the-art results among sentence encoding methods in Stanford Natural
Language Inference (SNLI) dataset with the least number of parameters, while
showing comparative results in Stanford Sentiment Treebank (SST) dataset.
</summary>
    <author>
      <name>Deunsol Yoon</name>
    </author>
    <author>
      <name>Dongbok Lee</name>
    </author>
    <author>
      <name>SangKeun Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07382v1</id>
    <updated>2018-08-22T14:28:57Z</updated>
    <published>2018-08-22T14:28:57Z</published>
    <title>Convergence of Cubic Regularization for Nonconvex Optimization under KL
  Property</title>
    <summary>  Cubic-regularized Newton's method (CR) is a popular algorithm that guarantees
to produce a second-order stationary solution for solving nonconvex
optimization problems. However, existing understandings of the convergence rate
of CR are conditioned on special types of geometrical properties of the
objective function. In this paper, we explore the asymptotic convergence rate
of CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of
nonconvex objective functions. In specific, we characterize the asymptotic
convergence rate of various types of optimality measures for CR including
function value gap, variable distance gap, gradient norm and least eigenvalue
of the Hessian matrix. Our results fully characterize the diverse convergence
behaviors of these optimality measures in the full parameter regime of the KL
property. Moreover, we show that the obtained asymptotic convergence rates of
CR are order-wise faster than those of first-order gradient descent algorithms
under the KL property.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <author>
      <name>Zhe Wang</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07475v1</id>
    <updated>2018-08-22T13:38:19Z</updated>
    <published>2018-08-22T13:38:19Z</published>
    <title>Capsule Networks for Protein Structure Classification and Prediction</title>
    <summary>  Capsule Networks have great potential to tackle problems in structural
biology because of their attention to hierarchical relationships. This paper
describes the implementation and application of a Capsule Network architecture
to the classification of RAS protein family structures on GPU-based
computational resources. The proposed Capsule Network trained on 2D and 3D
structural encodings can successfully classify HRAS and KRAS structures. The
Capsule Network can also classify a protein-based dataset derived from a
PSI-BLAST search on sequences of KRAS and HRAS mutations. Our results show an
accuracy improvement compared to traditional convolutional networks, while
improving interpretability through visualization of activation vectors.
</summary>
    <author>
      <name>Dan Rosa de Jesus</name>
    </author>
    <author>
      <name>Julian Cuevas</name>
    </author>
    <author>
      <name>Wilson Rivera</name>
    </author>
    <author>
      <name>Silvia Crivelli</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11470v2</id>
    <updated>2018-08-22T12:53:06Z</updated>
    <published>2018-07-30T17:59:28Z</published>
    <title>Deep Encoder-Decoder Models for Unsupervised Learning of Controllable
  Speech Synthesis</title>
    <summary>  Generating versatile and appropriate synthetic speech requires control over
the output expression separate from the spoken text. Important non-textual
speech variation is seldom annotated, in which case output control must be
learned in an unsupervised fashion. In this paper, we perform an in-depth study
of methods for unsupervised learning of control in statistical speech
synthesis. For example, we show that popular unsupervised training heuristics
can be interpreted as variational inference in certain autoencoder models. We
additionally connect these models to VQ-VAEs, another, recently-proposed class
of deep variational autoencoders, which we show can be derived from a very
similar mathematical argument. The implications of these new probabilistic
interpretations are discussed. We illustrate the utility of the various
approaches with an application to acoustic modelling for emotional speech
synthesis, where the unsupervised methods for learning expression control
(without access to emotional labels) are found to give results that in many
aspects match or surpass the previous best supervised approach.
</summary>
    <author>
      <name>Gustav Eje Henter</name>
    </author>
    <author>
      <name>Jaime Lorenzo-Trueba</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05527v2</id>
    <updated>2018-08-22T12:50:41Z</updated>
    <published>2018-08-16T15:01:01Z</published>
    <title>Deep Learning for Energy Markets</title>
    <summary>  Deep Learning (DL) provides a methodology to predict extreme loads observed
in energy grids. Forecasting energy loads and prices is challenging due to
sharp peaks and troughs that arise from intraday system constraints due to
supply and demand fluctuations. We propose deep spatio-temporal models and
extreme value theory (DL-EVT) to capture the tail behavior of load spikes. Deep
architectures, such as ReLU and LSTM can model generation trends and temporal
dependencies while EVT captures highly volatile load spikes. To illustrate our
methodology, we use hourly price and demand data from the PJM interconnection
for 4719 nodes and we develop a deep predictor. DL-EVT outperforms traditional
Fourier and time series methods, both in-and out-of-sample, by capturing the
nonlinearities in prices. Finally, we conclude with directions for future
research.
</summary>
    <author>
      <name>Michael Polson</name>
    </author>
    <author>
      <name>Vadim Sokolov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05527v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05527v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07292v1</id>
    <updated>2018-08-22T09:28:43Z</updated>
    <published>2018-08-22T09:28:43Z</published>
    <title>k-meansNet: When k-means Meets Differentiable Programming</title>
    <summary>  In this paper, we study how to make clustering benefiting from differentiable
programming whose basic idea is treating the neural network as a language
instead of a machine learning method. To this end, we recast the vanilla
$k$-means as a novel feedforward neural network in an elegant way. Our
contribution is two-fold. On the one hand, the proposed \textit{k}-meansNet is
a neural network implementation of the vanilla \textit{k}-means, which enjoys
four advantages highly desired, i.e., robustness to initialization, fast
inference speed, the capability of handling new coming data, and provable
convergence. On the other hand, this work may provide novel insights into
differentiable programming. More specifically, most existing differentiable
programming works unroll an \textbf{optimizer} as a \textbf{recurrent neural
network}, namely, the neural network is employed to solve an existing
optimization problem. In contrast, we reformulate the \textbf{objective
function} of \textit{k}-means as a \textbf{feedforward neural network}, namely,
we employ the neural network to describe a problem. In such a way, we advance
the boundary of differentiable programming by treating the neural network as
from an alternative optimization approach to the problem formulation. Extensive
experimental studies show that our method achieves promising performance
comparing with 12 clustering methods on some challenging datasets.
</summary>
    <author>
      <name>Xi Peng</name>
    </author>
    <author>
      <name>Joey Tianyi Zhou</name>
    </author>
    <author>
      <name>Hongyuan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07288v1</id>
    <updated>2018-08-22T09:15:25Z</updated>
    <published>2018-08-22T09:15:25Z</published>
    <title>Clustering and Labelling Auction Fraud Data</title>
    <summary>  Although shill bidding is a common auction fraud, it is however very tough to
detect. Due to the unavailability and lack of training data, in this study, we
build a high-quality labeled shill bidding dataset based on recently collected
auctions from eBay. Labeling shill biding instances with multidimensional
features is a critical phase for the fraud classification task. For this
purpose, we introduce a new approach to systematically label the fraud data
with the help of the hierarchical clustering CURE that returns remarkable
results as illustrated in the experiments.
</summary>
    <author>
      <name>Ahmad Alzahrani</name>
    </author>
    <author>
      <name>Samira Sadaoui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.6084/m9.figshare.6993308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.6084/m9.figshare.6993308" rel="related"/>
    <link href="http://arxiv.org/abs/1808.07288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07270v1</id>
    <updated>2018-08-22T08:29:16Z</updated>
    <published>2018-08-22T08:29:16Z</published>
    <title>Learning to Support: Exploiting Structure Information in Support Sets
  for One-Shot Learning</title>
    <summary>  Deep Learning shows very good performance when trained on large labeled data
sets. The problem of training a deep net on a few or one sample per class
requires a different learning approach which can generalize to unseen classes
using only a few representatives of these classes. This problem has previously
been approached by meta-learning. Here we propose a novel meta-learner which
shows state-of-the-art performance on common benchmarks for one/few shot
classification. Our model features three novel components: First is a
feed-forward embedding that takes random class support samples (after a
customary CNN embedding) and transfers them to a better class representation in
terms of a classification problem. Second is a novel attention mechanism,
inspired by competitive learning, which causes class representatives to compete
with each other to become a temporary class prototype with respect to the query
point. This mechanism allows switching between representatives depending on the
position of the query point. Once a prototype is chosen for each class, the
predicated label is computed using a simple attention mechanism over prototypes
of all considered classes. The third feature is the ability of our meta-learner
to incorporate deeper CNN embedding, enabling larger capacity. Finally, to ease
the training procedure and reduce overfitting, we averages the top $t$ models
(evaluated on the validation) over the optimization trajectory. We show that
this approach can be viewed as an approximation to an ensemble, which saves the
factor of $t$ in training and test times and the factor of of $t$ in the
storage of the final model.
</summary>
    <author>
      <name>Jinchao Liu</name>
    </author>
    <author>
      <name>Stuart J. Gibson</name>
    </author>
    <author>
      <name>Margarita Osadchy</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07260v1</id>
    <updated>2018-08-22T07:55:29Z</updated>
    <published>2018-08-22T07:55:29Z</published>
    <title>On an improvement of LASSO by scaling</title>
    <summary>  A sparse modeling is a major topic in machine learning and statistics. LASSO
(Least Absolute Shrinkage and Selection Operator) is a popular sparse modeling
method while it has been known to yield unexpected large bias especially at a
sparse representation. There have been several studies for improving this
problem such as the introduction of non-convex regularization terms. The
important point is that this bias problem directly affects model selection in
applications since a sparse representation cannot be selected by a prediction
error based model selection even if it is a good representation. In this
article, we considered to improve this problem by introducing a scaling that
expands LASSO estimator to compensate excessive shrinkage, thus a large bias in
LASSO estimator. We here gave an empirical value for the amount of scaling.
There are two advantages of this scaling method as follows. Since the proposed
scaling value is calculated by using LASSO estimator, we only need LASSO
estimator that is obtained by a fast and stable optimization procedure such as
LARS (Least Angle Regression) under LASSO modification or coordinate descent.
And, the simplicity of our scaling method enables us to derive SURE (Stein's
Unbiased Risk Estimate) under the modified LASSO estimator with scaling. Our
scaling method together with model selection based on SURE is fully empirical
and do not need additional hyper-parameters. In a simple numerical example, we
verified that our scaling method actually improves LASSO and the SURE based
model selection criterion can stably choose an appropriate sparse model.
</summary>
    <author>
      <name>Katsuyuki Hagiwara</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07258v1</id>
    <updated>2018-08-22T07:51:22Z</updated>
    <published>2018-08-22T07:51:22Z</published>
    <title>Escaping from Collapsing Modes in a Constrained Space</title>
    <summary>  Generative adversarial networks (GANs) often suffer from unpredictable
mode-collapsing during training. We study the issue of mode collapse of
Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of
the state-of-the-art generative models. Despite its potential of generating
high-quality images, we find that BEGAN tends to collapse at some modes after a
period of training. We propose a new model, called \emph{BEGAN with a
Constrained Space} (BEGAN-CS), which includes a latent-space constraint in the
loss function. We show that BEGAN-CS can significantly improve training
stability and suppress mode collapse without either increasing the model
complexity or degrading the image quality. Further, we visualize the
distribution of latent vectors to elucidate the effect of latent-space
constraint. The experimental results show that our method has additional
advantages of being able to train on small datasets and to generate images
similar to a given real image yet with variations of designated attributes
on-the-fly.
</summary>
    <author>
      <name>Chia-Che Chang</name>
    </author>
    <author>
      <name>Chieh Hubert Lin</name>
    </author>
    <author>
      <name>Che-Rung Lee</name>
    </author>
    <author>
      <name>Da-Cheng Juan</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Hwann-Tzong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06464v2</id>
    <updated>2018-08-22T07:28:28Z</updated>
    <published>2017-11-17T09:29:52Z</published>
    <title>A unified deep artificial neural network approach to partial
  differential equations in complex geometries</title>
    <summary>  In this paper we use deep feedforward artificial neural networks to
approximate solutions to partial differential equations in complex geometries.
We show how to modify the backpropagation algorithm to compute the partial
derivatives of the network output with respect to the space variables which is
needed to approximate the differential operator. The method is based on an
ansatz for the solution which requires nothing but feedforward neural networks
and an unconstrained gradient based optimization method such as gradient
descent or a quasi-Newton method.
  We show an example where classical mesh based methods cannot be used and
neural networks can be seen as an attractive alternative. Finally, we highlight
the benefits of deep compared to shallow neural networks and device some other
convergence enhancing techniques.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nyström</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2018.06.056</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2018.06.056" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06464v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06464v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07251v1</id>
    <updated>2018-08-22T07:27:26Z</updated>
    <published>2018-08-22T07:27:26Z</published>
    <title>Genie: An Open Box Counterfactual Policy Estimator for Optimizing
  Sponsored Search Marketplace</title>
    <summary>  In this paper, we propose an offline counterfactual policy estimation
framework called Genie to optimize Sponsored Search Marketplace. Genie employs
an open box simulation engine with click calibration model to compute the KPI
impact of any modification to the system. From the experimental results on Bing
traffic, we showed that Genie performs better than existing observational
approaches that employs randomized experiments for traffic slices that have
frequent policy updates. We also show that Genie can be used to tune completely
new policies efficiently without creating risky randomized experiments due to
cold start problem. As time of today, Genie hosts more than 10000 optimization
jobs yearly which runs more than 30 Million processing node hours of big data
jobs for Bing Ads. For the last 3 years, Genie has been proven to be the one of
the major platforms to optimize Bing Ads Marketplace due to its reliability
under frequent policy changes and its efficiency to minimize risks in real
experiments.
</summary>
    <author>
      <name>Murat Ali Bayir</name>
    </author>
    <author>
      <name>Mingsen Xu</name>
    </author>
    <author>
      <name>Yaojia Zhu</name>
    </author>
    <author>
      <name>Yifan Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07249v1</id>
    <updated>2018-08-22T07:20:04Z</updated>
    <published>2018-08-22T07:20:04Z</published>
    <title>Analysis of Network Lasso For Semi-Supervised Regression</title>
    <summary>  We characterize the statistical properties of network Lasso for
semi-supervised regression problems involving network- structured data. This
characterization is based on the con- nectivity properties of the empirical
graph which encodes the similarities between individual data points. Loosely
speaking, network Lasso is accurate if the available label informa- tion is
well connected with the boundaries between clusters of the network-structure
datasets. We make this property precise using the notion of network flows. In
particular, the existence of a sufficiently large network flow over the
empirical graph implies a network compatibility condition which, in turn, en-
sures accuracy of network Lasso.
</summary>
    <author>
      <name>Alexander Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06576v2</id>
    <updated>2018-08-22T07:18:38Z</updated>
    <published>2018-08-20T17:31:09Z</published>
    <title>Peptide-Spectra Matching from Weak Supervision</title>
    <summary>  As in many other scientific domains, we face a fundamental problem when using
machine learning to identify proteins from mass spectrometry data: large ground
truth datasets mapping inputs to correct outputs are extremely difficult to
obtain. Instead, we have access to imperfect hand-coded models crafted by
domain experts. In this paper, we apply deep neural networks to an important
step of the protein identification problem, the pairing of mass spectra with
short sequences of amino acids called peptides. We train our model to
differentiate between top scoring results from a state-of-the art classical
system and hard-negative second and third place results. Our resulting model is
much better at identifying peptides with spectra than the model used to
generate its training data. In particular, we achieve a 43% improvement over
standard matching methods and a 10% improvement over a combination of the
matching method and an industry standard cross-spectra reranking tool.
Importantly, in a more difficult experimental regime that reflects current
challenges facing biologists, our advantage over the previous state-of-the-art
grows to 15% even after reranking. We believe this approach will generalize to
other challenging scientific problems.
</summary>
    <author>
      <name>Samuel S. Schoenholz</name>
    </author>
    <author>
      <name>Sean Hackett</name>
    </author>
    <author>
      <name>Laura Deming</name>
    </author>
    <author>
      <name>Eugene Melamud</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Fiona McAllister</name>
    </author>
    <author>
      <name>Jonathon O'Brien</name>
    </author>
    <author>
      <name>George Dahl</name>
    </author>
    <author>
      <name>Bryson Bennett</name>
    </author>
    <author>
      <name>Andrew M. Dai</name>
    </author>
    <author>
      <name>Daphne Koller</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07243v1</id>
    <updated>2018-08-22T06:55:44Z</updated>
    <published>2018-08-22T06:55:44Z</published>
    <title>Controversy Rules - Discovering Regions Where Classifiers (Dis-)Agree
  Exceptionally</title>
    <summary>  Finding regions for which there is higher controversy among different
classifiers is insightful with regards to the domain and our models. Such
evaluation can falsify assumptions, assert some, or also, bring to the
attention unknown phenomena. The present work describes an algorithm, which is
based on the Exceptional Model Mining framework, and enables that kind of
investigations. We explore several public datasets and show the usefulness of
this approach in classification tasks. We show in this paper a few interesting
observations about those well explored datasets, some of which are general
knowledge, and other that as far as we know, were not reported before.
</summary>
    <author>
      <name>Oren Zeev-Ben-Mordehai</name>
    </author>
    <author>
      <name>Wouter Duivesteijn</name>
    </author>
    <author>
      <name>Mykola Pechenizkiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08280v1</id>
    <updated>2018-08-22T06:08:45Z</updated>
    <published>2018-08-22T06:08:45Z</published>
    <title>Deep multiscale convolutional feature learning for weakly supervised
  localization of chest pathologies in X-ray images</title>
    <summary>  Localization of chest pathologies in chest X-ray images is a challenging task
because of their varying sizes and appearances. We propose a novel weakly
supervised method to localize chest pathologies using class aware deep
multiscale feature learning. Our method leverages intermediate feature maps
from CNN layers at different stages of a deep network during the training of a
classification model using image level annotations of pathologies. During the
training phase, a set of \emph{layer relevance weights} are learned for each
pathology class and the CNN is optimized to perform pathology classification by
convex combination of feature maps from both shallow and deep layers using the
learned weights. During the test phase, to localize the predicted pathology,
the multiscale attention map is obtained by convex combination of class
activation maps from each stage using the \emph{layer relevance weights}
learned during the training phase. We have validated our method using 112000
X-ray images and compared with the state-of-the-art localization methods. We
experimentally demonstrate that the proposed weakly supervised method can
improve the localization performance of small pathologies such as nodule and
mass while giving comparable performance for bigger pathologies e.g.,
Cardiomegaly
</summary>
    <author>
      <name>Suman Sedai</name>
    </author>
    <author>
      <name>Dwarikanath Mahapatra</name>
    </author>
    <author>
      <name>Zongyuan Ge</name>
    </author>
    <author>
      <name>Rajib Chakravorty</name>
    </author>
    <author>
      <name>Rahil Garnavi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02419v4</id>
    <updated>2018-08-22T05:33:38Z</updated>
    <published>2017-06-08T00:47:46Z</published>
    <title>Estimating Mixture Entropy with Pairwise Distances</title>
    <summary>  Mixture distributions arise in many parametric and non-parametric settings --
for example, in Gaussian mixture models and in non-parametric estimation. It is
often necessary to compute the entropy of a mixture, but, in most cases, this
quantity has no closed-form expression, making some form of approximation
necessary. We propose a family of estimators based on a pairwise distance
function between mixture components, and show that this estimator class has
many attractive properties. For many distributions of interest, the proposed
estimators are efficient to compute, differentiable in the mixture parameters,
and become exact when the mixture components are clustered. We prove this
family includes lower and upper bounds on the mixture entropy. The Chernoff
$\alpha$-divergence gives a lower bound when chosen as the distance function,
with the Bhattacharyya distance providing the tightest lower bound for
components that are symmetric and members of a location family. The
Kullback-Leibler divergence gives an upper bound when used as the distance
function. We provide closed-form expressions of these bounds for mixtures of
Gaussians, and discuss their applications to the estimation of mutual
information. We then demonstrate that our bounds are significantly tighter than
well-known existing bounds using numeric simulations. This estimator class is
very useful in optimization problems involving maximization/minimization of
entropy and mutual information, such as MaxEnt and rate distortion problems.
</summary>
    <author>
      <name>Artemy Kolchinsky</name>
    </author>
    <author>
      <name>Brendan D. Tracey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/e19070361</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/e19070361" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrects several errata in published version, in particular in
  Section V (bounds on mutual information)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Entropy 2017, 19(7), 361</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.02419v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02419v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06314v2</id>
    <updated>2018-08-22T05:25:38Z</updated>
    <published>2018-08-20T05:47:41Z</published>
    <title>A General Framework of Multi-Armed Bandit Processes by Arm Switch
  Restrictions</title>
    <summary>  This paper proposes a general framework of multi-armed bandit (MAB) processes
by introducing a type of restrictions on the switches among arms evolving in
continuous time.
  The Gittins index process is constructed for any single arm subject to the
restrictions on switches and then the optimality of the corresponding Gittins
index rule is established. The Gittins indices defined in this paper are
consistent with the ones for MAB processes in continuous time, integer time,
semi-Markovian setting as well as general discrete time setting, so that the
new theory covers the classical models as special cases and also applies to
many other situations that have not yet been touched in the literature. While
the proof of the optimality of Gittins index policies benefits from ideas in
the existing theory of MAB processes in continuous time, new techniques are
introduced which drastically simplify the proof.
</summary>
    <author>
      <name>Wenqing Bao</name>
    </author>
    <author>
      <name>Xiaoqiang Cai</name>
    </author>
    <author>
      <name>Xianyi Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C39, 93E20, 93E35, 49L20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07217v1</id>
    <updated>2018-08-22T04:50:55Z</updated>
    <published>2018-08-22T04:50:55Z</published>
    <title>Don't Use Large Mini-Batches, Use Local SGD</title>
    <summary>  Mini-batch stochastic gradient methods are the current state of the art for
large-scale distributed training of neural networks and other machine learning
models. However, they fail to adapt to a changing communication vs computation
trade-off in a system, such as when scaling to a large number of workers or
devices. More so, the fixed requirement of communication bandwidth for gradient
exchange severely limits the scalability to multi-node training e.g. in
datacenters, and even more so for training on decentralized networks such as
mobile devices. We argue that variants of local SGD, which perform several
update steps on a local model before communicating to other nodes, offer
significantly improved overall performance and communication efficiency, as
well as adaptivity to the underlying system resources. Furthermore, we present
a new hierarchical extension of local SGD, and demonstrate that it can
efficiently adapt to several levels of computation costs in a heterogeneous
distributed system.
</summary>
    <author>
      <name>Tao Lin</name>
    </author>
    <author>
      <name>Sebastian U. Stich</name>
    </author>
    <author>
      <name>Martin Jaggi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07216v1</id>
    <updated>2018-08-22T04:24:30Z</updated>
    <published>2018-08-22T04:24:30Z</published>
    <title>Model Interpretation: A Unified Derivative-based Framework for
  Nonparametric Regression and Supervised Machine Learning</title>
    <summary>  Interpreting a nonparametric regression model with many predictors is known
to be a challenging problem. There has been renewed interest in this topic due
to the extensive use of machine learning algorithms and the difficulty in
understanding and explaining their input-output relationships. This paper
develops a unified framework using a derivative-based approach for existing
tools in the literature, including the partial-dependence plots, marginal plots
and accumulated effects plots. It proposes a new interpretation technique
called the accumulated total derivative effects plot and demonstrates how its
components can be used to develop extensive insights in complex regression
models with correlated predictors. The techniques are illustrated through
simulation results.
</summary>
    <author>
      <name>Xiaoyu Liu</name>
    </author>
    <author>
      <name>Jie Chen</name>
    </author>
    <author>
      <name>Vijayan Nair</name>
    </author>
    <author>
      <name>Agus Sudjianto</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06148v2</id>
    <updated>2018-08-22T01:50:47Z</updated>
    <published>2018-08-19T00:59:55Z</published>
    <title>Generalized Bregman and Jensen divergences which include some
  f-divergences</title>
    <summary>  In this paper, we introduce new classes of divergences by extending the
definitions of the Bregman divergence and the skew Jensen divergence. These new
divergence classes (g-Bregman divergence and skew g-Jensen divergence) satisfy
some properties similar to the Bregman or skew Jensen divergence. We show these
g-divergences include divergences which belong to a class of f-divergence (the
Hellinger distance, the chi-square divergence and the alpha-divergence in
addition to the Kullback-Leibler divergence). Moreover, we derive an inequality
between the skew g-Jensen divergence and the g-Bregman divergence and show this
inequality is a generalization of Lin's inequality.
</summary>
    <author>
      <name>Tomohiro Nishiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07172v1</id>
    <updated>2018-08-22T01:04:07Z</updated>
    <published>2018-08-22T01:04:07Z</published>
    <title>Fisher Information and Natural Gradient Learning of Random Deep Networks</title>
    <summary>  A deep neural network is a hierarchical nonlinear model transforming input
signals to output signals. Its input-output relation is considered to be
stochastic, being described for a given input by a parameterized conditional
probability distribution of outputs. The space of parameters consisting of
weights and biases is a Riemannian manifold, where the metric is defined by the
Fisher information matrix. The natural gradient method uses the steepest
descent direction in a Riemannian manifold, so it is effective in learning,
avoiding plateaus. It requires inversion of the Fisher information matrix,
however, which is practically impossible when the matrix has a huge number of
dimensions. Many methods for approximating the natural gradient have therefore
been introduced. The present paper uses statistical neurodynamical method to
reveal the properties of the Fisher information matrix in a net of random
connections under the mean field approximation. We prove that the Fisher
information matrix is unit-wise block diagonal supplemented by small order
terms of off-block-diagonal elements, which provides a justification for the
quasi-diagonal natural gradient method by Y. Ollivier. A unitwise
block-diagonal Fisher metrix reduces to the tensor product of the Fisher
information matrices of single units. We further prove that the Fisher
information matrix of a single unit has a simple reduced form, a sum of a
diagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the
inverse of Fisher information explicitly. We then have an explicit form of the
natural gradient, without relying on the numerical matrix inversion, which
drastically speeds up stochastic gradient learning.
</summary>
    <author>
      <name>Shun-ichi Amari</name>
    </author>
    <author>
      <name>Ryo Karakida</name>
    </author>
    <author>
      <name>Masafumi Oizumi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07169v1</id>
    <updated>2018-08-22T00:57:41Z</updated>
    <published>2018-08-22T00:57:41Z</published>
    <title>Statistical Neurodynamics of Deep Networks: Geometry of Signal Spaces</title>
    <summary>  Statistical neurodynamics studies macroscopic behaviors of randomly connected
neural networks. We consider a deep layered feedforward network where input
signals are processed layer by layer. The manifold of input signals is embedded
in a higher dimensional manifold of the next layer as a curved submanifold,
provided the number of neurons is larger than that of inputs. We show
geometrical features of the embedded manifold, proving that the manifold
enlarges or shrinks locally isotropically so that it is always embedded
conformally. We study the curvature of the embedded manifold. The scalar
curvature converges to a constant or diverges to infinity slowly. The distance
between two signals also changes, converging eventually to a stable fixed
value, provided both the number of neurons in a layer and the number of layers
tend to infinity. This causes a problem, since when we consider a curve in the
input space, it is mapped as a continuous curve of fractal nature, but our
theory contradictorily suggests that the curve eventually converges to a
discrete set of equally spaced points. In reality, the numbers of neurons and
layers are finite and thus, it is expected that the finite size effect causes
the discrepancies between our theory and reality. We need to further study the
discrepancies to understand their implications on information processing.
</summary>
    <author>
      <name>Shun-ichi Amari</name>
    </author>
    <author>
      <name>Ryo Karakida</name>
    </author>
    <author>
      <name>Masafumi Oizumi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07168v1</id>
    <updated>2018-08-22T00:51:57Z</updated>
    <published>2018-08-22T00:51:57Z</published>
    <title>On Deep Neural Networks for Detecting Heart Disease</title>
    <summary>  Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as "at risk." Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5's design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
</summary>
    <author>
      <name>Nathalie-Sofia Tomov</name>
    </author>
    <author>
      <name>Stanimire Tomov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04577v3</id>
    <updated>2018-08-21T22:41:34Z</updated>
    <published>2018-04-12T15:46:12Z</published>
    <title>Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and
  Some New Implementations</title>
    <summary>  In this paper we discuss policy iteration methods for approximate solution of
a finite-state discounted Markov decision problem, with a focus on
feature-based aggregation methods and their connection with deep reinforcement
learning schemes. We introduce features of the states of the original problem,
and we formulate a smaller "aggregate" Markov decision problem, whose states
relate to the features. We discuss properties and possible implementations of
this type of aggregation, including a new approach to approximate policy
iteration. In this approach the policy improvement operation combines
feature-based aggregation with feature construction using deep neural networks
or other calculations. We argue that the cost function of a policy may be
approximated much more accurately by the nonlinear function of the features
provided by aggregation, than by the linear function of the features provided
by neural network-based reinforcement learning, thereby potentially leading to
more effective policy improvement.
</summary>
    <author>
      <name>Dimitri P. Bertsekas</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04577v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04577v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="49, 90, 93" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05236v3</id>
    <updated>2018-08-21T22:16:21Z</updated>
    <published>2018-01-16T13:06:12Z</published>
    <title>MORF: A Framework for Predictive Modeling and Replication At Scale With
  Privacy-Restricted MOOC Data</title>
    <summary>  Big data repositories from online learning platforms such as Massive Open
Online Courses (MOOCs) represent an unprecedented opportunity to advance
research on education at scale and impact a global population of learners. To
date, such research has been hindered by poor reproducibility and a lack of
replication, largely due to three types of barriers: experimental, inferential,
and data. We present a novel system for large-scale computational research, the
MOOC Replication Framework (MORF), to jointly address these barriers. We
discuss MORF's architecture, an open-source platform-as-a-service (PaaS) which
includes a simple, flexible software API providing for multiple modes of
research (predictive modeling or production rule analysis) integrated with a
high-performance computing environment. All experiments conducted on MORF use
executable Docker containers which ensure complete reproducibility while
allowing for the use of any software or language which can be installed in the
linux-based Docker container. Each experimental artifact is assigned a DOI and
made publicly available. MORF has the potential to accelerate and democratize
research on its massive data repository, which currently includes over 200
MOOCs, as demonstrated by initial research conducted on the platform. We also
highlight ways in which MORF represents a solution template to a more general
class of problems faced by computational researchers in other domains.
</summary>
    <author>
      <name>Josh Gardner</name>
    </author>
    <author>
      <name>Christopher Brooks</name>
    </author>
    <author>
      <name>Juan Miguel L. Andres</name>
    </author>
    <author>
      <name>Ryan Baker</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05236v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05236v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05247v2</id>
    <updated>2018-08-21T21:33:40Z</updated>
    <published>2018-07-13T18:56:34Z</published>
    <title>Channel Charting: Locating Users within the Radio Environment using
  Channel State Information</title>
    <summary>  We propose channel charting (CC), a novel framework in which a multi-antenna
network element learns a chart of the radio geometry in its surrounding area.
The channel chart captures the local spatial geometry of the area so that
points that are close in space will also be close in the channel chart and vice
versa. CC works in a fully unsupervised manner, i.e., learning is only based on
channel state information (CSI) that is passively collected at a single point
in space, but from multiple transmit locations in the area over time. The
method then extracts channel features that characterize large-scale fading
properties of the wireless channel. Finally, the channel charts are generated
with tools from dimensionality reduction, manifold learning, and deep neural
networks. The network element performing CC may be, for example, a
multi-antenna base-station in a cellular system and the charted area in the
served cell. Logical relationships related to the position and movement of a
transmitter, e.g., a user equipment (UE), in the cell can then be directly
deduced from comparing measured radio channel characteristics to the channel
chart. The unsupervised nature of CC enables a range of new applications in UE
localization, network planning, user scheduling, multipoint connectivity,
hand-over, cell search, user grouping, and other cognitive tasks that rely on
CSI and UE movement relative to the base-station, without the need of
information from global navigation satellite systems.
</summary>
    <author>
      <name>Christoph Studer</name>
    </author>
    <author>
      <name>Saïd Medjkouh</name>
    </author>
    <author>
      <name>Emre Gönültaş</name>
    </author>
    <author>
      <name>Tom Goldstein</name>
    </author>
    <author>
      <name>Olav Tirkkonen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Access</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05247v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05247v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04295v2</id>
    <updated>2018-08-21T21:01:43Z</updated>
    <published>2018-08-13T15:40:41Z</published>
    <title>Understanding training and generalization in deep learning by Fourier
  analysis</title>
    <summary>  Background: It is still an open research area to theoretically understand why
Deep Neural Networks (DNNs)---equipped with many more parameters than training
data and trained by (stochastic) gradient-based methods---often achieve
remarkably low generalization error. Contribution: We study DNN training by
Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic)
gradient-based methods endows low-frequency components of the target function
with a higher priority during the training; ii) Small initialization leads to
good generalization ability of DNN while preserving the DNN's ability of
fitting any function. These results are further confirmed by experiments of
DNNs fitting the following datasets, i.e., natural images, one-dimensional
functions and MNIST dataset.
</summary>
    <author>
      <name>Zhiqin John Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04295v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04295v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32, 68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07105v1</id>
    <updated>2018-08-21T19:55:33Z</updated>
    <published>2018-08-21T19:55:33Z</published>
    <title>Non-asymptotic bounds for sampling algorithms without log-concavity</title>
    <summary>  Discrete time analogues of ergodic stochastic differential equations (SDEs)
are one of the most popular and flexible tools for sampling high-dimensional
probability measures. Non-asymptotic analysis in the $L^2$ Wasserstein distance
of sampling algorithms based on Euler discretisations of SDEs has been recently
developed by several authors for log-concave probability distributions. In this
work we replace the log-concavity assumption with a log-concavity at infinity
condition. We provide novel $L^2$ convergence rates for Euler schemes,
expressed explicitly in terms of problem parameters. From there we derive
non-asymptotic bounds on the distance between the laws induced by Euler schemes
and the invariant laws of SDEs, both for schemes with standard and with
randomised (inaccurate) drifts. We also obtain bounds for the hierarchy of
discretisation, which enables us to deploy a multi-level Monte Carlo estimator.
Our proof relies on a novel construction of a coupling for the Markov chains
that can be used to control both the $L^1$ and $L^2$ Wasserstein distances
simultaneously. Finally, we provide a weak convergence analysis that covers
both the standard and the randomised (inaccurate) drift case. In particular, we
reveal that the variance of the randomised drift does not influence the rate of
weak convergence of the Euler scheme to the SDE.
</summary>
    <author>
      <name>Mateusz B. Majka</name>
    </author>
    <author>
      <name>Aleksandar Mijatović</name>
    </author>
    <author>
      <name>Lukasz Szpruch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11374v2</id>
    <updated>2018-08-21T18:40:09Z</updated>
    <published>2018-07-24T17:58:56Z</published>
    <title>Weakly-Supervised Deep Learning of Heat Transport via Physics Informed
  Loss</title>
    <summary>  In typical machine learning tasks and applications, it is necessary to obtain
or create large labeled datasets in order to to achieve high performance.
Unfortunately, large labeled datasets are not always available and can be
expensive to source, creating a bottleneck towards more widely applicable
machine learning. The paradigm of weak supervision offers an alternative that
allows for integration of domain-specific knowledge by enforcing constraints
that a correct solution to the learning problem will obey over the output
space. In this work, we explore the application of this paradigm to 2-D
physical systems governed by non-linear differential equations. We demonstrate
that knowledge of the partial differential equations governing a system can be
encoded into the loss function of a neural network via an appropriately chosen
convolutional kernel. We demonstrate this by showing that the steady-state
solution to the 2-D heat equation can be learned directly from initial
conditions by a convolutional neural network, in the absence of labeled
training data. We also extend recent work in the progressive growing of fully
convolutional networks to achieve high accuracy (&lt; 1.5% error) at multiple
scales of the heat-flow problem, including at the very large scale (1024x1024).
Finally, we demonstrate that this method can be used to speed up exact
calculation of the solution to the differential equations via finite
difference.
</summary>
    <author>
      <name>Rishi Sharma</name>
    </author>
    <author>
      <name>Amir Barati Farimani</name>
    </author>
    <author>
      <name>Joe Gomes</name>
    </author>
    <author>
      <name>Peter Eastman</name>
    </author>
    <author>
      <name>Vijay Pande</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07069v1</id>
    <updated>2018-08-21T18:03:36Z</updated>
    <published>2018-08-21T18:03:36Z</published>
    <title>Machine learning non-local correlations</title>
    <summary>  The ability to witness non-local correlations lies at the core of
foundational aspects of quantum mechanics and its application in the processing
of information. Commonly, this is achieved via the violation of Bell
inequalities. Unfortunately, however, their systematic derivation quickly
becomes unfeasible as the scenario of interest grows in complexity. To cope
with that, we propose here a machine learning approach for the detection and
quantification of non-locality. It consists of an ensemble of multilayer
perceptrons blended with genetic algorithms achieving a high performance in a
number of relevant Bell scenarios. Our results offer a novel method and a
proof-of-principle for the relevance of machine learning for understanding
non-locality.
</summary>
    <author>
      <name>Askery Canabarro</name>
    </author>
    <author>
      <name>Samuraí Brito</name>
    </author>
    <author>
      <name>Rafael Chaves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01251v2</id>
    <updated>2018-08-21T17:53:43Z</updated>
    <published>2018-07-03T15:50:41Z</published>
    <title>Training behavior of deep neural network in frequency domain</title>
    <summary>  Why deep neural networks (DNNs) capable of overfitting often generalize well
in practice is a mystery in deep learning. Existing works indicate that this
observation holds for both complicated real datasets and simple datasets of
one-dimensional (1-d) functions. In this work, for natural images and
low-frequency dominant 1-d functions, we empirically found that a DNN with
common settings first quickly captures the dominant low-frequency components,
and then relatively slowly captures high-frequency ones. We call this
phenomenon Frequency Principle (F-Principle). F-Principle can be observed over
various DNN setups of different activation functions, layer structures and
training algorithms in our experiments. F-Principle can be used to understand
(i) the behavior of DNN training in the information plane and (ii) why DNNs
often generalize well albeit its ability of overfitting. This F-Principle
potentially can provide insights into understanding the general principle
underlying DNN optimization and generalization for real datasets.
</summary>
    <author>
      <name>Zhi-Qin J. Xu</name>
    </author>
    <author>
      <name>Yaoyu Zhang</name>
    </author>
    <author>
      <name>Yanyang Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04926v2</id>
    <updated>2018-08-21T16:48:54Z</updated>
    <published>2018-08-14T23:59:26Z</published>
    <title>How Much Reading Does Reading Comprehension Require? A Critical
  Investigation of Popular Benchmarks</title>
    <summary>  Many recent papers address reading comprehension, where examples consist of
(question, passage, answer) tuples. Presumably, a model must combine
information from both questions and passages to predict corresponding answers.
However, despite intense interest in the topic, with hundreds of published
papers vying for leaderboard dominance, basic questions about the difficulty of
many popular benchmarks remain unanswered. In this paper, we establish sensible
baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding
that question- and passage-only models often perform surprisingly well. On $14$
out of $20$ bAbI tasks, passage-only models achieve greater than $50\%$
accuracy, sometimes matching the full model. Interestingly, while CBT provides
$20$-sentence stories only the last is needed for comparably accurate
prediction. By comparison, SQuAD and CNN appear better-constructed.
</summary>
    <author>
      <name>Divyansh Kaushik</name>
    </author>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04926v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04926v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06996v1</id>
    <updated>2018-08-21T16:16:46Z</updated>
    <published>2018-08-21T16:16:46Z</published>
    <title>Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models
  and Phase Retrieval</title>
    <summary>  We study the fundamental tradeoffs between statistical accuracy and
computational tractability in the analysis of high dimensional heterogeneous
data. As examples, we study sparse Gaussian mixture model, mixture of sparse
linear regressions, and sparse phase retrieval model. For these models, we
exploit an oracle-based computational model to establish conjecture-free
computationally feasible minimax lower bounds, which quantify the minimum
signal strength required for the existence of any algorithm that is both
computationally tractable and statistically accurate. Our analysis shows that
there exist significant gaps between computationally feasible minimax risks and
classical ones. These gaps quantify the statistical price we must pay to
achieve computational tractability in the presence of data heterogeneity. Our
results cover the problems of detection, estimation, support recovery, and
clustering, and moreover, resolve several conjectures of Azizyan et al. (2013,
2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our
results reveal a new but counter-intuitive phenomenon in heterogeneous data
analysis that more data might lead to less computation complexity.
</summary>
    <author>
      <name>Jianqing Fan</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Zhuoran Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">75 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06992v1</id>
    <updated>2018-08-21T16:14:55Z</updated>
    <published>2018-08-21T16:14:55Z</published>
    <title>Optimizing the Union of Intersections LASSO ($UoI_{LASSO}$) and Vector
  Autoregressive ($UoI_{VAR}$) Algorithms for Improved Statistical Estimation
  at Scale</title>
    <summary>  The analysis of scientific data of increasing size and complexity requires
statistical machine learning methods that are both interpretable and
predictive. Union of Intersections (UoI), a recently developed framework, is a
two-step approach that separates model selection and model estimation. A linear
regression algorithm based on UoI, $UoI_{LASSO}$, simultaneously achieves low
false positives and low false negative feature selection as well as low bias
and low variance estimates. Together, these qualities make the results both
predictive and interpretable. In this paper, we optimize the $UoI_{LASSO}$
algorithm for single-node execution on NERSC's Cori Knights Landing, a Xeon Phi
based supercomputer. We then scale $UoI_{LASSO}$ to execute on cores ranging
from 68-278,528 cores on a range of dataset sizes demonstrating the weak and
strong scaling of the implementation. We also implement a variant of
$UoI_{LASSO}$, $UoI_{VAR}$ for vector autoregressive models, to analyze high
dimensional time-series data. We perform single node optimization and
multi-node scaling experiments for $UoI_{VAR}$ to demonstrate the effectiveness
of the algorithm for weak and strong scaling. Our implementations enable to use
estimate the largest VAR model (1000 nodes) we are aware of, and apply it to
large neurophysiology data 192 nodes).
</summary>
    <author>
      <name>Mahesh Balasubramanian</name>
    </author>
    <author>
      <name>Trevor Ruiz</name>
    </author>
    <author>
      <name>Brandon Cook</name>
    </author>
    <author>
      <name>Sharmodeep Bhattacharyya</name>
    </author>
    <author>
      <name> Prabhat</name>
    </author>
    <author>
      <name>Aviral Shrivastava</name>
    </author>
    <author>
      <name>Kristofer Bouchard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00939v2</id>
    <updated>2018-08-21T16:00:02Z</updated>
    <published>2018-07-02T04:21:10Z</published>
    <title>Mining Illegal Insider Trading of Stocks: A Proactive Approach</title>
    <summary>  Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
</summary>
    <author>
      <name>Sheikh Rabiul Islam</name>
    </author>
    <author>
      <name>Sheikh Khaled Ghafoor</name>
    </author>
    <author>
      <name>William Eberle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Previous version (A Deep Learning Based Illegal Insider-Trading
  Detection and Prediction Technique in Stock Market ) was accepted for ICDATA,
  2018. But we didn't publish it there as the work was not adequately complete
  and the writing style wasn't good enough. We revamped paper and extended to a
  new perspective for IEEE Big Data, 2018 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00939v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00939v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06934v1</id>
    <updated>2018-08-21T14:41:56Z</updated>
    <published>2018-08-21T14:41:56Z</published>
    <title>Backpropagation and Biological Plausibility</title>
    <summary>  By and large, Backpropagation (BP) is regarded as one of the most important
neural computation algorithms at the basis of the progress in machine learning,
including the recent advances in deep learning. However, its computational
structure has been the source of many debates on its arguable biological
plausibility. In this paper, it is shown that when framing supervised learning
in the Lagrangian framework, while one can see a natural emergence of
Backpropagation, biologically plausible local algorithms can also be devised
that are based on the search for saddle points in the learning adjoint space
composed of weights, neural outputs, and Lagrangian multipliers. This might
open the doors to a truly novel class of learning algorithms where, because of
the introduction of the notion of support neurons, the optimization scheme also
plays a fundamental role in the construction of the architecture.
</summary>
    <author>
      <name>Alessandro Betti</name>
    </author>
    <author>
      <name>Marco Gori</name>
    </author>
    <author>
      <name>Giuseppe Marra</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02833v3</id>
    <updated>2018-08-21T14:31:18Z</updated>
    <published>2015-10-09T21:51:09Z</published>
    <title>On the Definiteness of Earth Mover's Distance and Its Relation to Set
  Intersection</title>
    <summary>  Positive definite kernels are an important tool in machine learning that
enable efficient solutions to otherwise difficult or intractable problems by
implicitly linearizing the problem geometry. In this paper we develop a
set-theoretic interpretation of the Earth Mover's Distance (EMD) and propose
Earth Mover's Intersection (EMI), a positive definite analog to EMD for sets of
different sizes. We provide conditions under which EMD or certain
approximations to EMD are negative definite. We also present a
positive-definite-preserving transformation that can be applied to any kernel
and can also be used to derive positive definite EMD-based kernels and show
that the Jaccard index is simply the result of this transformation. Finally, we
evaluate kernels based on EMI and the proposed transformation versus EMD in
various computer vision tasks and show that EMD is generally inferior even with
indefinite kernel techniques.
</summary>
    <author>
      <name>Andrew Gardner</name>
    </author>
    <author>
      <name>Christian A. Duncan</name>
    </author>
    <author>
      <name>Jinko Kanno</name>
    </author>
    <author>
      <name>Rastko R. Selmic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCYB.2017.2761798</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCYB.2017.2761798" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Major revision based on referee comments. Includes significant
  reorganization of content, new title, new propositions, revised proofs of
  previous propositions, and additional experiments with new data, kernels, and
  indefinite kernel techniques</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02833v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02833v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06918v1</id>
    <updated>2018-08-21T14:25:09Z</updated>
    <published>2018-08-21T14:25:09Z</published>
    <title>On a New Improvement-Based Acquisition Function for Bayesian
  Optimization</title>
    <summary>  Bayesian optimization (BO) is a popular algorithm for solving challenging
optimization tasks. It is designed for problems where the objective function is
expensive to evaluate, perhaps not available in exact form, without gradient
information and possibly returning noisy values. Different versions of the
algorithm vary in the choice of the acquisition function, which recommends the
point to query the objective at next. Initially, researchers focused on
improvement-based acquisitions, while recently the attention has shifted to
more computationally expensive information-theoretical measures. In this paper
we present two major contributions to the literature. First, we propose a new
improvement-based acquisition function that recommends query points where the
improvement is expected to be high with high confidence. The proposed algorithm
is evaluated on a large set of benchmark functions from the global optimization
literature, where it turns out to perform at least as well as current
state-of-the-art acquisition functions, and often better. This suggests that it
is a powerful default choice for BO. The novel policy is then compared to
widely used global optimization solvers in order to confirm that BO methods
reduce the computational costs of the optimization by keeping the number of
function evaluations small. The second main contribution represents an
application to precision medicine, where the interest lies in the estimation of
parameters of a partial differential equations model of the human pulmonary
blood circulation system. Once inferred, these parameters can help clinicians
in diagnosing a patient with pulmonary hypertension without going through the
standard invasive procedure of right heart catheterization, which can lead to
side effects and complications (e.g. severe pain, internal bleeding,
thrombosis).
</summary>
    <author>
      <name>Umberto Noè</name>
    </author>
    <author>
      <name>Dirk Husmeier</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06910v1</id>
    <updated>2018-08-21T14:08:44Z</updated>
    <published>2018-08-21T14:08:44Z</published>
    <title>Scalable Population Synthesis with Deep Generative Modeling</title>
    <summary>  Population synthesis is concerned with the generation of synthetic yet
realistic representations of populations. It is a fundamental problem in the
modeling of transport where the synthetic populations of micro agents represent
a key input to most agent-based models. In this paper, a new methodological
framework for how to grow pools of micro agents is presented. This is
accomplished by adopting a deep generative modeling approach from machine
learning based on a Variational Autoencoder (VAE) framework. Compared to the
previous population synthesis approaches based on Iterative Proportional
Fitting (IPF), Markov Chain Monte Carlo (MCMC) sampling or traditional
generative models, the proposed method allows unparalleled scalability with
respect to the number and types of attributes. In contrast to the approaches
that rely on approximating the joint distribution in the observed data space,
VAE learns its compressed latent representation. The advantage of the
compressed representation is that it avoids the problem of the generated
samples being trapped in local minima when the number of attributes becomes
large. The problem is illustrated using the Danish National Travel Survey data,
where the Gibbs sampler fails to generate a population with 21 attributes
(corresponding to the 121-dimensional joint distribution). At the same time,
VAE shows acceptable performance when 47 attributes (corresponding to the
357-dimensional joint distribution) are used. Moreover, VAE allows for growing
agents that are virtually different from those in the original data but have
similar statistical properties and correlation structure. The presented
approach will help modelers to generate better and richer populations with a
high level of detail, including smaller zones, personal details and travel
preferences.
</summary>
    <author>
      <name>Stanislav S. Borysov</name>
    </author>
    <author>
      <name>Jeppe Rich</name>
    </author>
    <author>
      <name>Francisco C. Pereira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06865v1</id>
    <updated>2018-08-21T12:17:08Z</updated>
    <published>2018-08-21T12:17:08Z</published>
    <title>Machine Learning for Spatiotemporal Sequence Forecasting: A Survey</title>
    <summary>  Spatiotemporal systems are common in the real-world. Forecasting the
multi-step future of these spatiotemporal systems based on the past
observations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant
and challenging problem. Although lots of real-world problems can be viewed as
STSF and many research works have proposed machine learning based methods for
them, no existing work has summarized and compared these methods from a unified
perspective. This survey aims to provide a systematic review of machine
learning for STSF. In this survey, we define the STSF problem and classify it
into three subcategories: Trajectory Forecasting of Moving Point Cloud
(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).
We then introduce the two major challenges of STSF: 1) how to learn a model for
multi-step forecasting and 2) how to adequately model the spatial and temporal
structures. After that, we review the existing works for solving these
challenges, including the general learning strategies for multi-step
forecasting, the classical machine learning based methods for STSF, and the
deep learning based methods for STSF. We also compare these methods and point
out some potential research directions.
</summary>
    <author>
      <name>Xingjian Shi</name>
    </author>
    <author>
      <name>Dit-Yan Yeung</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06846v1</id>
    <updated>2018-08-21T11:19:06Z</updated>
    <published>2018-08-21T11:19:06Z</published>
    <title>Search for Common Minima in Joint Optimization of Multiple Cost
  Functions</title>
    <summary>  We present a novel optimization method, named the Combined Optimization
Method (COM), for the joint optimization of two or more cost functions. Unlike
the conventional joint optimization schemes, which try to find minima in a
weighted sum of cost functions, the COM explores search space for common minima
shared by all the cost functions. Given a set of multiple cost functions that
have qualitatively different distributions of local minima with each other, the
proposed method finds the common minima with a high success rate without the
help of any metaheuristics. As a demonstration, we apply the COM to the crystal
structure prediction in materials science. By introducing the concept of data
assimilation, i.e., adopting the theoretical potential energy of the crystal
and the crystallinity, which characterizes the agreement with the theoretical
and experimental X-ray diffraction patterns, as cost functions, we show that
the correct crystal structures of Si diamond, low quartz, and low cristobalite
can be predicted with significantly higher success rates than the previous
methods.
</summary>
    <author>
      <name>Daiki Adachi</name>
    </author>
    <author>
      <name>Naoto Tsujimoto</name>
    </author>
    <author>
      <name>Ryosuke Akashi</name>
    </author>
    <author>
      <name>Synge Todo</name>
    </author>
    <author>
      <name>Shinji Tsuneyuki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06809v1</id>
    <updated>2018-08-21T09:08:56Z</updated>
    <published>2018-08-21T09:08:56Z</published>
    <title>Are You Tampering With My Data?</title>
    <summary>  We propose a novel approach towards adversarial attacks on neural networks
(NN), focusing on tampering the data used for training instead of generating
attacks on trained models. Our network-agnostic method creates a backdoor
during training which can be exploited at test time to force a neural network
to exhibit abnormal behaviour. We demonstrate on two widely used datasets
(CIFAR-10 and SVHN) that a universal modification of just one pixel per image
for all the images of a class in the training set is enough to corrupt the
training procedure of several state-of-the-art deep neural networks causing the
networks to misclassify any images to which the modification is applied. Our
aim is to bring to the attention of the machine learning community, the
possibility that even learning-based methods that are personally trained on
public datasets can be subject to attacks by a skillful adversary.
</summary>
    <author>
      <name>Michele Alberti</name>
    </author>
    <author>
      <name>Vinaychandran Pondenkandath</name>
    </author>
    <author>
      <name>Marcel Würsch</name>
    </author>
    <author>
      <name>Manuel Bouillon</name>
    </author>
    <author>
      <name>Mathias Seuret</name>
    </author>
    <author>
      <name>Rolf Ingold</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Conference on Computer Vision (ECCV 2018), Workshop on
  Objectionable Content and Misinformation</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.06809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06797v1</id>
    <updated>2018-08-21T08:40:16Z</updated>
    <published>2018-08-21T08:40:16Z</published>
    <title>zoNNscan : a boundary-entropy index for zone inspection of neural models</title>
    <summary>  The training of deep neural network classifiers results in decision
boundaries which geometry is still not well understood. This is in direct
relation with classification problems such as so called adversarial examples.
We introduce zoNNscan, an index that is intended to inform on the boundary
uncertainty (in terms of the presence of other classes) around one given input
datapoint. It is based on confidence entropy, and is implemented through
sampling in the multidimensional ball surrounding that input. We detail the
zoNNscan index, give an algorithm for approximating it, and finally illustrate
its benefits on four applications, including two important problems for the
adoption of deep networks in critical systems: adversarial examples and corner
case inputs. We highlight that zoNNscan exhibits significantly higher values
than for standard inputs in those two problem classes.
</summary>
    <author>
      <name>Adel Jaouen</name>
    </author>
    <author>
      <name>Erwan Le Merrer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01576v2</id>
    <updated>2018-08-21T06:17:32Z</updated>
    <published>2018-03-05T09:59:04Z</published>
    <title>Asymptotic Equivalence of Fixed-size and Varying-size Determinantal
  Point Processes</title>
    <summary>  Determinantal Point Processes (DPPs) are popular models for point processes
with repulsion. They appear in numerous contexts, from physics to graph theory,
and display appealing theoretical properties. On the more practical side of
things, since DPPs tend to select sets of points that are some distance apart
(repulsion), they have been advocated as a way of producing random subsets with
high diversity. DPPs come in two variants: fixed-size and varying-size. A
sample from a varying-size DPP is a subset of random cardinality, while in
fixed-size "$k$-DPPs" the cardinality is fixed. The latter makes more sense in
many applications, but unfortunately their computational properties are less
attractive, since, among other things, inclusion probabilities are harder to
compute. In this work we show that as the size of the ground set grows,
$k$-DPPs and DPPs become equivalent, meaning that their inclusion probabilities
converge. As a by-product, we obtain saddlepoint formulas for inclusion
probabilities in $k$-DPPs. These turn out to be extremely accurate, and suffer
less from numerical difficulties than exact methods do. Our results also
suggest that $k$-DPPs and DPPs also have equivalent maximum likelihood
estimators. Finally, we obtain results on asymptotic approximations of
elementary symmetric polynomials which may be of independent interest.
</summary>
    <author>
      <name>Simon Barthelmé</name>
    </author>
    <author>
      <name>Pierre-Olivier Amblard</name>
    </author>
    <author>
      <name>Nicolas Tremblay</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06774v1</id>
    <updated>2018-08-21T05:50:33Z</updated>
    <published>2018-08-21T05:50:33Z</published>
    <title>Multi-task multiple kernel machines for personalized pain recognition
  from functional near-infrared spectroscopy brain signals</title>
    <summary>  Currently there is no validated objective measure of pain. Recent
neuroimaging studies have explored the feasibility of using functional
near-infrared spectroscopy (fNIRS) to measure alterations in brain function in
evoked and ongoing pain. In this study, we applied multi-task machine learning
methods to derive a practical algorithm for pain detection derived from fNIRS
signals in healthy volunteers exposed to a painful stimulus. Especially, we
employed multi-task multiple kernel learning to account for the inter-subject
variability in pain response. Our results support the use of fNIRS and machine
learning techniques in developing objective pain detection, and also highlight
the importance of adopting personalized analysis in the process.
</summary>
    <author>
      <name>Daniel Lopez-Martinez</name>
    </author>
    <author>
      <name>Ke Peng</name>
    </author>
    <author>
      <name>Sarah C. Steele</name>
    </author>
    <author>
      <name>Arielle J. Lee</name>
    </author>
    <author>
      <name>David Borsook</name>
    </author>
    <author>
      <name>Rosalind Picard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Pattern Recognition (ICPR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09702v3</id>
    <updated>2018-08-21T05:41:09Z</updated>
    <published>2018-03-26T16:29:03Z</published>
    <title>HAMLET: Interpretable Human And Machine co-LEarning Technique</title>
    <summary>  Efficient label acquisition processes are key to obtaining robust
classifiers. However, data labeling is often challenging and subject to high
levels of label noise. This can arise even when classification targets are well
defined, if instances to be labeled are more difficult than the prototypes used
to define the class, leading to disagreements among the expert community. Here,
we enable efficient training of deep neural networks. From low-confidence
labels, we iteratively improve their quality by simultaneous learning of
machines and experts. We call it Human And Machine co-LEarning Technique
(HAMLET). Throughout the process, experts become more consistent, while the
algorithm provides them with explainable feedback for confirmation. HAMLET uses
a neural embedding function and a memory module filled with diverse reference
embeddings from different classes. Its output includes classification labels
and highly relevant reference embeddings as explanation. We took the study of
brain monitoring at intensive care unit (ICU) as an application of HAMLET on
continuous electroencephalography (cEEG) data. Although cEEG monitoring yields
large volumes of data, labeling costs and difficulty make it hard to build a
classifier. Additionally, while experts agree on the labels of clear-cut
examples of cEEG patterns, labeling many real-world cEEG data can be extremely
challenging. Thus, a large minority of sequences might be mislabeled. HAMLET
has shown significant performance gain against deep learning and other
baselines, increasing accuracy from 7.03% to 68.75% on challenging inputs.
Besides improved performance, clinical experts confirmed the interpretability
of those reference embeddings in helping explaining the classification results
by HAMLET.
</summary>
    <author>
      <name>Olivier Deiss</name>
    </author>
    <author>
      <name>Siddharth Biswal</name>
    </author>
    <author>
      <name>Jing Jin</name>
    </author>
    <author>
      <name>Haoqi Sun</name>
    </author>
    <author>
      <name>M. Brandon Westover</name>
    </author>
    <author>
      <name>Jimeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Removed KDD template</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09702v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09702v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03782v2</id>
    <updated>2018-08-21T05:38:27Z</updated>
    <published>2018-04-11T02:10:55Z</published>
    <title>CoT: Cooperative Training for Generative Modeling of Discrete Data</title>
    <summary>  We propose Cooperative Training (CoT) for training generative models that
measure a tractable density for discrete data. CoT coordinately trains a
generator $G$ and an auxiliary predictive mediator $M$. The training target of
$M$ is to estimate a mixture density of the learned distribution $G$ and the
target distribution $P$, and that of $G$ is to minimize the Jensen-Shannon
divergence estimated through $M$. CoT achieves independent success without the
necessity of pre-training via Maximum Likelihood Estimation or involving
high-variance algorithms like REINFORCE. This low-variance algorithm is
theoretically proved to be unbiased for both generative and predictive tasks.
We also theoretically and empirically show the superiority of CoT over most
previous algorithms in terms of generative quality and diversity, predictive
generalization ability and computational cost.
</summary>
    <author>
      <name>Sidi Lu</name>
    </author>
    <author>
      <name>Lantao Yu</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Yong Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03782v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03782v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04129v2</id>
    <updated>2018-08-21T04:37:56Z</updated>
    <published>2017-12-12T04:57:40Z</published>
    <title>Outlier Detection by Consistent Data Selection Method</title>
    <summary>  Often the challenge associated with tasks like fraud and spam detection[1] is
the lack of all likely patterns needed to train suitable supervised learning
models. In order to overcome this limitation, such tasks are attempted as
outlier or anomaly detection tasks. We also hypothesize that out- liers have
behavioral patterns that change over time. Limited data and continuously
changing patterns makes learning significantly difficult. In this work we are
proposing an approach that detects outliers in large data sets by relying on
data points that are consistent. The primary contribution of this work is that
it will quickly help retrieve samples for both consistent and non-outlier data
sets and is also mindful of new outlier patterns. No prior knowledge of each
set is required to extract the samples. The method consists of two phases, in
the first phase, consistent data points (non- outliers) are retrieved by an
ensemble method of unsupervised clustering techniques and in the second phase a
one class classifier trained on the consistent data point set is ap- plied on
the remaining sample set to identify the outliers. The approach is tested on
three publicly available data sets and the performance scores are competitive.
</summary>
    <author>
      <name>Utkarsh Porwal</name>
    </author>
    <author>
      <name>Smruthi Mukund</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04129v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04129v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07412v1</id>
    <updated>2018-08-21T03:40:21Z</updated>
    <published>2018-08-21T03:40:21Z</published>
    <title>Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation
  using Deep Neural Networks</title>
    <summary>  Statically estimating the number of processor clock cycles it takes to
execute a basic block of assembly instructions in steady state (throughput) is
important for compiler backend optimizations such as register allocation,
instruction selection and instruction scheduling. This is complicated specially
in modern x86-64 Complex Instruction Set Computer (CISC) machines with
sophisticated processor microarchitectures. Traditionally, compiler writers
invest time experimenting and referring to processor manuals to analytically
model modern processors with incomplete specifications. This is tedious, error
prone and should be done for each processor generation. We present Ithemal, the
first automatically learnt estimator to statically predict throughput of a set
of basic block instructions using machine learning. Ithemal uses a novel
Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven
approach for throughput estimation. We show that Ithemal is accurate than
state-of-the-art hand written tools used in compiler backends and static
machine code analyzers. In particular, our model has a worst case average error
of 10.53% on actual throughput values when compared to best case average errors
of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's
machine code analyzer when compared on three different microarchitectures,
while predicting throughput values at a faster rate than aforementioned tools.
We also show that Ithemal is portable, learning throughput estimation for Intel
Nehalem, Haswell and Skylake microarchitectures without requiring changes to
its structure.
</summary>
    <author>
      <name>Charith Mendis</name>
    </author>
    <author>
      <name>Saman Amarasinghe</name>
    </author>
    <author>
      <name>Michael Carbin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02538v4</id>
    <updated>2018-08-21T02:54:23Z</updated>
    <published>2017-09-08T04:53:51Z</published>
    <title>DeepFense: Online Accelerated Defense Against Adversarial Deep Learning</title>
    <summary>  Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
</summary>
    <author>
      <name>Bita Darvish Rouhani</name>
    </author>
    <author>
      <name>Mohammad Samragh</name>
    </author>
    <author>
      <name>Mojan Javaheripi</name>
    </author>
    <author>
      <name>Tara Javidi</name>
    </author>
    <author>
      <name>Farinaz Koushanfar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Adding hardware acceleration for real-time execution of defender
  modules</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02538v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02538v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06733v1</id>
    <updated>2018-08-21T01:39:17Z</updated>
    <published>2018-08-21T01:39:17Z</published>
    <title>Wrapped Loss Function for Regularizing Nonconforming Residual
  Distributions</title>
    <summary>  Multi-output is essential in machine learning that it might suffer from
nonconforming residual distributions, i.e., the multi-output residual
distributions are not conforming to the expected distribution. In this paper we
propose "Wrapped Loss Function" to wrap the original loss function to alleviate
the problem. This wrapped loss function acts just like original loss function
that its gradient can be used for backpropagation optimization. Empirical
evaluations show wrapped loss function has advanced properties of faster
convergence, better accuracy and improving imbalanced data.
</summary>
    <author>
      <name>Chun Ting Liu</name>
    </author>
    <author>
      <name>Ming Chuan Yang</name>
    </author>
    <author>
      <name>Meng Chang Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06725v1</id>
    <updated>2018-08-21T00:13:12Z</updated>
    <published>2018-08-21T00:13:12Z</published>
    <title>Learning to Exploit Invariances in Clinical Time-Series Data using
  Sequence Transformer Networks</title>
    <summary>  Recently, researchers have started applying convolutional neural networks
(CNNs) with one-dimensional convolutions to clinical tasks involving
time-series data. This is due, in part, to their computational efficiency,
relative to recurrent neural networks and their ability to efficiently exploit
certain temporal invariances, (e.g., phase invariance). However, it is
well-established that clinical data may exhibit many other types of invariances
(e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping)
may successfully transform and align inputs, their use often requires one to
identify the types of invariances in advance. In contrast, we propose the use
of Sequence Transformer Networks, an end-to-end trainable architecture that
learns to identify and account for invariances in clinical time-series data.
Applied to the task of predicting in-hospital mortality, our proposed approach
achieves an improvement in the area under the receiver operating characteristic
curve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our
results suggest that a variety of valuable invariances can be learned directly
from the data.
</summary>
    <author>
      <name>Jeeheh Oh</name>
    </author>
    <author>
      <name>Jiaxuan Wang</name>
    </author>
    <author>
      <name>Jenna Wiens</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08774v2</id>
    <updated>2018-08-20T22:52:03Z</updated>
    <published>2018-04-23T23:01:50Z</published>
    <title>Neural-Brane: Neural Bayesian Personalized Ranking for Attributed
  Network Embedding</title>
    <summary>  Network embedding methodologies, which learn a distributed vector
representation for each vertex in a network, have attracted considerable
interest in recent years. Existing works have demonstrated that vertex
representation learned through an embedding method provides superior
performance in many real-world applications, such as node classification, link
prediction, and community detection. However, most of the existing methods for
network embedding only utilize topological information of a vertex, ignoring a
rich set of nodal attributes (such as, user profiles of an online social
network, or textual contents of a citation network), which is abundant in all
real-life networks. A joint network embedding that takes into account both
attributional and relational information entails a complete network information
and could further enrich the learned vector representations. In this work, we
present Neural-Brane, a novel Neural Bayesian Personalized Ranking based
Attributed Network Embedding. For a given network, Neural-Brane extracts latent
feature representation of its vertices using a designed neural network model
that unifies network topological information and nodal attributes; Besides, it
utilizes Bayesian personalized ranking objective, which exploits the proximity
ordering between a similar node-pair and a dissimilar node-pair. We evaluate
the quality of vertex embedding produced by Neural-Brane by solving the node
classification and clustering tasks on four real-world datasets. Experimental
results demonstrate the superiority of our proposed method over the
state-of-the-art existing methods.
</summary>
    <author>
      <name>Vachik S. Dave</name>
    </author>
    <author>
      <name>Baichuan Zhang</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Mohammad Al Hasan</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07049v1</id>
    <updated>2018-08-20T21:49:13Z</updated>
    <published>2018-08-20T21:49:13Z</published>
    <title>Catastrophic Importance of Catastrophic Forgetting</title>
    <summary>  This paper describes some of the possibilities of artificial neural networks
that open up after solving the problem of catastrophic forgetting. A simple
model and reinforcement learning applications of existing methods are also
proposed.
</summary>
    <author>
      <name>Albert Ierusalem</name>
    </author>
    <link href="http://arxiv.org/abs/1808.07049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06684v1</id>
    <updated>2018-08-20T20:39:24Z</updated>
    <published>2018-08-20T20:39:24Z</published>
    <title>Use Of Vapnik-Chervonenkis Dimension in Model Selection</title>
    <summary>  In this dissertation, I derive a new method to estimate the
Vapnik-Chervonenkis Dimension (VCD) for the class of linear functions. This
method is inspired by the technique developed by Vapnik et al. Vapnik et al.
(1994). My contribution rests on the approximation of the expected maximum
difference between two empirical Losses (EMDBTEL). In fact, I use a
cross-validated form of the error to compute the EMDBTEL, and I make the bound
on the EMDBTEL tighter by minimizing a constant in of its right upper bound. I
also derive two bounds for the true unknown risk using the additive (ERM1) and
the multiplicative (ERM2) Chernoff bounds. These bounds depend on the estimated
VCD and the empirical risk. These bounds can be used to perform model selection
and to declare with high probability, the chosen model will perform better
without making strong assumptions about the data generating process (DG).
  I measure the accuracy of my technique on simulated datasets and also on
three real datasets. The model selection provided by VCD was always as good as
if not better than the other methods under reasonable conditions.
</summary>
    <author>
      <name>Merlin Mpoudeu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">131 pages, 34 figures, and 44 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06671v1</id>
    <updated>2018-08-20T19:53:19Z</updated>
    <published>2018-08-20T19:53:19Z</published>
    <title>Adversarial Sampling for Active Learning</title>
    <summary>  This paper describes ASAL a new active learning strategy that uses
uncertainty sampling, adversarial sample generation and sample matching.
Compared to traditional pool-based uncertainty sampling strategies, ASAL
synthesizes uncertain samples instead of performing an exhaustive search in
each active learning cycle. Then, the sample matching efficiently selects
similar samples from the pool. We present a comprehensive set of experiments on
MNIST and CIFAR-10 and show that ASAL outperforms similar methods and clearly
exceeds passive learning. To the best of our knowledge this is the first
pool-based adversarial active learning technique and the first that is applied
for multi-label classification using deep convolutional classifiers.
</summary>
    <author>
      <name>Christoph Mayer</name>
    </author>
    <author>
      <name>Radu Timofte</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06664v1</id>
    <updated>2018-08-20T19:29:57Z</updated>
    <published>2018-08-20T19:29:57Z</published>
    <title>Out-of-Distribution Detection using Multiple Semantic Label
  Representations</title>
    <summary>  Deep Neural Networks are powerful models that attained remarkable results on
a variety of tasks. These models are shown to be extremely efficient when
training and test data are drawn from the same distribution. However, it is not
clear how a network will act when it is fed with an out-of-distribution
example. In this work, we consider the problem of out-of-distribution detection
in neural networks. We propose to use multiple semantic dense representations
instead of sparse representation as the target label. Specifically, we propose
to use several word representations obtained from different corpora or
architectures as target labels. We evaluated the proposed model on computer
vision, and speech commands detection tasks and compared it to previous
methods. Results suggest that our method compares favorably with previous work.
Besides, we present the efficiency of our approach for detecting wrongly
classified and adversarial examples.
</summary>
    <author>
      <name>Gabi Shalev</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <author>
      <name>Joseph Keshet</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06350v2</id>
    <updated>2018-08-20T19:26:49Z</updated>
    <published>2018-05-16T14:43:33Z</published>
    <title>Approximating the Void: Learning Stochastic Channel Models from
  Observation with Variational Generative Adversarial Networks</title>
    <summary>  Channel modeling is a critical topic when considering designing, learning, or
evaluating the performance of any communications system. Most prior work in
designing or learning new modulation schemes has focused on using highly
simplified analytic channel models such as additive white Gaussian noise
(AWGN), Rayleigh fading channels or similar. Recently, we proposed the usage of
a generative adversarial networks (GANs) to jointly approximate a wireless
channel response model (e.g. from real black box measurements) and optimize for
an efficient modulation scheme over it using machine learning. This approach
worked to some degree, but was unable to produce accurate probability
distribution functions (PDFs) representing the stochastic channel response. In
this paper, we focus specifically on the problem of accurately learning a
channel PDF using a variational GAN, introducing an architecture and loss
function which can accurately capture stochastic behavior. We illustrate where
our prior method failed and share results capturing the performance of such as
system over a range of realistic channel distributions.
</summary>
    <author>
      <name>Timothy J. O'Shea</name>
    </author>
    <author>
      <name>Tamoghna Roy</name>
    </author>
    <author>
      <name>Nathan West</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06350v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06350v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06651v1</id>
    <updated>2018-08-20T18:49:32Z</updated>
    <published>2018-08-20T18:49:32Z</published>
    <title>Privacy Amplification by Iteration</title>
    <summary>  Many commonly used learning algorithms work by iteratively updating an
intermediate solution using one or a few data points in each iteration.
Analysis of differential privacy for such algorithms often involves ensuring
privacy of each step and then reasoning about the cumulative privacy cost of
the algorithm. This is enabled by composition theorems for differential privacy
that allow releasing of all the intermediate results. In this work, we
demonstrate that for contractive iterations, not releasing the intermediate
results strongly amplifies the privacy guarantees. We describe several
applications of this new analysis technique to solving convex optimization
problems via noisy stochastic gradient descent. For example, we demonstrate
that a relatively small number of non-private data points from the same
distribution can be used to close the gap between private and non-private
convex optimization. In addition, we demonstrate that we can achieve guarantees
similar to those obtainable using the privacy-amplification-by-sampling
technique in several natural settings where that technique cannot be applied.
</summary>
    <author>
      <name>Vitaly Feldman</name>
    </author>
    <author>
      <name>Ilya Mironov</name>
    </author>
    <author>
      <name>Kunal Talwar</name>
    </author>
    <author>
      <name>Abhradeep Thakurta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract appears in Foundations of Computer Science (FOCS)
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06645v1</id>
    <updated>2018-08-20T18:39:04Z</updated>
    <published>2018-08-20T18:39:04Z</published>
    <title>Stochastic Combinatorial Ensembles for Defending Against Adversarial
  Examples</title>
    <summary>  Many deep learning algorithms can be easily fooled with simple adversarial
examples. To address the limitations of existing defenses, we devised a
probabilistic framework that can generate an exponentially large ensemble of
models from a single model with just a linear cost. This framework takes
advantage of neural network depth and stochastically decides whether or not to
insert noise removal operators such as VAEs between layers. We show empirically
the important role that model gradients have when it comes to determining
transferability of adversarial examples, and take advantage of this result to
demonstrate that it is possible to train models with limited adversarial attack
transferability. Additionally, we propose a detection method based on metric
learning in order to detect adversarial examples that have no hope of being
cleaned of maliciously engineered noise.
</summary>
    <author>
      <name>George A. Adam</name>
    </author>
    <author>
      <name>Petr Smirnov</name>
    </author>
    <author>
      <name>Anna Goldenberg</name>
    </author>
    <author>
      <name>David Duvenaud</name>
    </author>
    <author>
      <name>Benjamin Haibe-Kains</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06581v1</id>
    <updated>2018-08-20T17:41:39Z</updated>
    <published>2018-08-20T17:41:39Z</published>
    <title>The Deconfounded Recommender: A Causal Inference Approach to
  Recommendation</title>
    <summary>  The goal of a recommender system is to show its users items that they will
like. In forming its prediction, the recommender system tries to answer: "what
would the rating be if we 'forced' the user to watch the movie?" This is a
question about an intervention in the world, a causal question, and so
traditional recommender systems are doing causal inference from observational
data. This paper develops a causal inference approach to recommendation.
Traditional recommenders are likely biased by unobserved confounders, variables
that affect both the "treatment assignments" (which movies the users watch) and
the "outcomes" (how they rate them). We develop the deconfounded recommender, a
strategy to leverage classical recommendation models for causal predictions.
The deconfounded recommender uses Poisson factorization on which movies users
watched to infer latent confounders in the data; it then augments common
recommendation models to correct for potential confounding bias. The
deconfounded recommender improves recommendation and it enjoys stable
performance against interventions on test sets.
</summary>
    <author>
      <name>Yixin Wang</name>
    </author>
    <author>
      <name>Dawen Liang</name>
    </author>
    <author>
      <name>Laurent Charlin</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06573v1</id>
    <updated>2018-08-20T17:18:25Z</updated>
    <published>2018-08-20T17:18:25Z</published>
    <title>A Semi-Supervised and Inductive Embedding Model for Churn Prediction of
  Large-Scale Mobile Games</title>
    <summary>  Mobile gaming has emerged as a promising market with billion-dollar revenues.
A variety of mobile game platforms and services have been developed around the
world. One critical challenge for these platforms and services is to understand
user churn behavior in mobile games. Successful churn prediction will benefit
many stakeholders such as game developers and platform operators. In this
paper, we present the first large-scale churn prediction solution for mobile
games. In view of the common limitations of the state-of-the-art methods built
upon traditional machine learning models, we devise a novel semi-supervised and
inductive embedding model that jointly learns the prediction function and the
embedding function for user-app relationships. We model these two functions by
deep neural networks with a unique edge embedding technique that is able to
capture both contextual information and relationship dynamics. We also design a
novel attributed random walk technique that takes into consideration both
topological adjacency and attributes similarities. To evaluate the performance
of our solution, we collect the real-world data from a commercial mobile gaming
platform that includes tens of thousands of games and hundreds of millions of
user-app interactions. The experimental results with this data demonstrate the
superiority of our proposed model against existing state-of-the-art methods.
</summary>
    <author>
      <name>Xi Liu</name>
    </author>
    <author>
      <name>Muhe Xie</name>
    </author>
    <author>
      <name>Xidao Wen</name>
    </author>
    <author>
      <name>Rui Chen</name>
    </author>
    <author>
      <name>Yong Ge</name>
    </author>
    <author>
      <name>Nick Duffield</name>
    </author>
    <author>
      <name>Na Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in ICDM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01204v2</id>
    <updated>2018-08-20T16:43:28Z</updated>
    <published>2018-08-03T14:28:12Z</published>
    <title>Learning Overparameterized Neural Networks via Stochastic Gradient
  Descent on Structured Data</title>
    <summary>  Neural networks have many successful applications, while much less
theoretical understanding has been gained. Towards bridging this gap, we study
the problem of learning a two-layer overparameterized ReLU neural network for
multi-class classification via stochastic gradient descent (SGD) from random
initialization. In the overparameterized setting, when the data comes from
mixtures of well-separated distributions, we prove that SGD learns a network
with a small generalization error, albeit the network has enough capacity to
fit arbitrary labels. Furthermore, the analysis provides interesting insights
into several aspects of learning neural networks and can be verified based on
empirical studies on synthetic data and on the MNIST dataset.
</summary>
    <author>
      <name>Yuanzhi Li</name>
    </author>
    <author>
      <name>Yingyu Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06560v1</id>
    <updated>2018-08-20T16:40:35Z</updated>
    <published>2018-08-20T16:40:35Z</published>
    <title>Multi-View Graph Embedding Using Randomized Shortest Paths</title>
    <summary>  Real-world data sets often provide multiple types of information about the
same set of entities. This data is well represented by multi-view graphs, which
consist of several distinct sets of edges over the same nodes. These can be
used to analyze how entities interact from different viewpoints. Combining
multiple views improves the quality of inferences drawn from the underlying
data, which has increased interest in developing efficient multi-view graph
embedding methods. We propose an algorithm, C-RSP, that generates a common (C)
embedding of a multi-view graph using Randomized Shortest Paths (RSP). This
algorithm generates a dissimilarity measure between nodes by minimizing the
expected cost of a random walk between any two nodes across all views of a
multi-view graph, in doing so encoding both the local and global structure of
the graph. We test C-RSP on both real and synthetic data and show that it
outperforms benchmark algorithms at embedding and clustering tasks while
remaining computationally efficient.
</summary>
    <author>
      <name>Anuththari Gamage</name>
    </author>
    <author>
      <name>Brian Rappaport</name>
    </author>
    <author>
      <name>Shuchin Aeron</name>
    </author>
    <author>
      <name>Xiaozhe Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06556v1</id>
    <updated>2018-08-20T16:31:30Z</updated>
    <published>2018-08-20T16:31:30Z</published>
    <title>Triangle Lasso for Simultaneous Clustering and Optimization in Graph
  Datasets</title>
    <summary>  Recently, network lasso has drawn many attentions due to its remarkable
performance on simultaneous clustering and optimization. However, it usually
suffers from the imperfect data (noise, missing values etc), and yields
sub-optimal solutions. The reason is that it finds the similar instances
according to their features directly, which is usually impacted by the
imperfect data, and thus returns sub-optimal results. In this paper, we propose
triangle lasso to avoid its disadvantage. Triangle lasso finds the similar
instances according to their neighbours. If two instances have many common
neighbours, they tend to become similar. Although some instances are profiled
by the imperfect data, it is still able to find the similar counterparts.
Furthermore, we develop an efficient algorithm based on Alternating Direction
Method of Multipliers (ADMM) to obtain a moderately accurate solution. In
addition, we present a dual method to obtain the accurate solution with the low
additional time consumption. We demonstrate through extensive numerical
experiments that triangle lasso is robust to the imperfect data. It usually
yields a better performance than the state-of-the-art method when performing
data analysis tasks in practical scenarios.
</summary>
    <author>
      <name>Yawei Zhao</name>
    </author>
    <author>
      <name>Kai Xu</name>
    </author>
    <author>
      <name>Xinwang Liu</name>
    </author>
    <author>
      <name>En Zhu</name>
    </author>
    <author>
      <name>Xinzhong Zhu</name>
    </author>
    <author>
      <name>Jianping Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Knowledge and Data Engineering, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06942v1</id>
    <updated>2018-08-20T15:20:46Z</updated>
    <published>2018-08-20T15:20:46Z</published>
    <title>PACO: Signal Restoration via PAtch COnsensus</title>
    <summary>  Many signal processing algorithms operate by breaking the target signal into
possibly overlapping segments (typically called windows or patches), processing
them separately, and then stitching them back into place to produce a unified
output. In most cases where pach overlapping occurs, the final value of those
samples that are estimated by more than one patch is resolved by averaging
those estimates; this includes many recent image processing algorithms. In
other cases, typically frequency-based restoration methods, the average is
implicitly weighted by some window function such as Hanning, Blackman, etc.
which is applied prior to the Fourier/DCT transform in order to avoid Gibbs
oscillations in the processed patches. Such averaging may incidentally help in
covering up artifacts in the restoration process, but more often will simply
degrade the overall result, posing an upper limit to the size of the patches
that can be used. In order to avoid such drawbacks, we propose a new
methodology where the different estimates of any given sample are forced to be
identical. We show that, together, these consensus constraints constitute a
non-empty convex feasible set, provide a general formulation of the resulting
constrained optimization problem which can be applied to a wide variety of
signal restoration tasks, and propose an efficient algorithm for finding the
corresponding solutions. Finally, we describe in detail the application of the
proposed methodology to three different signal processing problems, in some
cases surpassing the state of the art by a significant margin.
</summary>
    <author>
      <name>Ignacio Francisco Ramírez Paulino</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06508v1</id>
    <updated>2018-08-20T15:15:32Z</updated>
    <published>2018-08-20T15:15:32Z</published>
    <title>Life-Long Disentangled Representation Learning with Cross-Domain Latent
  Homologies</title>
    <summary>  Intelligent behaviour in the real-world requires the ability to acquire new
knowledge from an ongoing sequence of experiences while preserving and reusing
past knowledge. We propose a novel algorithm for unsupervised representation
learning from piece-wise stationary visual data: Variational Autoencoder with
Shared Embeddings (VASE). Based on the Minimum Description Length principle,
VASE automatically detects shifts in the data distribution and allocates spare
representational capacity to new knowledge, while simultaneously protecting
previously learnt representations from catastrophic forgetting. Our approach
encourages the learnt representations to be disentangled, which imparts a
number of desirable properties: VASE can deal sensibly with ambiguous inputs,
it can enhance its own representations through imagination-based exploration,
and most importantly, it exhibits semantically meaningful sharing of latents
between different datasets. Compared to baselines with entangled
representations, our approach is able to reason beyond surface-level statistics
and perform semantically meaningful cross-domain inference.
</summary>
    <author>
      <name>Alessandro Achille</name>
    </author>
    <author>
      <name>Tom Eccles</name>
    </author>
    <author>
      <name>Loic Matthey</name>
    </author>
    <author>
      <name>Christopher P. Burgess</name>
    </author>
    <author>
      <name>Nick Watters</name>
    </author>
    <author>
      <name>Alexander Lerchner</name>
    </author>
    <author>
      <name>Irina Higgins</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06452v1</id>
    <updated>2018-08-20T13:54:03Z</updated>
    <published>2018-08-20T13:54:03Z</published>
    <title>Reproducible evaluation of classification methods in Alzheimer's
  disease: framework and application to MRI and PET data</title>
    <summary>  A large number of papers have introduced novel machine learning and feature
extraction methods for automatic classification of AD. However, they are
difficult to reproduce because key components of the validation are often not
readily available. These components include selected participants and input
data, image preprocessing and cross-validation procedures. The performance of
the different approaches is also difficult to compare objectively. In
particular, it is often difficult to assess which part of the method provides a
real improvement, if any. We propose a framework for reproducible and objective
classification experiments in AD using three publicly available datasets (ADNI,
AIBL and OASIS). The framework comprises: i) automatic conversion of the three
datasets into BIDS format, ii) a modular set of preprocessing pipelines,
feature extraction and classification methods, together with an evaluation
framework, that provide a baseline for benchmarking the different components.
We demonstrate the use of the framework for a large-scale evaluation on 1960
participants using T1 MRI and FDG PET data. In this evaluation, we assess the
influence of different modalities, preprocessing, feature types, classifiers,
training set sizes and datasets. Performances were in line with the
state-of-the-art. FDG PET outperformed T1 MRI for all classification tasks. No
difference in performance was found for the use of different atlases, image
smoothing, partial volume correction of FDG PET images, or feature type. Linear
SVM and L2-logistic regression resulted in similar performance and both
outperformed random forests. The classification performance increased along
with the number of subjects used for training. Classifiers trained on ADNI
generalized well to AIBL and OASIS. All the code of the framework and the
experiments is publicly available at:
https://gitlab.icm-institute.org/aramislab/AD-ML.
</summary>
    <author>
      <name>Jorge Samper-González</name>
    </author>
    <author>
      <name>Ninon Burgos</name>
    </author>
    <author>
      <name>Simona Bottani</name>
    </author>
    <author>
      <name>Sabrina Fontanella</name>
    </author>
    <author>
      <name>Pascal Lu</name>
    </author>
    <author>
      <name>Arnaud Marcoux</name>
    </author>
    <author>
      <name>Alexandre Routier</name>
    </author>
    <author>
      <name>Jérémy Guillon</name>
    </author>
    <author>
      <name>Michael Bacci</name>
    </author>
    <author>
      <name>Junhao Wen</name>
    </author>
    <author>
      <name>Anne Bertrand</name>
    </author>
    <author>
      <name>Hugo Bertin</name>
    </author>
    <author>
      <name>Marie-Odile Habert</name>
    </author>
    <author>
      <name>Stanley Durrleman</name>
    </author>
    <author>
      <name>Theodoros Evgeniou</name>
    </author>
    <author>
      <name>Olivier Colliot</name>
    </author>
    <author>
      <name>for the Alzheimer's Disease Neuroimaging Initiative</name>
    </author>
    <author>
      <name>the Australian Imaging Biomarkers</name>
    </author>
    <author>
      <name>Lifestyle flagship study of ageing</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neuroimage.2018.08.042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neuroimage.2018.08.042" rel="related"/>
    <link href="http://arxiv.org/abs/1808.06452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09477v3</id>
    <updated>2018-08-20T13:50:41Z</updated>
    <published>2017-08-30T21:19:30Z</published>
    <title>A Compressive Sensing Approach to Community Detection with Applications</title>
    <summary>  The community detection problem for graphs asks one to partition the n
vertices V of a graph G into k communities, or clusters, such that there are
many intracluster edges and few intercluster edges. Of course this is
equivalent to finding a permutation matrix P such that, if A denotes the
adjacency matrix of G, then PAP^T is approximately block diagonal. As there are
k^n possible partitions of n vertices into k subsets, directly determining the
optimal clustering is clearly infeasible. Instead one seeks to solve a more
tractable approximation to the clustering problem. In this paper we reformulate
the community detection problem via sparse solution of a linear system
associated with the Laplacian of a graph G and then develop a two-stage
approach based on a thresholding technique and a compressive sensing algorithm
to find a sparse solution which corresponds to the community containing a
vertex of interest in G. Crucially, our approach results in an algorithm which
is able to find a single cluster of size n_0 in O(nlog(n)n_0) operations and
all k clusters in fewer than O(n^2ln(n)) operations. This is a marked
improvement over the classic spectral clustering algorithm, which is unable to
find a single cluster at a time and takes approximately O(n^3) operations to
find all k clusters. Moreover, we are able to provide robust guarantees of
success for the case where G is drawn at random from the Stochastic Block
Model, a popular model for graphs with clusters. Extensive numerical results
are also provided, showing the efficacy of our algorithm on both synthetic and
real-world data sets.
</summary>
    <author>
      <name>Ming-Jun Lai</name>
    </author>
    <author>
      <name>Daniel Mckenzie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 10 figures Version 2, disabled 'showkeys' package. Note
  that there is an error in the proof of Lemma 5.1. A correct version of this
  lemma, as well as a greatly improved version of the central algorithm of this
  paper, is available at: arXiv:1808.05780</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.09477v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09477v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06444v1</id>
    <updated>2018-08-20T13:36:07Z</updated>
    <published>2018-08-20T13:36:07Z</published>
    <title>Synthetic Patient Generation: A Deep Learning Approach Using Variational
  Autoencoders</title>
    <summary>  Artificial Intelligence in healthcare is a new and exciting frontier and the
possibilities are endless. With deep learning approaches beating human
performances in many areas, the logical next step is to attempt their
application in the health space. For these and other Machine Learning
approaches to produce good results and have their potential realized, the need
for, and importance of, large amounts of accurate data is second to none. This
is a challenge faced by many industries and more so in the healthcare space. We
present an approach of using Variational Autoencoders (VAE's) as an approach to
generating more data for training deeper networks, as well as uncovering
underlying patterns in diagnoses and the patients suffering from them. By
training a VAE, on available data, it was able to learn the latent distribution
of the patient features given the diagnosis. It is then possible, after
training, to sample from the learnt latent distribution to generate new
accurate patient records given the patient diagnosis.
</summary>
    <author>
      <name>Ally Salim Jr</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T00" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01085v5</id>
    <updated>2018-08-20T13:05:59Z</updated>
    <published>2018-07-03T11:19:17Z</published>
    <title>One-Class Kernel Spectral Regression for Outlier Detection</title>
    <summary>  The paper introduces a new efficient nonlinear one-class classifier
formulated as the Rayleigh quotient criterion optimisation. The method,
operating in a reproducing kernel Hilbert subspace, minimises the scatter of
target distribution along an optimal projection direction while at the same
time keeping projections of positive observations distant from the mean of the
negative class. We provide a graph embedding view of the problem which can then
be solved efficiently using the spectral regression approach. In this sense,
unlike previous similar methods which often require costly eigen-computations
of dense matrices, the proposed approach casts the problem under consideration
into a regression framework which is computationally more efficient. In
particular, it is shown that the dominant complexity of the proposed method is
the complexity of computing the kernel matrix. Additional appealing
characteristics of the proposed one-class classifier are: 1-the ability to be
trained in an incremental fashion (allowing for application in streaming data
scenarios while also reducing the computational complexity in a non-streaming
operation mode); 2-being unsupervised, but providing the option for refining
the solution using negative training examples, when available; Last but not
least, 3-the use of the kernel trick which facilitates a nonlinear mapping of
the data into a high-dimensional feature space to seek better solutions.
</summary>
    <author>
      <name>Shervin Rahimzadeh Arashloo</name>
    </author>
    <author>
      <name>Josef Kittler</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01085v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01085v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03513v3</id>
    <updated>2018-08-20T12:29:17Z</updated>
    <published>2018-07-10T07:58:43Z</published>
    <title>Automatic trajectory recognition in Active Target Time Projection
  Chambers data by means of hierarchical clustering</title>
    <summary>  The automatic reconstruction of three-dimensional particle tracks from Active
Target Time Projection Chambers data can be a challenging task, especially in
the presence of noise. In this article, we propose a non-parametric algorithm
that is based on the idea of clustering point triplets instead of the original
points. We define an appropriate distance measure on point triplets and then
apply a single-link hierarchical clustering on the triplets. Compared to
parametric approaches like RANSAC or the Hough transform, the new algorithm has
the advantage of potentially finding trajectories even of shapes that are not
known beforehand. This feature is particularly important in low-energy nuclear
physics experiments with Active Targets operating inside a magnetic field. The
algorithm has been validated using data from experiments performed with the
Active Target Time Projection Chamber developed at the National Superconducting
Cyclotron Laboratory (NSCL).The results demonstrate the capability of the
algorithm to identify and isolate particle tracks that describe non-analytical
trajectories. For curved tracks, the vertex detection recall was 86\% and the
precision 94\%. For straight tracks, the vertex detection recall was 96\% and
the precision 98\%. In the case of a test set containing only straight linear
tracks, the algorithm performed better than an iterative Hough transform.
</summary>
    <author>
      <name>Christoph Dalitz</name>
    </author>
    <author>
      <name>Yassid Ayyad</name>
    </author>
    <author>
      <name>Jens Wilberg</name>
    </author>
    <author>
      <name>Lukas Aymans</name>
    </author>
    <author>
      <name>Daniel Bazin</name>
    </author>
    <author>
      <name>Wolfgang Mittig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; fixed up references</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03513v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03513v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01230v2</id>
    <updated>2018-08-20T11:41:49Z</updated>
    <published>2016-11-04T00:29:02Z</published>
    <title>Bayesian Optical Flow with Uncertainty Quantification</title>
    <summary>  Optical flow refers to the visual motion observed between two consecutive
images. Since the degree of freedom is typically much larger than the
constraints imposed by the image observations, the straightforward formulation
of optical flow as an inverse problem is ill-posed. Standard approaches to
determine optical flow rely on formulating and solving an optimization problem
that contains both a data fidelity term and a regularization term, the latter
effectively resolves the otherwise ill-posedness of the inverse problem. In
this work, we depart from the deterministic formalism, and instead treat
optical flow as a statistical inverse problem. We discuss how a classical
optical flow solution can be interpreted as a point estimate in this more
general framework. The statistical approach, whose "solution" is a distribution
of flow fields, which we refer to as Bayesian optical flow, allows not only
"point" estimates (e.g., the computation of average flow field), but also
statistical estimates (e.g., quantification of uncertainty) that are beyond any
standard method for optical flow. As application, we benchmark Bayesian optical
flow together with uncertainty quantification using several types of prescribed
ground-truth flow fields and images.
</summary>
    <author>
      <name>Jie Sun</name>
    </author>
    <author>
      <name>Fernando J. Quevedo</name>
    </author>
    <author>
      <name>Erik Bollt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1361-6420/aad7cc</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1361-6420/aad7cc" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published 20 August 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Inverse Problems 34, 105008 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.01230v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01230v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06394v1</id>
    <updated>2018-08-20T11:38:46Z</updated>
    <published>2018-08-20T11:38:46Z</published>
    <title>Faster Support Vector Machines</title>
    <summary>  The time complexity of support vector machines (SVMs) prohibits training on
huge data sets with millions of samples. Recently, multilevel approaches to
train SVMs have been developed to allow for time efficient training on huge
data sets. While regular SVMs perform the entire training in one - time
consuming - optimization step, multilevel SVMs first build a hierarchy of
problems decreasing in size that resemble the original problem and then train
an SVM model for each hierarchy level benefiting from the solved models of
previous levels. We present a faster multilevel support vector machine that
uses a label propagation algorithm to construct the problem hierarchy.
Extensive experiments show that our new algorithm achieves speed-ups up to two
orders of magnitude while having similar or better classification quality over
state-of-the-art algorithms.
</summary>
    <author>
      <name>Sebastian Schlag</name>
    </author>
    <author>
      <name>Matthias Schmitt</name>
    </author>
    <author>
      <name>Christian Schulz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11156v3</id>
    <updated>2018-08-20T10:32:55Z</updated>
    <published>2018-07-30T03:14:50Z</published>
    <title>Transformationally Identical and Invariant Convolutional Neural Networks
  by Combining Symmetric Operations or Input Vectors</title>
    <summary>  Transformationally invariant processors constructed by transformed input
vectors or operators have been suggested and applied to many applications. In
this study, transformationally identical processing based on combining results
of all sub-processes with corresponding transformations at one of the
processing steps or at the beginning step were found to be equivalent for a
given condition. This property can be applied to most convolutional neural
network (CNN) systems. Specifically, a transformationally identical CNN can be
constructed by arranging internally symmetric operations in parallel with the
same transformation family that includes a flatten layer with weights sharing
among their corresponding transformation elements. Other transformationally
identical CNNs can be constructed by averaging transformed input vectors of the
family at the input layer followed by an ordinary CNN process or by a set of
symmetric operations. Interestingly, we found that both types of
transformationally identical CNN systems are mathematically equivalent by
either applying an averaging operation to corresponding elements of all
sub-channels before the activation function or without using a non-linear
activation function.
</summary>
    <author>
      <name>ShihChung B. Lo</name>
    </author>
    <author>
      <name>Matthew T. Freedman</name>
    </author>
    <author>
      <name>Seong K. Mun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11156v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11156v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06356v1</id>
    <updated>2018-08-20T09:28:24Z</updated>
    <published>2018-08-20T09:28:24Z</published>
    <title>Causal Discovery by Telling Apart Parents and Children</title>
    <summary>  We consider the problem of inferring the directed, causal graph from
observational data, assuming no hidden confounders. We take an information
theoretic approach, and make three main contributions.
  First, we show how through algorithmic information theory we can obtain SCI,
a highly robust, effective and computationally efficient test for conditional
independence---and show it outperforms the state of the art when applied in
constraint-based inference methods such as stable PC.
  Second, building upon on SCI, we show how to tell apart the parents and
children of a given node based on the algorithmic Markov condition. We give the
Climb algorithm to efficiently discover the directed, causal Markov
blanket---and show it is at least as accurate as inferring the global network,
while being much more efficient.
  Last, but not least, we detail how we can use the Climb score to direct those
edges that state of the art causal discovery algorithms based on PC or GES
leave undirected---and show this improves their precision, recall and F1 scores
by up to 20%.
</summary>
    <author>
      <name>Alexander Marx</name>
    </author>
    <author>
      <name>Jilles Vreeken</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06940v1</id>
    <updated>2018-08-20T09:25:30Z</updated>
    <published>2018-08-20T09:25:30Z</published>
    <title>End to End Vehicle Lateral Control Using a Single Fisheye Camera</title>
    <summary>  Convolutional neural networks are commonly used to control the steering angle
for autonomous cars. Most of the time, multiple long range cameras are used to
generate lateral failure cases. In this paper we present a novel model to
generate this data and label augmentation using only one short range fisheye
camera. We present our simulator and how it can be used as a consistent metric
for lateral end-to-end control evaluation. Experiments are conducted on a
custom dataset corresponding to more than 10000 km and 200 hours of open road
driving. Finally we evaluate this model on real world driving scenarios, open
road and a custom test track with challenging obstacle avoidance and sharp
turns. In our simulator based on real-world videos, the final model was capable
of more than 99% autonomy on urban road
</summary>
    <author>
      <name>Marin Toromanoff</name>
    </author>
    <author>
      <name>Emilie Wirbel</name>
    </author>
    <author>
      <name>Frédéric Wilhelm</name>
    </author>
    <author>
      <name>Camilo Vejarano</name>
    </author>
    <author>
      <name>Xavier Perrotton</name>
    </author>
    <author>
      <name>Fabien Moutarde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages paper accepted at IROS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06347v1</id>
    <updated>2018-08-20T08:46:33Z</updated>
    <published>2018-08-20T08:46:33Z</published>
    <title>A Distribution Similarity Based Regularizer for Learning Bayesian
  Networks</title>
    <summary>  Probabilistic graphical models compactly represent joint distributions by
decomposing them into factors over subsets of random variables. In Bayesian
networks, the factors are conditional probability distributions. For many
problems, common information exists among those factors. Adding similarity
restrictions can be viewed as imposing prior knowledge for model
regularization. With proper restrictions, learned models usually generalize
better. In this work, we study methods that exploit such high-level
similarities to regularize the learning process and apply them to the task of
modeling the wave propagation in inhomogeneous media. We propose a novel
distribution-based penalization approach that encourages similar conditional
probability distribution rather than force the parameters to be similar
explicitly. We show in experiment that our proposed algorithm solves the
modeling wave propagation problem, which other baseline methods are not able to
solve.
</summary>
    <author>
      <name>Weirui Kong</name>
    </author>
    <author>
      <name>Wenyi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06329v1</id>
    <updated>2018-08-20T07:35:39Z</updated>
    <published>2018-08-20T07:35:39Z</published>
    <title>The Mismatch Principle: Statistical Learning Under Large Model
  Uncertainties</title>
    <summary>  We study the learning capacity of empirical risk minimization with regard to
the squared loss and a convex hypothesis class consisting of linear functions.
While these types of estimators were originally designed for noisy linear
regression problems, it recently turned out that they are in fact capable of
handling considerably more complicated situations, involving highly non-linear
distortions. This work intends to provide a comprehensive explanation of this
somewhat astonishing phenomenon. At the heart of our analysis stands the
mismatch principle, which is a simple, yet generic recipe to establish
theoretical error bounds for empirical risk minimization. The scope of our
results is fairly general, permitting arbitrary sub-Gaussian input-output
pairs, possibly with strongly correlated feature variables. Noteworthy, the
mismatch principle also generalizes to a certain extent the classical
orthogonality principle for ordinary least squares. This adaption allows us to
investigate problem setups of recent interest, most importantly,
high-dimensional parameter regimes and non-linear observation processes. In
particular, our theoretical framework is applied to various scenarios of
practical relevance, such as single-index models, variable selection, and
strongly correlated designs. We thereby demonstrate the key purpose of the
mismatch principle, that is, learning (semi-)parametric output rules under
large model uncertainties and misspecifications.
</summary>
    <author>
      <name>Martin Genzel</name>
    </author>
    <author>
      <name>Gitta Kutyniok</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06324v1</id>
    <updated>2018-08-20T06:41:01Z</updated>
    <published>2018-08-20T06:41:01Z</published>
    <title>PAC-learning is Undecidable</title>
    <summary>  The problem of attempting to learn the mapping between data and labels is the
crux of any machine learning task. It is, therefore, of interest to the machine
learning community on practical as well as theoretical counts to consider the
existence of a test or criterion for deciding the feasibility of attempting to
learn. We investigate the existence of such a criterion in the setting of
PAC-learning, basing the feasibility solely on whether the mapping to be learnt
lends itself to approximation by a given class of hypothesis functions. We show
that no such criterion exists, exposing a fundamental limitation in the
decidability of learning. In other words, we prove that testing for
PAC-learnability is undecidable in the Turing sense. We also briefly discuss
some of the probable implications of this result to the current practice of
machine learning.
</summary>
    <author>
      <name>Sairaam Venkatraman</name>
    </author>
    <author>
      <name>S Balasubramanian</name>
    </author>
    <author>
      <name>R Raghunatha Sarma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06288v1</id>
    <updated>2018-08-20T02:36:19Z</updated>
    <published>2018-08-20T02:36:19Z</published>
    <title>Multimodal speech synthesis architecture for unsupervised speaker
  adaptation</title>
    <summary>  This paper proposes a new architecture for speaker adaptation of
multi-speaker neural-network speech synthesis systems, in which an unseen
speaker's voice can be built using a relatively small amount of speech data
without transcriptions. This is sometimes called "unsupervised speaker
adaptation". More specifically, we concatenate the layers to the audio inputs
when performing unsupervised speaker adaptation while we concatenate them to
the text inputs when synthesizing speech from text. Two new training schemes
for the new architecture are also proposed in this paper. These training
schemes are not limited to speech synthesis, other applications are suggested.
Experimental results show that the proposed model not only enables adaptation
to unseen speakers using untranscribed speech but it also improves the
performance of multi-speaker modeling and speaker adaptation using transcribed
audio files.
</summary>
    <author>
      <name>Hieu-Thi Luong</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Interspeech 2018, India</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03698v2</id>
    <updated>2018-08-20T00:48:14Z</updated>
    <published>2018-08-10T20:37:52Z</published>
    <title>BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear
  Regressions</title>
    <summary>  In this paper we introduce a new machine learning (ML) model for nonlinear
regression called Boosting Smooth Transition Regression Trees (BooST). The main
advantage of the BooST model is that it estimates the derivatives (partial
effects) of very general nonlinear models, providing more interpretation about
the mapping between the covariates and the dependent variable than other tree
based models, such as Random Forests. We provide some asymptotic theory that
shows consistency of the partial derivative estimates and we present some
examples on both simulated and real data.
</summary>
    <author>
      <name>Yuri Fonseca</name>
    </author>
    <author>
      <name>Marcelo Medeiros</name>
    </author>
    <author>
      <name>Gabriel Vasconcelos</name>
    </author>
    <author>
      <name>Alvaro Veiga</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03698v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03698v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06275v1</id>
    <updated>2018-08-20T00:24:41Z</updated>
    <published>2018-08-20T00:24:41Z</published>
    <title>Applying Machine Learning To Maize Traits Prediction</title>
    <summary>  Heterosis is the improved or increased function of any biological quality in
a hybrid offspring. We have studied yet the largest maize SNP dataset for
traits prediction. We develop linear and non-linear models which consider
relationships between different hybrids as well as other effect. Specially
designed model proved to be efficient and robust in prediction maize's traits.
</summary>
    <author>
      <name>Binbin Shi</name>
    </author>
    <author>
      <name>Xupeng Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00309v2</id>
    <updated>2018-08-19T20:21:33Z</updated>
    <published>2017-07-02T15:36:07Z</published>
    <title>Variance Regularizing Adversarial Learning</title>
    <summary>  We introduce a novel approach for training adversarial models by replacing
the discriminator score with a bi-modal Gaussian distribution over the
real/fake indicator variables. In order to do this, we train the Gaussian
classifier to match the target bi-modal distribution implicitly through
meta-adversarial training. We hypothesize that this approach ensures a non-zero
gradient to the generator, even in the limit of a perfect classifier. We test
our method against standard benchmark image datasets as well as show the
classifier output distribution is smooth and has overlap between the real and
fake modes.
</summary>
    <author>
      <name>Karan Grewal</name>
    </author>
    <author>
      <name>R Devon Hjelm</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Method is out of date and some results are incorrect</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09001v3</id>
    <updated>2018-08-19T17:00:10Z</updated>
    <published>2018-05-23T08:08:23Z</published>
    <title>One-to-one Mapping between Stimulus and Neural State: Memory and
  Classification</title>
    <summary>  Synaptic strength can be seen as probability to propagate impulse, and
function could exist from propagation activity to synaptic strength. If the
function satisfies constraints such as continuity and monotonicity, neural
network under external stimulus would always go to fixed point, and there could
be one-one-one mapping between external stimulus and synaptic strength at fixed
point. In other words, neural network "memorizes" external stimulus in its
synapses. A biological classifier is proposed to utilize this mapping.
</summary>
    <author>
      <name>Sizhong Lan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09001v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09001v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06656v2</id>
    <updated>2018-08-19T16:33:36Z</updated>
    <published>2017-08-22T14:49:07Z</published>
    <title>Causally Regularized Learning with Agnostic Data Selection Bias</title>
    <summary>  Most of previous machine learning algorithms are proposed based on the i.i.d.
hypothesis. However, this ideal assumption is often violated in real
applications, where selection bias may arise between training and testing
process. Moreover, in many scenarios, the testing data is not even available
during the training process, which makes the traditional methods like transfer
learning infeasible due to their need on prior of test distribution. Therefore,
how to address the agnostic selection bias for robust model learning is of
paramount importance for both academic research and real applications. In this
paper, under the assumption that causal relationships among variables are
robust across domains, we incorporate causal technique into predictive modeling
and propose a novel Causally Regularized Logistic Regression (CRLR) algorithm
by jointly optimize global confounder balancing and weighted logistic
regression. Global confounder balancing helps to identify causal features,
whose causal effect on outcome are stable across domains, then performing
logistic regression on those causal features constructs a robust predictive
model against the agnostic bias. To validate the effectiveness of our CRLR
algorithm, we conduct comprehensive experiments on both synthetic and real
world datasets. Experimental results clearly demonstrate that our CRLR
algorithm outperforms the state-of-the-art methods, and the interpretability of
our method can be fully depicted by the feature visualization.
</summary>
    <author>
      <name>Zheyan Shen</name>
    </author>
    <author>
      <name>Peng Cui</name>
    </author>
    <author>
      <name>Kun Kuang</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Peixuan Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240508.3240577</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240508.3240577" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Oral paper of 2018 ACM Multimedia Conference (MM'18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06656v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06656v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06206v1</id>
    <updated>2018-08-19T13:14:01Z</updated>
    <published>2018-08-19T13:14:01Z</published>
    <title>TLR: Transfer Latent Representation for Unsupervised Domain Adaptation</title>
    <summary>  Domain adaptation refers to the process of learning prediction models in a
target domain by making use of data from a source domain. Many classic methods
solve the domain adaptation problem by establishing a common latent space,
which may cause the loss of many important properties across both domains. In
this manuscript, we develop a novel method, transfer latent representation
(TLR), to learn a better latent space. Specifically, we design an objective
function based on a simple linear autoencoder to derive the latent
representations of both domains. The encoder in the autoencoder aims to project
the data of both domains into a robust latent space. Besides, the decoder
imposes an additional constraint to reconstruct the original data, which can
preserve the common properties of both domains and reduce the noise that causes
domain shift. Experiments on cross-domain tasks demonstrate the advantages of
TLR over competing methods.
</summary>
    <author>
      <name>Pan Xiao</name>
    </author>
    <author>
      <name>Bo Du</name>
    </author>
    <author>
      <name>Jia Wu</name>
    </author>
    <author>
      <name>Lefei Zhang</name>
    </author>
    <author>
      <name>Ruimin Hu</name>
    </author>
    <author>
      <name>Xuelong Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06191v1</id>
    <updated>2018-08-19T09:41:21Z</updated>
    <published>2018-08-19T09:41:21Z</published>
    <title>Fourier analysis perspective for sufficient dimension reduction problem</title>
    <summary>  A theory of sufficient dimension reduction (SDR) is developed from an
optimizational perspective. In our formulation of the problem, instead of
dealing with raw data, we assume that our ground truth includes a mapping
${\mathbf f}: {\mathbb R}^n\rightarrow {\mathbb R}^m$ and a probability
distribution function $p$ over ${\mathbb R}^n$, both given analytically. We
formulate SDR as a problem of finding a function ${\mathbf g}: {\mathbb
R}^k\rightarrow {\mathbb R}^m$ and a matrix $P\in {\mathbb R}^{k\times n}$ such
that ${\mathbb E}_{{\mathbf x}\sim p({\mathbf x})} \left|{\mathbf f}({\mathbf
x}) - {\mathbf g}(P{\mathbf x})\right|^2$ is minimal. It turns out that the
latter problem allows a reformulation in the dual space, i.e. instead of
searching for ${\mathbf g}(P{\mathbf x})$ we suggest searching for its Fourier
transform. First, we characterize all tempered distributions that can serve as
the Fourier transform of such functions. The reformulation in the dual space
can be interpreted as a problem of finding a $k$-dimensional linear subspace
$S$ and a tempered distribution ${\mathbf t}$ supported in $S$ such that
${\mathbf t}$ is "close" in a certain sense to the Fourier transform of
${\mathbf f}$.
  Instead of optimizing over generalized functions with a $k$-dimensional
support, we suggest minimizing over ordinary functions but with an additional
term $R$ that penalizes a strong distortion of the support from any
$k$-dimensional linear subspace. For a specific case of $R$, we develop an
algorithm that can be formulated for functions given in the initial form as
well as for their Fourier transforms. Eventually, we report results of
numerical experiments with a discretized version of the latter algorithm.
</summary>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07963v2</id>
    <updated>2018-08-19T07:41:40Z</updated>
    <published>2018-07-20T06:09:44Z</published>
    <title>Deep Transfer Learning for Cross-domain Activity Recognition</title>
    <summary>  Human activity recognition plays an important role in people's daily life.
However, it is often expensive and time-consuming to acquire sufficient labeled
activity data. To solve this problem, transfer learning leverages the labeled
samples from the source domain to annotate the target domain which has few or
none labels. Unfortunately, when there are several source domains available, it
is difficult to select the right source domains for transfer. The right source
domain means that it has the most similar properties with the target domain,
thus their similarity is higher, which can facilitate transfer learning.
Choosing the right source domain helps the algorithm perform well and prevents
the negative transfer. In this paper, we propose an effective Unsupervised
Source Selection algorithm for Activity Recognition (USSAR). USSAR is able to
select the most similar $K$ source domains from a list of available domains.
After this, we propose an effective Transfer Neural Network to perform
knowledge transfer for Activity Recognition (TNNAR). TNNAR could capture both
the time and spatial relationship between activities while transferring
knowledge. Experiments on three public activity recognition datasets
demonstrate that: 1) The USSAR algorithm is effective in selecting the best
source domains. 2) The TNNAR method can reach high accuracy when performing
activity knowledge transfer.
</summary>
    <author>
      <name>Jindong Wang</name>
    </author>
    <author>
      <name>Vincent W. Zheng</name>
    </author>
    <author>
      <name>Yiqiang Chen</name>
    </author>
    <author>
      <name>Meiyu Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCSE 2018 best paper; 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07963v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07963v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06170v1</id>
    <updated>2018-08-19T06:21:58Z</updated>
    <published>2018-08-19T06:21:58Z</published>
    <title>Linked Recurrent Neural Networks</title>
    <summary>  Recurrent Neural Networks (RNNs) have been proven to be effective in modeling
sequential data and they have been applied to boost a variety of tasks such as
document classification, speech recognition and machine translation. Most of
existing RNN models have been designed for sequences assumed to be identically
and independently distributed (i.i.d). However, in many real-world
applications, sequences are naturally linked. For example, web documents are
connected by hyperlinks; and genes interact with each other. On the one hand,
linked sequences are inherently not i.i.d., which poses tremendous challenges
to existing RNN models. On the other hand, linked sequences offer link
information in addition to the sequential information, which enables
unprecedented opportunities to build advanced RNN models. In this paper, we
study the problem of RNN for linked sequences. In particular, we introduce a
principled approach to capture link information and propose a linked Recurrent
Neural Network (LinkedRNN), which models sequential and link information
coherently. We conduct experiments on real-world datasets from multiple domains
and the experimental results validate the effectiveness of the proposed
framework.
</summary>
    <author>
      <name>Zhiwei Wang</name>
    </author>
    <author>
      <name>Yao Ma</name>
    </author>
    <author>
      <name>Dawei Yin</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00560v2</id>
    <updated>2018-08-19T03:20:07Z</updated>
    <published>2018-02-02T05:09:10Z</published>
    <title>Interpretable Deep Convolutional Neural Networks via Meta-learning</title>
    <summary>  Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.
</summary>
    <author>
      <name>Xuan Liu</name>
    </author>
    <author>
      <name>Xiaoguang Wang</name>
    </author>
    <author>
      <name>Stan Matwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures, 2018 International Joint Conference on Neural
  Networks, in press</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00560v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00560v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06152v1</id>
    <updated>2018-08-19T02:16:40Z</updated>
    <published>2018-08-19T02:16:40Z</published>
    <title>On Design of Problem Token Questions in Quality of Experience Surveys</title>
    <summary>  User surveys for Quality of Experience (QoE) are a critical source of
information. In addition to the common "star rating" used to estimate Mean
Opinion Score (MOS), more detailed survey questions (problem tokens) about
specific areas provide valuable insight into the factors impacting QoE. This
paper explores two aspects of the problem token questionnaire design. First, we
study the bias introduced by fixed question order, and second, we study the
challenge of selecting a subset of questions to keep the token set small. Based
on 900,000 calls gathered using a randomized controlled experiment from a live
system, we find that the order bias can be significantly reduced by randomizing
the display order of tokens. The difference in response rate varies based on
token position and display design. It is worth noting that the users respond to
the randomized-order variant at levels that are comparable to the fixed-order
variant. The effective selection of a subset of token questions is achieved by
extracting tokens that provide the highest information gain over user ratings.
This selection is known to be in the class of NP-hard problems. We apply a
well-known greedy submodular maximization method on our dataset to capture 94%
of the information using just 30% of the questions.
</summary>
    <author>
      <name>Jayant Gupchup</name>
    </author>
    <author>
      <name>Ebrahim Beyrami</name>
    </author>
    <author>
      <name>Martin Ellis</name>
    </author>
    <author>
      <name>Yasaman Hosseinkashi</name>
    </author>
    <author>
      <name>Sam Johnson</name>
    </author>
    <author>
      <name>Ross Cutler</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08402v3</id>
    <updated>2018-08-18T19:43:52Z</updated>
    <published>2018-05-22T05:29:13Z</published>
    <title>Adapted Deep Embeddings: A Synthesis of Methods for $k$-Shot Inductive
  Transfer Learning</title>
    <summary>  The focus in machine learning has branched beyond training classifiers on a
single task to investigating how previously acquired knowledge in a source
domain can be leveraged to facilitate learning in a related target domain,
known as inductive transfer learning. Three active lines of research have
independently explored transfer learning using neural networks. In weight
transfer, a model trained on the source domain is used as an initialization
point for a network to be trained on the target domain. In deep metric
learning, the source domain is used to construct an embedding that captures
class structure in both the source and target domains. In few-shot learning,
the focus is on generalizing well in the target domain based on a limited
number of labeled examples. We compare state-of-the-art methods from these
three paradigms and also explore hybrid adapted-embedding methods that use
limited target-domain data to fine tune embeddings constructed from
source-domain data. We conduct a systematic comparison of methods in a variety
of domains, varying the number of labeled instances available in the target
domain ($k$), as well as the number of target-domain classes. We reach three
principal conclusions: (1) Deep embeddings are far superior, compared to weight
transfer, as a starting point for inter-domain transfer or model re-use (2) Our
hybrid methods robustly outperform every few-shot learning and every deep
metric learning method previously proposed, with a mean error reduction of 30%
over state-of-the-art. (3) Among loss functions for discovering embeddings, the
histogram loss (Ustinova &amp; Lempitsky, 2016) is most robust. We hope our results
will motivate a unification of research in weight transfer, deep metric
learning, and few-shot learning.
</summary>
    <author>
      <name>Tyler R. Scott</name>
    </author>
    <author>
      <name>Karl Ridgeway</name>
    </author>
    <author>
      <name>Michael C. Mozer</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08402v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08402v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06117v1</id>
    <updated>2018-08-18T19:07:42Z</updated>
    <published>2018-08-18T19:07:42Z</published>
    <title>Effect of secular trend in drug effectiveness study in real world data</title>
    <summary>  We discovered secular trend bias in a drug effectiveness study for a recently
approved drug. We compared treatment outcomes between patients who received the
newly approved drug and patients exposed to the standard treatment. All
patients diagnosed after the new drug's approval date were considered. We built
a machine learning causal inference model to determine patient subpopulations
likely to respond better to the newly approved drug. After identifying the
presence of secular trend bias in our data, we attempted to adjust for the bias
in two different ways. First, we matched patients on the number of days from
the new drug's approval date that the patient's treatment (new or standard)
began. Second, we included a covariate in the model for the number of days
between the date of approval of the new drug and the treatment (new or
standard) start date. Neither approach completely mitigated the bias. Residual
bias we attribute to differences in patient disease severity or other
unmeasured patient characteristics. Had we not identified the secular trend
bias in our data, the causal inference model would have been interpreted
without consideration for this underlying bias. Being aware of, testing for,
and handling potential bias in the data is essential to diminish the
uncertainty in AI modeling.
</summary>
    <author>
      <name>Sharon Hensley Alford</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Watson</arxiv:affiliation>
    </author>
    <author>
      <name>Piyush Madan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Shilpa Mahatma</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Italo Buleje</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Yanyan Han</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <author>
      <name>Fang Lu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 7th Causal Inference Workshop at Uncertainty in
  Artificial Intelligence (UAI) Conference 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06107v1</id>
    <updated>2018-08-18T17:49:36Z</updated>
    <published>2018-08-18T17:49:36Z</published>
    <title>Exact Passive-Aggressive Algorithms for Learning to Rank Using Interval
  Labels</title>
    <summary>  In this paper, we propose exact passive-aggressive (PA) online algorithms for
learning to rank. The proposed algorithms can be used even when we have
interval labels instead of actual labels for examples. The proposed algorithms
solve a convex optimization problem at every trial. We find exact solution to
those optimization problems to determine the updated parameters. We propose
support class algorithm (SCA) which finds the active constraints using the KKT
conditions of the optimization problems. These active constrains form support
set which determines the set of thresholds that need to be updated. We derive
update rules for PA, PA-I and PA-II. We show that the proposed algorithms
maintain the ordering of the thresholds after every trial. We provide the
mistake bounds of the proposed algorithms in both ideal and general settings.
We also show experimentally that the proposed algorithms successfully learn
accurate classifiers using interval labels as well as exact labels. Proposed
algorithms also do well compared to other approaches.
</summary>
    <author>
      <name>Naresh Manwani</name>
    </author>
    <author>
      <name>Mohit Chandra</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03314v2</id>
    <updated>2018-08-18T16:05:02Z</updated>
    <published>2018-08-09T19:31:42Z</published>
    <title>Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term
  Memory (LSTM) Network</title>
    <summary>  Because of their effectiveness in broad practical applications, LSTM networks
have received a wealth of coverage in scientific journals, technical blogs, and
implementation guides. However, in most articles, the inference formulas for
the LSTM network and its parent, RNN, are stated axiomatically, while the
training formulas are omitted altogether. In addition, the technique of
"unrolling" an RNN is routinely presented without justification throughout the
literature. The goal of this paper is to explain the essential RNN and LSTM
fundamentals in a single document. Drawing from concepts in signal processing,
we formally derive the canonical RNN formulation from differential equations.
We then propose and prove a precise statement, which yields the RNN unrolling
technique. We also review the difficulties with training the standard RNN and
address them by transforming the RNN into the "Vanilla LSTM" network through a
series of logical arguments. We provide all equations pertaining to the LSTM
system together with detailed descriptions of its constituent entities. Albeit
unconventional, our choice of notation and the method for presenting the LSTM
system emphasizes ease of understanding. As part of the analysis, we identify
new opportunities to enrich the LSTM system and incorporate these extensions
into the Vanilla LSTM network, producing the most general LSTM variant to date.
The target reader has already been exposed to RNNs and LSTM networks through
numerous available resources and is open to an alternative pedagogical
approach. A Machine Learning practitioner seeking guidance for implementing our
new augmented LSTM model in software for experimentation and research will find
the insights and derivations in this tutorial valuable as well.
</summary>
    <author>
      <name>Alex Sherstinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 10 figures, 65 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06603v1</id>
    <updated>2018-08-18T14:43:20Z</updated>
    <published>2018-08-18T14:43:20Z</published>
    <title>Network-based Biased Tree Ensembles (NetBiTE) for Drug Sensitivity
  Prediction and Drug Sensitivity Biomarker Identification in Cancer</title>
    <summary>  We present the Network-based Biased Tree Ensembles (NetBiTE) method for drug
sensitivity prediction and drug sensitivity biomarker identification in cancer
using a combination of prior knowledge and gene expression data. Our devised
method consists of a biased tree ensemble that is built according to a
probabilistic bias weight distribution. The bias weight distribution is
obtained from the assignment of high weights to the drug targets and
propagating the assigned weights over a protein-protein interaction network
such as STRING. The propagation of weights, defines neighborhoods of influence
around the drug targets and as such simulates the spread of perturbations
within the cell, following drug administration. Using a synthetic dataset, we
showcase how application of biased tree ensembles (BiTE) results in significant
accuracy gains at a much lower computational cost compared to the unbiased
random forests (RF) algorithm. We then apply NetBiTE to the Genomics of Drug
Sensitivity in Cancer (GDSC) dataset and demonstrate that NetBiTE outperforms
RF in predicting IC50 drug sensitivity, only for drugs that target membrane
receptor pathways (MRPs): RTK, EGFR and IGFR signaling pathways. We propose
based on the NetBiTE results, that for drugs that inhibit MRPs, the expression
of target genes prior to drug administration is a biomarker for IC50 drug
sensitivity following drug administration. We further verify and reinforce this
proposition through control studies on, PI3K/MTOR signaling pathway inhibitors,
a drug category that does not target MRPs, and through assignment of dummy
targets to MRP inhibiting drugs and investigating the variation in NetBiTE
accuracy.
</summary>
    <author>
      <name>Ali Oskooei</name>
    </author>
    <author>
      <name>Matteo Manica</name>
    </author>
    <author>
      <name>Roland Mathis</name>
    </author>
    <author>
      <name>Maria Rodriguez Martinez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 5 figures, 3 supplementary figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06088v1</id>
    <updated>2018-08-18T14:30:57Z</updated>
    <published>2018-08-18T14:30:57Z</published>
    <title>Tangent-Normal Adversarial Regularization for Semi-supervised Learning</title>
    <summary>  The ever-increasing size of modern datasets combined with the difficulty of
obtaining label information has made semi-supervised learning of significant
practical importance in modern machine learning applications. Compared with
supervised learning, the key difficulty in semi-supervised learning is how to
make full use of the unlabeled data. In order to utilize manifold information
provided by unlabeled data, we propose a novel regularization called the
tangent-normal adversarial regularization, which is composed by two parts. The
two terms complement with each other and jointly enforce the smoothness along
two different directions that are crucial for semi-supervised learning. One is
applied along the tangent space of the data manifold, aiming to enforce local
invariance of the classifier on the manifold, while the other is performed on
the normal space orthogonal to the tangent space, intending to impose
robustness on the classifier against the noise causing the observed data
deviating from the underlying data manifold. Both of the two regularizers are
achieved by the strategy of virtual adversarial training. Our method has
achieved state-of-the-art performance on semi-supervised learning tasks on both
artificial dataset and FashionMNIST dataset.
</summary>
    <author>
      <name>Bing Yu</name>
    </author>
    <author>
      <name>Jingfeng Wu</name>
    </author>
    <author>
      <name>Zhanxing Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07379v1</id>
    <updated>2018-08-18T10:42:52Z</updated>
    <published>2018-08-18T10:42:52Z</published>
    <title>Privacy Mining from IoT-based Smart Homes</title>
    <summary>  Recently, a wide range of smart devices are deployed in a variety of
environments to improve the quality of human life. One of the important
IoT-based applications is smart homes for healthcare, especially for elders.
IoT-based smart homes enable elders' health to be properly monitored and taken
care of. However, elders' privacy might be disclosed from smart homes due to
non-fully protected network communication or other reasons. To demonstrate how
serious this issue is, we introduce in this paper a Privacy Mining Approach
(PMA) to mine privacy from smart homes by conducting a series of deductions and
analyses on sensor datasets generated by smart homes. The experimental results
demonstrate that PMA is able to deduce a global sensor topology for a smart
home and disclose elders' privacy in terms of their house layouts.
</summary>
    <author>
      <name>Ming-Chang Lee</name>
    </author>
    <author>
      <name>Jia-Chun Lin</name>
    </author>
    <author>
      <name>Olaf Owe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper, which has 11 pages and 7 figures, has been accepted BWCCA
  2018 on 13th August 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00877v2</id>
    <updated>2018-08-18T04:19:15Z</updated>
    <published>2018-06-03T21:07:34Z</published>
    <title>Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual
  Optimization</title>
    <summary>  Despite the success of single-agent reinforcement learning, multi-agent
reinforcement learning (MARL) remains challenging due to complex interactions
between agents. Motivated by decentralized applications such as sensor
networks, swarm robotics, and power grids, we study policy evaluation in MARL,
where agents with jointly observed state-action pairs and private local rewards
collaborate to learn the value of a given policy.
  In this paper, we propose a double averaging scheme, where each agent
iteratively performs averaging over both space and time to incorporate
neighboring gradient information and local reward information, respectively. We
prove that the proposed algorithm converges to the optimal solution at a global
geometric rate. In particular, such an algorithm is built upon a primal-dual
reformulation of the mean squared projected Bellman error minimization problem,
which gives rise to a decentralized convex-concave saddle-point problem. To the
best of our knowledge, the proposed double averaging primal-dual optimization
algorithm is the first to achieve fast finite-time convergence on decentralized
convex-concave saddle-point problems.
</summary>
    <author>
      <name>Hoi-To Wai</name>
    </author>
    <author>
      <name>Zhuoran Yang</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Mingyi Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures; updated with improved convergence results</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06022v1</id>
    <updated>2018-08-18T00:00:19Z</updated>
    <published>2018-08-18T00:00:19Z</published>
    <title>Characterizing Transgender Health Issues in Twitter</title>
    <summary>  Although there are millions of transgender people in the world, a lack of
information exists about their health issues. This issue has consequences for
the medical field, which only has a nascent understanding of how to identify
and meet this population's health-related needs. Social media sites like
Twitter provide new opportunities for transgender people to overcome these
barriers by sharing their personal health experiences. Our research employs a
computational framework to collect tweets from self-identified transgender
users, detect those that are health-related, and identify their information
needs. This framework is significant because it provides a macro-scale
perspective on an issue that lacks investigation at national or demographic
levels. Our findings identified 54 distinct health-related topics that we
grouped into 7 broader categories. Further, we found both linguistic and
topical differences in the health-related information shared by transgender men
(TM) as com-pared to transgender women (TW). These findings can help inform
medical and policy-based strategies for health interventions within transgender
communities. Also, our proposed approach can inform the development of
computational strategies to identify the health-related information needs of
other marginalized populations.
</summary>
    <author>
      <name>Amir Karami</name>
    </author>
    <author>
      <name>Frank Webb</name>
    </author>
    <author>
      <name>Vanessa L. Kitzie</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10079v3</id>
    <updated>2018-08-17T23:51:52Z</updated>
    <published>2018-07-26T11:54:48Z</published>
    <title>Automatic Detection of Node-Replication Attack in Vehicular Ad-hoc
  Networks</title>
    <summary>  Recent advances in smart cities applications enforce security threads such as
node replication attacks. Such attack is take place when the attacker plants a
replicated network node within the network. Vehicular Ad hoc networks are
connecting sensors that have limited resources and required the response time
to be as low as possible. In this type networks, traditional detection
algorithms of node replication attacks are not efficient. In this paper, we
propose an initial idea to apply a newly adapted statistical methodology that
can detect node replication attacks with high performance as compared to
state-of-the-art techniques. We provide a sufficient description of this
methodology and a road-map for testing and experiment its performance.
</summary>
    <author>
      <name>Mohammed GH. I. AL Zamil</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10079v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10079v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06021v1</id>
    <updated>2018-08-17T23:50:01Z</updated>
    <published>2018-08-17T23:50:01Z</published>
    <title>What do the US West Coast Public Libraries Post on Twitter?</title>
    <summary>  Twitter has provided a great opportunity for public libraries to disseminate
information for a variety of purposes. Twitter data have been applied in
different domains such as health, politics, and history. There are thousands of
public libraries in the US, but no study has yet investigated the content of
their social media posts like tweets to find their interests. Moreover,
traditional content analysis of Twitter content is not an efficient task for
exploring thousands of tweets. Therefore, there is a need for automatic methods
to overcome the limitations of manual methods. This paper proposes a
computational approach to collecting and analyzing using Twitter Application
Programming Interfaces (API) and investigates more than 138,000 tweets from 48
US west coast libraries using topic modeling. We found 20 topics and assigned
them to five categories including public relations, book, event, training, and
social good. Our results show that the US west coast libraries are more
interested in using Twitter for public relations and book-related events. This
research has both practical and theoretical applications for libraries as well
as other organizations to explore social media actives of their customer and
themselves.
</summary>
    <author>
      <name>Amir Karami</name>
    </author>
    <author>
      <name>Matthew Collins</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10102v2</id>
    <updated>2018-08-17T21:38:55Z</updated>
    <published>2017-05-29T10:29:23Z</published>
    <title>Structural Conditions for Projection-Cost Preservation via Randomized
  Matrix Multiplication</title>
    <summary>  Projection-cost preservation is a low-rank approximation guarantee which
ensures that the cost of any rank-$k$ projection can be preserved using a
smaller sketch of the original data matrix. We present a general structural
result outlining four sufficient conditions to achieve projection-cost
preservation. These conditions can be satisfied using tools from the Randomized
Linear Algebra literature.
</summary>
    <author>
      <name>Agniva Chowdhury</name>
    </author>
    <author>
      <name>Jiasen Yang</name>
    </author>
    <author>
      <name>Petros Drineas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.10102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05979v1</id>
    <updated>2018-08-17T20:12:26Z</updated>
    <published>2018-08-17T20:12:26Z</published>
    <title>Optimizing Deep Neural Network Architecture: A Tabu Search Based
  Approach</title>
    <summary>  The performance of Feedforward neural network (FNN) fully de-pends upon the
selection of architecture and training algorithm. FNN architecture can be
tweaked using several parameters, such as the number of hidden layers, number
of hidden neurons at each hidden layer and number of connections between
layers. There may be exponential combinations for these architectural
attributes which may be unmanageable manually, so it requires an algorithm
which can automatically design an optimal architecture with high generalization
ability. Numerous optimization algorithms have been utilized for FNN
architecture determination. This paper proposes a new methodology which can
work on the estimation of hidden layers and their respective neurons for FNN.
This work combines the advantages of Tabu search (TS) and Gradient descent with
momentum backpropagation (GDM) training algorithm to demonstrate how Tabu
search can automatically select the best architecture from the populated
architectures based on minimum testing error criteria. The proposed approach
has been tested on four classification benchmark dataset of different size.
</summary>
    <author>
      <name>Tarun Kumar Gupta</name>
    </author>
    <author>
      <name>Khalid Raza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, 2 algorithms, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08340v2</id>
    <updated>2018-08-17T17:13:01Z</updated>
    <published>2018-05-22T01:08:40Z</published>
    <title>Reducing Parameter Space for Neural Network Training</title>
    <summary>  For neural networks (NNs) with rectified linear unit (ReLU) or binary
activation functions, we show that their training can be accomplished in a
reduced parameter space. Specifically, the weights in each neuron can be
trained on the unit sphere, as opposed to the entire space, and the threshold
can be trained in a bounded interval, as opposed to the real line. We show that
the NNs in the reduced parameter space are mathematically equivalent to the
standard NNs with parameters in the whole space. The reduced parameter space
shall facilitate the optimization procedure for the network training, as the
search space becomes (much) smaller. We demonstrate the improved training
performance using numerical examples.
</summary>
    <author>
      <name>Tong Qin</name>
    </author>
    <author>
      <name>Ling Zhou</name>
    </author>
    <author>
      <name>Dongbin Xiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10692v2</id>
    <updated>2018-08-17T17:10:17Z</updated>
    <published>2018-06-10T13:04:53Z</published>
    <title>ActiveRemediation: The Search for Lead Pipes in Flint, Michigan</title>
    <summary>  We detail our ongoing work in Flint, Michigan to detect pipes made of lead
and other hazardous metals. After elevated levels of lead were detected in
residents' drinking water, followed by an increase in blood lead levels in area
children, the state and federal governments directed over $125 million to
replace water service lines, the pipes connecting each home to the water
system. In the absence of accurate records, and with the high cost of
determining buried pipe materials, we put forth a number of predictive and
procedural tools to aid in the search and removal of lead infrastructure.
Alongside these statistical and machine learning approaches, we describe our
interactions with government officials in recommending homes for both
inspection and replacement, with a focus on the statistical model that adapts
to incoming information. Finally, in light of discussions about increased
spending on infrastructure development by the federal government, we explore
how our approach generalizes beyond Flint to other municipalities nationwide.
</summary>
    <author>
      <name>Jacob Abernethy</name>
    </author>
    <author>
      <name>Alex Chojnacki</name>
    </author>
    <author>
      <name>Arya Farahi</name>
    </author>
    <author>
      <name>Eric Schwartz</name>
    </author>
    <author>
      <name>Jared Webb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219896</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219896" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, To appear in KDD 2018, For associated
  promotional video, see https://www.youtube.com/watch?v=YbIn_axYu9E</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05924v1</id>
    <updated>2018-08-17T16:42:09Z</updated>
    <published>2018-08-17T16:42:09Z</published>
    <title>Randomized Least Squares Regression: Combining Model- and
  Algorithm-Induced Uncertainties</title>
    <summary>  We analyze the uncertainties in the minimum norm solution of full-rank
regression problems, arising from Gaussian linear models, computed by
randomized (row-wise sampling and, more generally, sketching) algorithms. From
a deterministic perspective, our structural perturbation bounds imply that
least squares problems are less sensitive to multiplicative perturbations than
to additive perturbations. From a probabilistic perspective, our expressions
for the total expectation and variance with regard to both model- and
algorithm-induced uncertainties, are exact, hold for general sketching
matrices, and make no assumptions on the rank of the sketched matrix. The
relative differences between the total bias and variance on the one hand, and
the model bias and variance on the other hand, are governed by two factors: (i)
the expected rank deficiency of the sketched matrix, and (ii) the expected
difference between projectors associated with the original and the sketched
problems. A simple example, based on uniform sampling with replacement,
illustrates the statistical quantities.
</summary>
    <author>
      <name>Jocelyn T. Chi</name>
    </author>
    <author>
      <name>Ilse C. F. Ipsen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05917v1</id>
    <updated>2018-08-17T16:22:09Z</updated>
    <published>2018-08-17T16:22:09Z</published>
    <title>A bagging and importance sampling approach to Support Vector Machines</title>
    <summary>  An importance sampling and bagging approach to solving the support vector
machine (SVM) problem in the context of large databases is presented and
evaluated. Our algorithm builds on the nearest neighbors ideas presented in
Camelo at al. (2015). As in that reference, the goal of the present proposal is
to achieve a faster solution of the SVM problem without a significance loss in
the prediction error. The performance of the methodology is evaluated in
benchmark examples and theoretical aspects of subsample methods are discussed.
</summary>
    <author>
      <name>R. Bárcenas</name>
    </author>
    <author>
      <name>M. D. Gónzalez--Lima</name>
    </author>
    <author>
      <name>A. J. Quiroz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05904v1</id>
    <updated>2018-08-17T15:48:52Z</updated>
    <published>2018-08-17T15:48:52Z</published>
    <title>Correlated Multi-armed Bandits with a Latent Random Source</title>
    <summary>  We consider a novel multi-armed bandit framework where the rewards obtained
by pulling the arms are functions of a common latent random variable. The
correlation between arms due to the common random source can be used to design
a generalized upper-confidence-bound (UCB) algorithm that identifies certain
arms as $non-competitive$, and avoids exploring them. As a result, we reduce a
$K$-armed bandit problem to a $C+1$-armed problem, where $C+1$ includes the
best arm and $C$ $competitive$ arms. Our regret analysis shows that the
competitive arms need to be pulled $\mathcal{O}(\log T)$ times, while the
non-competitive arms are pulled only $\mathcal{O}(1)$ times. As a result, there
are regimes where our algorithm achieves a $\mathcal{O}(1)$ regret as opposed
to the typical logarithmic regret scaling of multi-armed bandit algorithms. We
also evaluate lower bounds on the expected regret and prove that our
correlated-UCB algorithm is order-wise optimal.
</summary>
    <author>
      <name>Samarth Gupta</name>
    </author>
    <author>
      <name>Gauri Joshi</name>
    </author>
    <author>
      <name>Osman Yağan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05902v1</id>
    <updated>2018-08-17T15:32:24Z</updated>
    <published>2018-08-17T15:32:24Z</published>
    <title>Learning Supervised Topic Models for Classification and Regression from
  Crowds</title>
    <summary>  The growing need to analyze large collections of documents has led to great
developments in topic modeling. Since documents are frequently associated with
other related variables, such as labels or ratings, much interest has been
placed on supervised topic models. However, the nature of most annotation
tasks, prone to ambiguity and noise, often with high volumes of documents, deem
learning under a single-annotator assumption unrealistic or unpractical for
most real-world applications. In this article, we propose two supervised topic
models, one for classification and another for regression problems, which
account for the heterogeneity and biases among different annotators that are
encountered in practice when learning from crowds. We develop an efficient
stochastic variational inference algorithm that is able to scale to very large
datasets, and we empirically demonstrate the advantages of the proposed model
over state-of-the-art approaches.
</summary>
    <author>
      <name>Filipe Rodrigues</name>
    </author>
    <author>
      <name>Mariana Lourenço</name>
    </author>
    <author>
      <name>Bernardete Ribeiro</name>
    </author>
    <author>
      <name>Francisco Pereira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2017.2648786</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2017.2648786" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Rodrigues, F., Lourenco, M., Ribeiro, B. and Pereira, F.C., 2017.
  Learning supervised topic models for classification and regression from
  crowds. IEEE transactions on pattern analysis and machine intelligence,
  39(12), pp.2409-2422</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.05902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02287v2</id>
    <updated>2018-08-17T15:27:29Z</updated>
    <published>2018-07-06T07:12:56Z</published>
    <title>Outperforming Good-Turing: Preliminary Report</title>
    <summary>  Estimating a large alphabet probability distribution from a limited number of
samples is a fundamental problem in machine learning and statistics. A variety
of estimation schemes have been proposed over the years, mostly inspired by the
early work of Laplace and the seminal contribution of Good and Turing. One of
the basic assumptions shared by most commonly-used estimators is the unique
correspondence between the symbol's sample frequency and its estimated
probability. In this work we tackle this paradigmatic assumption; we claim that
symbols with "similar" frequencies shall be assigned the same estimated
probability value. This way we regulate the number of parameters and improve
generalization. In this preliminary report we show that by applying an ensemble
of such regulated estimators, we introduce a dramatic enhancement in the
estimation accuracy (typically up to 50%), compared to currently known methods.
An implementation of our suggested method is publicly available at the first
author's web-page.
</summary>
    <author>
      <name>Amichai Painsky</name>
    </author>
    <author>
      <name>Meir Feder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has several inaccuracies in the experimental setup which
  require additional work</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05889v1</id>
    <updated>2018-08-17T14:51:09Z</updated>
    <published>2018-08-17T14:51:09Z</published>
    <title>Data Consistency Approach to Model Validation</title>
    <summary>  In scientific inference problems, the underlying statistical modeling
assumptions have a crucial impact on the end results. There exist, however,
only a few automatic means for validating these fundamental modelling
assumptions. The contribution in this paper is a general criterion to evaluate
the consistency of a set of statistical models with respect to observed data.
This is achieved by automatically gauging the models' ability to generate data
that is similar to the observed data. Importantly, the criterion follows from
the model class itself and is therefore directly applicable to a broad range of
inference problems with varying data types. The proposed data consistency
criterion is illustrated and evaluated using three synthetic and two real data
sets.
</summary>
    <author>
      <name>Andreas Svensson</name>
    </author>
    <author>
      <name>Dave Zachariah</name>
    </author>
    <author>
      <name>Petre Stoica</name>
    </author>
    <author>
      <name>Thomas B. Schön</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06503v1</id>
    <updated>2018-08-17T14:26:47Z</updated>
    <published>2018-08-17T14:26:47Z</published>
    <title>Collaborative Pressure Ulcer Prevention: An Automated Skin Damage and
  Pressure Ulcer Assessment Tool for Nursing Professionals, Patients, Family
  Members and Carers</title>
    <summary>  This paper describes the Pressure Ulcers Online Website, which is a first
step solution towards a new and innovative platform for helping people to
detect, understand and manage pressure ulcers. It outlines the reasons why the
project has been developed and provides a central point of contact for pressure
ulcer analysis and ongoing research. Using state-of-the-art technologies in
convolutional neural networks and transfer learning along with end-to-end web
technologies, this platform allows pressure ulcers to be analysed and findings
to be reported. As the system evolves through collaborative partnerships,
future versions will provide decision support functions to describe the complex
characteristics of pressure ulcers along with information on wound care across
multiple user boundaries. This project is therefore intended to raise awareness
and support for people suffering with or providing care for pressure ulcers.
</summary>
    <author>
      <name>Paul Fergus</name>
    </author>
    <author>
      <name>Carl Chalmers</name>
    </author>
    <author>
      <name>David Tully</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 7 figures, Position Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05854v1</id>
    <updated>2018-08-17T13:31:30Z</updated>
    <published>2018-08-17T13:31:30Z</published>
    <title>Robust Compressive Phase Retrieval via Deep Generative Priors</title>
    <summary>  This paper proposes a new framework to regularize the highly ill-posed and
non-linear phase retrieval problem through deep generative priors using simple
gradient descent algorithm. We experimentally show effectiveness of proposed
algorithm for random Gaussian measurements (practically relevant in imaging
through scattering media) and Fourier friendly measurements (relevant in
optical set ups). We demonstrate that proposed approach achieves impressive
results when compared with traditional hand engineered priors including
sparsity and denoising frameworks for number of measurements and robustness
against noise. Finally, we show the effectiveness of the proposed approach on a
real transmission matrix dataset in an actual application of multiple
scattering media imaging.
</summary>
    <author>
      <name>Fahad Shamshad</name>
    </author>
    <author>
      <name>Ali Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03538v3</id>
    <updated>2018-08-17T13:12:02Z</updated>
    <published>2015-07-13T18:24:16Z</published>
    <title>Classifying X-ray Binaries: A Probabilistic Approach</title>
    <summary>  In X-ray binary star systems consisting of a compact object that accretes
material from an orbiting secondary star, there is no straightforward means to
decide if the compact object is a black hole or a neutron star. To assist this
classification, we develop a Bayesian statistical model that makes use of the
fact that X-ray binary systems appear to cluster based on their compact object
type when viewed from a 3-dimensional coordinate system derived from X-ray
spectral data. The first coordinate of this data is the ratio of counts in mid
to low energy band (color 1), the second coordinate is the ratio of counts in
high to low energy band (color 2), and the third coordinate is the sum of
counts in all three bands. We use this model to estimate the probabilities that
an X-ray binary system contains a black hole, non-pulsing neutron star, or
pulsing neutron star. In particular, we utilize a latent variable model in
which the latent variables follow a Gaussian process prior distribution, and
hence we are able to induce the spatial correlation we believe exists between
systems of the same type. The utility of this approach is evidenced by the
accurate prediction of system types using Rossi X-ray Timing Explorer All Sky
Monitor data, but it is not flawless. In particular, non-pulsing neutron
systems containing "bursters" that are close to the boundary demarcating
systems containing black holes tend to be classified as black hole systems. As
a byproduct of our analyses, we provide the astronomer with public R code that
can be used to predict the compact object type of X-ray binaries given training
data.
</summary>
    <author>
      <name>Giri Gopalan</name>
    </author>
    <author>
      <name>Saeqa Dil Vrtilek</name>
    </author>
    <author>
      <name>Luke Bornn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0004-637X/809/1/40</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0004-637X/809/1/40" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Editing of figure captions and correction of y-axis labels for bar
  charts in figures 6 and 7</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 ApJ 809 40</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.03538v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03538v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07562v2</id>
    <updated>2018-08-17T11:40:54Z</updated>
    <published>2018-06-20T05:53:32Z</published>
    <title>Efficient inference in stochastic block models with vertex labels</title>
    <summary>  We study the stochastic block model with two communities where vertices
contain side information in the form of a vertex label. These vertex labels may
have arbitrary label distributions, depending on the community memberships. We
analyze a linearized version of the popular belief propagation algorithm. We
show that this algorithm achieves the highest accuracy possible whenever a
certain function of the network parameters has a unique fixed point. Whenever
this function has multiple fixed points, the belief propagation algorithm may
not perform optimally. We show that increasing the information in the vertex
labels may reduce the number of fixed points and hence lead to optimality of
belief propagation.
</summary>
    <author>
      <name>Clara Stegehuis</name>
    </author>
    <author>
      <name>Laurent Massoulié</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07562v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07562v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05832v1</id>
    <updated>2018-08-17T11:25:19Z</updated>
    <published>2018-08-17T11:25:19Z</published>
    <title>Importance mixing: Improving sample reuse in evolutionary policy search
  methods</title>
    <summary>  Deep neuroevolution, that is evolutionary policy search methods based on deep
neural networks, have recently emerged as a competitor to deep reinforcement
learning algorithms due to their better parallelization capabilities. However,
these methods still suffer from a far worse sample efficiency. In this paper we
investigate whether a mechanism known as "importance mixing" can significantly
improve their sample efficiency. We provide a didactic presentation of
importance mixing and we explain how it can be extended to reuse more samples.
Then, from an empirical comparison based on a simple benchmark, we show that,
though it actually provides better sample efficiency, it is still far from the
sample efficiency of deep reinforcement learning, though it is more stable.
</summary>
    <author>
      <name>Aloïs Pourchot</name>
    </author>
    <author>
      <name>Nicolas Perrin</name>
    </author>
    <author>
      <name>Olivier Sigaud</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05819v1</id>
    <updated>2018-08-17T10:37:51Z</updated>
    <published>2018-08-17T10:37:51Z</published>
    <title>Motion Prediction of Traffic Actors for Autonomous Driving using Deep
  Convolutional Networks</title>
    <summary>  Recent algorithmic improvements and hardware breakthroughs resulted in a
number of success stories in the field of AI impacting our daily lives.
However, despite its ubiquity AI is only just starting to make advances in what
may arguably have the largest impact thus far, the nascent field of autonomous
driving. In this work we discuss this important topic and address one of
crucial aspects of the emerging area, the problem of predicting future state of
autonomous vehicle's surrounding necessary for safe and efficient operations.
We introduce a deep learning-based approach that takes into account current
state of traffic actors and produces rasterized representations of each actor's
vicinity. The raster images are then used by deep convolutional models to infer
future movement of actors while accounting for inherent uncertainty of the
prediction task. Extensive experiments on real-world data strongly suggest
benefits of the proposed approach. Moreover, following successful tests the
system was deployed to a fleet of autonomous vehicles.
</summary>
    <author>
      <name>Nemanja Djuric</name>
    </author>
    <author>
      <name>Vladan Radosavljevic</name>
    </author>
    <author>
      <name>Henggang Cui</name>
    </author>
    <author>
      <name>Thi Nguyen</name>
    </author>
    <author>
      <name>Fang-Chieh Chou</name>
    </author>
    <author>
      <name>Tsung-Han Lin</name>
    </author>
    <author>
      <name>Jeff Schneider</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0208v4</id>
    <updated>2018-08-17T09:09:37Z</updated>
    <published>2014-07-01T12:08:10Z</published>
    <title>A Bayes consistent 1-NN classifier</title>
    <summary>  We show that a simple modification of the 1-nearest neighbor classifier
yields a strongly Bayes consistent learner. Prior to this work, the only
strongly Bayes consistent proximity-based method was the k-nearest neighbor
classifier, for k growing appropriately with sample size. We will argue that a
margin-regularized 1-NN enjoys considerable statistical and algorithmic
advantages over the k-NN classifier. These include user-friendly finite-sample
error bounds, as well as time- and memory-efficient learning and test-point
evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging
empirical results are reported.
</summary>
    <author>
      <name>Aryeh Kontorovich</name>
    </author>
    <author>
      <name>Roi Weiss</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0208v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0208v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05770v1</id>
    <updated>2018-08-17T06:34:53Z</updated>
    <published>2018-08-17T06:34:53Z</published>
    <title>Reinforcement Learning for Autonomous Defence in Software-Defined
  Networking</title>
    <summary>  Despite the successful application of machine learning (ML) in a wide range
of domains, adaptability---the very property that makes machine learning
desirable---can be exploited by adversaries to contaminate training and evade
classification. In this paper, we investigate the feasibility of applying a
specific class of machine learning algorithms, namely, reinforcement learning
(RL) algorithms, for autonomous cyber defence in software-defined networking
(SDN). In particular, we focus on how an RL agent reacts towards different
forms of causative attacks that poison its training process, including
indiscriminate and targeted, white-box and black-box attacks. In addition, we
also study the impact of the attack timing, and explore potential
countermeasures such as adversarial training.
</summary>
    <author>
      <name>Yi Han</name>
    </author>
    <author>
      <name>Benjamin I. P. Rubinstein</name>
    </author>
    <author>
      <name>Tamas Abraham</name>
    </author>
    <author>
      <name>Tansu Alpcan</name>
    </author>
    <author>
      <name>Olivier De Vel</name>
    </author>
    <author>
      <name>Sarah Erfani</name>
    </author>
    <author>
      <name>David Hubczenko</name>
    </author>
    <author>
      <name>Christopher Leckie</name>
    </author>
    <author>
      <name>Paul Montague</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02016v2</id>
    <updated>2018-08-17T06:00:41Z</updated>
    <published>2018-08-04T15:48:39Z</published>
    <title>MCRM: Mother Compact Recurrent Memory</title>
    <summary>  LSTMs and GRUs are the most common recurrent neural network architectures
used to solve temporal sequence problems. The two architectures have differing
data flows dealing with a common component called the cell state (also referred
to as the memory). We attempt to enhance the memory by presenting a
modification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are
a type of a nested LSTM-GRU architecture where the cell state is the GRU hidden
state. The concatenation of the forget gate and input gate interactions from
the LSTM are considered an input to the GRU cell. Because MCRMs has this type
of nesting, MCRMs have a compact memory pattern consisting of neurons that acts
explicitly in both long-term and short-term fashions. For some specific tasks,
empirical results show that MCRMs outperform previously used architectures.
</summary>
    <author>
      <name>Abduallah A. Mohamed</name>
    </author>
    <author>
      <name>Christian Claudel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to AAAI-19</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08271v1</id>
    <updated>2018-08-17T02:33:55Z</updated>
    <published>2018-08-17T02:33:55Z</published>
    <title>An elementary introduction to information geometry</title>
    <summary>  We describe the fundamental differential-geometric structures of information
manifolds, state the fundamental theorem of information geometry, and
illustrate some uses of these information manifolds in information sciences.
The exposition is self-contained by concisely introducing the necessary
concepts of differential geometry with proofs omitted for brevity.
</summary>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05731v1</id>
    <updated>2018-08-17T02:24:23Z</updated>
    <published>2018-08-17T02:24:23Z</published>
    <title>Efficiently Learning Mixtures of Mallows Models</title>
    <summary>  Mixtures of Mallows models are a popular generative model for ranking data
coming from a heterogeneous population. They have a variety of applications
including social choice, recommendation systems and natural language
processing. Here we give the first polynomial time algorithm for provably
learning the parameters of a mixture of Mallows models with any constant number
of components. Prior to our work, only the two component case had been settled.
Our analysis revolves around a determinantal identity of Zagier which was
proven in the context of mathematical physics, which we use to show polynomial
identifiability and ultimately to construct test functions to peel off one
component at a time.
  To complement our upper bounds, we show information-theoretic lower bounds on
the sample complexity as well as lower bounds against restricted families of
algorithms that make only local queries. Together, these results demonstrate
various impediments to improving the dependence on the number of components.
They also motivate the study of learning mixtures of Mallows models from the
perspective of beyond worst-case analysis. In this direction, we show that when
the scaling parameters of the Mallows models have separation, there are much
faster learning algorithms.
</summary>
    <author>
      <name>Allen Liu</name>
    </author>
    <author>
      <name>Ankur Moitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">FOCS 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.05731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06492v1</id>
    <updated>2018-08-17T02:15:39Z</updated>
    <published>2018-08-17T02:15:39Z</published>
    <title>Benchmarking Automatic Machine Learning Frameworks</title>
    <summary>  AutoML serves as the bridge between varying levels of expertise when
designing machine learning systems and expedites the data science process. A
wide range of techniques is taken to address this, however there does not exist
an objective comparison of these techniques. We present a benchmark of current
open source AutoML solutions using open source datasets. We test auto-sklearn,
TPOT, auto_ml, and H2O's AutoML solution against a compiled set of regression
and classification datasets sourced from OpenML and find that auto-sklearn
performs the best across classification datasets and TPOT performs the best
across regression datasets.
</summary>
    <author>
      <name>Adithya Balaji</name>
    </author>
    <author>
      <name>Alexander Allen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05726v1</id>
    <updated>2018-08-17T01:59:57Z</updated>
    <published>2018-08-17T01:59:57Z</published>
    <title>An N Time-Slice Dynamic Chain Event Graph</title>
    <summary>  The Dynamic Chain Event Graph (DCEG) is able to depict many classes of
discrete random processes exhibiting asymmetries in their developments and
context-specific conditional probabilities structures. However, paradoxically,
this very generality has so far frustrated its wide application. So in this
paper we develop an object-oriented method to fully analyse a particularly
useful and feasibly implementable new subclass of these graphical models called
the N Time-Slice DCEG (NT-DCEG). After demonstrating a close relationship
between an NT-DCEG and a specific class of Markov processes, we discuss how
graphical modellers can exploit this connection to gain a deep understanding of
their processes. We also show how to read from the topology of this graph
context-specific independence statements that can then be checked by domain
experts. Our methods are illustrated throughout using examples of dynamic
multivariate processes describing inmate radicalisation in a prison.
</summary>
    <author>
      <name>Rodrigo A. Collazo</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04659v3</id>
    <updated>2018-08-17T01:57:44Z</updated>
    <published>2018-04-12T14:06:05Z</published>
    <title>Asynch-SGBDT: Asynchronous Parallel Stochastic Gradient Boosting
  Decision Tree based on Parameters Server</title>
    <summary>  In AI research and industry, machine learning is the most widely used tool.
One of the most important machine learning algorithms is Gradient Boosting
Decision Tree, i.e. GBDT whose training process needs considerable
computational resources and time. To shorten GBDT training time, many works
tried to apply GBDT on Parameter Server. However, those GBDT algorithms are
synchronous parallel algorithms which fail to make full use of Parameter
Server. In this paper, we examine the possibility of using asynchronous
parallel methods to train GBDT model and name this algorithm as asynch-SGBDT
(asynchronous parallel stochastic gradient boosting decision tree). Our
theoretical and experimental results indicate that the scalability of
asynch-SGBDT is influenced by the sample diversity of datasets, sampling rate,
step length and the setting of GBDT tree. Experimental results also show
asynch-SGBDT training process reaches a linear speedup in asynchronous parallel
manner when datasets and GBDT trees meet high scalability requirements.
</summary>
    <author>
      <name>Cheng Daning</name>
    </author>
    <author>
      <name>Xia Fen</name>
    </author>
    <author>
      <name>Li Shigang</name>
    </author>
    <author>
      <name>Zhang Yunquan</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04659v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04659v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07755v2</id>
    <updated>2018-08-17T00:20:11Z</updated>
    <published>2018-06-19T14:01:27Z</published>
    <title>An empirical study on evaluation metrics of generative adversarial
  networks</title>
    <summary>  Evaluating generative adversarial networks (GANs) is inherently challenging.
In this paper, we revisit several representative sample-based evaluation
metrics for GANs, and address the problem of how to evaluate the evaluation
metrics. We start with a few necessary conditions for metrics to produce
meaningful scores, such as distinguishing real from generated samples,
identifying mode dropping and mode collapsing, and detecting overfitting. With
a series of carefully designed experiments, we comprehensively investigate
existing sample-based metrics and identify their strengths and limitations in
practical settings. Based on these results, we observe that kernel Maximum Mean
Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to
satisfy most of the desirable properties, provided that the distances between
samples are computed in a suitable feature space. Our experiments also unveil
interesting properties about the behavior of several popular GAN models, such
as whether they are memorizing training samples, and how far they are from
learning the target distribution.
</summary>
    <author>
      <name>Qiantong Xu</name>
    </author>
    <author>
      <name>Gao Huang</name>
    </author>
    <author>
      <name>Yang Yuan</name>
    </author>
    <author>
      <name>Chuan Guo</name>
    </author>
    <author>
      <name>Yu Sun</name>
    </author>
    <author>
      <name>Felix Wu</name>
    </author>
    <author>
      <name>Kilian Weinberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1802.03446 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07755v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07755v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05705v1</id>
    <updated>2018-08-16T23:39:55Z</updated>
    <published>2018-08-16T23:39:55Z</published>
    <title>Mitigation of Adversarial Attacks through Embedded Feature Selection</title>
    <summary>  Machine learning has become one of the main components for task automation in
many application domains. Despite the advancements and impressive achievements
of machine learning, it has been shown that learning algorithms can be
compromised by attackers both at training and test time. Machine learning
systems are especially vulnerable to adversarial examples where small
perturbations added to the original data points can produce incorrect or
unexpected outputs in the learning algorithms at test time. Mitigation of these
attacks is hard as adversarial examples are difficult to detect. Existing
related work states that the security of machine learning systems against
adversarial examples can be weakened when feature selection is applied to
reduce the systems' complexity. In this paper, we empirically disprove this
idea, showing that the relative distortion that the attacker has to introduce
to succeed in the attack is greater when the target is using a reduced set of
features. We also show that the minimal adversarial examples differ
statistically more strongly from genuine examples with a lower number of
features. However, reducing the feature count can negatively impact the
system's performance. We illustrate the trade-off between security and accuracy
with specific examples. We propose a design methodology to evaluate the
security of machine learning classifiers with embedded feature selection
against adversarial examples crafted using different attack strategies.
</summary>
    <author>
      <name>Ziyi Bao</name>
    </author>
    <author>
      <name>Luis Muñoz-González</name>
    </author>
    <author>
      <name>Emil C. Lupu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07025v3</id>
    <updated>2018-08-16T22:56:41Z</updated>
    <published>2017-05-19T14:42:48Z</published>
    <title>Effective Representations of Clinical Notes</title>
    <summary>  Clinical notes are a rich source of information about patient state. However,
using them to predict clinical events with machine learning models is
challenging. They are very high dimensional, sparse and have complex structure.
Furthermore, training data is often scarce because it is expensive to obtain
reliable labels for many clinical events. These difficulties have traditionally
been addressed by manual feature engineering encoding task specific domain
knowledge. We explored the use of neural networks and transfer learning to
learn representations of clinical notes that are useful for predicting future
clinical events of interest, such as all causes mortality, inpatient
admissions, and emergency room visits. Our data comprised 2.7 million notes and
115 thousand patients at Stanford Hospital. We used the learned
representations, along with commonly used bag of words and topic model
representations, as features for predictive models of clinical events. We
evaluated the effectiveness of these representations with respect to the
performance of the models trained on small datasets. Models using the neural
network derived representations performed significantly better than models
using the baseline representations with small ($N &lt; 1000$) training datasets.
The learned representations offer significant performance gains over commonly
used baseline representations for a range of predictive modeling tasks and
cohort sizes, offering an effective alternative to task specific feature
engineering when plentiful labeled training data is not available.
</summary>
    <author>
      <name>Sebastien Dubois</name>
    </author>
    <author>
      <name>Nathanael Romano</name>
    </author>
    <author>
      <name>David C. Kale</name>
    </author>
    <author>
      <name>Nigam Shah</name>
    </author>
    <author>
      <name>Kenneth Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07025v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07025v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05935v2</id>
    <updated>2018-08-16T22:48:20Z</updated>
    <published>2018-07-16T15:56:55Z</published>
    <title>Siamese Survival Analysis with Competing Risks</title>
    <summary>  Survival analysis in the presence of multiple possible adverse events, i.e.,
competing risks, is a pervasive problem in many industries (healthcare,
finance, etc.). Since only one event is typically observed, the incidence of an
event of interest is often obscured by other related competing events. This
nonidentifiability, or inability to estimate true cause-specific survival
curves from empirical data, further complicates competing risk survival
analysis. We introduce Siamese Survival Prognosis Network (SSPN), a novel deep
learning architecture for estimating personalized risk scores in the presence
of competing risks. SSPN circumvents the nonidentifiability problem by avoiding
the estimation of cause-specific survival curves and instead determines
pairwise concordant time-dependent risks, where longer event times are assigned
lower risks. Furthermore, SSPN is able to directly optimize an approximation to
the C-discrimination index, rather than relying on well-known metrics which are
unable to capture the unique requirements of survival analysis with competing
risks.
</summary>
    <author>
      <name>Anton Nemchenko</name>
    </author>
    <author>
      <name>Trent Kyono</name>
    </author>
    <author>
      <name>Mihaela Van Der Schaar</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05935v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05935v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05671v1</id>
    <updated>2018-08-16T20:25:28Z</updated>
    <published>2018-08-16T20:25:28Z</published>
    <title>On the Convergence of Adaptive Gradient Methods for Nonconvex
  Optimization</title>
    <summary>  Adaptive gradient methods are workhorses in deep learning. However, the
convergence guarantees of adaptive gradient methods for nonconvex optimization
have not been sufficiently studied. In this paper, we provide a sharp analysis
of a recently proposed adaptive gradient method namely partially adaptive
momentum estimation method (Padam) (Chen and Gu, 2018), which admits many
existing adaptive gradient methods such as AdaGrad, RMSProp and AMSGrad as
special cases. Our analysis shows that, for smooth nonconvex functions, Padam
converges to a first-order stationary point at the rate of
$O\big((\sum_{i=1}^d\|\mathbf{g}_{1:T,i}\|_2)^{1/2}/T^{3/4} + d/T\big)$, where
$T$ is the number of iterations, $d$ is the dimension,
$\mathbf{g}_1,\ldots,\mathbf{g}_T$ are the stochastic gradients, and
$\mathbf{g}_{1:T,i} = [g_{1,i},g_{2,i},\ldots,g_{T,i}]^\top$. Our theoretical
result also suggests that in order to achieve faster convergence rate, it is
necessary to use Padam instead of AMSGrad. This is well-aligned with the
empirical results of deep learning reported in Chen and Gu (2018).
</summary>
    <author>
      <name>Dongruo Zhou</name>
    </author>
    <author>
      <name>Yiqi Tang</name>
    </author>
    <author>
      <name>Ziyan Yang</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Quanquan Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09350v3</id>
    <updated>2018-08-16T18:55:14Z</updated>
    <published>2017-07-28T17:47:00Z</published>
    <title>Centrality measures for graphons: Accounting for uncertainty in networks</title>
    <summary>  As relational datasets modeled as graphs keep increasing in size and their
data-acquisition is permeated by uncertainty, graph-based analysis techniques
can become computationally and conceptually challenging. In particular, node
centrality measures rely on the assumption that the graph is perfectly known --
a premise not necessarily fulfilled for large, uncertain networks. Accordingly,
centrality measures may fail to faithfully extract the importance of nodes in
the presence of uncertainty. To mitigate these problems, we suggest a
statistical approach based on graphon theory: we introduce formal definitions
of centrality measures for graphons and establish their connections to
classical graph centrality measures. A key advantage of this approach is that
centrality measures defined at the modeling level of graphons are inherently
robust to stochastic variations of specific graph realizations. Using the
theory of linear integral operators, we define degree, eigenvector, Katz and
PageRank centrality functions for graphons and establish concentration
inequalities demonstrating that graphon centrality functions arise naturally as
limits of their counterparts defined on sequences of graphs of increasing size.
The same concentration inequalities also provide high-probability bounds
between the graphon centrality functions and the centrality measures on any
sampled graph, thereby establishing a measure of uncertainty of the measured
centrality score.
</summary>
    <author>
      <name>Marco Avella-Medina</name>
    </author>
    <author>
      <name>Francesca Parise</name>
    </author>
    <author>
      <name>Michael T. Schaub</name>
    </author>
    <author>
      <name>Santiago Segarra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Authors ordered alphabetically, all authors contributed equally. 20
  pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09350v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09350v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08272v1</id>
    <updated>2018-08-16T18:11:56Z</updated>
    <published>2018-08-16T18:11:56Z</published>
    <title>Probabilistic Model of Object Detection Based on Convolutional Neural
  Network</title>
    <summary>  The combination of a CNN detector and a search framework forms the basis for
local object/pattern detection. To handle the waste of regional information and
the defective compromise between efficiency and accuracy, this paper proposes a
probabilistic model with a powerful search framework. By mapping an image into
a probabilistic distribution of objects, this new model gives more informative
outputs with less computation. The setting and analytic traits are elaborated
in this paper, followed by a series of experiments carried out on FDDB, which
show that the proposed model is sound, efficient and analytic.
</summary>
    <author>
      <name>Fang-Qi Li</name>
    </author>
    <author>
      <name>Xu-Die Ren</name>
    </author>
    <author>
      <name>Hao-Nan Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, International Conference on Communication, Signal
  Processing and Systems (CSPS 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05599v1</id>
    <updated>2018-08-16T17:41:00Z</updated>
    <published>2018-08-16T17:41:00Z</published>
    <title>Improving Conditional Sequence Generative Adversarial Networks by
  Stepwise Evaluation</title>
    <summary>  Sequence generative adversarial networks (SeqGAN) have been used to improve
conditional sequence generation tasks, for example, chit-chat dialogue
generation. To stabilize the training of SeqGAN, Monte Carlo tree search (MCTS)
or reward at every generation step (REGS) is used to evaluate the goodness of a
generated subsequence. MCTS is computationally intensive, but the performance
of REGS is worse than MCTS. In this paper, we propose stepwise GAN (StepGAN),
in which the discriminator is modified to automatically assign scores
quantifying the goodness of each subsequence at every generation step. StepGAN
has significantly less computational costs than MCTS. We demonstrate that
StepGAN outperforms previous GAN-based methods on both synthetic experiment and
chit-chat dialogue generation.
</summary>
    <author>
      <name>Yi-Lin Tuan</name>
    </author>
    <author>
      <name>Hung-Yi Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05587v1</id>
    <updated>2018-08-16T17:20:58Z</updated>
    <published>2018-08-16T17:20:58Z</published>
    <title>Deep Convolutional Networks as shallow Gaussian Processes</title>
    <summary>  We show that the output of a (residual) convolutional neural network (CNN)
with an appropriate prior over the weights and biases is a Gaussian process
(GP) in the limit of infinitely many convolutional filters, extending similar
results for dense networks. For a CNN, the equivalent kernel can be computed
exactly and, unlike "deep kernels", has very few parameters: only the
hyperparameters of the original CNN. Further, we show that this kernel has two
properties that allow it to be computed efficiently; the cost of evaluating the
kernel for a pair of images is similar to a single forward pass through the
original CNN with only one filter per layer. The kernel equivalent to a
32-layer ResNet obtains 0.84% classification error on MNIST, a new record for
GPs with a comparable number of parameters.
</summary>
    <author>
      <name>Adrià Garriga-Alonso</name>
    </author>
    <author>
      <name>Laurence Aitchison</name>
    </author>
    <author>
      <name>Carl Edward Rasmussen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03300v2</id>
    <updated>2018-08-16T17:15:44Z</updated>
    <published>2018-08-09T18:43:18Z</published>
    <title>$α$-Approximation Density-based Clustering of Multi-valued Objects</title>
    <summary>  Multi-valued data are commonly found in many real applications. During the
process of clustering multi-valued data, most existing methods use sampling or
aggregation mechanisms that cannot reflect the real distribution of objects and
their instances and thus fail to obtain high-quality clusters. In this paper, a
concept of $\alpha$-approximation distance is introduced to measure the
connectivity between multi-valued objects by taking account of the distribution
of the instances. An $\alpha$-approximation density-based clustering algorithm
(DBCMO) is proposed to efficiently cluster the multi-valued objects by using
global and local R* tree structures. To speed up the algorithm, four pruning
rules on the tree structures are implemented. Empirical studies on synthetic
and real datasets demonstrate that DBCMO can efficiently and effectively
discover the multi-valued object clusters. A comparison with two existing
methods further shows that DBCMO can better handle a continuous decrease in the
cluster density and detect clusters of varying density.
</summary>
    <author>
      <name>Zhilin Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03300v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03300v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05578v1</id>
    <updated>2018-08-16T16:48:56Z</updated>
    <published>2018-08-16T16:48:56Z</published>
    <title>LARNN: Linear Attention Recurrent Neural Network</title>
    <summary>  The Linear Attention Recurrent Neural Network (LARNN) is a recurrent
attention module derived from the Long Short-Term Memory (LSTM) cell and ideas
from the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The
LARNN uses attention on its past cell state values for a limited window size
$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)
cell and the Transformer Network for its Multi-Head Attention Mechanism. The
Multi-Head Attention Mechanism is used inside the cell such that it can query
its own $k$ past values with the attention window. This has the effect of
augmenting the rank of the tensor with the attention mechanism, such that the
cell can perform complex queries to question its previous inner memories, which
should augment the long short-term effect of the memory. With a clever trick,
the LARNN cell with attention can be easily used inside a loop on the cell
state, just like how any other Recurrent Neural Network (RNN) cell can be
looped linearly through time series. This is due to the fact that its state,
which is looped upon throughout time steps within time series, stores the inner
states in a "first in, first out" queue which contains the $k$ most recent
states and on which it is easily possible to add static positional encoding
when the queue is represented as a tensor. This neural architecture yields
better results than the vanilla LSTM cells. It can obtain results of 91.92% for
the test accuracy, compared to the previously attained 91.65% using vanilla
LSTM cells. Note that this is not to compare to other research, where up to
93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3
cells as analyzed here. Finally, an interesting discovery is made, such that
adding activation within the multi-head attention mechanism's linear layers can
yield better results in the context researched hereto.
</summary>
    <author>
      <name>Guillaume Chevalier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05563v1</id>
    <updated>2018-08-16T16:12:39Z</updated>
    <published>2018-08-16T16:12:39Z</published>
    <title>Learning Invariances using the Marginal Likelihood</title>
    <summary>  Generalising well in supervised learning tasks relies on correctly
extrapolating the training data to a large region of the input space. One way
to achieve this is to constrain the predictions to be invariant to
transformations on the input that are known to be irrelevant (e.g.
translation). Commonly, this is done through data augmentation, where the
training set is enlarged by applying hand-crafted transformations to the
inputs. We argue that invariances should instead be incorporated in the model
structure, and learned using the marginal likelihood, which correctly rewards
the reduced complexity of invariant models. We demonstrate this for Gaussian
process models, due to the ease with which their marginal likelihood can be
estimated. Our main contribution is a variational inference scheme for Gaussian
processes containing invariances described by a sampling procedure. We learn
the sampling procedure by back-propagating through it to maximise the marginal
likelihood.
</summary>
    <author>
      <name>Mark van der Wilk</name>
    </author>
    <author>
      <name>Matthias Bauer</name>
    </author>
    <author>
      <name>ST John</name>
    </author>
    <author>
      <name>James Hensman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05537v1</id>
    <updated>2018-08-16T15:20:55Z</updated>
    <published>2018-08-16T15:20:55Z</published>
    <title>Distributionally Adversarial Attack</title>
    <summary>  Recent work on adversarial attack has shown that Projected Gradient Descent
(PGD) Adversary is a universal first-order adversary, and the classifier
adversarially trained by PGD is robust against a wide range of first-order
attacks. However, it is worth noting that the objective of an attacking/defense
model relies on a data distribution, typically in the form of risk
maximization/minimization: $\max\!/\!\min \mathbb{E}_{p(\mathbf{x})}
\mathcal{L}(\mathbf{x})$, with $p(\mathbf{x})$ the data distribution and
$\mathcal{L}(\cdot)$ a loss function. While PGD generates attack samples
independently for each data point, the procedure does not necessary lead to
good generalization in terms of risk maximization. In the paper, we achieve the
goal by proposing distributionally adversarial attack (DAA), a framework to
solve an optimal {\em adversarial data distribution}, a perturbed distribution
that is close to the original data distribution but increases the
generalization risk maximally. Algorithmically, DAA performs optimization on
the space of probability measures, which introduces direct dependency between
all data points when generating adversarial samples. DAA is evaluated by
attacking state-of-the-art defense models, including the adversarially trained
models provided by MadryLab. Notably, DAA outperforms all the attack algorithms
listed in MadryLab's white-box leaderboard, reducing the accuracy of their
secret MNIST model to $88.79\%$ (with $l_\infty$ perturbations of $\epsilon =
0.3$) and the accuracy of their secret CIFAR model to $44.73\%$ (with
$l_\infty$ perturbations of $\epsilon = 8.0$). Code for the experiments is
released on https://github.com/tianzheng4/Distributionally-Adversarial-Attack
</summary>
    <author>
      <name>Tianhang Zheng</name>
    </author>
    <author>
      <name>Changyou Chen</name>
    </author>
    <author>
      <name>Kui Ren</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05535v1</id>
    <updated>2018-08-16T15:19:34Z</updated>
    <published>2018-08-16T15:19:34Z</published>
    <title>Combining time-series and textual data for taxi demand prediction in
  event areas: a deep learning approach</title>
    <summary>  Accurate time-series forecasting is vital for numerous areas of application
such as transportation, energy, finance, economics, etc. However, while modern
techniques are able to explore large sets of temporal data to build forecasting
models, they typically neglect valuable information that is often available
under the form of unstructured text. Although this data is in a radically
different format, it often contains contextual explanations for many of the
patterns that are observed in the temporal data. In this paper, we propose two
deep learning architectures that leverage word embeddings, convolutional layers
and attention mechanisms for combining text information with time-series data.
We apply these approaches for the problem of taxi demand forecasting in event
areas. Using publicly available taxi data from New York, we empirically show
that by fusing these two complementary cross-modal sources of information, the
proposed models are able to significantly reduce the error in the forecasts.
</summary>
    <author>
      <name>Filipe Rodrigues</name>
    </author>
    <author>
      <name>Ioulia Markou</name>
    </author>
    <author>
      <name>Francisco Pereira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.inffus.2018.07.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.inffus.2018.07.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Rodrigues, F., Markou, I., Pereira, F. Combining time-series and
  textual data for taxi demand prediction in event areas: a deep learning
  approach. In Information Fusion, Elsevier, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.05535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11271v2</id>
    <updated>2018-08-16T14:51:32Z</updated>
    <published>2018-04-30T15:21:23Z</published>
    <title>Gaussian Process Behaviour in Wide Deep Neural Networks</title>
    <summary>  Whilst deep neural networks have shown great empirical success, there is
still much work to be done to understand their theoretical properties. In this
paper, we study the relationship between random, wide, fully connected,
feedforward networks with more than one hidden layer and Gaussian processes
with a recursive kernel definition. We show that, under broad conditions, as we
make the architecture increasingly wide, the implied random function converges
in distribution to a Gaussian process, formalising and extending existing
results by Neal (1996) to deep networks. To evaluate convergence rates
empirically, we use maximum mean discrepancy. We then compare finite Bayesian
deep networks from the literature to Gaussian processes in terms of the key
predictive quantities of interest, finding that in some cases the agreement can
be very close. We discuss the desirability of Gaussian process behaviour and
review non-Gaussian alternative models from the literature.
</summary>
    <author>
      <name>Alexander G. de G. Matthews</name>
    </author>
    <author>
      <name>Mark Rowland</name>
    </author>
    <author>
      <name>Jiri Hron</name>
    </author>
    <author>
      <name>Richard E. Turner</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work substantially extends the work of Matthews et al. (2018)
  published at the International Conference on Learning Representations (ICLR)
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.11271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07828v2</id>
    <updated>2018-08-16T14:36:25Z</updated>
    <published>2018-05-20T21:46:29Z</published>
    <title>A VEST of the Pseudoinverse Learning Algorithm</title>
    <summary>  In this paper, we briefly review the basic scheme of the pseudoinverse
learning (PIL) algorithm and present some discussions on the PIL, as well as
its variants. The PIL algorithm, first presented in 1995, is a non-gradient
descent and non-iterative learning algorithm for multi-layer neural networks
and has several advantages compared with gradient descent based algorithms.
Some new viewpoints to PIL algorithm are presented, and several common pitfalls
in practical implementation of the neural network learning task are also
addressed. In addition, we show that so called extreme learning machine is a
Variant crEated by Simple name alTernation (VEST) of the PIL algorithm for
single hidden layer feedforward neural networks.
</summary>
    <author>
      <name>Ping Guo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">School of Systems Science, Beijing Normal University, Beijing, China</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ELM is another name of the PIL</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05443v1</id>
    <updated>2018-08-16T12:28:11Z</updated>
    <published>2018-08-16T12:28:11Z</published>
    <title>Transfer Learning and Organic Computing for Autonomous Vehicles</title>
    <summary>  Autonomous Vehicles(AV) are one of the brightest promises of the future which
would help cut down fatalities and improve travel time while working in
harmony. Autonomous vehicles will face with challenging situations and
experiences not seen before. These experiences should be converted to knowledge
and help the vehicle prepare better in the future. Online Transfer Learning
will help transferring prior knowledge to a new task and also keep the
knowledge updated as the task evolves. This paper presents the different
methods of transfer learning, online transfer learning and organic computing
that could be adapted to the domain of autonomous vehicles.
</summary>
    <author>
      <name>Christofer Fellicious</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, survey of papers and methods in transfer
  learning, organic computing and online transfer learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01020v2</id>
    <updated>2018-08-16T10:50:05Z</updated>
    <published>2018-07-03T08:33:01Z</published>
    <title>Coopetitive Soft Gating Ensemble</title>
    <summary>  In this article, we propose the Coopetititve Soft Gating Ensemble or CSGE for
general machine learning tasks and interwoven systems. The goal of machine
learning is to create models that generalize well for unknown datasets. Often,
however, the problems are too complex to be solved with a single model, so
several models are combined. Similar, Autonomic Computing requires the
integration of different systems. Here, especially, the local, temporal online
evaluation and the resulting (re-)weighting scheme of the CSGE makes the
approach highly applicable for self-improving system integrations. To achieve
the best potential performance the CSGE can be optimized according to arbitrary
loss functions making it accessible for a broader range of problems. We
introduce a novel training procedure including a hyper-parameter initialisation
at its heart. We show that the CSGE approach reaches state-of-the-art
performance for both classification and regression tasks. Further on, the CSGE
provides a human-readable quantification on the influence of all base
estimators employing the three weighting aspects. Moreover, we provide a
scikit-learn compatible implementation.
</summary>
    <author>
      <name>Stephan Deist</name>
    </author>
    <author>
      <name>Maarten Bieshaar</name>
    </author>
    <author>
      <name>Jens Schreiber</name>
    </author>
    <author>
      <name>Andre Gensler</name>
    </author>
    <author>
      <name>Bernhard Sick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures, 4 tables, submitted (accepted for publication) -
  SISSY 2018 - Workshop on Self-Improving System Integration at IEEE ICAC/ SASO
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05403v1</id>
    <updated>2018-08-16T10:25:48Z</updated>
    <published>2018-08-16T10:25:48Z</published>
    <title>Nonconvex Regularization Based Sparse and Low-Rank Recovery in Signal
  Processing, Statistics, and Machine Learning</title>
    <summary>  In the past decade, sparse and low-rank recovery have drawn much attention in
many areas such as signal/image processing, statistics, bioinformatics and
machine learning. To achieve sparsity and/or low-rankness inducing, the
$\ell_1$ norm and nuclear norm are of the most popular regularization penalties
due to their convexity. While the $\ell_1$ and nuclear norm are convenient as
the related convex optimization problems are usually tractable, it has been
shown in many applications that a nonconvex penalty can yield significantly
better performance. In recent, nonconvex regularization based sparse and
low-rank recovery is of considerable interest and it in fact is a main driver
of the recent progress in nonconvex and nonsmooth optimization. This paper
gives an overview of this topic in various fields in signal processing,
statistics and machine learning, including compressive sensing (CS), sparse
regression and variable selection, sparse signals separation, sparse principal
component analysis (PCA), large covariance and inverse covariance matrices
estimation, matrix completion, and robust PCA. We present recent developments
of nonconvex regularization based sparse and low-rank recovery in these fields,
addressing the issues of penalty selection, applications and the convergence of
nonconvex algorithms.
</summary>
    <author>
      <name>Fei Wen</name>
    </author>
    <author>
      <name>Lei Chu</name>
    </author>
    <author>
      <name>Peilin Liu</name>
    </author>
    <author>
      <name>Robert C. Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05906v1</id>
    <updated>2018-08-16T09:02:37Z</updated>
    <published>2018-08-16T09:02:37Z</published>
    <title>Story Disambiguation: Tracking Evolving News Stories across News and
  Social Streams</title>
    <summary>  Following a particular news story online is an important but difficult task,
as the relevant information is often scattered across different domains/sources
(e.g., news articles, blogs, comments, tweets), presented in various formats
and language styles, and may overlap with thousands of other stories. In this
work we join the areas of topic tracking and entity disambiguation, and propose
a framework named Story Disambiguation - a cross-domain story tracking approach
that builds on real-time entity disambiguation and a learning-to-rank framework
to represent and update the rich semantic structure of news stories. Given a
target news story, specified by a seed set of documents, the goal is to
effectively select new story-relevant documents from an incoming document
stream. We represent stories as entity graphs and we model the story tracking
problem as a learning-to-rank task. This enables us to track content with high
accuracy, from multiple domains, in real-time. We study a range of text, entity
and graph based features to understand which type of features are most
effective for representing stories. We further propose new semi-supervised
learning techniques to automatically update the story representation over time.
Our empirical study shows that we outperform the accuracy of state-of-the-art
methods for tracking mixed-domain document streams, while requiring fewer
labeled data to seed the tracked stories. This is particularly the case for
local news stories that are easily over shadowed by other trending stories, and
for complex news stories with ambiguous content in noisy stream environments.
</summary>
    <author>
      <name>Bichen Shi</name>
    </author>
    <author>
      <name>Thanh-Binh Le</name>
    </author>
    <author>
      <name>Neil Hurley</name>
    </author>
    <author>
      <name>Georgiana Ifrim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00121v2</id>
    <updated>2018-08-16T08:16:50Z</updated>
    <published>2018-04-30T22:38:05Z</published>
    <title>A Missing Information Loss function for implicit feedback datasets</title>
    <summary>  Latent factor models for Recommender Systems with implicit feedback typically
treat unobserved user-item interactions (i.e. missing information) as negative
feedback. This is frequently done either through negative sampling (point--wise
loss) or with a ranking loss function (pair-- or list--wise estimation). Since
a zero preference recommendation is a valid solution for most common objective
functions, regarding unknown values as actual zeros results in users having a
zero preference recommendation for most of the available items. In this paper
we propose a novel objective function, the \emph{Missing Information Loss}
(MIL), that explicitly forbids treating unobserved user-item interactions as
positive or negative feedback. We apply this loss to both traditional Matrix
Factorization and user--based Denoising Autoencoder, and compare it with other
established objective functions such as cross-entropy (both point- and
pair-wise) or the recently proposed multinomial log-likelihood. MIL achieves
competitive performance in ranking-aware metrics when applied to three
datasets. Furthermore, we show that such a relevance in the recommendation is
obtained while displaying popular items less frequently (up to a $20 \%$
decrease with respect to the best competing method). This debiasing from the
recommendation of popular items favours the appearance of infrequent items (up
to a $50 \%$ increase of long-tail recommendations), a valuable feature for
Recommender Systems with a large catalogue of products.
</summary>
    <author>
      <name>Juan Arévalo</name>
    </author>
    <author>
      <name>Juan Ramón Duque</name>
    </author>
    <author>
      <name>Marco Creatura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00121v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00121v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05355v1</id>
    <updated>2018-08-16T06:25:24Z</updated>
    <published>2018-08-16T06:25:24Z</published>
    <title>Conceptual Domain Adaptation Using Deep Learning</title>
    <summary>  Deep learning has recently been shown to be instrumental in the problem of
domain adaptation, where the goal is to learn a model on a target domain using
a similar --but not identical-- source domain. The rationale for coupling both
techniques is the possibility of extracting common concepts across domains.
Considering (strictly) local representations, traditional deep learning assumes
common concepts must be captured in the same hidden units. We contend that
jointly training a model with source and target data using a single deep
network is prone to failure when there is inherently lower-level
representational discrepancy between the two domains; such discrepancy leads to
a misalignment of corresponding concepts in separate hidden units. We introduce
a search framework to correctly align high-level representations when training
deep networks; such framework leads to the notion of conceptual --as opposed to
representational-- domain adaptation.
</summary>
    <author>
      <name>Behrang Mehrparvar</name>
    </author>
    <author>
      <name>Ricardo Vilalta</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01367v2</id>
    <updated>2018-08-16T06:21:56Z</updated>
    <published>2018-06-26T03:51:53Z</published>
    <title>EmbNum: Semantic labeling for numerical values with deep metric learning</title>
    <summary>  Semantic labeling for numerical values is a task of assigning semantic labels
to unknown numerical attributes. The semantic labels could be numerical
properties in ontologies, instances in knowledge bases, or labeled data that
are manually annotated by domain experts. In this paper, we refer to semantic
labeling as a retrieval setting where the label of an unknown attribute is
assigned by the label of the most relevant attribute in labeled data. One of
the greatest challenges is that an unknown attribute rarely has the same set of
values with the similar one in the labeled data. To overcome the issue,
statistical interpretation of value distribution is taken into account.
However, the existing studies assume a specific form of distribution. It is not
appropriate in particular to apply open data where there is no knowledge of
data in advance. To address these problems, we propose a neural numerical
embedding model (EmbNum) to learn useful representation vectors for numerical
attributes without prior assumptions on the distribution of data. Then, the
"semantic similarities" between the attributes are measured on these
representation vectors by the Euclidean distance. Our empirical experiments on
City Data and Open Data show that EmbNum significantly outperforms
state-of-the-art methods for the task of numerical attribute semantic labeling
regarding effectiveness and efficiency.
</summary>
    <author>
      <name>Phuc Nguyen</name>
    </author>
    <author>
      <name>Khai Nguyen</name>
    </author>
    <author>
      <name>Ryutaro Ichise</name>
    </author>
    <author>
      <name>Hideaki Takeda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05347v1</id>
    <updated>2018-08-16T04:45:15Z</updated>
    <published>2018-08-16T04:45:15Z</published>
    <title>Tool Breakage Detection using Deep Learning</title>
    <summary>  In manufacture, steel and other metals are mainly cut and shaped during the
fabrication process by computer numerical control (CNC) machines. To keep high
productivity and efficiency of the fabrication process, engineers need to
monitor the real-time process of CNC machines, and the lifetime management of
machine tools. In a real manufacturing process, breakage of machine tools
usually happens without any indication, this problem seriously affects the
fabrication process for many years. Previous studies suggested many different
approaches for monitoring and detecting the breakage of machine tools. However,
there still exists a big gap between academic experiments and the complex real
fabrication processes such as the high demands of real-time detections, the
difficulty in data acquisition and transmission. In this work, we use the
spindle current approach to detect the breakage of machine tools, which has the
high performance of real-time monitoring, low cost, and easy to install. We
analyze the features of the current of a milling machine spindle through tools
wearing processes, and then we predict the status of tool breakage by a
convolutional neural network(CNN). In addition, we use a BP neural network to
understand the reliability of the CNN. The results show that our CNN approach
can detect tool breakage with an accuracy of 93%, while the best performance of
BP is 80%.
</summary>
    <author>
      <name>Guang Li</name>
    </author>
    <author>
      <name>Xin Yang</name>
    </author>
    <author>
      <name>Duanbing Chen</name>
    </author>
    <author>
      <name>Anxing Song</name>
    </author>
    <author>
      <name>Yuke Fang</name>
    </author>
    <author>
      <name>Junlin Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,BCD2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09055v2</id>
    <updated>2018-08-16T04:10:47Z</updated>
    <published>2018-01-27T08:45:07Z</published>
    <title>Solving for multi-class using orthogonal coding matrices</title>
    <summary>  We describe a method of solving for the conditional probabilities in
multi-class classification using orthogonal error-correcting codes. The method
is tested on six different datasets using support vector machines as the binary
classifiers and both the classification results as well as the probability
estimates are found to be more accurate than using random coding matrices, as
predicted by recent literature. Probability estimates are desirable in
statistical classification both for gauging the accuracy of a classification
result and for calibration. Probability estimation using orthogonal codes is
simple and elegant making it faster than the more general constrained
optimization solutions required for arbitrary codes.
</summary>
    <author>
      <name>Peter Mills</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">keywords: multi-class classifier, conditional probabilities, SVM,
  constrained linear least squares, quadratic optimization, error-correcting
  codes</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09055v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09055v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04936v2</id>
    <updated>2018-08-16T03:51:53Z</updated>
    <published>2018-07-13T06:47:06Z</published>
    <title>Non-Gaussian Component Analysis using Entropy Methods</title>
    <summary>  Non-Gaussian component analysis (NGCA) is a problem in multidimensional data
analysis. Since its formulation in 2006, NGCA has attracted considerable
attention in statistics and machine learning. In this problem, we have a random
variable $X$ in $n$-dimensional Euclidean space. There is an unknown subspace
$U$ of the $n$-dimensional Euclidean space such that the orthogonal projection
of $X$ onto $U$ is standard multidimensional Gaussian and the orthogonal
projection of $X$ onto $V$, the orthogonal complement of $U$, is non-Gaussian,
in the sense that all its one-dimensional marginals are different from the
Gaussian in a certain metric defined in terms of moments. The NGCA problem is
to approximate the non-Gaussian subspace $V$ given samples of $X$.
  Vectors in $V$ corresponds to "interesting" directions, whereas vectors in
$U$ correspond to the directions where data is very noisy. The most interesting
applications of the NGCA model is for the case when the magnitude of the noise
is comparable to that of the true signal, a setting in which traditional noise
reduction techniques such as PCA don't apply directly. NGCA is also related to
dimensionality reduction and to other data analysis problems such as ICA.
NGCA-like problems have been studied in statistics for a long time using
techniques such as projection pursuit.
  We give an algorithm that takes polynomial time in the dimension $n$ and has
an inverse polynomial dependence on the error parameter measuring the angle
distance between the non-Gaussian subspace and the subspace output by the
algorithm. Our algorithm is based on relative entropy as the contrast function
and fits under the projection pursuit framework. The techniques we develop for
analyzing our algorithm maybe of use for other related problems.
</summary>
    <author>
      <name>Navin Goyal</name>
    </author>
    <author>
      <name>Abhishek Shetty</name>
    </author>
    <link href="http://arxiv.org/abs/1807.04936v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04936v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03333v3</id>
    <updated>2018-08-16T03:29:23Z</updated>
    <published>2018-08-09T20:11:09Z</published>
    <title>Linked Causal Variational Autoencoder for Inferring Paired Spillover
  Effects</title>
    <summary>  Modeling spillover effects from observational data is an important problem in
economics, business, and other fields of research. % It helps us infer the
causality between two seemingly unrelated set of events. For example, if
consumer spending in the United States declines, it has spillover effects on
economies that depend on the U.S. as their largest export market. In this
paper, we aim to infer the causation that results in spillover effects between
pairs of entities (or units), we call this effect as \textit{paired spillover}.
To achieve this, we leverage the recent developments in variational inference
and deep learning techniques to propose a generative model called Linked Causal
Variational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA
incorporates an encoder neural network to learn the latent attributes and a
decoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the
\textit{latent attributes as confounders that are assumed to affect both the
treatment and the outcome of units}. Specifically, given a pair of units $u$
and $\bar{u}$, their individual treatment and outcomes, the encoder network of
LCVA samples the confounders by conditioning on the observed covariates of $u$,
the treatments of both $u$ and $\bar{u}$ and the outcome of $u$. Once inferred,
the latent attributes (or confounders) of $u$ captures the spillover effect of
$\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde
(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that
LCVA is significantly more robust than existing methods in capturing spillover
effects.
</summary>
    <author>
      <name>Vineeth Rakesh</name>
    </author>
    <author>
      <name>Ruocheng Guo</name>
    </author>
    <author>
      <name>Raha Moraffah</name>
    </author>
    <author>
      <name>Nitin Agarwal</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First two authors contributed equally, 4 pages, CIKM'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03333v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03333v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05334v1</id>
    <updated>2018-08-16T03:25:09Z</updated>
    <published>2018-08-16T03:25:09Z</published>
    <title>Active Distribution Learning from Indirect Samples</title>
    <summary>  This paper studies the problem of {\em learning} the probability distribution
$P_X$ of a discrete random variable $X$ using indirect and sequential samples.
At each time step, we choose one of the possible $K$ functions, $g_1, \ldots,
g_K$ and observe the corresponding sample $g_i(X)$. The goal is to estimate the
probability distribution of $X$ by using a minimum number of such sequential
samples. This problem has several real-world applications including inference
under non-precise information and privacy-preserving statistical estimation. We
establish necessary and sufficient conditions on the functions $g_1, \ldots,
g_K$ under which asymptotically consistent estimation is possible. We also
derive lower bounds on the estimation error as a function of total samples and
show that it is order-wise achievable. Leveraging these results, we propose an
iterative algorithm that i) chooses the function to observe at each step based
on past observations; and ii) combines the obtained samples to estimate $p_X$.
The performance of this algorithm is investigated numerically under various
scenarios, and shown to outperform baseline approaches.
</summary>
    <author>
      <name>Samarth Gupta</name>
    </author>
    <author>
      <name>Gauri Joshi</name>
    </author>
    <author>
      <name>Osman Yağan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Allerton Conference on Communication, Control and Computing, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05329v1</id>
    <updated>2018-08-16T02:58:54Z</updated>
    <published>2018-08-16T02:58:54Z</published>
    <title>Sequential Behavioral Data Processing Using Deep Learning and the Markov
  Transition Field in Online Fraud Detection</title>
    <summary>  Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.
</summary>
    <author>
      <name>Ruinan Zhang</name>
    </author>
    <author>
      <name>Fanglan Zheng</name>
    </author>
    <author>
      <name>Wei Min</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD2018 Data Science in Fintech Workshop Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06536v1</id>
    <updated>2018-08-15T23:49:51Z</updated>
    <published>2018-08-15T23:49:51Z</published>
    <title>Study of Set-Membership Adaptive Kernel Algorithms</title>
    <summary>  In the last decade, a considerable research effort has been devoted to
developing adaptive algorithms based on kernel functions. One of the main
features of these algorithms is that they form a family of universal
approximation techniques, solving problems with nonlinearities elegantly. In
this paper, we present data-selective adaptive kernel normalized least-mean
square (KNLMS) algorithms that can increase their learning rate and reduce
their computational complexity. In fact, these methods deal with kernel
expansions, creating a growing structure also known as the dictionary, whose
size depends on the number of observations and their innovation. The algorithms
described herein use an adaptive step-size to accelerate the learning and can
offer an excellent tradeoff between convergence speed and steady state, which
allows them to solve nonlinear filtering and estimation problems with a large
number of parameters without requiring a large computational cost. The
data-selective update scheme also limits the number of operations performed and
the size of the dictionary created by the kernel expansion, saving
computational resources and dealing with one of the major problems of kernel
adaptive algorithms. A statistical analysis is carried out along with a
computational complexity analysis of the proposed algorithms. Simulations show
that the proposed KNLMS algorithms outperform existing algorithms in examples
of nonlinear system identification and prediction of a time series originating
from a nonlinear difference equation.
</summary>
    <author>
      <name>A. Flores</name>
    </author>
    <author>
      <name>R. C. de Lamare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10251v2</id>
    <updated>2018-08-15T20:55:00Z</updated>
    <published>2018-07-26T17:22:29Z</published>
    <title>Aggregated Learning: A Vector Quantization Approach to Learning with
  Neural Networks</title>
    <summary>  We establish an equivalence between information bottleneck (IB) learning and
an unconventional quantization problem, `IB quantization'. Under this
equivalence, standard neural network models correspond to scalar IB quantizers.
We prove a coding theorem for IB quantization, which implies that scalar IB
quantizers are in general inferior to vector IB quantizers. This inspires us to
develop a learning framework for neural networks, AgrLearn, that corresponds to
vector IB quantizers. We experimentally verify that AgrLearn applied to some
deep network models of current art improves upon them, while requiring less
training data. With a heuristic smoothing, AgrLearn further improves its
performance, resulting in new state of the art in image classification on
Cifar10.
</summary>
    <author>
      <name>Hongyu Guo</name>
    </author>
    <author>
      <name>Yongyi Mao</name>
    </author>
    <author>
      <name>Richong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05274v1</id>
    <updated>2018-08-15T19:55:41Z</updated>
    <published>2018-08-15T19:55:41Z</published>
    <title>Frank-Wolfe Style Algorithms for Large Scale Optimization</title>
    <summary>  We introduce a few variants on Frank-Wolfe style algorithms suitable for
large scale optimization. We show how to modify the standard Frank-Wolfe
algorithm using stochastic gradients, approximate subproblem solutions, and
sketched decision variables in order to scale to enormous problems while
preserving (up to constants) the optimal convergence rate
$\mathcal{O}(\frac{1}{k})$.
</summary>
    <author>
      <name>Lijun Ding</name>
    </author>
    <author>
      <name>Madeleine Udell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures, a chapter of the book "Large-Scale and
  Distributed Optimization", Springer's Lecture Notes in Mathematics Series,
  volume 2227, https://www.springer.com/us/book/9783319974774</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10588v2</id>
    <updated>2018-08-15T19:41:38Z</updated>
    <published>2018-07-18T20:16:00Z</published>
    <title>A Modality-Adaptive Method for Segmenting Brain Tumors and
  Organs-at-Risk in Radiation Therapy Planning</title>
    <summary>  In this paper we present a method for simultaneously segmenting brain tumors
and an extensive set of organs-at-risk for radiation therapy planning of
glioblastomas. The method combines a contrast-adaptive generative model for
whole-brain segmentation with a new spatial regularization model of tumor shape
using convolutional restricted Boltzmann machines. We demonstrate
experimentally that the method is able to adapt to image acquisitions that
differ substantially from any available training data, ensuring its
applicability across treatment sites; that its tumor segmentation accuracy is
comparable to that of the current state of the art; and that it captures most
organs-at-risk sufficiently well for radiation therapy planning purposes. The
proposed method may be a valuable step towards automating the delineation of
brain tumors and organs-at-risk in glioblastoma patients undergoing radiation
therapy.
</summary>
    <author>
      <name>Mikael Agn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Per Munck af Rosenschöld</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radiation Physics, Department of Hematology, Oncology and Radiation Physics, Skåne University Hospital, Lund, Sweden</arxiv:affiliation>
    </author>
    <author>
      <name>Oula Puonti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Michael J. Lundemann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Oncology, Copenhagen University Hospital Rigshospitalet, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Laura Mancini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroradiological Academic Unit, Department of Brain Repair and Rehabilitation, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Anastasia Papadaki</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroradiological Academic Unit, Department of Brain Repair and Rehabilitation, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Steffi Thust</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroradiological Academic Unit, Department of Brain Repair and Rehabilitation, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, UK</arxiv:affiliation>
    </author>
    <author>
      <name>John Ashburner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Wellcome Centre for Human Neuroimaging, UCL Institute of Neurology, University College London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Law</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Clinical Physiology, Nuclear Medicine and PET, Copenhagen University Hospital Rigshospitalet, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Koen Van Leemput</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Harvard Medical School, USA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">corrected one reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05264v1</id>
    <updated>2018-08-15T19:22:15Z</updated>
    <published>2018-08-15T19:22:15Z</published>
    <title>DeepDownscale: a Deep Learning Strategy for High-Resolution Weather
  Forecast</title>
    <summary>  Running high-resolution physical models is computationally expensive and
essential for many disciplines. Agriculture, transportation, and energy are
sectors that depend on high-resolution weather models, which typically consume
many hours of large High Performance Computing (HPC) systems to deliver timely
results. Many users cannot afford to run the desired resolution and are forced
to use low resolution output. One simple solution is to interpolate results for
visualization. It is also possible to combine an ensemble of low resolution
models to obtain a better prediction. However, these approaches fail to capture
the redundant information and patterns in the low-resolution input that could
help improve the quality of prediction. In this paper, we propose and evaluate
a strategy based on a deep neural network to learn a high-resolution
representation from low-resolution predictions using weather forecast as a
practical use case. We take a supervised learning approach, since obtaining
labeled data can be done automatically. Our results show significant
improvement when compared with standard practices and the strategy is still
lightweight enough to run on modest computer systems.
</summary>
    <author>
      <name>Eduardo R. Rodrigues</name>
    </author>
    <author>
      <name>Igor Oliveira</name>
    </author>
    <author>
      <name>Renato L. F. Cunha</name>
    </author>
    <author>
      <name>Marco A. S. Netto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, accepted for publication at 14th IEEE eScience</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05394v2</id>
    <updated>2018-08-15T17:19:40Z</updated>
    <published>2018-06-14T07:04:31Z</published>
    <title>Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables
  Signal Propagation in Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks have gained widespread use in modeling sequence
data across various domains. While many successful recurrent architectures
employ a notion of gating, the exact mechanism that enables such remarkable
performance is not well understood. We develop a theory for signal propagation
in recurrent networks after random initialization using a combination of mean
field theory and random matrix theory. To simplify our discussion, we introduce
a new RNN cell with a simple gating mechanism that we call the minimalRNN and
compare it with vanilla RNNs. Our theory allows us to define a maximum
timescale over which RNNs can remember an input. We show that this theory
predicts trainability for both recurrent architectures. We show that gated
recurrent networks feature a much broader, more robust, trainable region than
vanilla RNNs, which corroborates recent experimental findings. Finally, we
develop a closed-form critical initialization scheme that achieves dynamical
isometry in both vanilla RNNs and minimalRNNs. We show that this results in
significantly improvement in training dynamics. Finally, we demonstrate that
the minimalRNN achieves comparable performance to its more complex
counterparts, such as LSTMs or GRUs, on a language modeling task.
</summary>
    <author>
      <name>Minmin Chen</name>
    </author>
    <author>
      <name>Jeffrey Pennington</name>
    </author>
    <author>
      <name>Samuel S. Schoenholz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018 Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05394v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05394v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04272v2</id>
    <updated>2018-08-15T16:24:39Z</updated>
    <published>2018-05-11T08:28:55Z</published>
    <title>An $O(N)$ Sorting Algorithm: Machine Learning Sort</title>
    <summary>  We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method,
which shows a huge potential sorting big data. This sorting algorithm can be
applied to parallel sorting and is suitable for GPU or TPU acceleration.
Furthermore, we discuss the application of this algorithm to sparse hash table.
</summary>
    <author>
      <name>Hanqing Zhao</name>
    </author>
    <author>
      <name>Yuehan Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02234v2</id>
    <updated>2018-08-15T15:44:25Z</updated>
    <published>2018-08-07T07:15:54Z</published>
    <title>Deep Stacked Stochastic Configuration Networks for Non-Stationary Data
  Streams</title>
    <summary>  The concept of stochastic configuration networks (SCNs) others a solid
framework for fast implementation of feedforward neural networks through
randomized learning. Unlike conventional randomized approaches, SCNs provide an
avenue to select appropriate scope of random parameters to ensure the universal
approximation property. In this paper, a deep version of stochastic
configuration networks, namely deep stacked stochastic configuration network
(DSSCN), is proposed for modeling non-stationary data streams. As an extension
of evolving stochastic connfiguration networks (eSCNs), this work contributes a
way to grow and shrink the structure of deep stochastic configuration networks
autonomously from data streams. The performance of DSSCN is evaluated by six
benchmark datasets. Simulation results, compared with prominent data stream
algorithms, show that the proposed method is capable of achieving comparable
accuracy and evolving compact and parsimonious deep stacked network
architecture.
</summary>
    <author>
      <name>Mahardhika Pratama</name>
    </author>
    <author>
      <name>Dianhui Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02234v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02234v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05128v1</id>
    <updated>2018-08-15T15:20:49Z</updated>
    <published>2018-08-15T15:20:49Z</published>
    <title>Using Regular Languages to Explore the Representational Capacity of
  Recurrent Neural Architectures</title>
    <summary>  The presence of Long Distance Dependencies (LDDs) in sequential data poses
significant challenges for computational models. Various recurrent neural
architectures have been designed to mitigate this issue. In order to test these
state-of-the-art architectures, there is growing need for rich benchmarking
datasets. However, one of the drawbacks of existing datasets is the lack of
experimental control with regards to the presence and/or degree of LDDs. This
lack of control limits the analysis of model performance in relation to the
specific challenge posed by LDDs. One way to address this is to use synthetic
data having the properties of subregular languages. The degree of LDDs within
the generated data can be controlled through the k parameter, length of the
generated strings, and by choosing appropriate forbidden strings. In this
paper, we explore the capacity of different RNN extensions to model LDDs, by
evaluating these models on a sequence of SPk synthesized datasets, where each
subsequent dataset exhibits a longer degree of LDD. Even though SPk are simple
languages, the presence of LDDs does have significant impact on the performance
of recurrent neural architectures, thus making them prime candidate in
benchmarking tasks.
</summary>
    <author>
      <name>Abhijit Mahalunkar</name>
    </author>
    <author>
      <name>John D. Kelleher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference of Artificial Neural Networks (ICANN) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09762v2</id>
    <updated>2018-08-15T15:11:58Z</updated>
    <published>2018-06-26T02:22:13Z</published>
    <title>Boulevard: Regularized Stochastic Gradient Boosted Trees and Their
  Limiting Distribution</title>
    <summary>  This paper examines a novel gradient boosting framework for regression. We
regularize gradient boosted trees by introducing subsampling and employ a
modified shrinkage algorithm so that at every boosting stage the estimate is
given by an average of trees. The resulting algorithm, titled Boulevard, is
shown to converge as the number of trees grows. We also demonstrate a central
limit theorem for this limit, allowing a characterization of uncertainty for
predictions. A simulation study and real world examples provide support for
both the predictive accuracy of the model and its limiting behavior.
</summary>
    <author>
      <name>Yichen Zhou</name>
    </author>
    <author>
      <name>Giles Hooker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 5 figures. Fixed a typo in Theorem 4.2</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09762v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09762v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05110v1</id>
    <updated>2018-08-15T14:54:44Z</updated>
    <published>2018-08-15T14:54:44Z</published>
    <title>Joint &amp; Progressive Learning from High-Dimensional Data for Multi-Label
  Classification</title>
    <summary>  Despite the fact that nonlinear subspace learning techniques (e.g. manifold
learning) have successfully applied to data representation, there is still room
for improvement in explainability (explicit mapping), generalization
(out-of-samples), and cost-effectiveness (linearization). To this end, a novel
linearized subspace learning technique is developed in a joint and progressive
way, called \textbf{j}oint and \textbf{p}rogressive \textbf{l}earning
str\textbf{a}teg\textbf{y} (J-Play), with its application to multi-label
classification. The J-Play learns high-level and semantically meaningful
feature representation from high-dimensional data by 1) jointly performing
multiple subspace learning and classification to find a latent subspace where
samples are expected to be better classified; 2) progressively learning
multi-coupled projections to linearly approach the optimal mapping bridging the
original space with the most discriminative subspace; 3) locally embedding
manifold structure in each learnable latent subspace. Extensive experiments are
performed to demonstrate the superiority and effectiveness of the proposed
method in comparison with previous state-of-the-art methods.
</summary>
    <author>
      <name>Danfeng Hong</name>
    </author>
    <author>
      <name>Naoto Yokoya</name>
    </author>
    <author>
      <name>Jian Xu</name>
    </author>
    <author>
      <name>Xiaoxiang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted in ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05480v1</id>
    <updated>2018-08-15T12:59:14Z</updated>
    <published>2018-08-15T12:59:14Z</published>
    <title>A novel Empirical Bayes with Reversible Jump Markov Chain in User-Movie
  Recommendation system</title>
    <summary>  In this article we select the unknown dimension of the feature by re-
versible jump MCMC inside a simulated annealing in bayesian set up of
collaborative filter. We implement the same in MovieLens small dataset. We also
tune the hyper parameter by using a modified empirical bayes. It can also be
used to guess an initial choice for hyper-parameters in grid search procedure
even for the datasets where MCMC oscillates around the true value or takes long
time to converge.
</summary>
    <author>
      <name>Arabin Kumar Dey</name>
    </author>
    <author>
      <name>Himanshu Jhamb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1707.02294</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05054v1</id>
    <updated>2018-08-15T12:25:02Z</updated>
    <published>2018-08-15T12:25:02Z</published>
    <title>Shedding Light on Black Box Machine Learning Algorithms: Development of
  an Axiomatic Framework to Assess the Quality of Methods that Explain
  Individual Predictions</title>
    <summary>  From self-driving vehicles and back-flipping robots to virtual assistants who
book our next appointment at the hair salon or at that restaurant for dinner -
machine learning systems are becoming increasingly ubiquitous. The main reason
for this is that these methods boast remarkable predictive capabilities.
However, most of these models remain black boxes, meaning that it is very
challenging for humans to follow and understand their intricate inner workings.
Consequently, interpretability has suffered under this ever-increasing
complexity of machine learning models. Especially with regards to new
regulations, such as the General Data Protection Regulation (GDPR), the
necessity for plausibility and verifiability of predictions made by these black
boxes is indispensable. Driven by the needs of industry and practice, the
research community has recognised this interpretability problem and focussed on
developing a growing number of so-called explanation methods over the past few
years. These methods explain individual predictions made by black box machine
learning models and help to recover some of the lost interpretability. With the
proliferation of these explanation methods, it is, however, often unclear,
which explanation method offers a higher explanation quality, or is generally
better-suited for the situation at hand. In this thesis, we thus propose an
axiomatic framework, which allows comparing the quality of different
explanation methods amongst each other. Through experimental validation, we
find that the developed framework is useful to assess the explanation quality
of different explanation methods and reach conclusions that are consistent with
independent research.
</summary>
    <author>
      <name>Milo Honegger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: black box, machine learning, interpretability, explanation
  methods, explanation quality, axiomatic explanation consistency</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00245v2</id>
    <updated>2018-08-15T10:06:20Z</updated>
    <published>2018-08-01T09:43:50Z</published>
    <title>Robbins-Mobro conditions for persistent exploration learning strategies</title>
    <summary>  We formulate simple assumptions, implying the Robbins-Monro conditions for
the $Q$-learning algorithm with the local learning rate, depending on the
number of visits of a particular state-action pair (local clock) and the number
of iteration (global clock). It is assumed that the Markov decision process is
communicating and the learning policy ensures the persistent exploration. The
restrictions are imposed on the functional dependence of the learning rate on
the local and global clocks. The result partially confirms the conjecture of
Bradkte (1994).
</summary>
    <author>
      <name>Dmitry B. Rokhlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Theorem 2 was improved</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00245v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00245v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="93E35, 62L20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09802v1</id>
    <updated>2018-08-15T07:36:01Z</updated>
    <published>2018-08-15T07:36:01Z</published>
    <title>Modelling Irregular Spatial Patterns using Graph Convolutional Neural
  Networks</title>
    <summary>  The understanding of geographical reality is a process of data representation
and pattern discovery. Former studies mainly adopted continuous-field models to
represent spatial variables and to investigate the underlying spatial
continuity/heterogeneity in the regular spatial domain. In this article, we
introduce a more generalized model based on graph convolutional neural networks
(GCNs) that can capture the complex parameters of spatial patterns underlying
graph-structured spatial data, which generally contain both Euclidean spatial
information and non-Euclidean feature information. A trainable semi-supervised
prediction framework is proposed to model the spatial distribution patterns of
intra-urban points of interest(POI) check-ins. This work demonstrates the
feasibility of GCNs in complex geographic decision problems and provides a
promising tool to analyze irregular spatial data.
</summary>
    <author>
      <name>Di Zhu</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, preprint for arxiv</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06537v1</id>
    <updated>2018-08-15T04:23:31Z</updated>
    <published>2018-08-15T04:23:31Z</published>
    <title>Ricean K-factor Estimation based on Channel Quality Indicator in OFDM
  Systems using Neural Network</title>
    <summary>  Ricean channel model is widely used in wireless communications to
characterize the channels with a line-of-sight path. The Ricean K factor,
defined as the ratio of direct path and scattered paths, provides a good
indication of the link quality. Most existing works estimate K factor based on
either maximum-likelihood criterion or higher-order moments, and the existing
works are targeted at K-factor estimation at receiver side. In this work, a
novel approach is proposed. Cast as a classification problem, the estimation of
K factor by neural network provides high accuracy. Moreover, the proposed
K-factor estimation is done at transmitter side for transmit processing, thus
saving the limited feedback bandwidth.
</summary>
    <author>
      <name>Kun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12111v3</id>
    <updated>2018-08-15T03:00:45Z</updated>
    <published>2018-05-24T04:03:39Z</published>
    <title>Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend
  Prediction of Critical Metal Companies</title>
    <summary>  The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more "critical," and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different "advisors." creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
</summary>
    <author>
      <name>Zhengyang Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Major revision. Methodology was optimized, more case study companies
  were added, and more experiments were conducted for more detailed method
  analyses. Changed the writing to focus more on stock prediction and less on
  critical metals</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.12111v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12111v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04947v1</id>
    <updated>2018-08-15T02:15:45Z</updated>
    <published>2018-08-15T02:15:45Z</published>
    <title>Collapse of Deep and Narrow Neural Nets</title>
    <summary>  Recent theoretical work has demonstrated that deep neural networks have
superior performance over shallow networks, but their training is more
difficult, e.g., they suffer from the vanishing gradient problem. This problem
can be typically resolved by the rectified linear unit (ReLU) activation.
However, here we show that even for such activation, deep and narrow neural
networks will converge to erroneous mean or median states of the target
function depending on the loss with high probability. We demonstrate this
collapse of deep and narrow neural networks both numerically and theoretically,
and provide estimates of the probability of collapse. We also construct a
diagram of a safe region of designing neural networks that avoid the collapse
to erroneous states. Finally, we examine different ways of initialization and
normalization that may avoid the collapse problem.
</summary>
    <author>
      <name>Lu Lu</name>
    </author>
    <author>
      <name>Yanhui Su</name>
    </author>
    <author>
      <name>George Em Karniadakis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04855v2</id>
    <updated>2018-08-15T01:15:27Z</updated>
    <published>2018-07-12T22:57:19Z</published>
    <title>A feature agnostic approach for glaucoma detection in OCT volumes</title>
    <summary>  Optical coherence tomography (OCT) based measurements of retinal layer
thickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell
with inner plexiform layer (GCIPL) are commonly used for the diagnosis and
monitoring of glaucoma. Previously, machine learning techniques have utilized
segmentation-based imaging features such as the peripapillary RNFL thickness
and the cup-to-disc ratio. Here, we propose a deep learning technique that
classifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT
volumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network
(CNN). We compared the accuracy of this technique with various feature-based
machine learning algorithms and demonstrated the superiority of the proposed
deep learning based method.
  Logistic regression was found to be the best performing classical machine
learning technique with an AUC of 0.89. In direct comparison, the deep learning
approach achieved a substantially higher AUC of 0.94 with the additional
advantage of providing insight into which regions of an OCT volume are
important for glaucoma detection.
  Computing Class Activation Maps (CAM), we found that the CNN identified
neuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and
its surrounding areas as the regions significantly associated with the glaucoma
classification. These regions anatomically correspond to the well established
and commonly used clinical markers for glaucoma diagnosis such as increased cup
volume, cup diameter, and neuroretinal rim thinning at the superior and
inferior segments.
</summary>
    <author>
      <name>Stefan Maetschke</name>
    </author>
    <author>
      <name>Bhavna Antony</name>
    </author>
    <author>
      <name>Hiroshi Ishikawa</name>
    </author>
    <author>
      <name>Gadi Wollstein</name>
    </author>
    <author>
      <name>Joel S. Schuman</name>
    </author>
    <author>
      <name>Rahil Garvani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04855v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04855v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04928v1</id>
    <updated>2018-08-15T00:10:55Z</updated>
    <published>2018-08-15T00:10:55Z</published>
    <title>Deep EHR: Chronic Disease Prediction Using Medical Notes</title>
    <summary>  Early detection of preventable diseases is important for better disease
management, improved inter-ventions, and more efficient health-care resource
allocation. Various machine learning approacheshave been developed to utilize
information in Electronic Health Record (EHR) for this task. Majorityof
previous attempts, however, focus on structured fields and lose the vast amount
of information inthe unstructured notes. In this work we propose a general
multi-task framework for disease onsetprediction that combines both free-text
medical notes and structured information. We compareperformance of different
deep learning architectures including CNN, LSTM and hierarchical models.In
contrast to traditional text-based prediction models, our approach does not
require disease specificfeature engineering, and can handle negations and
numerical values that exist in the text. Ourresults on a cohort of about 1
million patients show that models using text outperform modelsusing just
structured data, and that models capable of using numerical values and
negations in thetext, in addition to the raw text, further improve performance.
Additionally, we compare differentvisualization methods for medical
professionals to interpret model predictions.
</summary>
    <author>
      <name>Jingshu Liu</name>
    </author>
    <author>
      <name>Zachariah Zhang</name>
    </author>
    <author>
      <name>Narges Razavian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Health Care conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05637v2</id>
    <updated>2018-08-15T00:02:46Z</updated>
    <published>2018-02-15T16:19:21Z</published>
    <title>cGANs with Projection Discriminator</title>
    <summary>  We propose a novel, projection based way to incorporate the conditional
information into the discriminator of GANs that respects the role of the
conditional information in the underlining probabilistic model. This approach
is in contrast with most frameworks of conditional GANs used in application
today, which use the conditional information by concatenating the (embedded)
conditional vector to the feature vectors. With this modification, we were able
to significantly improve the quality of the class conditional image generation
on ILSVRC2012 (ImageNet) 1000-class image dataset from the current
state-of-the-art result, and we achieved this with a single pair of a
discriminator and a generator. We were also able to extend the application to
super-resolution and succeeded in producing highly discriminative
super-resolution images. This new structure also enabled high quality category
transformation based on parametric functional transformation of conditional
batch normalization layers in the generator.
</summary>
    <author>
      <name>Takeru Miyato</name>
    </author>
    <author>
      <name>Masanori Koyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05637v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05637v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07519v4</id>
    <updated>2018-08-14T23:07:39Z</updated>
    <published>2018-03-20T16:52:12Z</published>
    <title>DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems</title>
    <summary>  Deep learning (DL) defines a new data-driven programming paradigm that
constructs the internal system logic of a crafted neuron network through a set
of training data. We have seen wide adoption of DL in many safety-critical
scenarios. However, a plethora of studies have shown that the state-of-the-art
DL systems suffer from various vulnerabilities which can lead to severe
consequences when applied to real-world applications. Currently, the testing
adequacy of a DL system is usually measured by the accuracy of test data.
Considering the limitation of accessible high quality test data, good accuracy
performance on test data can hardly provide confidence to the testing adequacy
and generality of DL systems. Unlike traditional software systems that have
clear and controllable logic and functionality, the lack of interpretability in
a DL system makes system analysis and defect detection difficult, which could
potentially hinder its real-world deployment. In this paper, we propose
DeepGauge, a set of multi-granularity testing criteria for DL systems, which
aims at rendering a multi-faceted portrayal of the testbed. The in-depth
evaluation of our proposed testing criteria is demonstrated on two well-known
datasets, five DL systems, and with four state-of-the-art adversarial attack
techniques against DL. The potential usefulness of DeepGauge sheds light on the
construction of more generic and robust DL systems.
</summary>
    <author>
      <name>Lei Ma</name>
    </author>
    <author>
      <name>Felix Juefei-Xu</name>
    </author>
    <author>
      <name>Fuyuan Zhang</name>
    </author>
    <author>
      <name>Jiyuan Sun</name>
    </author>
    <author>
      <name>Minhui Xue</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Chunyang Chen</name>
    </author>
    <author>
      <name>Ting Su</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Jianjun Zhao</name>
    </author>
    <author>
      <name>Yadong Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3238147.3238202</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3238147.3238202" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 33rd IEEE/ACM International Conference on Automated Software
  Engineering (ASE 2018)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DeepGauge: Multi-Granularity Testing Criteria for Deep Learning
  Systems. In Proceedings of the 33rd ACM/IEEE International Conference on
  Automated Software Engineering (ASE 18), September 3-7, 2018, Montpellier,
  France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.07519v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07519v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04888v1</id>
    <updated>2018-08-14T20:39:17Z</updated>
    <published>2018-08-14T20:39:17Z</published>
    <title>Skill Rating for Generative Models</title>
    <summary>  We explore a new way to evaluate generative models using insights from
evaluation of competitive games between human players. We show experimentally
that tournaments between generators and discriminators provide an effective way
to evaluate generative models. We introduce two methods for summarizing
tournament outcomes: tournament win rate and skill rating. Evaluations are
useful in different contexts, including monitoring the progress of a single
model as it learns during the training process, and comparing the capabilities
of two different fully trained models. We show that a tournament consisting of
a single model playing against past and future versions of itself produces a
useful measure of training progress. A tournament containing multiple separate
models (using different seeds, hyperparameters, and architectures) provides a
useful relative comparison between different trained GANs. Tournament-based
rating methods are conceptually distinct from numerous previous categories of
approaches to evaluation of generative models, and have complementary
advantages and disadvantages.
</summary>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Surya Bhupatiraju</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Augustus Odena</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04880v1</id>
    <updated>2018-08-14T20:08:33Z</updated>
    <published>2018-08-14T20:08:33Z</published>
    <title>A Precision Environment-Wide Association Study of Hypertension via
  Supervised Cadre Models</title>
    <summary>  We consider the problem in precision health of grouping people into
subpopulations based on their degree of vulnerability to a risk factor. These
subpopulations cannot be discovered with traditional clustering techniques
because their quality is evaluated with a supervised metric: the ease of
modeling a response variable over observations within them. Instead, we apply
the supervised cadre model (SCM), which does use this metric. We extend the SCM
formalism so that it may be applied to multivariate regression and binary
classification problems. We also develop a way to use conditional entropy to
assess the confidence in the process by which a subject is assigned their
cadre. Using the SCM, we generalize the environment-wide association study
(EWAS) workflow to be able to model heterogeneity in population risk. In our
EWAS, we consider more than two hundred environmental exposure factors and find
their association with diastolic blood pressure, systolic blood pressure, and
hypertension. This requires adapting the SCM to be applicable to data generated
by a complex survey design. After correcting for false positives, we found 25
exposure variables that had a significant association with at least one of our
response variables. Eight of these were significant for a discovered
subpopulation but not for the overall population. Some of these associations
have been identified by previous researchers, while others appear to be novel
associations. We examine several learned subpopulations in detail, and we find
that they are interpretable and that they suggest further research questions.
</summary>
    <author>
      <name>Alexander New</name>
    </author>
    <author>
      <name>Kristin P. Bennett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04875v1</id>
    <updated>2018-08-14T19:55:38Z</updated>
    <published>2018-08-14T19:55:38Z</published>
    <title>Multi-user Communication Networks: A Coordinated Multi-armed Bandit
  Approach</title>
    <summary>  Communication networks shared by many users are a widespread challenge
nowadays. In this paper we address several aspects of this challenge
simultaneously: learning unknown stochastic network characteristics, sharing
resources with other users while keeping coordination overhead to a minimum.
The proposed solution combines Multi-Armed Bandit learning with a lightweight
signalling-based coordination scheme, and ensures convergence to a stable
allocation of resources. Our work considers single-user level algorithms for
two scenarios: an unknown fixed number of users, and a dynamic number of users.
Analytic performance guarantees, proving convergence to stable marriage
configurations, are presented for both setups. The algorithms are designed
based on a system-wide perspective, rather than focusing on single user
welfare. Thus, maximal resource utilization is ensured. An extensive
experimental analysis covers convergence to a stable configuration as well as
reward maximization. Experiments are carried out over a wide range of setups,
demonstrating the advantages of our approach over existing state-of-the-art
methods.
</summary>
    <author>
      <name>Orly Avner</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04873v1</id>
    <updated>2018-08-14T19:41:12Z</updated>
    <published>2018-08-14T19:41:12Z</published>
    <title>Generalization of Equilibrium Propagation to Vector Field Dynamics</title>
    <summary>  The biological plausibility of the backpropagation algorithm has long been
doubted by neuroscientists. Two major reasons are that neurons would need to
send two different types of signal in the forward and backward phases, and that
pairs of neurons would need to communicate through symmetric bidirectional
connections. We present a simple two-phase learning procedure for fixed point
recurrent networks that addresses both these issues. In our model, neurons
perform leaky integration and synaptic weights are updated through a local
mechanism. Our learning method generalizes Equilibrium Propagation to vector
field dynamics, relaxing the requirement of an energy function. As a
consequence of this generalization, the algorithm does not compute the true
gradient of the objective function, but rather approximates it at a precision
which is proven to be directly related to the degree of symmetry of the
feedforward and feedback weights. We show experimentally that our algorithm
optimizes the objective function.
</summary>
    <author>
      <name>Benjamin Scellier</name>
    </author>
    <author>
      <name>Anirudh Goyal</name>
    </author>
    <author>
      <name>Jonathan Binas</name>
    </author>
    <author>
      <name>Thomas Mesnard</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04866v1</id>
    <updated>2018-08-14T19:20:35Z</updated>
    <published>2018-08-14T19:20:35Z</published>
    <title>Mitigating Sybils in Federated Learning Poisoning</title>
    <summary>  Machine learning (ML) over distributed data is relevant to a variety of
domains. Existing approaches, such as federated learning, compose the outputs
computed by a group of devices at a central aggregator and run multi-round
algorithms to generate a globally shared model. Unfortunately, such approaches
are susceptible to a variety of attacks, including model poisoning, which is
made substantially worse in the presence of sybils.
  In this paper we first evaluate the vulnerability of federated learning to
sybil-based poisoning attacks. We then describe FoolsGold, a novel defense to
this problem that identifies poisoning sybils based on the diversity of client
contributions in the distributed learning process. Unlike prior work, our
system does not assume that the attackers are in the minority, requires no
auxiliary information outside of the learning process, and makes fewer
assumptions about clients and their data.
  In our evaluation we show that FoolsGold exceeds the capabilities of existing
state of the art approaches to countering ML poisoning attacks. Our results
hold for a variety of conditions, including different distributions of data,
varying poisoning targets, and various attack strategies.
</summary>
    <author>
      <name>Clement Fung</name>
    </author>
    <author>
      <name>Chris J. M. Yoon</name>
    </author>
    <author>
      <name>Ivan Beschastnikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10837v2</id>
    <updated>2018-08-14T19:03:32Z</updated>
    <published>2018-03-28T20:14:08Z</published>
    <title>Learning Deep Representations with Probabilistic Knowledge Transfer</title>
    <summary>  Knowledge Transfer (KT) techniques tackle the problem of transferring the
knowledge from a large and complex neural network into a smaller and faster
one. However, existing KT methods are tailored towards classification tasks and
they cannot be used efficiently for other representation learning tasks. In
this paper a novel knowledge transfer technique, that is capable of training a
student model that maintains the same amount of mutual information between the
learned representation and a set of (possible unknown) labels as the teacher
model, is proposed. Apart from outperforming existing KT techniques, the
proposed method allows for overcoming several limitations of existing methods
providing new insight into KT as well as novel KT applications, ranging from
knowledge transfer from handcrafted feature extractors to {cross-modal} KT from
the textual modality into the representation extracted from the visual modality
of the data.
</summary>
    <author>
      <name>Nikolaos Passalis</name>
    </author>
    <author>
      <name>Anastasios Tefas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.10837v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10837v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04732v2</id>
    <updated>2018-08-14T18:44:12Z</updated>
    <published>2018-04-12T21:17:54Z</published>
    <title>Multimodal Unsupervised Image-to-Image Translation</title>
    <summary>  Unsupervised image-to-image translation is an important and challenging
problem in computer vision. Given an image in the source domain, the goal is to
learn the conditional distribution of corresponding images in the target
domain, without seeing any pairs of corresponding images. While this
conditional distribution is inherently multimodal, existing approaches make an
overly simplified assumption, modeling it as a deterministic one-to-one
mapping. As a result, they fail to generate diverse outputs from a given source
domain image. To address this limitation, we propose a Multimodal Unsupervised
Image-to-image Translation (MUNIT) framework. We assume that the image
representation can be decomposed into a content code that is domain-invariant,
and a style code that captures domain-specific properties. To translate an
image to another domain, we recombine its content code with a random style code
sampled from the style space of the target domain. We analyze the proposed
framework and establish several theoretical results. Extensive experiments with
comparisons to the state-of-the-art approaches further demonstrates the
advantage of the proposed framework. Moreover, our framework allows users to
control the style of translation outputs by providing an example style image.
Code and pretrained models are available at https://github.com/nvlabs/MUNIT
</summary>
    <author>
      <name>Xun Huang</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04732v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04732v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02513v2</id>
    <updated>2018-08-14T18:34:04Z</updated>
    <published>2016-09-08T17:52:26Z</published>
    <title>Functorial Hierarchical Clustering with Overlaps</title>
    <summary>  This work draws inspiration from three important sources of research on
dissimilarity-based clustering and intertwines those three threads into a
consistent principled functorial theory of clustering. Those three are the
overlapping clustering of Jardine and Sibson, the functorial approach of
Carlsson and M\'{e}moli to partition-based clustering, and the Isbell/Dress
school's study of injective envelopes. Carlsson and M\'{e}moli introduce the
idea of viewing clustering methods as functors from a category of metric spaces
to a category of clusters, with functoriality subsuming many desirable
properties. Our first series of results extends their theory of functorial
clustering schemes to methods that allow overlapping clusters in the spirit of
Jardine and Sibson. This obviates some of the unpleasant effects of chaining
that occur, for example with single-linkage clustering. We prove an equivalence
between these general overlapping clustering functors and projections of weight
spaces to what we term clustering domains, by focusing on the order structure
determined by the morphisms. As a specific application of this machinery, we
are able to prove that there are no functorial projections to cut metrics, or
even to tree metrics. Finally, although we focus less on the construction of
clustering methods (clustering domains) derived from injective envelopes, we
lay out some preliminary results, that hopefully will give a feel for how the
third leg of the stool comes into play.
</summary>
    <author>
      <name>Jared Culbertson</name>
    </author>
    <author>
      <name>Dan P. Guralnik</name>
    </author>
    <author>
      <name>Peter F. Stiller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dam.2017.10.015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dam.2017.10.015" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor revisions. 24 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete Applied Mathematics, Volume 236, 19 February 2018,
  pp.108--123</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.02513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="51K05, 68P01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04839v1</id>
    <updated>2018-08-14T18:06:58Z</updated>
    <published>2018-08-14T18:06:58Z</published>
    <title>Discrete gradient descent differs qualitatively from gradient flow</title>
    <summary>  We consider gradient descent on functions of the form $L_1 = |f|$ and $L_2 =
f^2$, where $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is any smooth function
with 0 as a regular value. We show that gradient descent implemented with a
discrete step size $\tau$ behaves qualitatively differently from continuous
gradient descent. We show that over long time scales, continuous and discrete
gradient descent on $L_1$ find different minima of $L_1$, and we can
characterize the difference - the minima that tend to be found by discrete
gradient descent lie in a secondary critical submanifold $M' \subset M$, the
locus within $M$ where the function $K=|\nabla f|^2 \big|_M$ is minimized. In
this paper, we explain this behavior. We also study the more subtle behavior of
discrete gradient descent on $L_2$.
</summary>
    <author>
      <name>Y. Cooper</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02483v4</id>
    <updated>2018-08-14T17:46:39Z</updated>
    <published>2018-05-07T12:50:36Z</published>
    <title>The Logistic Network Lasso</title>
    <summary>  We apply the network Lasso to solve binary classification and clustering
problems for network-structured data. To this end, we generalize ordinary
logistic regression to non-Euclidean data with an intrinsic network structure.
The resulting "logistic network Lasso" amounts to solving a non-smooth convex
regularized empirical risk minimization. The risk is measured using the
logistic loss incurred over a small set of labeled nodes. For the
regularization, we propose to use the total variation of the classifier
requiring it to conform to the underlying network structure. A scalable
implementation of the learning method is obtained using an inexact variant of
the alternating direction methods of multipliers which results in a scalable
learning algorithm
</summary>
    <author>
      <name>Henrik Ambos</name>
    </author>
    <author>
      <name>Nguyen Tran</name>
    </author>
    <author>
      <name>Alexander Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02483v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02483v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00868v4</id>
    <updated>2018-08-14T17:25:59Z</updated>
    <published>2018-05-02T15:35:52Z</published>
    <title>A Dynamic Model for Traffic Flow Prediction Using Improved DRN</title>
    <summary>  Real-time traffic flow prediction can not only provide travelers with
reliable traffic information so that it can save people's time, but also assist
the traffic management agency to manage traffic system. It can greatly improve
the efficiency of the transportation system. Traditional traffic flow
prediction approaches usually need a large amount of data but still give poor
performances. With the development of deep learning, researchers begin to pay
attention to artificial neural networks (ANNs) such as RNN and LSTM. However,
these ANNs are very time-consuming. In our research, we improve the Deep
Residual Network and build a dynamic model which previous researchers hardly
use. We firstly integrate the input and output of the $i^{th}$ layer to the
input of the $i+1^{th}$ layer and prove that each layer will fit a simpler
function so that the error rate will be much smaller. Then, we use the concept
of online learning in our model to update pre-trained model during prediction.
Our result shows that our model has higher accuracy than some state-of-the-art
models. In addition, our dynamic model can perform better in practical
applications.
</summary>
    <author>
      <name>Zeren Tan</name>
    </author>
    <author>
      <name>Ruimin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00868v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00868v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62L10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04768v1</id>
    <updated>2018-08-14T16:07:41Z</updated>
    <published>2018-08-14T16:07:41Z</published>
    <title>Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical
  Models</title>
    <summary>  We introduce a method which enables a recurrent dynamics model to be
temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI),
is based on the observation that in many sequential prediction tasks, the exact
time at which events occur is irrelevant to the underlying objective. Moreover,
in many situations, there exist prediction intervals which result in
particularly easy-to-predict transitions. We show that there are prediction
tasks for which we gain both computational efficiency and prediction accuracy
by allowing the model to make predictions at a sampling rate which it can
choose itself.
</summary>
    <author>
      <name>Alexander Neitz</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Stefan Bauer</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00931v2</id>
    <updated>2018-08-14T15:47:44Z</updated>
    <published>2018-08-02T17:31:54Z</published>
    <title>Machine Learning of Space-Fractional Differential Equations</title>
    <summary>  Data-driven discovery of "hidden physics" -- i.e., machine learning of
differential equation models underlying observed data -- has recently been
approached by embedding the discovery problem into a Gaussian Process
regression of spatial data, treating and discovering unknown equation
parameters as hyperparameters of a modified "physics informed" Gaussian Process
kernel. This kernel includes the parametrized differential operators applied to
a prior covariance kernel. We extend this framework to linear space-fractional
differential equations. The methodology is compatible with a wide variety of
fractional operators in $\mathbb{R}^d$ and stationary covariance kernels,
including the Matern class, and can optimize the Matern parameter during
training. We provide a user-friendly and feasible way to perform fractional
derivatives of kernels, via a unified set of d-dimensional Fourier integral
formulas amenable to generalized Gauss-Laguerre quadrature.
  The implementation of fractional derivatives has several benefits. First, it
allows for discovering fractional-order PDEs for systems characterized by heavy
tails or anomalous diffusion, bypassing the analytical difficulty of fractional
calculus. Data sets exhibiting such features are of increasing prevalence in
physical and financial domains. Second, a single fractional-order archetype
allows for a derivative of arbitrary order to be learned, with the order itself
being a parameter in the regression. This is advantageous even when used for
discovering integer-order equations; the user is not required to assume a
"dictionary" of derivatives of various orders, and directly controls the
parsimony of the models being discovered. We illustrate on several examples,
including fractional-order interpolation of advection-diffusion and modeling
relative stock performance in the S&amp;P 500 with alpha-stable motion via a
fractional diffusion equation.
</summary>
    <author>
      <name>Mamikon Gulian</name>
    </author>
    <author>
      <name>Maziar Raissi</name>
    </author>
    <author>
      <name>Paris Perdikaris</name>
    </author>
    <author>
      <name>George Karniadakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 10 figures. In v2, a minor change to the formatting of a
  handful of references was made in the bibliography; the main text was
  unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35R11, 65N21, 62M10, 62F15, 60G15, 60G52" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04760v1</id>
    <updated>2018-08-14T15:47:32Z</updated>
    <published>2018-08-14T15:47:32Z</published>
    <title>Parallel Statistical and Machine Learning Methods for Estimation of
  Physical Load</title>
    <summary>  Several statistical and machine learning methods are proposed to estimate the
type and intensity of physical load and accumulated fatigue . They are based on
the statistical analysis of accumulated and moving window data subsets with
construction of a kurtosis-skewness diagram. This approach was applied to the
data gathered by the wearable heart monitor for various types and levels of
physical activities, and for people with various physical conditions. The
different levels of physical activities, loads, and fitness can be
distinguished from the kurtosis-skewness diagram, and their evolution can be
monitored. Several metrics for estimation of the instant effect and accumulated
effect (physical fatigue) of physical loads were proposed. The data and results
presented allow to extend application of these methods for modeling and
characterization of complex human activity patterns, for example, to estimate
the actual and accumulated physical load and fatigue, model the potential
dangerous development, and give cautions and advice in real time.
</summary>
    <author>
      <name>Sergii Stirenko</name>
    </author>
    <author>
      <name>Gang Peng</name>
    </author>
    <author>
      <name>Wei Zeng</name>
    </author>
    <author>
      <name>Yuri Gordienko</name>
    </author>
    <author>
      <name>Oleg Alienin</name>
    </author>
    <author>
      <name>Oleksandr Rokovyi</name>
    </author>
    <author>
      <name>Nikita Gordienko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures, accepted for 18th International Conference on
  Algorithms and Architectures for Parallel Processing (ICA3PP) 15-17 November,
  2018 (Guangzhou, China)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04759v1</id>
    <updated>2018-08-14T15:45:48Z</updated>
    <published>2018-08-14T15:45:48Z</published>
    <title>An Overview and a Benchmark of Active Learning for One-Class
  Classification</title>
    <summary>  Active learning stands for methods which increase classification quality by
means of user feedback. An important subcategory is active learning for
one-class classifiers, i.e., for imbalanced class distributions. While various
methods in this category exist, selecting one for a given application scenario
is difficult. This is because existing methods rely on different assumptions,
have different objectives, and often are tailored to a specific use case. All
this calls for a comprehensive comparison, the topic of this article. This
article starts with a categorization of the various methods. We then propose
ways to evaluate active learning results. Next, we run extensive experiments to
compare existing methods, for a broad variety of scenarios. One result is that
the practicality and the performance of an active learning method strongly
depend on its category and on the assumptions behind it. Another observation is
that there only is a small subset of our experiments where existing approaches
outperform random baselines. Finally, we show that a well-laid-out
categorization and a rigorous specification of assumptions can facilitate the
selection of a good method for one-class classification.
</summary>
    <author>
      <name>Holger Trittenbach</name>
    </author>
    <author>
      <name>Adrian Englhardt</name>
    </author>
    <author>
      <name>Klemens Böhm</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04750v1</id>
    <updated>2018-08-14T15:27:22Z</updated>
    <published>2018-08-14T15:27:22Z</published>
    <title>Quantifying the Influences on Probabilistic Wind Power Forecasts</title>
    <summary>  In recent years, probabilistic forecasts techniques were proposed in research
as well as in applications to integrate volatile renewable energy resources
into the electrical grid. These techniques allow decision makers to take the
uncertainty of the prediction into account and, therefore, to devise optimal
decisions, e.g., related to costs and risks in the electrical grid. However, it
was yet not studied how the input, such as numerical weather predictions,
affects the model output of forecasting models in detail. Therefore, we examine
the potential influences with techniques from the field of sensitivity analysis
on three different black-box models to obtain insights into differences and
similarities of these probabilistic models. The analysis shows a considerable
number of potential influences in those models depending on, e.g., the
predicted probability and the type of model. These effects motivate the need to
take various influences into account when models are tested, analyzed, or
compared. Nevertheless, results of the sensitivity analysis will allow us to
select a model with advantages in the practical application.
</summary>
    <author>
      <name>Jens Schreiber</name>
    </author>
    <author>
      <name>Bernhard Sick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; 1 table; 3 figures; This work has been submitted to the IEEE
  for possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04730v1</id>
    <updated>2018-08-14T14:58:59Z</updated>
    <published>2018-08-14T14:58:59Z</published>
    <title>Analyzing Inverse Problems with Invertible Neural Networks</title>
    <summary>  In many tasks, in particular in natural science, the goal is to determine
hidden system parameters from a set of measurements. Often, the forward process
from parameter- to measurement-space is a well-defined function, whereas the
inverse problem is ambiguous: one measurement may map to multiple different
sets of parameters. In this setting, the posterior parameter distribution,
conditioned on an input measurement, has to be determined. We argue that a
particular class of neural networks is well suited for this task -- so-called
Invertible Neural Networks (INNs). Although INNs are not new, they have, so
far, received little attention in literature. While classical neural networks
attempt to solve the ambiguous inverse problem directly, INNs are able to learn
it jointly with the well-defined forward process, using additional latent
output variables to capture the information otherwise lost. Given a specific
measurement and sampled latent variables, the inverse pass of the INN provides
a full distribution over parameter space. We verify experimentally, on
artificial data and real-world problems from astrophysics and medicine, that
INNs are a powerful analysis tool to find multi-modalities in parameter space,
to uncover parameter correlations, and to identify unrecoverable parameters.
</summary>
    <author>
      <name>Lynton Ardizzone</name>
    </author>
    <author>
      <name>Jakob Kruse</name>
    </author>
    <author>
      <name>Sebastian Wirkert</name>
    </author>
    <author>
      <name>Daniel Rahner</name>
    </author>
    <author>
      <name>Eric W. Pellegrini</name>
    </author>
    <author>
      <name>Ralf S. Klessen</name>
    </author>
    <author>
      <name>Lena Maier-Hein</name>
    </author>
    <author>
      <name>Carsten Rother</name>
    </author>
    <author>
      <name>Ullrich Köthe</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12528v4</id>
    <updated>2018-08-14T14:38:50Z</updated>
    <published>2018-05-31T15:57:33Z</published>
    <title>Fusion Graph Convolutional Networks</title>
    <summary>  Semi-supervised node classification involves learning to classify unlabelled
nodes given a partially labeled graph. In transductive learning, all unlabelled
nodes to be classified are observed during training and in inductive learning,
predictions are to be made for nodes not seen at training. In this paper, we
focus on both these settings for node classification in attributed graphs,
i.e., graphs in which nodes have additional features. State-of-the-art models
for node classification on such attributed graphs use differentiable recursive
functions. These differentiable recursive functions enable aggregation and
filtering of neighborhood information from multiple hops (depths). Despite
being powerful, these variants are limited in their ability to combine
information from different hops efficiently. In this work, we analyze this
limitation of recursive graph functions in terms of their representation
capacity to effectively capture multi-hop neighborhood information. Further, we
provide a simple fusion component which is mathematically motivated to address
this limitation and improve the existing models to explicitly learn the
importance of information from different hops. This proposed mechanism is shown
to improve over existing methods across 8 popular datasets from different
domains. Specifically, our model improves the Graph Convolutional Network (GCN)
and a variant of Graph SAGE by a significant margin providing highly
competitive state-of-the-art results.
</summary>
    <author>
      <name>Priyesh Vijayan</name>
    </author>
    <author>
      <name>Yash Chandak</name>
    </author>
    <author>
      <name>Mitesh M. Khapra</name>
    </author>
    <author>
      <name>Balaraman Ravindran</name>
    </author>
    <link href="http://arxiv.org/abs/1805.12528v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12528v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09859v2</id>
    <updated>2018-08-14T14:02:55Z</updated>
    <published>2017-10-26T18:38:28Z</published>
    <title>Kernel k-Groups via Hartigan's Method</title>
    <summary>  Energy statistics was proposed by Sz\'{e}kely in the 80's inspired by
Newton's gravitational potential in classical mechanics, and it provides a
model-free hypothesis test for equality of distributions. In its original form,
energy statistics was formulated in Euclidean spaces. More recently, it was
generalized to metric spaces of negative type. In this paper, we consider a
formulation for the clustering problem using a weighted version of energy
statistics in spaces of negative type. We show that this approach leads to a
quadratically constrained quadratic program in the associated kernel space,
establishing connections with graph partitioning problems and kernel methods in
unsupervised machine learning. To find local solutions of such an optimization
problem, we propose an extension of Hartigan's method to kernel spaces. Our
method has the same computational cost as kernel k-means algorithm, which is
based on Lloyd's heuristic, but our numerical results show an improved
performance, especially in high dimensions.
</summary>
    <author>
      <name>Guilherme França</name>
    </author>
    <author>
      <name>Maria L. Rizzo</name>
    </author>
    <author>
      <name>Joshua T. Vogelstein</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09859v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09859v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04685v1</id>
    <updated>2018-08-14T13:45:34Z</updated>
    <published>2018-08-14T13:45:34Z</published>
    <title>Learning ReLU Networks on Linearly Separable Data: Algorithm,
  Optimality, and Generalization</title>
    <summary>  Neural networks with ReLU activations have achieved great empirical success
in various domains. However, existing results for learning ReLU networks either
pose assumptions on the underlying data distribution being e.g. Gaussian, or
require the network size and/or training size to be sufficiently large. In this
context, the problem of learning a two-layer ReLU network is approached in a
binary classification setting, where the data are linearly separable and a
hinge loss criterion is adopted. Leveraging the power of random noise, this
contribution presents a novel stochastic gradient descent (SGD) algorithm,
which can provably train any single-hidden-layer ReLU network to attain global
optimality, despite the presence of infinitely many bad local minima and saddle
points in general. This result is the first of its kind, requiring no
assumptions on the data distribution, training/network size, or initialization.
Convergence of the resultant iterative algorithm to a global minimum is
analyzed by establishing both an upper bound and a lower bound on the number of
effective (non-zero) updates to be performed. Furthermore, generalization
guarantees are developed for ReLU networks trained with the novel SGD. These
guarantees highlight a fundamental difference (at least in the worst case)
between learning a ReLU network as well as a leaky ReLU network in terms of
sample complexity. Numerical tests using synthetic data and real images
validate the effectiveness of the algorithm and the practical merits of the
theory.
</summary>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Georgios B. Giannakis</name>
    </author>
    <author>
      <name>Jie Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures, work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07572v2</id>
    <updated>2018-08-14T13:06:04Z</updated>
    <published>2018-02-21T14:01:20Z</published>
    <title>Information Theoretic Co-Training</title>
    <summary>  This paper introduces an information theoretic co-training objective for
unsupervised learning. We consider the problem of predicting the future. Rather
than predict future sensations (image pixels or sound waves) we predict
"hypotheses" to be confirmed by future sensations. More formally, we assume a
population distribution on pairs $(x,y)$ where we can think of $x$ as a past
sensation and $y$ as a future sensation. We train both a predictor model
$P_\Phi(z|x)$ and a confirmation model $P_\Psi(z|y)$ where we view $z$ as
hypotheses (when predicted) or facts (when confirmed). For a population
distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual
information between $x$ and $y$. By the data processing inequality this mutual
information is at least as large as the mutual information between $x$ and $z$
under the distribution on triples $(x,z,y)$ defined by the confirmation model
$P_\Psi(z|y)$. The information theoretic training objective for $P_\Phi(z|x)$
and $P_\Psi(z|y)$ can be viewed as a form of co-training where we want the
prediction from $x$ to match the confirmation from $y$.
</summary>
    <author>
      <name>David McAllester</name>
    </author>
    <link href="http://arxiv.org/abs/1802.07572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06470v1</id>
    <updated>2018-08-14T09:34:16Z</updated>
    <published>2018-08-14T09:34:16Z</published>
    <title>predictSLUMS: A new model for identifying and predicting informal
  settlements and slums in cities from street intersections using machine
  learning</title>
    <summary>  Identifying current and future informal regions within cities remains a
crucial issue for policymakers and governments in developing countries. The
delineation process of identifying such regions in cities requires a lot of
resources. While there are various studies that identify informal settlements
based on satellite image classification, relying on both supervised or
unsupervised machine learning approaches, these models either require multiple
input data to function or need further development with regards to precision.
In this paper, we introduce a novel method for identifying and predicting
informal settlements using only street intersections data, regardless of the
variation of urban form, number of floors, materials used for construction or
street width. With such minimal input data, we attempt to provide planners and
policy-makers with a pragmatic tool that can aid in identifying informal zones
in cities. The algorithm of the model is based on spatial statistics and a
machine learning approach, using Multinomial Logistic Regression (MNL) and
Artificial Neural Networks (ANN). The proposed model relies on defining
informal settlements based on two ubiquitous characteristics that these regions
tend to be filled in with smaller subdivided lots of housing relative to the
formal areas within the local context, and the paucity of services and
infrastructure within the boundary of these settlements that require relatively
bigger lots. We applied the model in five major cities in Egypt and India that
have spatial structures in which informality is present. These cities are
Greater Cairo, Alexandria, Hurghada and Minya in Egypt, and Mumbai in India.
The predictSLUMS model shows high validity and accuracy for identifying and
predicting informality within the same city the model was trained on or in
different ones of a similar context.
</summary>
    <author>
      <name>Mohamed R. Ibrahim</name>
    </author>
    <author>
      <name>Helena Titheridge</name>
    </author>
    <author>
      <name>Tao Cheng</name>
    </author>
    <author>
      <name>James Haworth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04580v1</id>
    <updated>2018-08-14T08:24:01Z</updated>
    <published>2018-08-14T08:24:01Z</published>
    <title>NFFT meets Krylov methods: Fast matrix-vector products for the graph
  Laplacian of fully connected networks</title>
    <summary>  The graph Laplacian is a standard tool in data science, machine learning, and
image processing. The corresponding matrix inherits the complex structure of
the underlying network and is in certain applications densely populated. This
makes computations, in particular matrix-vector products, with the graph
Laplacian a hard task. A typical application is the computation of a number of
its eigenvalues and eigenvectors. Standard methods become infeasible as the
number of nodes in the graph is too large. We propose the use of the fast
summation based on the nonequispaced fast Fourier transform (NFFT) to perform
the dense matrix-vector product with the graph Laplacian fast without ever
forming the whole matrix. The enormous flexibility of the NFFT algorithm allows
us to embed the accelerated multiplication into Lanczos-based eigenvalues
routines or iterative linear system solvers. We illustrate the feasibility of
our approach on a number of test problems from image segmentation to
semi-supervised learning based on graph-based PDEs. In particular, we compare
our approach with the Nystr\"om method. Moreover, we present and test an
enhanced, hybrid version of the Nystr\"om method, which internally uses the
NFFT.
</summary>
    <author>
      <name>Dominik Alfke</name>
    </author>
    <author>
      <name>Daniel Potts</name>
    </author>
    <author>
      <name>Martin Stoll</name>
    </author>
    <author>
      <name>Toni Volkmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R10, 05C50, 65F15, 65T50, 68T05, 62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09133v2</id>
    <updated>2018-08-14T07:30:11Z</updated>
    <published>2018-04-24T16:50:54Z</published>
    <title>Improving Native Ads CTR Prediction by Large Scale Event Embedding and
  Recurrent Networks</title>
    <summary>  Click through rate (CTR) prediction is very important for Native
advertisement but also hard as there is no direct query intent. In this paper
we propose a large-scale event embedding scheme to encode the each user
browsing event by training a Siamese network with weak supervision on the
users' consecutive events. The CTR prediction problem is modeled as a
supervised recurrent neural network, which naturally model the user history as
a sequence of events. Our proposed recurrent models utilizing pretrained event
embedding vectors and an attention layer to model the user history. Our
experiments demonstrate that our model significantly outperforms the baseline
and some variants.
</summary>
    <author>
      <name>Mehul Parsana</name>
    </author>
    <author>
      <name>Krishna Poola</name>
    </author>
    <author>
      <name>Yajun Wang</name>
    </author>
    <author>
      <name>Zhiguang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version has some language error, and the authors all agree to
  withdraw it at the moment to further edit and update with some new results</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.09133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03298v2</id>
    <updated>2018-08-14T06:49:53Z</updated>
    <published>2018-06-26T04:48:57Z</published>
    <title>Probabilistic Ensemble of Collaborative Filters</title>
    <summary>  Collaborative filtering is an important technique for recommendation. Whereas
it has been repeatedly shown to be effective in previous work, its performance
remains unsatisfactory in many real-world applications, especially those where
the items or users are highly diverse. In this paper, we explore an
ensemble-based framework to enhance the capability of a recommender in handling
diverse data. Specifically, we formulate a probabilistic model which integrates
the items, the users, as well as the associations between them into a
generative process. On top of this formulation, we further derive a progressive
algorithm to construct an ensemble of collaborative filters. In each iteration,
a new filter is derived from re-weighted entries and incorporated into the
ensemble. It is noteworthy that while the algorithmic procedure of our
algorithm is apparently similar to boosting, it is derived from an essentially
different formulation and thus differs in several key technical aspects. We
tested the proposed method on three large datasets, and observed substantial
improvement over the state of the art, including L2Boost, an effective method
based on boosting.
</summary>
    <author>
      <name>Zhiyu Min</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. In Proceedings of AAAI-2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03298v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03298v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04550v1</id>
    <updated>2018-08-14T06:40:01Z</updated>
    <published>2018-08-14T06:40:01Z</published>
    <title>SciSports: Learning football kinematics through two-dimensional tracking
  data</title>
    <summary>  SciSports is a Dutch startup company specializing in football analytics. This
paper describes a joint research effort with SciSports, during the Study Group
Mathematics with Industry 2018 at Eindhoven, the Netherlands. The main
challenge that we addressed was to automatically process empirical football
players' trajectories, in order to extract useful information from them. The
data provided to us was two-dimensional positional data during entire matches.
We developed methods based on Newtonian mechanics and the Kalman filter,
Generative Adversarial Nets and Variational Autoencoders. In addition, we
trained a discriminator network to recognize and discern different movement
patterns of players. The Kalman-filter approach yields an interpretable model,
in which a small number of player-dependent parameters can be fit; in theory
this could be used to distinguish among players. The
Generative-Adversarial-Nets approach appears promising in theory, and some
initial tests showed an improvement with respect to the baseline, but the
limits in time and computational power meant that we could not fully explore
it. We also trained a Discriminator network to distinguish between two players
based on their trajectories; after training, the network managed to distinguish
between some pairs of players, but not between others. After training, the
Variational Autoencoders generated trajectories that are difficult to
distinguish, visually, from the data. These experiments provide an indication
that deep generative models can learn the underlying structure and statistics
of football players' trajectories. This can serve as a starting point for
determining player qualities based on such trajectory data.
</summary>
    <author>
      <name>Anatoliy Babic</name>
    </author>
    <author>
      <name>Harshit Bansal</name>
    </author>
    <author>
      <name>Gianluca Finocchio</name>
    </author>
    <author>
      <name>Julian Golak</name>
    </author>
    <author>
      <name>Mark Peletier</name>
    </author>
    <author>
      <name>Jim Portegies</name>
    </author>
    <author>
      <name>Clara Stegehuis</name>
    </author>
    <author>
      <name>Anuj Tyagi</name>
    </author>
    <author>
      <name>Roland Vincze</name>
    </author>
    <author>
      <name>William Weimin Yoo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This report was made for the Study Group Mathematics with Industry
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04545v1</id>
    <updated>2018-08-14T06:21:03Z</updated>
    <published>2018-08-14T06:21:03Z</published>
    <title>MT-VAE: Learning Motion Transformations to Generate Multimodal Human
  Dynamics</title>
    <summary>  Long-term human motion can be represented as a series of motion
modes---motion sequences that capture short-term temporal dynamics---with
transitions between them. We leverage this structure and present a novel Motion
Transformation Variational Auto-Encoders (MT-VAE) for learning motion sequence
generation. Our model jointly learns a feature embedding for motion modes (that
the motion sequence can be reconstructed from) and a feature transformation
that represents the transition of one motion mode to the next motion mode. Our
model is able to generate multiple diverse and plausible motion sequences in
the future from the same input. We apply our approach to both facial and full
body motion, and demonstrate applications like analogy-based motion transfer
and video synthesis.
</summary>
    <author>
      <name>Xinchen Yan</name>
    </author>
    <author>
      <name>Akash Rastogi</name>
    </author>
    <author>
      <name>Ruben Villegas</name>
    </author>
    <author>
      <name>Kalyan Sunkavalli</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Sunil Hadap</name>
    </author>
    <author>
      <name>Ersin Yumer</name>
    </author>
    <author>
      <name>Honglak Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04538v1</id>
    <updated>2018-08-14T05:45:25Z</updated>
    <published>2018-08-14T05:45:25Z</published>
    <title>Text-to-Image-to-Text Translation using Cycle Consistent Adversarial
  Networks</title>
    <summary>  Text-to-Image translation has been an active area of research in the recent
past. The ability for a network to learn the meaning of a sentence and generate
an accurate image that depicts the sentence shows ability of the model to think
more like humans. Popular methods on text to image translation make use of
Generative Adversarial Networks (GANs) to generate high quality images based on
text input, but the generated images don't always reflect the meaning of the
sentence given to the model as input. We address this issue by using a
captioning network to caption on generated images and exploit the distance
between ground truth captions and generated captions to improve the network
further. We show extensive comparisons between our method and existing methods.
</summary>
    <author>
      <name>Satya Krishna Gorti</name>
    </author>
    <author>
      <name>Jeremy Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03133v2</id>
    <updated>2018-08-14T04:24:35Z</updated>
    <published>2018-06-29T18:00:03Z</published>
    <title>Outfit Generation and Style Extraction via Bidirectional LSTM and
  Autoencoder</title>
    <summary>  When creating an outfit, style is a criterion in selecting each fashion item.
This means that style can be regarded as a feature of the overall outfit.
However, in various previous studies on outfit generation, there have been few
methods focusing on global information obtained from an outfit. To address this
deficiency, we have incorporated an unsupervised style extraction module into a
model to learn outfits. Using the style information of an outfit as a whole,
the proposed model succeeded in generating outfits more flexibly without
requiring additional information. Moreover, the style information extracted by
the proposed model is easy to interpret. The proposed model was evaluated on
two human-generated outfit datasets. In a fashion item prediction task (missing
prediction task), the proposed model outperformed a baseline method. In a style
extraction task, the proposed model extracted some easily distinguishable
styles. In an outfit generation task, the proposed model generated an outfit
while controlling its styles. This capability allows us to generate fashionable
outfits according to various preferences.
</summary>
    <author>
      <name>Takuma Nakamura</name>
    </author>
    <author>
      <name>Ryosuke Goto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, KDD Workshop AI for fashion</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11622v2</id>
    <updated>2018-08-14T02:56:53Z</updated>
    <published>2018-07-31T01:25:44Z</published>
    <title>Count-Based Exploration with the Successor Representation</title>
    <summary>  The problem of exploration in reinforcement learning is well-understood in
the tabular case and many sample-efficient algorithms are known. Nevertheless,
it is often unclear how the algorithms in the tabular setting can be extended
to tasks with large state-spaces where generalization is required. Recent
promising developments generally depend on problem-specific density models or
handcrafted features. In this paper we introduce a simple approach for
exploration that allows us to develop theoretically justified algorithms in the
tabular case but that also give us intuitions for new algorithms applicable to
settings where function approximation is required. Our approach and its
underlying theory is based on the substochastic successor representation, a
concept we develop here. While the traditional successor representation is a
representation that defines state generalization by the similarity of successor
states, the substochastic successor representation is also able to implicitly
count the number of times each state (or feature) has been observed. This
extension connects two until now disjoint areas of research. We show in
traditional tabular domains (RiverSwim and SixArms) that our algorithm
empirically performs as well as other sample-efficient algorithms. We then
describe a deep reinforcement learning algorithm inspired by these ideas and
show that it matches the performance of recent pseudo-count-based methods in
hard exploration Atari 2600 games.
</summary>
    <author>
      <name>Marlos C. Machado</name>
    </author>
    <author>
      <name>Marc G. Bellemare</name>
    </author>
    <author>
      <name>Michael Bowling</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11622v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11622v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07268v2</id>
    <updated>2018-08-14T01:07:04Z</updated>
    <published>2016-06-23T10:53:05Z</published>
    <title>Semi-supervised Inference: General Theory and Estimation of Means</title>
    <summary>  We propose a general semi-supervised inference framework focused on the
estimation of the population mean. As usual in semi-supervised settings, there
exists an unlabeled sample of covariate vectors and a labeled sample consisting
of covariate vectors along with real-valued responses ("labels"). Otherwise,
the formulation is "assumption-lean" in that no major conditions are imposed on
the statistical or functional form of the data. We consider both the ideal
semi-supervised setting where infinitely many unlabeled samples are available,
as well as the ordinary semi-supervised setting in which only a finite number
of unlabeled samples is available.
  Estimators are proposed along with corresponding confidence intervals for the
population mean. Theoretical analysis on both the asymptotic distribution and
$\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed
estimators, based on a simple form of the least squares method, outperform the
ordinary sample mean. The simple, transparent form of the estimator lends
confidence to the perception that its asymptotic improvement over the ordinary
sample mean also nearly holds even for moderate size samples. The method is
further extended to a nonparametric setting, in which the oracle rate can be
achieved asymptotically. The proposed estimators are further illustrated by
simulation studies and a real data example involving estimation of the homeless
population.
</summary>
    <author>
      <name>Anru Zhang</name>
    </author>
    <author>
      <name>Lawrence D. Brown</name>
    </author>
    <author>
      <name>T. Tony Cai</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07268v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07268v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04900v2</id>
    <updated>2018-08-14T01:02:12Z</updated>
    <published>2018-06-13T09:00:36Z</published>
    <title>A Machine-Learning Item Recommendation System for Video Games</title>
    <summary>  Video-game players generate huge amounts of data, as everything they do
within a game is recorded. In particular, among all the stored actions and
behaviors, there is information on the in-game purchases of virtual products.
Such information is of critical importance in modern free-to-play titles, where
gamers can select or buy a profusion of items during the game in order to
progress and fully enjoy their experience.
  To try to maximize these kind of purchases, one can use a recommendation
system so as to present players with items that might be interesting for them.
Such systems can better achieve their goal by employing machine learning
algorithms that are able to predict the rating of an item or product by a
particular user. In this paper we evaluate and compare two of these algorithms,
an ensemble-based model (extremely randomized trees) and a deep neural network,
both of which are promising candidates for operational video-game recommender
engines.
  Item recommenders can help developers improve the game. But, more
importantly, it should be possible to integrate them into the game, so that
users automatically get personalized recommendations while playing. The
presented models are not only able to meet this challenge, providing accurate
predictions of the items that a particular player will find attractive, but
also sufficiently fast and robust to be used in operational settings.
</summary>
    <author>
      <name>Paul Bertens</name>
    </author>
    <author>
      <name>Anna Guitart</name>
    </author>
    <author>
      <name>Pei Pei Chen</name>
    </author>
    <author>
      <name>África Periáñez</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05140v1</id>
    <updated>2018-08-13T22:46:41Z</updated>
    <published>2018-08-13T22:46:41Z</published>
    <title>A Framework for Automated Cellular Network Tuning with Reinforcement
  Learning</title>
    <summary>  Tuning cellular network performance against always occurring wireless
impairments can dramatically improve reliability to end users. In this paper,
we formulate cellular network performance tuning as a reinforcement learning
(RL) problem and provide a solution to improve the signal to
interference-plus-noise ratio (SINR) for indoor and outdoor environments. By
leveraging the ability of Q-learning to estimate future SINR improvement
rewards, we propose two algorithms: (1) voice over LTE (VoLTE) downlink closed
loop power control (PC) and (2) self-organizing network (SON) fault management.
The VoLTE PC algorithm uses RL to adjust the indoor base station transmit power
so that the effective SINR meets the target SINR. The SON fault management
algorithm uses RL to improve the performance of an outdoor cluster by resolving
faults in the network through configuration management. Both algorithms exploit
measurements from the connected users, wireless impairments, and relevant
configuration parameters to solve a non-convex SINR optimization problem using
RL. Simulation results show that our proposed RL based algorithms outperform
the industry standards today in realistic cellular communication environments.
</summary>
    <author>
      <name>Faris B. Mismar</name>
    </author>
    <author>
      <name>Jinseok Choi</name>
    </author>
    <author>
      <name>Brian L. Evans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 9 figures, submitted to IEEE Transactions on Communications
  on August 13, 2018. arXiv admin note: text overlap with arXiv:1707.03269</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04475v1</id>
    <updated>2018-08-13T21:43:20Z</updated>
    <published>2018-08-13T21:43:20Z</published>
    <title>Kernel Flows: from learning kernels from data into the abyss</title>
    <summary>  Learning can be seen as approximating an unknown function by interpolating
the training data. Kriging offers a solution to this problem based on the prior
specification of a kernel. We explore a numerical approximation approach to
kernel selection/construction based on the simple premise that a kernel must be
good if the number of interpolation points can be halved without significant
loss in accuracy (measured using the intrinsic RKHS norm $\|\cdot\|$ associated
with the kernel). We first test and motivate this idea on a simple problem of
recovering the Green's function of an elliptic PDE (with inhomogeneous
coefficients) from the sparse observation of one of its solutions. Next we
consider the problem of learning non-parametric families of deep kernels of the
form $K_1(F_n(x),F_n(x'))$ with $F_{n+1}=(I_d+\epsilon G_{n+1})\circ F_n$ and
$G_{n+1} \in \operatorname{Span}\{K_1(F_n(x_i),\cdot)\}$. With the proposed
approach constructing the kernel becomes equivalent to integrating a stochastic
data driven dynamical system, which allows for the training of very deep
(bottomless) networks and the exploration of their properties. These networks
learn by constructing flow maps in the kernel and input spaces via incremental
data-dependent deformations/perturbations (appearing as the cooperative
counterpart of adversarial examples) and, at profound depths, they (1) can
achieve accurate classification from only one data point per class (2) appear
to learn archetypes of each class (3) expand distances between points that are
in different classes and contract distances between points in the same class.
For kernels parameterized by the weights of Convolutional Neural Network,
minimizing approximation errors incurred by halving random subsets of
interpolation points, appears to outperform training (the same CNN
architecture) with relative entropy and dropout.
</summary>
    <author>
      <name>Houman Owhadi</name>
    </author>
    <author>
      <name>Gene Ryan Yoo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 28 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J02, 68T01, 91C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04468v1</id>
    <updated>2018-08-13T21:08:46Z</updated>
    <published>2018-08-13T21:08:46Z</published>
    <title>Risk-Sensitive Generative Adversarial Imitation Learning</title>
    <summary>  We study risk-sensitive imitation learning where the agent's goal is to
perform at least as well as the expert in terms of a risk profile. We first
formulate our risk-sensitive imitation learning setting. We consider the
generative adversarial approach to imitation learning (GAIL) and derive an
optimization problem for our formulation, which we call risk-sensitive GAIL
(RS-GAIL). We then derive two different versions of our RS-GAIL optimization
problem that aim at matching the risk profiles of the agent and the expert
w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop
risk-sensitive generative adversarial imitation learning algorithms based on
these optimization problems. We evaluate the performance of our JS-based
algorithm and compare it with GAIL and the risk-averse imitation learning
(RAIL) algorithm in two MuJoCo tasks.
</summary>
    <author>
      <name>Jonathan Lacotte</name>
    </author>
    <author>
      <name>Yinlam Chow</name>
    </author>
    <author>
      <name>Mohammad Ghavamzadeh</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04456v1</id>
    <updated>2018-08-13T20:36:08Z</updated>
    <published>2018-08-13T20:36:08Z</published>
    <title>Multimodal Deep Neural Networks using Both Engineered and Learned
  Representations for Biodegradability Prediction</title>
    <summary>  Deep learning algorithms excel at extracting patterns from raw data. Through
representation learning and automated feature engineering on large datasets,
such models have been highly successful in computer vision and natural language
applications. However, in many other technical domains, large datasets on which
to learn representations from may not be feasible. In this work, we develop a
novel multimodal CNN-MLP neural network architecture that utilizes both
domain-specific feature engineering as well as learned representations from raw
data. We illustrate the effectiveness of such an approach in the chemical
sciences, for predicting chemical properties, where labeled data is scarce
owing to the high costs associated with acquiring labels through experimental
measurements. By training on both raw chemical data and using engineered
chemical features, while leveraging weak supervised learning and transfer
learning methods, we show that the multimodal CNN-MLP network is more accurate
than either a standalone CNN or MLP network that uses only raw data or
engineered features respectively. Using this multimodal network, we then
develop the DeepBioD model for predicting chemical biodegradability, which
achieves an error classification rate of 0.125 that is 27% lower than the
current state-of-the-art. Thus, our work indicates that combining traditional
feature engineering with representation learning on raw data can be an
effective approach, particularly in situations where labeled training data is
limited. Such a framework can also be potentially applied to other technical
fields, where substantial research efforts into feature engineering has been
established.
</summary>
    <author>
      <name>Garrett B. Goh</name>
    </author>
    <author>
      <name>Khusheemn Sakloth</name>
    </author>
    <author>
      <name>Charles Siegel</name>
    </author>
    <author>
      <name>Abhinav Vishnu</name>
    </author>
    <author>
      <name>Jim Pfaendtner</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05409v2</id>
    <updated>2018-08-13T20:02:42Z</updated>
    <published>2017-09-15T21:07:46Z</published>
    <title>Gaussian Process Latent Force Models for Learning and Stochastic Control
  of Physical Systems</title>
    <summary>  This article is concerned with learning and stochastic control in physical
systems which contain unknown input signals. These unknown signals are modeled
as Gaussian processes (GP) with certain parametrized covariance structures. The
resulting latent force models (LFMs) can be seen as hybrid models that contain
a first-principles physical model part and a non-parametric GP model part. We
briefly review the statistical inference and learning methods for this kind of
models, introduce stochastic control methodology for the models, and provide
new theoretical observability and controllability results for them.
</summary>
    <author>
      <name>Simo Särkkä</name>
    </author>
    <author>
      <name>Mauricio A. Álvarez</name>
    </author>
    <author>
      <name>Neil D. Lawrence</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05409v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05409v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04411v1</id>
    <updated>2018-08-13T19:21:41Z</updated>
    <published>2018-08-13T19:21:41Z</published>
    <title>Murmur Detection Using Parallel Recurrent &amp; Convolutional Neural
  Networks</title>
    <summary>  In this article, we propose a novel technique for classification of the
Murmurs in heart sound. We introduce a novel deep neural network architecture
using parallel combination of the Recurrent Neural Network (RNN) based
Bidirectional Long Short-Term Memory (BiLSTM) &amp; Convolutional Neural Network
(CNN) to learn visual and time-dependent characteristics of Murmur in PCG
waveform. Set of acoustic features are presented to our proposed deep neural
network to discriminate between Normal and Murmur class. The proposed method
was evaluated on a large dataset using 5-fold cross-validation, resulting in a
sensitivity and specificity of 96 +- 0.6 % , 100 +- 0 % respectively and F1
Score of 98 +- 0.3 %.
</summary>
    <author>
      <name>Shahnawaz Alam</name>
    </author>
    <author>
      <name>Rohan Banerjee</name>
    </author>
    <author>
      <name>Soma Bandyopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Machine Learning for Medicine and Healthcare Workshop, KDD
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06475v1</id>
    <updated>2018-08-13T18:43:11Z</updated>
    <published>2018-08-13T18:43:11Z</published>
    <title>The perceived quality of process discovery tools</title>
    <summary>  Process discovery has seen a rise in popularity in the last decade for both
researchers and businesses. Recent developments mainly focused on the power and
the functionalities of the discovery algorithm. While continuous improvement of
these functional aspects is very important, non-functional aspects such as
visualization and usability are often overlooked. However, these aspects are
considered valuable for end-users and play an important part in the experience
of these end-users when working with a process discovery tool. A questionnaire
has been sent out to give end-users the opportunity to voice their opinion on
available process discovery tools and about the state of process discovery as a
domain in general. The results of 66 respondents are presented and compared
with the answers of 63 respondents that were contacted through one particular
software vendor's employee and customer base (i.e., Celonis).
</summary>
    <author>
      <name>Francis Bru</name>
    </author>
    <author>
      <name>Jan Claes</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04355v1</id>
    <updated>2018-08-13T17:58:01Z</updated>
    <published>2018-08-13T17:58:01Z</published>
    <title>Large-Scale Study of Curiosity-Driven Learning</title>
    <summary>  Reinforcement learning algorithms rely on carefully engineering environment
rewards that are extrinsic to the agent. However, annotating each environment
with hand-designed, dense rewards is not scalable, motivating the need for
developing reward functions that are intrinsic to the agent. Curiosity is a
type of intrinsic reward function which uses prediction error as reward signal.
In this paper: (a) We perform the first large-scale study of purely
curiosity-driven learning, i.e. without any extrinsic rewards, across 54
standard benchmark environments, including the Atari game suite. Our results
show surprisingly good performance, and a high degree of alignment between the
intrinsic curiosity objective and the hand-designed extrinsic rewards of many
game environments. (b) We investigate the effect of using different feature
spaces for computing prediction error and show that random features are
sufficient for many popular RL game benchmarks, but learned features appear to
generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We
demonstrate limitations of the prediction-based rewards in stochastic setups.
Game-play videos and code are at
https://pathak22.github.io/large-scale-curiosity/
</summary>
    <author>
      <name>Yuri Burda</name>
    </author>
    <author>
      <name>Harri Edwards</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <author>
      <name>Amos Storkey</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First three authors contributed equally and ordered alphabetically.
  Website at https://pathak22.github.io/large-scale-curiosity/</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10377v2</id>
    <updated>2018-08-13T17:27:52Z</updated>
    <published>2018-05-25T21:55:12Z</published>
    <title>Ergodic Measure Preserving Flows</title>
    <summary>  Probabilistic modelling is a general and elegant framework to capture the
uncertainty, ambiguity and diversity of data. Probabilistic inference is the
core technique for developing training and simulation algorithms on
probabilistic models. However, the classic inference methods, like Markov chain
Monte Carlo (MCMC) methods and mean-field variational inference (VI), are not
computationally scalable for the recent developed probabilistic models with
neural networks (NNs). This motivates many recent works on improving classic
inference methods using NNs, especially, NN empowered VI. However, even with
powerful NNs, VI still suffers its fundamental limitations. In this work, we
propose a novel computational scalable general inference framework. With the
theoretical foundation in ergodic theory, the proposed methods are not only
computationally scalable like NN-based VI methods but also asymptotically
accurate like MCMC. We test our method on popular benchmark problems and the
results suggest that our methods can outperform NN-based VI and MCMC on deep
generative models and Bayesian neural networks.
</summary>
    <author>
      <name>Yichuan Zhang</name>
    </author>
    <author>
      <name>Jose Miguel Hernandez-Lobato</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04334v1</id>
    <updated>2018-08-13T17:20:20Z</updated>
    <published>2018-08-13T17:20:20Z</published>
    <title>Angular-Based Word Meta-Embedding Learning</title>
    <summary>  Ensembling word embeddings to improve distributed word representations has
shown good success for natural language processing tasks in recent years. These
approaches either carry out straightforward mathematical operations over a set
of vectors or use unsupervised learning to find a lower-dimensional
representation. This work compares meta-embeddings trained for different
losses, namely loss functions that account for angular distance between the
reconstructed embedding and the target and those that account normalized
distances based on the vector length. We argue that meta-embeddings are better
to treat the ensemble set equally in unsupervised learning as the respective
quality of each embedding is unknown for upstream tasks prior to
meta-embedding. We show that normalization methods that account for this such
as cosine and KL-divergence objectives outperform meta-embedding trained on
standard $\ell_1$ and $\ell_2$ loss on \textit{defacto} word similarity and
relatedness datasets and find it outperforms existing meta-learning strategies.
</summary>
    <author>
      <name>James O' Neill</name>
    </author>
    <author>
      <name>Danushka Bollegala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06007v3</id>
    <updated>2018-08-13T16:54:07Z</updated>
    <published>2018-07-17T07:05:57Z</published>
    <title>On Lebesgue Integral Quadrature</title>
    <summary>  A new type of quadrature is developed. The Gauss quadrature, for a given
measure, finds optimal values of a function's argument (nodes) and the
corresponding weights. In contrast, the Lebesgue quadrature developed in this
paper, finds optimal values of function (value-nodes) and the corresponding
weights. The Gauss quadrature groups sums by function argument, it can be
viewed as a $n$-point discrete measure, producing the Riemann integral. The
Lebesgue quadrature groups sums by function value, it can be viewed as a
$n$-point discrete distribution, producing the Lebesgue integral.
Mathematically, the problem is reduced to a generalized eigenvalue problem:
Lebesgue quadrature value-nodes are the eigenvalues and the corresponding
weights are the square of the averaged eigenvectors. A numerical estimation of
an integral as the Lebesgue integral is especially advantageous when analyzing
irregular and stochastic processes. The approach separates the outcome
(value-nodes) and the probability of the outcome (weight). For this reason, it
is especially well-suited for the study of non--Gaussian processes. The
software implementing the theory is available from the authors.
</summary>
    <author>
      <name>Vladislav Gennadievich Malyshkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Relation to density matrix added. Images fixed. Density matrix
  appendix fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06007v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06007v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04327v1</id>
    <updated>2018-08-13T16:37:56Z</updated>
    <published>2018-08-13T16:37:56Z</published>
    <title>Hidden Fluid Mechanics: A Navier-Stokes Informed Deep Learning Framework
  for Assimilating Flow Visualization Data</title>
    <summary>  We present hidden fluid mechanics (HFM), a physics informed deep learning
framework capable of encoding an important class of physical laws governing
fluid motions, namely the Navier-Stokes equations. In particular, we seek to
leverage the underlying conservation laws (i.e., for mass, momentum, and
energy) to infer hidden quantities of interest such as velocity and pressure
fields merely from spatio-temporal visualizations of a passive scaler (e.g.,
dye or smoke), transported in arbitrarily complex domains (e.g., in human
arteries or brain aneurysms). Our approach towards solving the aforementioned
data assimilation problem is unique as we design an algorithm that is agnostic
to the geometry or the initial and boundary conditions. This makes HFM highly
flexible in choosing the spatio-temporal domain of interest for data
acquisition as well as subsequent training and predictions. Consequently, the
predictions made by HFM are among those cases where a pure machine learning
strategy or a mere scientific computing approach simply cannot reproduce. The
proposed algorithm achieves accurate predictions of the pressure and velocity
fields in both two and three dimensional flows for several benchmark problems
motivated by real-world applications. Our results demonstrate that this
relatively simple methodology can be used in physical and biomedical problems
to extract valuable quantitative information (e.g., lift and drag forces or
wall shear stresses in arteries) for which direct measurements may not be
possible.
</summary>
    <author>
      <name>Maziar Raissi</name>
    </author>
    <author>
      <name>Alireza Yazdani</name>
    </author>
    <author>
      <name>George Em Karniadakis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04308v1</id>
    <updated>2018-08-13T16:04:34Z</updated>
    <published>2018-08-13T16:04:34Z</published>
    <title>What is Unique in Individual Gait Patterns? Understanding and
  Interpreting Deep Learning in Gait Analysis</title>
    <summary>  Machine learning (ML) techniques such as (deep) artificial neural networks
(DNN) are solving very successfully a plethora of tasks and provide new
predictive models for complex physical, chemical, biological and social
systems. However, in most cases this comes with the disadvantage of acting as a
black box, rarely providing information about what made them arrive at a
particular prediction. This black box aspect of ML techniques can be
problematic especially in medical diagnoses, so far hampering a clinical
acceptance. The present paper studies the uniqueness of individual gait
patterns in clinical biomechanics using DNNs. By attributing portions of the
model predictions back to the input variables (ground reaction forces and
full-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique
reliably demonstrates which variables at what time windows of the gait cycle
are most relevant for the characterisation of gait patterns from a certain
individual. By measuring the timeresolved contribution of each input variable
to the prediction of ML techniques such as DNNs, our method describes the first
general framework that enables to understand and interpret non-linear ML
methods in (biomechanical) gait analysis and thereby supplies a powerful tool
for analysis, diagnosis and treatment of human gait.
</summary>
    <author>
      <name>Fabian Horst</name>
    </author>
    <author>
      <name>Sebastian Lapuschkin</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <author>
      <name>Klaus-Robert Müller</name>
    </author>
    <author>
      <name>Wolfgang I. Schöllhorn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages (23 pages including references, 24 pages including
  references and auxiliary statements, 33 pages including references, auxiliary
  statements and and supplementary material). 5 figures, 3 tables, 4
  supplementary figures, 9 supplementary tables. Under review at Scientific
  Reports</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04302v1</id>
    <updated>2018-08-13T15:54:50Z</updated>
    <published>2018-08-13T15:54:50Z</published>
    <title>Simple Root Cause Analysis by Separable Likelihoods</title>
    <summary>  Root Cause Analysis for Anomalies is challenging because of the trade-off
between the accuracy and its explanatory friendliness, required for industrial
applications. In this paper we propose a framework for simple and friendly RCA
within the Bayesian regime under certain restrictions (that Hessian at the mode
is diagonal, here referred to as \emph{separability}) imposed on the predictive
posterior. We show that this assumption is satisfied for important base models,
including Multinomal, Dirichlet-Multinomial and Naive Bayes. To demonstrate the
usefulness of the framework, we embed it into the Bayesian Net and validate on
web server error logs (real world data set).
</summary>
    <author>
      <name>Maciej Skorski</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05327v3</id>
    <updated>2018-08-13T15:46:23Z</updated>
    <published>2017-02-17T13:05:04Z</published>
    <title>Solving Equations of Random Convex Functions via Anchored Regression</title>
    <summary>  We consider the question of estimating a solution to a system of equations
that involve convex nonlinearities, a problem that is common in machine
learning and signal processing. Because of these nonlinearities, conventional
estimators based on empirical risk minimization generally involve solving a
non-convex optimization program. We propose anchored regression, a new approach
based on convex programming that amounts to maximizing a linear functional
(perhaps augmented by a regularizer) over a convex set. The proposed convex
program is formulated in the natural space of the problem, and avoids the
introduction of auxiliary variables, making it computationally favorable.
Working in the native space also provides great flexibility as structural
priors (e.g., sparsity) can be seamlessly incorporated.
  For our analysis, we model the equations as being drawn from a fixed set
according to a probability law. Our main results provide guarantees on the
accuracy of the estimator in terms of the number of equations we are solving,
the amount of noise present, a measure of statistical complexity of the random
equations, and the geometry of the regularizer at the true solution. We also
provide recipes for constructing the anchor vector (that determines the linear
functional to maximize) directly from the observed data.
</summary>
    <author>
      <name>Sohail Bahmani</name>
    </author>
    <author>
      <name>Justin Romberg</name>
    </author>
    <link href="http://arxiv.org/abs/1702.05327v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05327v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04293v1</id>
    <updated>2018-08-13T15:37:29Z</updated>
    <published>2018-08-13T15:37:29Z</published>
    <title>Fast, Better Training Trick -- Random Gradient</title>
    <summary>  In this paper, we will show an unprecedented method to accelerate training
and improve performance, which called random gradient (RG). This method can be
easier to the training of any model without extra calculation cost, we use
Image classification, Semantic segmentation, and GANs to confirm this method
can improve speed which is training model in computer vision. The central idea
is using the loss multiplied by a random number to random reduce the
back-propagation gradient. We can use this method to produce a better result in
Pascal VOC, Cifar, Cityscapes datasets.
</summary>
    <author>
      <name>Jiakai Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1708.07120 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04287v1</id>
    <updated>2018-08-13T15:24:01Z</updated>
    <published>2018-08-13T15:24:01Z</published>
    <title>Visual Sensor Network Reconfiguration with Deep Reinforcement Learning</title>
    <summary>  We present an approach for reconfiguration of dynamic visual sensor networks
with deep reinforcement learning (RL). Our RL agent uses a modified
asynchronous advantage actor-critic framework and the recently proposed
Relational Network module at the foundation of its network architecture. To
address the issue of sample inefficiency in current approaches to model-free
reinforcement learning, we train our system in an abstract simulation
environment that represents inputs from a dynamic scene. Our system is
validated using inputs from a real-world scenario and preexisting object
detection and tracking algorithms.
</summary>
    <author>
      <name>Paul Jasek</name>
    </author>
    <author>
      <name>Bernard Abayowa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04281v1</id>
    <updated>2018-08-13T15:07:55Z</updated>
    <published>2018-08-13T15:07:55Z</published>
    <title>Estimating Heterogeneous Causal Effects in the Presence of Irregular
  Assignment Mechanisms</title>
    <summary>  This paper provides a link between causal inference and machine learning
techniques - specifically, Classification and Regression Trees (CART) - in
observational studies where the receipt of the treatment is not randomized, but
the assignment to the treatment can be assumed to be randomized (irregular
assignment mechanism). The paper contributes to the growing applied machine
learning literature on causal inference, by proposing a modified version of the
Causal Tree (CT) algorithm to draw causal inference from an irregular
assignment mechanism. The proposed method is developed by merging the CT
approach with the instrumental variable framework to causal inference, hence
the name Causal Tree with Instrumental Variable (CT-IV). As compared to CT, the
main strength of CT-IV is that it can deal more efficiently with the
heterogeneity of causal effects, as demonstrated by a series of numerical
results obtained on synthetic data. Then, the proposed algorithm is used to
evaluate a public policy implemented by the Tuscan Regional Administration
(Italy), which aimed at easing the access to credit for small firms. In this
context, CT-IV breaks fresh ground for target-based policies, identifying
interesting heterogeneous causal effects.
</summary>
    <author>
      <name>Falco J. Bargagli Stoffi</name>
    </author>
    <author>
      <name>Giorgio Gnecco</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04260v1</id>
    <updated>2018-08-13T14:23:15Z</updated>
    <published>2018-08-13T14:23:15Z</published>
    <title>iNNvestigate neural networks!</title>
    <summary>  In recent years, deep neural networks have revolutionized many application
domains of machine learning and are key components of many critical decision or
predictive processes. Therefore, it is crucial that domain specialists can
understand and analyze actions and pre- dictions, even of the most complex
neural network architectures. Despite these arguments neural networks are often
treated as black boxes. In the attempt to alleviate this short- coming many
analysis methods were proposed, yet the lack of reference implementations often
makes a systematic comparison between the methods a major effort. The presented
library iNNvestigate addresses this by providing a common interface and
out-of-the- box implementation for many analysis methods, including the
reference implementation for PatternNet and PatternAttribution as well as for
LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an
analysis of image classifications for variety of state-of-the-art neural
network architectures.
</summary>
    <author>
      <name>Maximilian Alber</name>
    </author>
    <author>
      <name>Sebastian Lapuschkin</name>
    </author>
    <author>
      <name>Philipp Seegerer</name>
    </author>
    <author>
      <name>Miriam Hägele</name>
    </author>
    <author>
      <name>Kristof T. Schütt</name>
    </author>
    <author>
      <name>Grégoire Montavon</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <author>
      <name>Klaus-Robert Müller</name>
    </author>
    <author>
      <name>Sven Dähne</name>
    </author>
    <author>
      <name>Pieter-Jan Kindermans</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04752v1</id>
    <updated>2018-08-13T14:11:43Z</updated>
    <published>2018-08-13T14:11:43Z</published>
    <title>A Survey on Methods and Theories of Quantized Neural Networks</title>
    <summary>  Deep neural networks are the state-of-the-art methods for many real-world
tasks, such as computer vision, natural language processing and speech
recognition. For all its popularity, deep neural networks are also criticized
for consuming a lot of memory and draining battery life of devices during
training and inference. This makes it hard to deploy these models on mobile or
embedded devices which have tight resource constraints. Quantization is
recognized as one of the most effective approaches to satisfy the extreme
memory requirements that deep neural network models demand. Instead of adopting
32-bit floating point format to represent weights, quantized representations
store weights using more compact formats such as integers or even binary
numbers. Despite a possible degradation in predictive performance, quantization
provides a potential solution to greatly reduce the model size and the energy
consumption. In this survey, we give a thorough review of different aspects of
quantized neural networks. Current challenges and trends of quantized neural
networks are also discussed.
</summary>
    <author>
      <name>Yunhui Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 0.0</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09865v2</id>
    <updated>2018-08-13T13:17:15Z</updated>
    <published>2018-07-25T21:19:48Z</published>
    <title>Predicting Acute Kidney Injury at Hospital Re-entry Using
  High-dimensional Electronic Health Record Data</title>
    <summary>  Acute Kidney Injury (AKI), a sudden decline in kidney function, is associated
with increased mortality, morbidity, length of stay, and hospital cost. Since
AKI is sometimes preventable, there is great interest in prediction. Most
existing studies consider all patients and therefore restrict to features
available in the first hours of hospitalization. Here, the focus is instead on
rehospitalized patients, a cohort in which rich longitudinal features from
prior hospitalizations can be analyzed. Our objective is to provide a risk
score directly at hospital re-entry. Gradient boosting, penalized logistic
regression (with and without stability selection), and a recurrent neural
network are trained on two years of adult inpatient EHR data (3,387 attributes
for 34,505 patients who generated 90,013 training samples with 5,618 cases and
84,395 controls). Predictions are internally evaluated with 50 iterations of
5-fold grouped cross-validation with special emphasis on calibration, an
analysis of which is performed at the patient as well as hospitalization level.
Error is assessed with respect to diagnosis, race, age, gender, AKI
identification method, and hospital utilization. In an additional experiment,
the regularization penalty is severely increased to induce parsimony and
interpretability. Predictors identified for rehospitalized patients are also
reported with a special analysis of medications that might be modifiable risk
factors. Insights from this study might be used to construct a predictive tool
for AKI in rehospitalized patients. An accurate estimate of AKI risk at
hospital entry might serve as a prior for an admitting provider or another
predictive algorithm.
</summary>
    <author>
      <name>Samuel J. Weisenthal</name>
    </author>
    <author>
      <name>Caroline Quill</name>
    </author>
    <author>
      <name>Samir Farooq</name>
    </author>
    <author>
      <name>Henry Kautz</name>
    </author>
    <author>
      <name>Martin S. Zand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In revision</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04883v1</id>
    <updated>2018-08-13T11:45:16Z</updated>
    <published>2018-08-13T11:45:16Z</published>
    <title>COLA: Communication-Efficient Decentralized Linear Learning</title>
    <summary>  Decentralized machine learning is a promising emerging paradigm in view of
global challenges of data ownership and privacy. We consider learning of linear
classification and regression models, in the setting where the training data is
decentralized over many user devices, and the learning algorithm must run
on-device, on an arbitrary communication network, without a central
coordinator. We propose COLA, a new decentralized training algorithm with
strong theoretical guarantees and superior practical performance. Our framework
overcomes many limitations of existing methods, and achieves communication
efficiency, scalability, elasticity as well as resilience to changes in data
and participating devices.
</summary>
    <author>
      <name>Lie He</name>
    </author>
    <author>
      <name>An Bian</name>
    </author>
    <author>
      <name>Martin Jaggi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04152v1</id>
    <updated>2018-08-13T11:18:49Z</updated>
    <published>2018-08-13T11:18:49Z</published>
    <title>Learning Discriminative Hashing Codes for Cross-Modal Retrieval based on
  Multiorder Statistical Features</title>
    <summary>  Hashing techniques have been applied broadly in large-scale retrieval tasks
due to their low storage requirements and high speed of processing. Many
hashing methods have shown promising performance but as they fail to exploit
all structural information in learning the hashing function, they leave a scope
for improvement. The paper proposes a novel discrete hashing learning framework
which jointly performs classifier learning and subspace learning for
cross-modal retrieval. Concretely, the framework proposed in the paper includes
two stages, namely a kernelization process and a quantization process. The aim
of kernelization is to learn a common subspace where heterogeneous data can be
fused. The quantization process is designed to learn discriminative unified
hashing codes. Extensive experiments on three publicly available datasets
clearly indicate the superiority of our method compared with the
state-of-the-art methods.
</summary>
    <author>
      <name>Jun Yu</name>
    </author>
    <author>
      <name>Xiao-Jun Wu</name>
    </author>
    <author>
      <name>Josef Kittler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10pages,6figures,4tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04096v1</id>
    <updated>2018-08-13T08:12:22Z</updated>
    <published>2018-08-13T08:12:22Z</published>
    <title>Directed Policy Gradient for Safe Reinforcement Learning with Human
  Advice</title>
    <summary>  Many currently deployed Reinforcement Learning agents work in an environment
shared with humans, be them co-workers, users or clients. It is desirable that
these agents adjust to people's preferences, learn faster thanks to their help,
and act safely around them. We argue that most current approaches that learn
from human feedback are unsafe: rewarding or punishing the agent a-posteriori
cannot immediately prevent it from wrong-doing. In this paper, we extend Policy
Gradient to make it robust to external directives, that would otherwise break
the fundamentally on-policy nature of Policy Gradient. Our technique, Directed
Policy Gradient (DPG), allows a teacher or backup policy to override the agent
before it acts undesirably, while allowing the agent to leverage human advice
or directives to learn faster. Our experiments demonstrate that DPG makes the
agent learn much faster than reward-based approaches, while requiring an order
of magnitude less advice.
</summary>
    <author>
      <name>Hélène Plisnier</name>
    </author>
    <author>
      <name>Denis Steckelmacher</name>
    </author>
    <author>
      <name>Tim Brys</name>
    </author>
    <author>
      <name>Diederik M. Roijers</name>
    </author>
    <author>
      <name>Ann Nowé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the European Workshop on Reinforcement Learning 2018
  (EWRL14)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06396v3</id>
    <updated>2018-08-13T03:15:02Z</updated>
    <published>2018-03-16T20:57:36Z</published>
    <title>Reviving and Improving Recurrent Back-Propagation</title>
    <summary>  In this paper, we revisit the recurrent back-propagation (RBP) algorithm,
discuss the conditions under which it applies as well as how to satisfy them in
deep neural networks. We show that RBP can be unstable and propose two variants
based on conjugate gradient on the normal equations (CG-RBP) and Neumann series
(Neumann-RBP). We further investigate the relationship between Neumann-RBP and
back propagation through time (BPTT) and its truncated version (TBPTT). Our
Neumann-RBP has the same time complexity as TBPTT but only requires constant
memory, whereas TBPTT's memory cost scales linearly with the number of
truncation steps. We examine all RBP variants along with BPTT and TBPTT in
three different application domains: associative memory with continuous
Hopfield networks, document classification in citation networks using graph
neural networks and hyperparameter optimization for fully connected networks.
All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are
efficient and effective for optimizing convergent recurrent neural networks.
</summary>
    <author>
      <name>Renjie Liao</name>
    </author>
    <author>
      <name>Yuwen Xiong</name>
    </author>
    <author>
      <name>Ethan Fetaya</name>
    </author>
    <author>
      <name>Lisa Zhang</name>
    </author>
    <author>
      <name>KiJung Yoon</name>
    </author>
    <author>
      <name>Xaq Pitkow</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <author>
      <name>Richard Zemel</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06396v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06396v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04048v1</id>
    <updated>2018-08-13T02:53:00Z</updated>
    <published>2018-08-13T02:53:00Z</published>
    <title>Relax, and Accelerate: A Continuous Perspective on ADMM</title>
    <summary>  The acceleration technique first introduced by Nesterov for gradient descent
is widely used in many machine learning applications, however it is not yet
well-understood. Recently, significant progress has been made to close this
understanding gap by using a continuous-time dynamical system perspective
associated with gradient-based methods. In this paper, we extend this
perspective by considering the continuous limit of the family of relaxed
Alternating Direction Method of Multipliers (ADMM). We also introduce two new
families of relaxed and accelerated ADMM algorithms, one follows Nesterov's
acceleration approach and the other is inspired by Polyak's Heavy Ball method,
and derive the continuous limit of these families of relaxed and accelerated
algorithms as differential equations. Then, using a Lyapunov stability analysis
of the dynamical systems, we obtain rate-of-convergence results for convex and
strongly convex objective functions.
</summary>
    <author>
      <name>Guilherme França</name>
    </author>
    <author>
      <name>Daniel P. Robinson</name>
    </author>
    <author>
      <name>René Vidal</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02229v2</id>
    <updated>2018-08-13T02:19:08Z</updated>
    <published>2018-08-07T06:54:06Z</published>
    <title>Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep
  Learning</title>
    <summary>  Modern machine learning algorithms have been adopted in a range of
signal-processing applications spanning computer vision, natural language
processing, and artificial intelligence. Many relevant problems involve
subspace-structured features, orthogonality constrained or low-rank constrained
objective functions, or subspace distances. These mathematical characteristics
are expressed naturally using the Grassmann manifold. Unfortunately, this fact
is not yet explored in many traditional learning algorithms. In the last few
years, there have been growing interests in studying Grassmann manifold to
tackle new learning problems. Such attempts have been reassured by substantial
performance improvements in both classic learning and learning using deep
neural networks. We term the former as shallow and the latter deep Grassmannian
learning. The aim of this paper is to introduce the emerging area of
Grassmannian learning by surveying common mathematical problems and primary
solution approaches, and overviewing various applications. We hope to inspire
practitioners in different fields to adopt the powerful tool of Grassmannian
learning in their research.
</summary>
    <author>
      <name>Jiayao Zhang</name>
    </author>
    <author>
      <name>Guangxu Zhu</name>
    </author>
    <author>
      <name>Robert W. Heath Jr.</name>
    </author>
    <author>
      <name>Kaibin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Signal Processing Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02229v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02229v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04022v1</id>
    <updated>2018-08-12T23:47:10Z</updated>
    <published>2018-08-12T23:47:10Z</published>
    <title>Interpretable Time Series Classification using All-Subsequence Learning
  and Symbolic Representations in Time and Frequency Domains</title>
    <summary>  The time series classification literature has expanded rapidly over the last
decade, with many new classification approaches published each year. The
research focus has mostly been on improving the accuracy and efficiency of
classifiers, while their interpretability has been somewhat neglected.
Classifier interpretability has become a critical constraint for many
application domains and the introduction of the 'right to explanation' GDPR EU
legislation in May 2018 is likely to further emphasize the importance of
explainable learning algorithms. In this work we analyse the state-of-the-art
for time series classification, and propose new algorithms that aim to maintain
the classifier accuracy and efficiency, but keep interpretability as a key
design constraint. We present new time series classification algorithms that
advance the state-of-the-art by implementing the following three key ideas: (1)
Multiple resolutions of symbolic approximations: we combine symbolic
representations obtained using different parameters; (2) Multiple domain
representations: we combine symbolic approximations in time (e.g., SAX) and
frequency (e.g., SFA) domains; (3) Efficient navigation of a huge
symbolic-words space: we adapt a symbolic sequence classifier named SEQL, to
make it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL),
and use its greedy feature selection strategy to effectively filter the best
features for each representation. We show that a multi-resolution multi-domain
linear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the
state-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but
uses a fraction of the time required by either COTE or FCN. We discuss the
accuracy, efficiency and interpretability of our proposed algorithms. To
further analyse the interpretability aspect of our classifiers, we present a
case study on an ecology benchmark.
</summary>
    <author>
      <name>Thach Le Nguyen</name>
    </author>
    <author>
      <name>Severin Gsponer</name>
    </author>
    <author>
      <name>Iulia Ilie</name>
    </author>
    <author>
      <name>Georgiana Ifrim</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04008v1</id>
    <updated>2018-08-12T22:23:43Z</updated>
    <published>2018-08-12T22:23:43Z</published>
    <title>PAC-Battling Bandits with Plackett-Luce: Tradeoff between Sample
  Complexity and Subset Size</title>
    <summary>  We introduce the probably approximately correct (PAC) version of the problem
of {Battling-bandits} with the Plackett-Luce (PL) model -- an online learning
framework where in each trial, the learner chooses a subset of $k \le n$ arms
from a pool of fixed set of $n$ arms, and subsequently observes a stochastic
feedback indicating preference information over the items in the chosen subset;
e.g., the most preferred item or ranking of the top $m$ most preferred items
etc. The objective is to recover an `approximate-best' item of the underlying
PL model with high probability. This framework is motivated by practical
settings such as recommendation systems and information retrieval, where it is
easier and more efficient to collect relative feedback for multiple arms at
once. Our framework can be seen as a generalization of the well-studied
PAC-{Dueling-Bandit} problem over set of $n$ arms. We propose two different
feedback models: just the winner information (WI), and ranking of top-$m$ items
(TR), for any $2\le m \le k$. We show that with just the winner information
(WI), one cannot recover the `approximate-best' item with sample complexity
lesser than $\Omega\bigg( \frac{n}{\epsilon^2} \ln \frac{1}{\delta}\bigg)$,
which is independent of $k$, and same as the one required for standard dueling
bandit setting ($k=2$). However with top-$m$ ranking (TR) feedback, our lower
analysis proves an improved sample complexity guarantee of $\Omega\bigg(
\frac{n}{m\epsilon^2} \ln \frac{1}{\delta}\bigg)$, which shows a relative
improvement of $\frac{1}{m}$ factor compared to WI feedback, rightfully
justifying the additional information gain due to the knowledge of ranking of
topmost $m$ items. We also provide algorithms for each of the above feedback
models, our theoretical analyses proves the {optimality} of their sample
complexities which matches the derived lower bounds (upto logarithmic factors).
</summary>
    <author>
      <name>Aditya Gopalan</name>
    </author>
    <author>
      <name>Aadirupa Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05114v3</id>
    <updated>2018-08-12T16:45:06Z</updated>
    <published>2017-10-14T00:51:18Z</published>
    <title>Arbitrage-Free Regularization</title>
    <summary>  We introduce an unsupervised and non-anticipative machine learning algorithm
which is able to detect and remove arbitrage from a wide variety models. In
this framework, fundamental results and techniques from risk-neutral pricing
theory such as NFLVR, market completeness, and changes of measure are given an
equivalent formulation and extended to models which are deformable into
arbitrage-free models. We use this scheme to construct a meta-algorithm which
ensures that a wide range of factor estimation schemes return arbitrage-free
estimates and incorporate this additional information into their estimation
procedure. We show that using our meta-algorithm we are able to produce more
accurate estimates of forward-rate curves, specifically at the long-end. The
spread between a model and its arbitrage-free regularization is then used to
construct a mis-pricing detection or classification algorithm, which is in turn
used to develop a pairs trading strategy. Our theory provides a sound
theoretical foundation for a risk-neutral pricing theory capable of handling
models which potentially admit arbitrage but which can which can be deformed
into arbitrage-free models.
</summary>
    <author>
      <name>Anastasis Kratsios</name>
    </author>
    <author>
      <name>Cody B. Hyndman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">51 pages, revised paper now includes numerics and additional results</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.05114v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05114v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91G80, 91G30, 68T05, 94A15, 65J20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03965v1</id>
    <updated>2018-08-12T16:22:12Z</updated>
    <published>2018-08-12T16:22:12Z</published>
    <title>Large-Scale Learnable Graph Convolutional Networks</title>
    <summary>  Convolutional neural networks (CNNs) have achieved great success on grid-like
data such as images, but face tremendous challenges in learning from more
generic data such as graphs. In CNNs, the trainable local filters enable the
automatic extraction of high-level features. The computation with filters
requires a fixed number of ordered units in the receptive fields. However, the
number of neighboring units is neither fixed nor are they ordered in generic
graphs, thereby hindering the applications of convolutional operations. Here,
we address these challenges by proposing the learnable graph convolutional
layer (LGCL). LGCL automatically selects a fixed number of neighboring nodes
for each feature based on value ranking in order to transform graph data into
grid-like structures in 1-D format, thereby enabling the use of regular
convolutional operations on generic graphs. To enable model training on
large-scale graphs, we propose a sub-graph training method to reduce the
excessive memory and computational resource requirements suffered by prior
methods on graph convolutions. Our experimental results on node classification
tasks in both transductive and inductive learning settings demonstrate that our
methods can achieve consistently better performance on the Cora, Citeseer,
Pubmed citation network, and protein-protein interaction network datasets. Our
results also indicate that the proposed methods using sub-graph training
strategy are more efficient as compared to prior approaches.
</summary>
    <author>
      <name>Hongyang Gao</name>
    </author>
    <author>
      <name>Zhengyang Wang</name>
    </author>
    <author>
      <name>Shuiwang Ji</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219947</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219947" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 24th ACM SIGKDD International Conference on
  Knowledge Discovery &amp; Data Mining (pp. 1416-1424). ACM (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.03965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00967v2</id>
    <updated>2018-08-12T16:08:00Z</updated>
    <published>2018-03-02T17:40:18Z</published>
    <title>Active model learning and diverse action sampling for task and motion
  planning</title>
    <summary>  The objective of this work is to augment the basic abilities of a robot by
learning to use new sensorimotor primitives to enable the solution of complex
long-horizon problems. Solving long-horizon problems in complex domains
requires flexible generative planning that can combine primitive abilities in
novel combinations to solve problems as they arise in the world. In order to
plan to combine primitive actions, we must have models of the preconditions and
effects of those actions: under what circumstances will executing this
primitive achieve some particular effect in the world?
  We use, and develop novel improvements on, state-of-the-art methods for
active learning and sampling. We use Gaussian process methods for learning the
conditions of operator effectiveness from small numbers of expensive training
examples collected by experimentation on a robot. We develop adaptive sampling
methods for generating diverse elements of continuous sets (such as robot
configurations and object poses) during planning for solving a new task, so
that planning is as efficient as possible. We demonstrate these methods in an
integrated system, combining newly learned models with an efficient
continuous-space robot task and motion planner to learn to solve long horizon
problems more efficiently than was previously possible.
</summary>
    <author>
      <name>Zi Wang</name>
    </author>
    <author>
      <name>Caelan Reed Garrett</name>
    </author>
    <author>
      <name>Leslie Pack Kaelbling</name>
    </author>
    <author>
      <name>Tomás Lozano-Pérez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2018 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS), Madrid, Spain.
  https://www.youtube.com/playlist?list=PLoWhBFPMfSzDbc8CYelsbHZa1d3uz-W_c</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00967v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00967v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.06423v4</id>
    <updated>2018-08-12T16:03:00Z</updated>
    <published>2015-10-21T20:35:13Z</published>
    <title>Optimization as Estimation with Gaussian Processes in Bandit Settings</title>
    <summary>  Recently, there has been rising interest in Bayesian optimization -- the
optimization of an unknown function with assumptions usually expressed by a
Gaussian Process (GP) prior. We study an optimization strategy that directly
uses an estimate of the argmax of the function. This strategy offers both
practical and theoretical advantages: no tradeoff parameter needs to be
selected, and, moreover, we establish close connections to the popular GP-UCB
and GP-PI strategies. Our approach can be understood as automatically and
adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We
illustrate the effects of this adaptive tuning via bounds on the regret as well
as an extensive empirical evaluation on robotics and vision tasks,
demonstrating the robustness of this strategy for a range of performance
criteria.
</summary>
    <author>
      <name>Zi Wang</name>
    </author>
    <author>
      <name>Bolei Zhou</name>
    </author>
    <author>
      <name>Stefanie Jegelka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 19th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.06423v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06423v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03958v1</id>
    <updated>2018-08-12T15:37:40Z</updated>
    <published>2018-08-12T15:37:40Z</published>
    <title>Characterizing Neuronal Circuits with Spike-triggered Non-negative
  Matrix Factorization</title>
    <summary>  Neuronal circuits formed in the brain are complex with intricate connection
patterns. Such a complexity is also observed in the retina as a relatively
simple neuronal circuit. A retinal ganglion cell receives excitatory inputs
from neurons in previous layers as driving forces to fire spikes. Analytical
methods are required that can decipher these components in a systematic manner.
Recently a method termed spike-triggered non-negative matrix factorization
(STNMF) has been proposed for this purpose. In this study, we extend the scope
of the STNMF method. By using the retinal ganglion cell as a model system, we
show that STNMF can detect various biophysical properties of upstream bipolar
cells, including spatial receptive fields, temporal filters, and transfer
nonlinearity. In addition, we recover synaptic connection strengths from the
weight matrix of STNMF. Furthermore, we show that STNMF can separate spikes of
a ganglion cell into a few subsets of spikes where each subset is contributed
by one presynaptic bipolar cell. Taken together, these results corroborate that
STNMF is a useful method for deciphering the structure of neuronal circuits.
</summary>
    <author>
      <name>Shanshan Jia</name>
    </author>
    <author>
      <name>Zhaofei Yu</name>
    </author>
    <author>
      <name>Arno Onken</name>
    </author>
    <author>
      <name>Yonghong Tian</name>
    </author>
    <author>
      <name>Tiejun Huang</name>
    </author>
    <author>
      <name>Jian K. Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages,11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03953v1</id>
    <updated>2018-08-12T15:14:04Z</updated>
    <published>2018-08-12T15:14:04Z</published>
    <title>A Fourier View of REINFORCE</title>
    <summary>  We show a connection between the Fourier spectrum of Boolean functions and
the REINFORCE gradient estimator for binary latent variable models. We show
that REINFORCE estimates (up to a factor) the degree-1 Fourier coefficients of
a Boolean function. Using this connection we offer a new perspective on
variance reduction in gradient estimation for latent variable models: namely,
that variance reduction involves eliminating or reducing Fourier coefficients
that do not have degree 1. We then use this connection to develop low-variance
unbiased gradient estimators for binary latent variable models such as sigmoid
belief networks. The estimator is based upon properties of the noise operator
from Boolean Fourier theory and involves a sample-dependent baseline added to
the REINFORCE estimator in a way that keeps the estimator unbiased. The
baseline can be plugged into existing gradient estimators for further variance
reduction.
</summary>
    <author>
      <name>Adeel Pervez</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03920v1</id>
    <updated>2018-08-12T10:04:45Z</updated>
    <published>2018-08-12T10:04:45Z</published>
    <title>Multimodal Language Analysis with Recurrent Multistage Fusion</title>
    <summary>  Computational modeling of human multimodal language is an emerging research
area in natural language processing spanning the language, visual and acoustic
modalities. Comprehending multimodal language requires modeling not only the
interactions within each modality (intra-modal interactions) but more
importantly the interactions between modalities (cross-modal interactions). In
this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which
decomposes the fusion problem into multiple stages, each of them focused on a
subset of multimodal signals for specialized, effective fusion. Cross-modal
interactions are modeled using this multistage fusion approach which builds
upon intermediate representations of previous stages. Temporal and intra-modal
interactions are modeled by integrating our proposed fusion approach with a
system of recurrent neural networks. The RMFN displays state-of-the-art
performance in modeling human multimodal language across three public datasets
relating to multimodal sentiment analysis, emotion recognition, and speaker
traits recognition. We provide visualizations to show that each stage of fusion
focuses on a different subset of multimodal signals, learning increasingly
discriminative multimodal representations.
</summary>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Ziyin Liu</name>
    </author>
    <author>
      <name>Amir Zadeh</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03912v1</id>
    <updated>2018-08-12T08:56:06Z</updated>
    <published>2018-08-12T08:56:06Z</published>
    <title>Outer Product-based Neural Collaborative Filtering</title>
    <summary>  In this work, we contribute a new multi-layer neural network architecture
named ONCF to perform collaborative filtering. The idea is to use an outer
product to explicitly model the pairwise correlations between the dimensions of
the embedding space. In contrast to existing neural recommender models that
combine user embedding and item embedding via a simple concatenation or
element-wise product, our proposal of using outer product above the embedding
layer results in a two-dimensional interaction map that is more expressive and
semantically plausible. Above the interaction map obtained by outer product, we
propose to employ a convolutional neural network to learn high-order
correlations among embedding dimensions. Extensive experiments on two public
implicit feedback data demonstrate the effectiveness of our proposed ONCF
framework, in particular, the positive effect of using outer product to model
the correlations between embedding dimensions in the low level of multi-layer
neural recommender model. The experiment codes are available at:
https://github.com/duxy-me/ConvNCF
</summary>
    <author>
      <name>Xiangnan He</name>
    </author>
    <author>
      <name>Xiaoyu Du</name>
    </author>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Feng Tian</name>
    </author>
    <author>
      <name>Jinhui Tang</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03908v1</id>
    <updated>2018-08-12T08:26:25Z</updated>
    <published>2018-08-12T08:26:25Z</published>
    <title>Adversarial Personalized Ranking for Recommendation</title>
    <summary>  Item recommendation is a personalized ranking task. To this end, many
recommender systems optimize models with pairwise ranking objectives, such as
the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) ---
the most widely used model in recommendation --- as a demonstration, we show
that optimizing it with BPR leads to a recommender model that is not robust. In
particular, we find that the resultant model is highly vulnerable to
adversarial perturbations on its model parameters, which implies the possibly
large error in generalization.
  To enhance the robustness of a recommender model and thus improve its
generalization performance, we propose a new optimization framework, namely
Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise
ranking method BPR by performing adversarial training. It can be interpreted as
playing a minimax game, where the minimization of the BPR objective function
meanwhile defends an adversary, which adds adversarial perturbations on model
parameters to maximize the BPR objective function. To illustrate how it works,
we implement APR on MF by adding adversarial perturbations on the embedding
vectors of users and items. Extensive experiments on three public real-world
datasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it
outperforms BPR with a relative improvement of 11.2% on average and achieves
state-of-the-art performance for item recommendation. Our implementation is
available at: https://github.com/hexiangnan/adversarial_personalized_ranking.
</summary>
    <author>
      <name>Xiangnan He</name>
    </author>
    <author>
      <name>Zhankui He</name>
    </author>
    <author>
      <name>Xiaoyu Du</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3209978.3209981</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3209978.3209981" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03889v1</id>
    <updated>2018-08-12T04:34:24Z</updated>
    <published>2018-08-12T04:34:24Z</published>
    <title>Robust high dimensional factor models with applications to statistical
  machine learning</title>
    <summary>  Factor models are a class of powerful statistical models that have been
widely used to deal with dependent measurements that arise frequently from
various applications from genomics and neuroscience to economics and finance.
As data are collected at an ever-growing scale, statistical machine learning
faces some new challenges: high dimensionality, strong dependence among
observed variables, heavy-tailed variables and heterogeneity. High-dimensional
robust factor analysis serves as a powerful toolkit to conquer these
challenges.
  This paper gives a selective overview on recent advance on high-dimensional
factor models and their applications to statistics including Factor-Adjusted
Robust Model selection (FarmSelect) and Factor-Adjusted Robust Multiple testing
(FarmTest). We show that classical methods, especially principal component
analysis (PCA), can be tailored to many new problems and provide powerful tools
for statistical estimation and inference. We highlight PCA and its connections
to matrix perturbation theory, robust statistics, random projection, false
discovery rate, etc., and illustrate through several applications how insights
from these fields yield solutions to modern challenges. We also present
far-reaching connections between factor models and popular statistical learning
problems, including network analysis and low-rank matrix recovery.
</summary>
    <author>
      <name>Jianqing Fan</name>
    </author>
    <author>
      <name>Kaizheng Wang</name>
    </author>
    <author>
      <name>Yiqiao Zhong</name>
    </author>
    <author>
      <name>Ziwei Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03880v1</id>
    <updated>2018-08-12T01:56:17Z</updated>
    <published>2018-08-12T01:56:17Z</published>
    <title>Parallelization does not Accelerate Convex Optimization: Adaptivity
  Lower Bounds for Non-smooth Convex Minimization</title>
    <summary>  In this paper we study the limitations of parallelization in convex
optimization. A convenient approach to study parallelization is through the
prism of \emph{adaptivity} which is an information theoretic measure of the
parallel runtime of an algorithm. Informally, adaptivity is the number of
sequential rounds an algorithm needs to make when it can execute
polynomially-many queries in parallel at every round. For combinatorial
optimization with black-box oracle access, the study of adaptivity has recently
led to exponential accelerations in parallel runtime and the natural question
is whether dramatic accelerations are achievable for convex optimization.
  Our main result is a spoiler. We show that, in general, parallelization does
not accelerate convex optimization. In particular, for the problem of
minimizing a non-smooth Lipschitz and strongly convex function with black-box
oracle access we give information theoretic lower bounds that indicate that the
number of adaptive rounds of any randomized algorithm exactly match the upper
bounds of single-query-per-round (i.e. non-parallel) algorithms.
</summary>
    <author>
      <name>Eric Balkanski</name>
    </author>
    <author>
      <name>Yaron Singer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03874v1</id>
    <updated>2018-08-11T22:32:55Z</updated>
    <published>2018-08-11T22:32:55Z</published>
    <title>Orders-of-magnitude speedup in atmospheric chemistry modeling through
  neural network-based emulation</title>
    <summary>  Chemical transport models (CTMs), which simulate air pollution transport,
transformation, and removal, are computationally expensive, largely because of
the computational intensity of the chemical mechanisms: systems of coupled
differential equations representing atmospheric chemistry. Here we investigate
the potential for machine learning to reproduce the behavior of a chemical
mechanism, yet with reduced computational expense. We create a 17-layer
residual multi-target regression neural network to emulate the Carbon Bond
Mechanism Z (CBM-Z) gas-phase chemical mechanism. We train the network to match
CBM-Z predictions of changes in concentrations of 77 chemical species after one
hour, given a range of chemical and meteorological input conditions, which it
is able to do with root-mean-square error (RMSE) of less than 1.97 ppb (median
RMSE = 0.02 ppb), while achieving a 250x computational speedup. An additional
17x speedup (total 4250x speedup) is achieved by running the neural network on
a graphics-processing unit (GPU). The neural network is able to reproduce the
emergent behavior of the chemical system over diurnal cycles using Euler
integration, but additional work is needed to constrain the propagation of
errors as simulation time progresses.
</summary>
    <author>
      <name>Makoto M. Kelp</name>
    </author>
    <author>
      <name>Christopher W. Tessum</name>
    </author>
    <author>
      <name>Julian D. Marshall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer code for training the neural network emulator and the
  trained model (CSV and python scripts) and supplemental text and figures
  (PDF) are available by request by emailing corresponding author Christopher
  Tessum</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03873v1</id>
    <updated>2018-08-11T22:28:11Z</updated>
    <published>2018-08-11T22:28:11Z</published>
    <title>A Consistent Method for Learning OOMs from Asymptotically Stationary
  Time Series Data Containing Missing Values</title>
    <summary>  In the traditional framework of spectral learning of stochastic time series
models, model parameters are estimated based on trajectories of fully recorded
observations. However, real-world time series data often contain missing
values, and worse, the distributions of missingness events over time are often
not independent of the visible process. Recently, a spectral OOM learning
algorithm for time series with missing data was introduced and proved to be
consistent, albeit under quite strong conditions. Here we refine the algorithm
and prove that the original strong conditions can be very much relaxed. We
validate our theoretical findings by numerical experiments, showing that the
algorithm can consistently handle missingness patterns whose dynamic interacts
with the visible process.
</summary>
    <author>
      <name>Tianlin Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03857v1</id>
    <updated>2018-08-11T20:15:28Z</updated>
    <published>2018-08-11T20:15:28Z</published>
    <title>Ranking with Features: Algorithm and A Graph Theoretic Analysis</title>
    <summary>  We consider the problem of ranking a set of items from pairwise comparisons
in the presence of features associated with the items. Recent works have
established that $O(n\log(n))$ samples are needed to rank well when there is no
feature information present. However, this might be sub-optimal in the presence
of associated features. We introduce a new probabilistic preference model
called feature-Bradley-Terry-Luce (f-BTL) model that generalizes the standard
BTL model to incorporate feature information. We present a new least squares
based algorithm called fBTL-LS which we show requires much lesser than
$O(n\log(n))$ pairs to obtain a good ranking -- precisely our new sample
complexity bound is of $O(\alpha\log \alpha)$, where $\alpha$ denotes the
number of `independent items' of the set, in general $\alpha &lt;&lt; n$. Our
analysis is novel and makes use of tools from classical graph matching theory
to provide tighter bounds that sheds light on the true complexity of the
ranking problem, capturing the item dependencies in terms of their feature
representations. This was not possible with earlier matrix completion based
tools used for this problem. We also prove an information theoretic lower bound
on the required sample complexity for recovering the underlying ranking, which
essentially shows the tightness of our proposed algorithms. The efficacy of our
proposed algorithms are validated through extensive experimental evaluations on
a variety of synthetic and real world datasets.
</summary>
    <author>
      <name>Aadirupa Saha</name>
    </author>
    <author>
      <name>Arun Rajkumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03856v1</id>
    <updated>2018-08-11T20:12:49Z</updated>
    <published>2018-08-11T20:12:49Z</published>
    <title>Neural Importance Sampling</title>
    <summary>  We propose to use deep neural networks for generating samples in Monte Carlo
integration. Our work is based on non-linear independent component analysis,
which we extend in numerous ways to improve performance and enable its
application to integration problems. First, we introduce piecewise-polynomial
coupling transforms that greatly increase the modeling power of individual
coupling layers. Second, we propose to preprocess the inputs of neural networks
using one-blob encoding, which stimulates localization of computation and
improves inference. Third, we derive a gradient-descent-based optimization for
the KL and the $\chi^2$ divergence for the specific application of Monte Carlo
integration with stochastic estimates of the target distribution. Our approach
enables fast and accurate inference and efficient sample generation independent
of the dimensionality of the integration domain. We demonstrate the benefits of
our approach for generating natural images and in two applications to
light-transport simulation. First, we show how to learn joint path-sampling
densities in primary sample space and how to importance sample
multi-dimensional path prefixes thereof. Second, we use our technique to
extract conditional directional densities driven by the triple product of the
rendering equation, and leverage them for path guiding. In all applications,
our approach yields on-par or higher performance at equal sample count than
competing techniques.
</summary>
    <author>
      <name>Thomas Müller</name>
    </author>
    <author>
      <name>Brian McWilliams</name>
    </author>
    <author>
      <name>Fabrice Rousselle</name>
    </author>
    <author>
      <name>Markus Gross</name>
    </author>
    <author>
      <name>Jan Novák</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04362v1</id>
    <updated>2018-08-11T19:43:22Z</updated>
    <published>2018-08-11T19:43:22Z</published>
    <title>A Domain Guided CNN Architecture for Predicting Age from Structural
  Brain Images</title>
    <summary>  Given the wide success of convolutional neural networks (CNNs) applied to
natural images, researchers have begun to apply them to neuroimaging data. To
date, however, exploration of novel CNN architectures tailored to neuroimaging
data has been limited. Several recent works fail to leverage the 3D structure
of the brain, instead treating the brain as a set of independent 2D slices.
Approaches that do utilize 3D convolutions rely on architectures developed for
object recognition tasks in natural 2D images. Such architectures make
assumptions about the input that may not hold for neuroimaging. For example,
existing architectures assume that patterns in the brain exhibit translation
invariance. However, a pattern in the brain may have different meaning
depending on where in the brain it is located. There is a need to explore novel
architectures that are tailored to brain images. We present two simple
modifications to existing CNN architectures based on brain image structure.
Applied to the task of brain age prediction, our network achieves a mean
absolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline
that achieves a MAE of 1.6 years. Our results suggest that lessons learned from
developing models on natural images may not directly transfer to neuroimaging
tasks. Instead, there remains a large space of unexplored questions regarding
model development in this area, whose answers may differ from conventional
wisdom.
</summary>
    <author>
      <name>Pascal Sturmfels</name>
    </author>
    <author>
      <name>Saige Rutherford</name>
    </author>
    <author>
      <name>Mike Angstadt</name>
    </author>
    <author>
      <name>Mark Peterson</name>
    </author>
    <author>
      <name>Chandra Sripada</name>
    </author>
    <author>
      <name>Jenna Wiens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Healthcare 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11048v2</id>
    <updated>2018-08-11T17:26:17Z</updated>
    <published>2018-05-25T04:46:48Z</published>
    <title>Scalable Spectral Clustering Using Random Binning Features</title>
    <summary>  Spectral clustering is one of the most effective clustering approaches that
capture hidden cluster structures in the data. However, it does not scale well
to large-scale problems due to its quadratic complexity in constructing
similarity graphs and computing subsequent eigendecomposition. Although a
number of methods have been proposed to accelerate spectral clustering, most of
them compromise considerable information loss in the original data for reducing
computational bottlenecks. In this paper, we present a novel scalable spectral
clustering method using Random Binning features (RB) to simultaneously
accelerate both similarity graph construction and the eigendecomposition.
Specifically, we implicitly approximate the graph similarity (kernel) matrix by
the inner product of a large sparse feature matrix generated by RB. Then we
introduce a state-of-the-art SVD solver to effectively compute eigenvectors of
this large matrix for spectral clustering. Using these two building blocks, we
reduce the computational cost from quadratic to linear in the number of data
points while achieving similar accuracy. Our theoretical analysis shows that
spectral clustering via RB converges faster to the exact spectral clustering
than the standard Random Feature approximation. Extensive experiments on 8
benchmarks show that the proposed method either outperforms or matches the
state-of-the-art methods in both accuracy and runtime. Moreover, our method
exhibits linear scalability in both the number of data samples and the number
of RB features.
</summary>
    <author>
      <name>Lingfei Wu</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Ian En-Hsu Yen</name>
    </author>
    <author>
      <name>Fangli Xu</name>
    </author>
    <author>
      <name>Yinglong Xia</name>
    </author>
    <author>
      <name>Charu Aggarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD'18, Oral Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11048v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11048v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03835v1</id>
    <updated>2018-08-11T16:47:58Z</updated>
    <published>2018-08-11T16:47:58Z</published>
    <title>jLDADMM: A Java package for the LDA and DMM topic models</title>
    <summary>  In this technical report, we present jLDADMM---an easy-to-use Java toolkit
for conventional topic models. jLDADMM is released to provide alternatives for
topic modeling on normal or short texts. It provides implementations of the
Latent Dirichlet Allocation topic model and the one-topic-per-document
Dirichlet Multinomial Mixture model (i.e. mixture of unigrams), using collapsed
Gibbs sampling. In addition, jLDADMM supplies a document clustering evaluation
to compare topic models. jLDADMM is open-source and available to download at:
https://github.com/datquocnguyen/jLDADMM
</summary>
    <author>
      <name>Dat Quoc Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04530v2</id>
    <updated>2018-08-11T16:30:35Z</updated>
    <published>2016-01-18T14:31:12Z</published>
    <title>Domain based classification</title>
    <summary>  The majority of traditional classification ru les minimizing the expected
probability of error (0-1 loss) are inappropriate if the class probability
distributions are ill-defined or impossible to estimate. We argue that in such
cases class domains should be used instead of class distributions or densities
to construct a reliable decision function. Proposals are presented for some
evaluation criteria and classifier learning schemes, illustrated by an example.
</summary>
    <author>
      <name>Robert P. W. Duin</name>
    </author>
    <author>
      <name>Elzbieta Pekalska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, unpublished paper written in 2005, discussing a significant,
  still almost not studied problem. missing reference links corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09704v3</id>
    <updated>2018-08-11T14:24:07Z</updated>
    <published>2018-03-26T16:36:37Z</published>
    <title>MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time
  Series Forecasting</title>
    <summary>  Time series forecasting is ubiquitous in the modern world. Applications range
from health care to astronomy, and include climate modelling, financial trading
and monitoring of critical engineering equipment. To offer value over this
range of activities, models must not only provide accurate forecasts, but also
quantify and adjust their uncertainty over time. In this work, we directly
tackle this task with a novel, fully end-to-end deep learning method for time
series forecasting. By recasting time series forecasting as an ordinal
regression task, we develop a principled methodology to assess long-term
predictive uncertainty and describe rich multimodal, non-Gaussian behaviour,
which arises regularly in applied settings.
  Notably, our framework is a wholly general-purpose approach that requires
little to no user intervention to be used. We showcase this key feature in a
large-scale benchmark test with 45 datasets drawn from both, a wide range of
real-world application domains, as well as a comprehensive list of synthetic
maps. This wide comparison encompasses state-of-the-art methods in both the
Machine Learning and Statistics modelling literature, such as the Gaussian
Process. We find that our approach does not only provide excellent predictive
forecasts, shadowing true future values, but also allows us to infer valuable
information, such as the predictive distribution of the occurrence of critical
events of interest, accurately and reliably even over long time horizons.
</summary>
    <author>
      <name>Bernardo Pérez Orozco</name>
    </author>
    <author>
      <name>Gabriele Abbati</name>
    </author>
    <author>
      <name>Stephen Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 4 figures * This version includes a new large-scale
  experiment where we evaluate our model over 45 datasets and 4 different state
  of the art baselines</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09704v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09704v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03753v1</id>
    <updated>2018-08-11T05:05:26Z</updated>
    <published>2018-08-11T05:05:26Z</published>
    <title>MARVIN: An Open Machine Learning Corpus and Environment for Automated
  Machine Learning Primitive Annotation and Execution</title>
    <summary>  In this demo paper, we introduce the DARPA D3M program for automatic machine
learning (ML) and JPL's MARVIN tool that provides an environment to locate,
annotate, and execute machine learning primitives for use in ML pipelines.
MARVIN is a web-based application and associated back-end interface written in
Python that enables composition of ML pipelines from hundreds of primitives
from the world of Scikit-Learn, Keras, DL4J and other widely used libraries.
MARVIN allows for the creation of Docker containers that run on Kubernetes
clusters within DARPA to provide an execution environment for automated machine
learning. MARVIN currently contains over 400 datasets and challenge problems
from a wide array of ML domains including routine classification and regression
to advanced video/image classification and remote sensing.
</summary>
    <author>
      <name>Chris A. Mattmann</name>
    </author>
    <author>
      <name>Sujen Shah</name>
    </author>
    <author>
      <name>Brian Wilson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03749v1</id>
    <updated>2018-08-11T04:36:53Z</updated>
    <published>2018-08-11T04:36:53Z</published>
    <title>Neural Network Encapsulation</title>
    <summary>  A capsule is a collection of neurons which represents different variants of a
pattern in the network. The routing scheme ensures only certain capsules which
resemble lower counterparts in the higher layer should be activated. However,
the computational complexity becomes a bottleneck for scaling up to larger
networks, as lower capsules need to correspond to each and every higher
capsule. To resolve this limitation, we approximate the routing process with
two branches: a master branch which collects primary information from its
direct contact in the lower layer and an aide branch that replenishes master
based on pattern variants encoded in other lower capsules. Compared with
previous iterative and unsupervised routing scheme, these two branches are
communicated in a fast, supervised and one-time pass fashion. The complexity
and runtime of the model are therefore decreased by a large margin. Motivated
by the routing to make higher capsule have agreement with lower capsule, we
extend the mechanism as a compensation for the rapid loss of information in
nearby layers. We devise a feedback agreement unit to send back higher capsules
as feedback. It could be regarded as an additional regularization to the
network. The feedback agreement is achieved by comparing the optimal transport
divergence between two distributions (lower and higher capsules). Such an
add-on witnesses a unanimous gain in both capsule and vanilla networks. Our
proposed EncapNet performs favorably better against previous state-of-the-arts
on CIFAR10/100, SVHN and a subset of ImageNet.
</summary>
    <author>
      <name>Hongyang Li</name>
    </author>
    <author>
      <name>Xiaoyang Guo</name>
    </author>
    <author>
      <name>Bo Dai</name>
    </author>
    <author>
      <name>Wanli Ouyang</name>
    </author>
    <author>
      <name>Xiaogang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01371v2</id>
    <updated>2018-08-11T01:59:23Z</updated>
    <published>2018-08-03T21:44:29Z</published>
    <title>Large Scale Language Modeling: Converging on 40GB of Text in Four Hours</title>
    <summary>  Recent work has shown how to train Convolutional Neural Networks (CNNs)
rapidly on large image datasets, then transfer the knowledge gained from these
models to a variety of tasks. Following [Radford 2017], in this work, we
demonstrate similar scalability and transfer for Recurrent Neural Networks
(RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and
a 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to
train a character-level 4096-dimension multiplicative LSTM (mLSTM) for
unsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews
dataset in four hours. This runtime compares favorably with previous work
taking one month to train the same size and configuration for one epoch over
the same dataset. Converging large batch RNN models can be challenging. Recent
work has suggested scaling the learning rate as a function of batch size, but
we find that simply scaling the learning rate as a function of batch size leads
either to significantly worse convergence or immediate divergence for this
problem. We provide a learning rate schedule that allows our model to converge
with a 32k batch size. Since our model converges over the Amazon Reviews
dataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while
substantial, is commercially available, this work opens up large scale
unsupervised NLP training to most commercial applications and deep learning
researchers. A model can be trained over most public or private text datasets
overnight.
</summary>
    <author>
      <name>Raul Puri</name>
    </author>
    <author>
      <name>Robert Kirby</name>
    </author>
    <author>
      <name>Nikolai Yakovenko</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; To appear in High Performance Machine Learning Workshop
  (HPML) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04072v5</id>
    <updated>2018-08-10T21:26:28Z</updated>
    <published>2017-09-12T22:28:18Z</published>
    <title>A convergence framework for inexact nonconvex and nonsmooth algorithms
  and its applications to several iterations</title>
    <summary>  In this paper, we consider the convergence of an abstract inexact nonconvex
and nonsmooth algorithm. We promise a pseudo sufficient descent condition and a
pseudo relative error condition, which are both related to an auxiliary
sequence, for the algorithm; and a continuity condition is assumed to hold. In
fact, a lot of classical inexact nonconvex and nonsmooth algorithms allow these
three conditions. Under a special kind of summable assumption on the auxiliary
sequence, we prove the sequence generated by the general algorithm converges to
a critical point of the objective function if being assumed Kurdyka-
Lojasiewicz property. The core of the proofs lies in building a new Lyapunov
function, whose successive difference provides a bound for the successive
difference of the points generated by the algorithm. And then, we apply our
findings to several classical nonconvex iterative algorithms and derive the
corresponding convergence results
</summary>
    <author>
      <name>Tao Sun</name>
    </author>
    <author>
      <name>Hao Jiang</name>
    </author>
    <author>
      <name>Lizhi Cheng</name>
    </author>
    <author>
      <name>Wei Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04072v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04072v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03620v1</id>
    <updated>2018-08-10T16:55:33Z</updated>
    <published>2018-08-10T16:55:33Z</published>
    <title>Ensemble Kalman Inversion: A Derivative-Free Technique For Machine
  Learning Tasks</title>
    <summary>  The standard probabilistic perspective on machine learning gives rise to
empirical risk-minimization tasks that are frequently solved by stochastic
gradient descent (SGD) and variants thereof. We present a formulation of these
tasks as classical inverse or filtering problems and, furthermore, we propose
an efficient, gradient-free algorithm for finding a solution to these problems
using ensemble Kalman inversion (EKI). Applications of our approach include
offline and online supervised learning with deep neural networks, as well as
graph-based semi-supervised learning. The essence of the EKI procedure is an
ensemble based approximate gradient descent in which derivatives are replaced
by differences from within the ensemble. We suggest several modifications to
the basic method, derived from empirically successful heuristics developed in
the context of SGD. Numerical results demonstrate wide applicability and
robustness of the proposed algorithm.
</summary>
    <author>
      <name>Nikola B. Kovachki</name>
    </author>
    <author>
      <name>Andrew M. Stuart</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20, 65L09, 65K10, 49M15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03666v3</id>
    <updated>2018-08-10T16:37:59Z</updated>
    <published>2018-03-09T19:26:11Z</published>
    <title>Standing Wave Decomposition Gaussian Process</title>
    <summary>  We propose a Standing Wave Decomposition (SWD) approximation to Gaussian
Process regression (GP). GP involves a costly matrix inversion operation, which
limits applicability to large data analysis. For an input space that can be
approximated by a grid and when correlations among data are short-ranged, the
kernel matrix inversion can be replaced by analytic diagonalization using the
SWD. We show that this approach applies to uni- and multi-dimensional input
data, extends to include longer-range correlations, and the grid can be in a
latent space and used as inducing points. Through simulations, we show that our
approximate method applied to the squared exponential kernel outperforms
existing methods in predictive accuracy per unit time in the regime where data
are plentiful. Our SWD-GP is recommended for regression analyses where there is
a relatively large amount of data and/or there are constraints on computation
time.
</summary>
    <author>
      <name>Chi-Ken Lu</name>
    </author>
    <author>
      <name>Scott Cheng-Hsin Yang</name>
    </author>
    <author>
      <name>Patrick Shafto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures; updated version includes a modified introduction
  and a new discussion on time complexity of our approximated GP method. New
  references are added. Simulation package will be announced later; updated
  with discussion of validity of perturbation treatment of Eq. (25) with added
  Fig. 6 as evidence</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03666v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03666v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10070v2</id>
    <updated>2018-08-10T16:35:17Z</updated>
    <published>2018-04-26T14:01:12Z</published>
    <title>Adaptive pooling operators for weakly labeled sound event detection</title>
    <summary>  Sound event detection (SED) methods are tasked with labeling segments of
audio recordings by the presence of active sound sources. SED is typically
posed as a supervised machine learning problem, requiring strong annotations
for the presence or absence of each sound source at every time instant within
the recording. However, strong annotations of this type are both labor- and
cost-intensive for human annotators to produce, which limits the practical
scalability of SED methods.
  In this work, we treat SED as a multiple instance learning (MIL) problem,
where training labels are static over a short excerpt, indicating the presence
or absence of sound sources but not their temporal locality. The models,
however, must still produce temporally dynamic predictions, which must be
aggregated (pooled) when comparing against static labels during training. To
facilitate this aggregation, we develop a family of adaptive pooling
operators---referred to as auto-pool---which smoothly interpolate between
common pooling operators, such as min-, max-, or average-pooling, and
automatically adapt to the characteristics of the sound sources in question. We
evaluate the proposed pooling operators on three datasets, and demonstrate that
in each case, the proposed methods outperform non-adaptive pooling operators
for static prediction, and nearly match the performance of models trained with
strong, dynamic annotations. The proposed method is evaluated in conjunction
with convolutional neural networks, but can be readily applied to any
differentiable model for time-series label prediction.
</summary>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Justin Salamon</name>
    </author>
    <author>
      <name>Juan Pablo Bello</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10070v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10070v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03604v1</id>
    <updated>2018-08-10T16:08:27Z</updated>
    <published>2018-08-10T16:08:27Z</published>
    <title>Disease Progression Timeline Estimation for Alzheimer's Disease using
  Discriminative Event Based Modeling</title>
    <summary>  Alzheimer's Disease (AD) is characterized by a cascade of biomarkers becoming
abnormal, the pathophysiology of which is very complex and largely unknown.
Event-based modeling (EBM) is a data-driven technique to estimate the sequence
in which biomarkers for a disease become abnormal based on cross-sectional
data. It can help in understanding the dynamics of disease progression and
facilitate early diagnosis and prognosis. In this work we propose a novel
discriminative approach to EBM, which is shown to be more accurate than
existing state-of-the-art EBM methods. The method first estimates for each
subject an approximate ordering of events. Subsequently, the central ordering
over all subjects is estimated by fitting a generalized Mallows model to these
approximate subject-specific orderings. We also introduce the concept of
relative distance between events which helps in creating a disease progression
timeline. Subsequently, we propose a method to stage subjects by placing them
on the estimated disease progression timeline. We evaluated the proposed method
on Alzheimer's Disease Neuroimaging Initiative (ADNI) data and compared the
results with existing state-of-the-art EBM methods. We also performed extensive
experiments on synthetic data simulating the progression of Alzheimer's
disease. The event orderings obtained on ADNI data seem plausible and are in
agreement with the current understanding of progression of AD. The proposed
patient staging algorithm performed consistently better than that of
state-of-the-art EBM methods. Event orderings obtained in simulation
experiments were more accurate than those of other EBM methods and the
estimated disease progression timeline was observed to correlate with the
timeline of actual disease progression. The results of these experiments are
encouraging and suggest that discriminative EBM is a promising approach to
disease progression modeling.
</summary>
    <author>
      <name>Vikram Venkatraghavan</name>
    </author>
    <author>
      <name>Esther E. Bron</name>
    </author>
    <author>
      <name>Wiro J. Niessen</name>
    </author>
    <author>
      <name>Stefan Klein</name>
    </author>
    <author>
      <name>for the Alzheimer's Disease Neuroimaging Initiative</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03601v1</id>
    <updated>2018-08-10T15:59:31Z</updated>
    <published>2018-08-10T15:59:31Z</published>
    <title>Using Randomness to Improve Robustness of Machine-Learning Models
  Against Evasion Attacks</title>
    <summary>  Machine learning models have been widely used in security applications such
as intrusion detection, spam filtering, and virus or malware detection.
However, it is well-known that adversaries are always trying to adapt their
attacks to evade detection. For example, an email spammer may guess what
features spam detection models use and modify or remove those features to avoid
detection. There has been some work on making machine learning models more
robust to such attacks. However, one simple but promising approach called {\em
randomization} is underexplored. This paper proposes a novel
randomization-based approach to improve robustness of machine learning models
against evasion attacks. The proposed approach incorporates randomization into
both model training time and model application time (meaning when the model is
used to detect attacks). We also apply this approach to random forest, an
existing ML method which already has some degree of randomness. Experiments on
intrusion detection and spam filtering data show that our approach further
improves robustness of random-forest method. We also discuss how this approach
can be applied to other ML models.
</summary>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Zhiyuan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03591v1</id>
    <updated>2018-08-10T15:38:14Z</updated>
    <published>2018-08-10T15:38:14Z</published>
    <title>How Complex is your classification problem? A survey on measuring
  classification complexity</title>
    <summary>  Extracting characteristics from the training datasets of classification
problems has proven effective in a number of meta-analyses. Among them,
measures of classification complexity can estimate the difficulty in separating
the data points into their expected classes. Descriptors of the spatial
distribution of the data and estimates of the shape and size of the decision
boundary are among the existent measures for this characterization. This
information can support the formulation of new data-driven pre-processing and
pattern recognition techniques, which can in turn be focused on challenging
characteristics of the problems. This paper surveys and analyzes measures which
can be extracted from the training datasets in order to characterize the
complexity of the respective classification problems. Their use in recent
literature is also reviewed and discussed, allowing to prospect opportunities
for future work in the area. Finally, descriptions are given on an R package
named Extended Complexity Library (ECoL) that implements a set of complexity
measures and is made publicly available.
</summary>
    <author>
      <name>Ana C. Lorena</name>
    </author>
    <author>
      <name>Luís P. F. Garcia</name>
    </author>
    <author>
      <name>Jens Lehmann</name>
    </author>
    <author>
      <name>Marcilio C. P. Souto</name>
    </author>
    <author>
      <name>Tin K. Ho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Survey paper, 27 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04439v1</id>
    <updated>2018-08-10T15:25:10Z</updated>
    <published>2018-08-10T15:25:10Z</published>
    <title>Image Registration and Predictive Modeling: Learning the Metric on the
  Space of Diffeomorphisms</title>
    <summary>  We present a method for metric optimization in the Large Deformation
Diffeomorphic Metric Mapping (LDDMM) framework, by treating the induced
Riemannian metric on the space of diffeomorphisms as a kernel in a machine
learning context. For simplicity, we choose the kernel Fischer Linear
Discriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters
in an Expectation-Maximization framework, we define model fidelity via the
hinge loss of the decision function. The resulting algorithm optimizes the
parameters of the LDDMM norm-inducing differential operator as a solution to a
group-wise registration and classification problem. In practice, this may lead
to a biology-aware registration, focusing its attention on the predictive task
at hand such as identifying the effects of disease. We first tested our
algorithm on a synthetic dataset, showing that our parameter selection improves
registration quality and classification accuracy. We then tested the algorithm
on 3D subcortical shapes from the Schizophrenia cohort Schizconnect. Our
Schizpohrenia-Control predictive model showed significant improvement in ROC
AUC compared to baseline parameters.
</summary>
    <author>
      <name>Ayagoz Mussabayeva</name>
    </author>
    <author>
      <name>Alexey Kroshnin</name>
    </author>
    <author>
      <name>Anvar Kurmukov</name>
    </author>
    <author>
      <name>Yulia Dodonova</name>
    </author>
    <author>
      <name>Li Shen</name>
    </author>
    <author>
      <name>Shan Cong</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Boris A. Gutman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ShapeMI workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03578v1</id>
    <updated>2018-08-10T15:06:05Z</updated>
    <published>2018-08-10T15:06:05Z</published>
    <title>Dropout is a special case of the stochastic delta rule: faster and more
  accurate deep learning</title>
    <summary>  Multi-layer neural networks have lead to remarkable performance on many kinds
of benchmark tasks in text, speech and image processing. Nonlinear parameter
estimation in hierarchical models is known to be subject to overfitting. One
approach to this overfitting and related problems (local minima, colinearity,
feature discovery etc.) is called dropout (Srivastava, et al 2014, Baldi et al
2016). This method removes hidden units with a Bernoulli random variable with
probability $p$ over updates. In this paper we will show that Dropout is a
special case of a more general model published originally in 1990 called the
stochastic delta rule ( SDR, Hanson, 1990). SDR parameterizes each weight in
the network as a random variable with mean $\mu_{w_{ij}}$ and standard
deviation $\sigma_{w_{ij}}$. These random variables are sampled on each forward
activation, consequently creating an exponential number of potential networks
with shared weights. Both parameters are updated according to prediction error,
thus implementing weight noise injections that reflect a local history of
prediction error and efficient model averaging. SDR therefore implements a
local gradient-dependent simulated annealing per weight converging to a bayes
optimal network. Tests on standard benchmarks (CIFAR) using a modified version
of DenseNet shows the SDR outperforms standard dropout in error by over 50% and
in loss by over 50%. Furthermore, the SDR implementation converges on a
solution much faster, reaching a training error of 5 in just 15 epochs with
DenseNet-40 compared to standard DenseNet-40's 94 epochs.
</summary>
    <author>
      <name>Noah Frazier-Logue</name>
    </author>
    <author>
      <name>Stephen José Hanson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures; submitted to NIPS</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03566v1</id>
    <updated>2018-08-10T14:35:38Z</updated>
    <published>2018-08-10T14:35:38Z</published>
    <title>Greedy Algorithms for Approximating the Diameter of Machine Learning
  Datasets in Multidimensional Euclidean Space</title>
    <summary>  Finding the diameter of a dataset in multidimensional Euclidean space is a
well-established problem, with well-known algorithms. However, most of the
algorithms found in the literature do not scale well with large values of data
dimension, so the time complexity grows exponentially in most cases, which
makes these algorithms impractical. Therefore, we implemented 4 simple greedy
algorithms to be used for approximating the diameter of a multidimensional
dataset; these are based on minimum/maximum l2 norms, hill climbing search,
Tabu search and Beam search approaches, respectively. The time complexity of
the implemented algorithms is near-linear, as they scale near-linearly with
data size and its dimensions. The results of the experiments (conducted on
different machine learning data sets) prove the efficiency of the implemented
algorithms and can therefore be recommended for finding the diameter to be used
by different machine learning applications when needed.
</summary>
    <author>
      <name>Ahmad B. Hassanat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09917v3</id>
    <updated>2018-08-10T12:41:24Z</updated>
    <published>2018-05-23T16:55:16Z</published>
    <title>Machine-learning inference of fluid variables from data using reservoir
  computing</title>
    <summary>  We infer both microscopic and macroscopic behaviors of a three-dimensional
chaotic fluid flow using reservoir computing. In our procedure of the
inference, we assume no prior knowledge of a physical process of a fluid flow
except that its behavior is complex but deterministic. We present two ways of
inference of the complex behavior; the first called partial-inference requires
continued knowledge of partial time-series data during the inference as well as
past time-series data, while the second called full-inference requires only
past time-series data as training data. For the first case, we are able to
infer long-time motion of microscopic fluid variables. For the second case, we
show that the reservoir dynamics constructed from only past data of energy
functions can infer the future behavior of energy functions and reproduce the
energy spectrum. It is also shown that we can infer a time-series data from
only one measurement by using the delay coordinates. These implies that the
obtained two reservoir systems constructed without the knowledge of microscopic
data are equivalent to the dynamical systems describing macroscopic behavior of
energy functions.
</summary>
    <author>
      <name>Kengo Nakai</name>
    </author>
    <author>
      <name>Yoshitaka Saiki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09917v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09917v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="37N10, 35R99, 68T05, 76F55" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03504v1</id>
    <updated>2018-08-10T12:21:47Z</updated>
    <published>2018-08-10T12:21:47Z</published>
    <title>Model Approximation Using Cascade of Tree Decompositions</title>
    <summary>  In this paper, we present a general, multistage framework for graphical model
approximation using a cascade of models such as trees. In particular, we look
at the problem of covariance matrix approximation for Gaussian distributions as
linear transformations of tree models. This is a new way to decompose the
covariance matrix. Here, we propose an algorithm which incorporates the
Cholesky factorization method to compute the decomposition matrix and thus can
approximate a simple graphical model using a cascade of the Cholesky
factorization of the tree approximation transformations. The Cholesky
decomposition enables us to achieve a tree structure factor graph at each
cascade stage of the algorithm which facilitates the use of the message passing
algorithm since the approximated graph has less loops compared to the original
graph. The overall graph is a cascade of factor graphs with each factor graph
being a tree. This is a different perspective on the approximation model, and
algorithms such as Gaussian belief propagation can be used on this overall
graph. Here, we present theoretical result that guarantees the convergence of
the proposed model approximation using the cascade of tree decompositions. In
the simulations, we look at synthetic and real data and measure the performance
of the proposed framework by comparing the KL divergences.
</summary>
    <author>
      <name>Navid Tafaghodi Khajavi</name>
    </author>
    <author>
      <name>Anthony Kuh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01280v3</id>
    <updated>2018-08-10T11:26:09Z</updated>
    <published>2018-08-03T02:27:40Z</published>
    <title>Geared Rotationally Identical and Invariant Convolutional Neural Network
  Systems</title>
    <summary>  Theorems and techniques to form different types of transformationally
invariant processing and to produce the same output quantitatively based on
either transformationally invariant operators or symmetric operations have
recently been introduced by the authors. In this study, we further propose to
compose a geared rotationally identical CNN system (GRI-CNN) with a small step
angle by connecting networks of participated processes at the first flatten
layer. Using an ordinary CNN structure as a base, requirements for constructing
a GRI-CNN include the use of either symmetric input vector or kernels with an
angle increment that can form a complete cycle as a "gearwheel". Four basic
GRI-CNN structures were studied. Each of them can produce quantitatively
identical output results when a rotation angle of the input vector is evenly
divisible by the step angle of the gear. Our study showed when an input vector
rotated with an angle does not match to a step angle, the GRI-CNN can also
produce a highly consistent result. With a design of using an ultra-fine
gear-tooth step angle (e.g., 1 degree or 0.1 degree), all four GRI-CNN systems
can be constructed virtually isotropically.
</summary>
    <author>
      <name>ShihChung B. Lo</name>
    </author>
    <author>
      <name>Ph. D.</name>
    </author>
    <author>
      <name>Matthew T. Freedman</name>
    </author>
    <author>
      <name>M. D.</name>
    </author>
    <author>
      <name>Seong K. Mun</name>
    </author>
    <author>
      <name>Ph. D.</name>
    </author>
    <author>
      <name>Heang-Ping Chan</name>
    </author>
    <author>
      <name>Ph. D</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01280v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01280v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04216v1</id>
    <updated>2018-08-10T10:09:54Z</updated>
    <published>2018-08-10T10:09:54Z</published>
    <title>Effective Unsupervised Author Disambiguation with Relative Frequencies</title>
    <summary>  This work addresses the problem of author name homonymy in the Web of
Science. Aiming for an efficient, simple and straightforward solution, we
introduce a novel probabilistic similarity measure for author name
disambiguation based on feature overlap. Using the researcher-ID available for
a subset of the Web of Science, we evaluate the application of this measure in
the context of agglomeratively clustering author mentions. We focus on a
concise evaluation that shows clearly for which problem setups and at which
time during the clustering process our approach works best. In contrast to most
other works in this field, we are sceptical towards the performance of author
name disambiguation methods in general and compare our approach to the trivial
single-cluster baseline. Our results are presented separately for each correct
clustering size as we can explain that, when treating all cases together, the
trivial baseline and more sophisticated approaches are hardly distinguishable
in terms of evaluation results. Our model shows state-of-the-art performance
for all correct clustering sizes without any discriminative training and with
tuning only one convergence parameter.
</summary>
    <author>
      <name>Tobias Backes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3197026.3197036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3197026.3197036" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of JCDL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04433v1</id>
    <updated>2018-08-10T09:30:52Z</updated>
    <published>2018-08-10T09:30:52Z</published>
    <title>Out of the Black Box: Properties of deep neural networks and their
  applications</title>
    <summary>  Deep neural networks are powerful machine learning approaches that have
exhibited excellent results on many classification tasks. However, they are
considered as black boxes and some of their properties remain to be formalized.
In the context of image recognition, it is still an arduous task to understand
why an image is recognized or not. In this study, we formalize some properties
shared by eight state-of-the-art deep neural networks in order to grasp the
principles allowing a given deep neural network to classify an image. Our
results, tested on these eight networks, show that an image can be sub-divided
into several regions (patches) responding at different degrees of probability
(local property). With the same patch, some locations in the image can answer
two (or three) orders of magnitude higher than other locations (spatial
property). Some locations are activators and others inhibitors
(activation-inhibition property). The repetition of the same patch can increase
(or decrease) the probability of recognition of an object (cumulative
property). Furthermore, we propose a new approach called Deepception that
exploits these properties to deceive a deep neural network. We obtain for the
VGG-VDD-19 neural network a fooling ratio of 88\%. Thanks to our
"Psychophysics" approach, no prior knowledge on the networks architectures is
required.
</summary>
    <author>
      <name>Nizar Ouarti</name>
    </author>
    <author>
      <name>David Carmona</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01840v2</id>
    <updated>2018-08-10T09:07:40Z</updated>
    <published>2018-03-02T19:26:16Z</published>
    <title>TACO: Learning Task Decomposition via Temporal Alignment for Control</title>
    <summary>  Many advanced Learning from Demonstration (LfD) methods consider the
decomposition of complex, real-world tasks into simpler sub-tasks. By reusing
the corresponding sub-policies within and between tasks, they provide training
data for each policy from different high-level tasks and compose them to
perform novel ones. Existing approaches to modular LfD focus either on learning
a single high-level task or depend on domain knowledge and temporal
segmentation. In contrast, we propose a weakly supervised, domain-agnostic
approach based on task sketches, which include only the sequence of sub-tasks
performed in each demonstration. Our approach simultaneously aligns the
sketches with the observed demonstrations and learns the required sub-policies.
This improves generalisation in comparison to separate optimisation procedures.
We evaluate the approach on multiple domains, including a simulated 3D robot
arm control task using purely image-based observations. The results show that
our approach performs commensurately with fully supervised approaches, while
requiring significantly less annotation effort.
</summary>
    <author>
      <name>Kyriacos Shiarlis</name>
    </author>
    <author>
      <name>Markus Wulfmeier</name>
    </author>
    <author>
      <name>Sasha Salter</name>
    </author>
    <author>
      <name>Shimon Whiteson</name>
    </author>
    <author>
      <name>Ingmar Posner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages. Published at ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01840v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01840v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04262v1</id>
    <updated>2018-08-10T08:54:31Z</updated>
    <published>2018-08-10T08:54:31Z</published>
    <title>Connectivity-Driven Brain Parcellation via Consensus Clustering</title>
    <summary>  We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
</summary>
    <author>
      <name>Anvar Kurmukov</name>
    </author>
    <author>
      <name>Ayagoz Mussabayeva</name>
    </author>
    <author>
      <name>Yulia Denisova</name>
    </author>
    <author>
      <name>Daniel Moyer</name>
    </author>
    <author>
      <name>Boris Gutman</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04258v1</id>
    <updated>2018-08-10T07:16:49Z</updated>
    <published>2018-08-10T07:16:49Z</published>
    <title>Model Reduction with Memory and the Machine Learning of Dynamical
  Systems</title>
    <summary>  The well-known Mori-Zwanzig theory tells us that model reduction leads to
memory effect. For a long time, modeling the memory effect accurately and
efficiently has been an important but nearly impossible task in developing a
good reduced model. In this work, we explore a natural analogy between
recurrent neural networks and the Mori-Zwanzig formalism to establish a
systematic approach for developing reduced models with memory. Two training
models-a direct training model and a dynamically coupled training model-are
proposed and compared. We apply these methods to the Kuramoto-Sivashinsky
equation and the Navier-Stokes equation. Numerical experiments show that the
proposed method can produce reduced model with good performance on both
short-term prediction and long-term statistical properties.
</summary>
    <author>
      <name>Chao Ma</name>
    </author>
    <author>
      <name>Jianchun Wang</name>
    </author>
    <author>
      <name>Weinan E</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03425v1</id>
    <updated>2018-08-10T06:36:40Z</updated>
    <published>2018-08-10T06:36:40Z</published>
    <title>Learning and Inference on Generative Adversarial Quantum Circuits</title>
    <summary>  Quantum mechanics is inherently probabilistic in light of Born's rule. Using
quantum circuits as probabilistic generative models for classical data exploits
their superior expressibility and efficient direct sampling ability. However,
training of quantum circuits can be more challenging compared to classical
neural networks due to lack of efficient differentiable learning algorithm. We
devise an adversarial quantum-classical hybrid training scheme via coupling a
quantum circuit generator and a classical neural network discriminator
together. After training, the quantum circuit generative model can infer
missing data with quadratic speed up via amplitude amplification. We
numerically simulate the learning and inference of generative adversarial
quantum circuit using the prototypical Bars-and-Stripes dataset. Generative
adversarial quantum circuits is a fresh approach to machine learning which may
enjoy the practically useful quantum advantage on near-term quantum devices.
</summary>
    <author>
      <name>Jinfeng Zeng</name>
    </author>
    <author>
      <name>Yufeng Wu</name>
    </author>
    <author>
      <name>Jin-Guo Liu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Jiangping Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03064v2</id>
    <updated>2018-08-10T06:30:10Z</updated>
    <published>2018-08-09T09:19:34Z</published>
    <title>Gradient and Newton Boosting for Classification and Regression</title>
    <summary>  Boosting algorithms enjoy large popularity due to their high predictive
accuracy on a wide array of datasets. In this article, we argue that it is
important to distinguish between three types of statistical boosting
algorithms: gradient and Newton boosting as well as a hybrid variant of the
two. To date, both researchers and practitioners often do not discriminate
between these boosting variants. We compare the different boosting algorithms
on a wide range of real and simulated datasets for various choices of loss
functions using trees as base learners. In addition, we introduce a novel
tuning parameter for Newton boosting. We find that Newton boosting performs
substantially better than the other boosting variants for classification, and
that the novel tuning parameter is important for predictive accuracy
</summary>
    <author>
      <name>Fabio Sigrist</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03420v1</id>
    <updated>2018-08-10T05:53:12Z</updated>
    <published>2018-08-10T05:53:12Z</published>
    <title>Hierarchical Block Sparse Neural Networks</title>
    <summary>  Sparse deep neural networks(DNNs) are efficient in both memory and compute
when compared to dense DNNs. But due to irregularity in computation of sparse
DNNs, their efficiencies are much lower than that of dense DNNs on general
purpose hardwares. This leads to poor/no performance benefits for sparse DNNs.
Performance issue for sparse DNNs can be alleviated by bringing structure to
the sparsity and leveraging it for improving runtime efficiency. But such
structural constraints often lead to sparse models with suboptimal accuracies.
In this work, we jointly address both accuracy and performance of sparse DNNs
using our proposed class of neural networks called HBsNN ( Hierarchical Block
Sparse Neural Networks).
</summary>
    <author>
      <name>Dharma Teja Vooturi</name>
    </author>
    <author>
      <name>Dheevatsa Mudigree</name>
    </author>
    <author>
      <name>Sasikanth Avancha</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03408v1</id>
    <updated>2018-08-10T04:18:48Z</updated>
    <published>2018-08-10T04:18:48Z</published>
    <title>On the Convergence of AdaGrad with Momentum for Training Deep Neural
  Networks</title>
    <summary>  Adaptive stochastic gradient descent methods, such as AdaGrad, Adam,
AdaDelta, Nadam, AMSGrad, \textit{etc.}, have been demonstrated efficacious in
solving non-convex stochastic optimization, such as training deep neural
networks. However, their convergence rates have not been touched under the
non-convex stochastic circumstance except recent breakthrough results on
AdaGrad \cite{ward2018adagrad} and perturbed AdaGrad \cite{li2018convergence}.
In this paper, we propose two new adaptive stochastic gradient methods called
AdaHB and AdaNAG which integrate coordinate-wise AdaGrad with heavy ball
momentum and Nesterov accelerated gradient momentum, respectively. The
$\mathcal{O}(\frac{\log{T}}{\sqrt{T}})$ non-asymptotic convergence rates of
AdaHB and AdaNAG in non-convex stochastic setting are also jointly
characterized by leveraging a newly developed unified formulation of these two
momentum mechanisms. In particular, when momentum term vanishes we obtain
convergence rate of coordinate-wise AdaGrad in non-convex stochastic setting as
a byproduct.
</summary>
    <author>
      <name>Fangyu Zou</name>
    </author>
    <author>
      <name>Li Shen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00672v4</id>
    <updated>2018-08-10T03:39:13Z</updated>
    <published>2016-09-02T17:21:25Z</published>
    <title>The Inflation Technique for Causal Inference with Latent Variables</title>
    <summary>  The problem of causal inference is to determine if a given probability
distribution on observed variables is compatible with some causal structure.
The difficult case is when the causal structure includes latent variables. We
here introduce the $\textit{inflation technique}$ for tackling this problem. An
inflation of a causal structure is a new causal structure that can contain
multiple copies of each of the original variables, but where the ancestry of
each copy mirrors that of the original. To every distribution of the observed
variables that is compatible with the original causal structure, we assign a
family of marginal distributions on certain subsets of the copies that are
compatible with the inflated causal structure. It follows that compatibility
constraints for the inflation can be translated into compatibility constraints
for the original causal structure. Even if the constraints at the level of
inflation are weak, such as observable statistical independences implied by
disjoint causal ancestry, the translated constraints can be strong. We apply
this method to derive new inequalities whose violation by a distribution
witnesses that distribution's incompatibility with the causal structure (of
which Bell inequalities and Pearl's instrumental inequality are prominent
examples). We describe an algorithm for deriving all such inequalities for the
original causal structure that follow from ancestral independences in the
inflation. For three observed binary variables with pairwise common causes, it
yields inequalities that are stronger in at least some aspects than those
obtainable by existing methods. We also describe an algorithm that derives a
weaker set of inequalities but is more efficient. Finally, we discuss which
inflations are such that the inequalities one obtains from them remain valid
even for quantum (and post-quantum) generalizations of the notion of a causal
model.
</summary>
    <author>
      <name>Elie Wolfe</name>
    </author>
    <author>
      <name>Robert W. Spekkens</name>
    </author>
    <author>
      <name>Tobias Fritz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Notation and content organization have been improved so as to make
  the paper more accessible to non-physicists</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00672v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00672v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06501v3</id>
    <updated>2018-08-10T02:33:08Z</updated>
    <published>2018-02-19T02:30:10Z</published>
    <title>Recommendations with Negative Feedback via Pairwise Deep Reinforcement
  Learning</title>
    <summary>  Recommender systems play a crucial role in mitigating the problem of
information overload by suggesting users' personalized items or services. The
vast majority of traditional recommender systems consider the recommendation
procedure as a static process and make recommendations following a fixed
strategy. In this paper, we propose a novel recommender system with the
capability of continuously improving its strategies during the interactions
with users. We model the sequential interactions between users and a
recommender system as a Markov Decision Process (MDP) and leverage
Reinforcement Learning (RL) to automatically learn the optimal strategies via
recommending trial-and-error items and receiving reinforcements of these items
from users' feedback. Users' feedback can be positive and negative and both
types of feedback have great potentials to boost recommendations. However, the
number of negative feedback is much larger than that of positive one; thus
incorporating them simultaneously is challenging since positive feedback could
be buried by negative one. In this paper, we develop a novel approach to
incorporate them into the proposed deep recommender system (DEERS) framework.
The experimental results based on real-world e-commerce data demonstrate the
effectiveness of the proposed framework. Further experiments have been
conducted to understand the importance of both positive and negative feedback
in recommendations.
</summary>
    <author>
      <name>Xiangyu Zhao</name>
    </author>
    <author>
      <name>Liang Zhang</name>
    </author>
    <author>
      <name>Zhuoye Ding</name>
    </author>
    <author>
      <name>Long Xia</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <author>
      <name>Dawei Yin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219886</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219886" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1801.00209</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06501v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06501v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03388v1</id>
    <updated>2018-08-10T01:58:27Z</updated>
    <published>2018-08-10T01:58:27Z</published>
    <title>Code-division multiplexed resistive pulse sensor networks for
  spatio-temporal detection of particles in microfluidic devices</title>
    <summary>  Spatial separation of suspended particles based on contrast in their physical
or chemical properties forms the basis of various biological assays performed
on lab-on-achip devices. To electronically acquire this information, we have
recently introduced a microfluidic sensing platform, called Microfluidic CODES,
which combines the resistive pulse sensing with the code division multiple
access in multiplexing a network of integrated electrical sensors. In this
paper, we enhance the multiplexing capacity of the Microfluidic CODES by
employing sensors that generate non-orthogonal code waveforms and a new
decoding algorithm that combines machine learning techniques with minimum
mean-squared error estimation. As a proof of principle, we fabricated a
microfluidic device with a network of 10 code-multiplexed sensors and
characterized it using cells suspended in phosphate buffer saline solution.
</summary>
    <author>
      <name>Ningquan Wang</name>
    </author>
    <author>
      <name>Ruxiu Liu</name>
    </author>
    <author>
      <name>Roozbeh Khodambashi</name>
    </author>
    <author>
      <name>Norh Asmare</name>
    </author>
    <author>
      <name>A. Fatih Sarioglu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MEMSYS.2017.7863416</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MEMSYS.2017.7863416" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE 30th International Conference on Micro Electro Mechanical
  Systems (MEMS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03351v1</id>
    <updated>2018-08-09T21:36:02Z</updated>
    <published>2018-08-09T21:36:02Z</published>
    <title>Exploiting Structure for Fast Kernel Learning</title>
    <summary>  We propose two methods for exact Gaussian process (GP) inference and learning
on massive image, video, spatial-temporal, or multi-output datasets with
missing values (or "gaps") in the observed responses. The first method ignores
the gaps using sparse selection matrices and a highly effective low-rank
preconditioner is introduced to accelerate computations. The second method
introduces a novel approach to GP training whereby response values are inferred
on the gaps before explicitly training the model. We find this second approach
to be greatly advantageous for the class of problems considered. Both of these
novel approaches make extensive use of Kronecker matrix algebra to design
massively scalable algorithms which have low memory requirements. We
demonstrate exact GP inference for a spatial-temporal climate modelling problem
with 3.7 million training points as well as a video reconstruction problem with
1 billion points.
</summary>
    <author>
      <name>Trefor W. Evans</name>
    </author>
    <author>
      <name>Prasanth B. Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the proceedings of the SIAM International Conference on
  Data Mining (SDM), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03350v1</id>
    <updated>2018-08-09T21:34:12Z</updated>
    <published>2018-08-09T21:34:12Z</published>
    <title>Uncovering the Spread of Chagas Disease in Argentina and Mexico</title>
    <summary>  Chagas disease is a neglected disease, and information about its geographical
spread is very scarse. We analyze here mobility and calling patterns in order
to identify potential risk zones for the disease, by using public health
information and mobile phone records. Geolocalized call records are rich in
social and mobility information, which can be used to infer whether an
individual has lived in an endemic area. We present two case studies in Latin
American countries. Our objective is to generate risk maps which can be used by
public health campaign managers to prioritize detection campaigns and target
specific areas. Finally, we analyze the value of mobile phone data to infer
long-term migrations, which play a crucial role in the geographical spread of
Chagas disease.
</summary>
    <author>
      <name>Juan de Monasterio</name>
    </author>
    <author>
      <name>Alejo Salles</name>
    </author>
    <author>
      <name>Carolina Lang</name>
    </author>
    <author>
      <name>Diego Weinberg</name>
    </author>
    <author>
      <name>Martin Minnoni</name>
    </author>
    <author>
      <name>Matias Travizano</name>
    </author>
    <author>
      <name>Carlos Sarraute</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in NetMob 2017 (Fifth Conference on the Scientific Analysis
  of Mobile Phone Datasets), Milan, Italy. April 5, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03331v1</id>
    <updated>2018-08-09T20:08:13Z</updated>
    <published>2018-08-09T20:08:13Z</published>
    <title>The Effectiveness of Multitask Learning for Phenotyping with Electronic
  Health Records Data</title>
    <summary>  Electronic phenotyping, which is the task of ascertaining whether an
individual has a medical condition of interest by analyzing their medical
records, is a foundational task in clinical informatics. Increasingly,
electronic phenotyping is performed via supervised learning. We investigate the
effectiveness of multitask learning for phenotyping using electronic health
records (EHR) data. Multitask learning aims to improve model performance on a
target task by jointly learning additional auxiliary tasks, and has been used
to good effect in disparate areas of machine learning. However, its utility
when applied to EHR data has not been established, and prior work suggests that
its benefits are inconsistent. Here we present experiments that elucidate when
multitask learning with neural networks can improve performance for electronic
phenotyping using EHR data relative to well-tuned single task neural networks.
We find that multitask networks consistently outperform single task networks
for rare phenotypes but underperform for more common phenotypes. The effect
size increases as more auxiliary tasks are added.
</summary>
    <author>
      <name>Daisy Yi Ding</name>
    </author>
    <author>
      <name>Chloé Simpson</name>
    </author>
    <author>
      <name>Stephen Pfohl</name>
    </author>
    <author>
      <name>Dave C. Kale</name>
    </author>
    <author>
      <name>Kenneth Jung</name>
    </author>
    <author>
      <name>Nigam H. Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of an article submitted for consideration in Pacific
  Symposium on Biocomputing 2018, https://psb.stanford.edu/psb-online/, 12
  pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04444v1</id>
    <updated>2018-08-09T18:44:38Z</updated>
    <published>2018-08-09T18:44:38Z</published>
    <title>Character-Level Language Modeling with Deeper Self-Attention</title>
    <summary>  LSTMs and other RNN variants have shown strong performance on character-level
language modeling. These models are typically trained using truncated
backpropagation through time, and it is common to assume that their success
stems from their ability to remember long-term contexts. In this paper, we show
that a deep (64-layer) transformer model with fixed context outperforms RNN
variants by a large margin, achieving state of the art on two popular
benchmarks- 1.13 bits per character on text8 and 1.06 on enwik8. To get good
results at this depth, we show that it is important to add auxiliary losses,
both at intermediate network layers and intermediate sequence positions.
</summary>
    <author>
      <name>Rami Al-Rfou</name>
    </author>
    <author>
      <name>Dokook Choe</name>
    </author>
    <author>
      <name>Noah Constant</name>
    </author>
    <author>
      <name>Mandy Guo</name>
    </author>
    <author>
      <name>Llion Jones</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08898v5</id>
    <updated>2018-08-09T18:24:09Z</updated>
    <published>2018-02-24T19:23:21Z</published>
    <title>Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo</title>
    <summary>  Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from
high-dimensional distributions in Statistics and Machine learning. HMC is known
to run very efficiently in practice and its popular second-order "leapfrog"
implementation has long been conjectured to run in $d^{1/4}$ gradient
evaluations. Here we show that this conjecture is true when sampling from
strongly log-concave target distributions that satisfy a weak third-order
regularity property associated with the input data. Our regularity condition is
weaker than the Lipschitz Hessian property and allows us to show faster
convergence bounds for a much larger class of distributions than would be
possible with the usual Lipschitz Hessian constant alone. Important
distributions that satisfy our regularity condition include posterior
distributions used in Bayesian logistic regression for which the data satisfies
an "incoherence" property. Our result compares favorably with the best
available bounds for the class of strongly log-concave distributions, which
grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our
simulations on synthetic data suggest that, when our regularity condition is
satisfied, leapfrog HMC performs better than its competitors -- both in terms
of accuracy and in terms of the number of gradient evaluations it requires.
</summary>
    <author>
      <name>Oren Mangoubi</name>
    </author>
    <author>
      <name>Nisheeth K. Vishnoi</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08898v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08898v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04441v1</id>
    <updated>2018-08-09T17:55:31Z</updated>
    <published>2018-08-09T17:55:31Z</published>
    <title>Deep Morphing: Detecting bone structures in fluoroscopic X-ray images
  with prior knowledge</title>
    <summary>  We propose approaches based on deep learning to localize objects in images
when only a small training dataset is available and the images have low
quality. That applies to many problems in medical image processing, and in
particular to the analysis of fluoroscopic (low-dose) X-ray images, where the
images have low contrast. We solve the problem by incorporating high-level
information about the objects, which could be a simple geometrical model, like
a circular outline, or a more complex statistical model. A simple geometrical
representation can sufficiently describe some objects and only requires minimal
labeling. Statistical shape models can be used to represent more complex
objects. We propose computationally efficient two-stage approaches, which we
call deep morphing, for both representations by fitting the representation to
the output of a deep segmentation network.
</summary>
    <author>
      <name>Aaron Pries</name>
    </author>
    <author>
      <name>Peter J. Schreier</name>
    </author>
    <author>
      <name>Artur Lamm</name>
    </author>
    <author>
      <name>Stefan Pede</name>
    </author>
    <author>
      <name>Jürgen Schmidt</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03253v1</id>
    <updated>2018-08-09T17:35:52Z</updated>
    <published>2018-08-09T17:35:52Z</published>
    <title>Counterfactual Normalization: Proactively Addressing Dataset Shift and
  Improving Reliability Using Causal Mechanisms</title>
    <summary>  Predictive models can fail to generalize from training to deployment
environments because of dataset shift, posing a threat to model reliability and
the safety of downstream decisions made in practice. Instead of using samples
from the target distribution to reactively correct dataset shift, we use
graphical knowledge of the causal mechanisms relating variables in a prediction
problem to proactively remove relationships that do not generalize across
environments, even when these relationships may depend on unobserved variables
(violations of the "no unobserved confounders" assumption). To accomplish this,
we identify variables with unstable paths of statistical influence and remove
them from the model. We also augment the causal graph with latent
counterfactual variables that isolate unstable paths of statistical influence,
allowing us to retain stable paths that would otherwise be removed. Our
experiments demonstrate that models that remove vulnerable variables and use
estimates of the latent variables transfer better, often outperforming in the
target domain despite some accuracy loss in the training domain.
</summary>
    <author>
      <name>Adarsh Subbaswamy</name>
    </author>
    <author>
      <name>Suchi Saria</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 34th Conference on Uncertainty in Artificial
  Intelligence (UAI), 2018. Revised from print version</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03233v1</id>
    <updated>2018-08-09T16:56:04Z</updated>
    <published>2018-08-09T16:56:04Z</published>
    <title>OBOE: Collaborative Filtering for AutoML Initialization</title>
    <summary>  Algorithm selection and hyperparameter tuning remain two of the most
challenging tasks in machine learning. The number of machine learning
applications is growing much faster than the number of machine learning
experts, hence we see an increasing demand for efficient automation of learning
processes. Here, we introduce OBOE, an algorithm for time-constrained model
selection and hyperparameter tuning. Taking advantage of similarity between
datasets, OBOE finds promising algorithm and hyperparameter configurations
through collaborative filtering. Our system explores these models under time
constraints, so that rapid initializations can be provided to warm-start more
fine-grained optimization methods. One novel aspect of our approach is a new
heuristic for active learning in time-constrained matrix completion based on
optimal experiment design. Our experiments demonstrate that OBOE delivers
state-of-the-art performance faster than competing approaches on a test bed of
supervised learning problems.
</summary>
    <author>
      <name>Chengrun Yang</name>
    </author>
    <author>
      <name>Yuji Akimoto</name>
    </author>
    <author>
      <name>Dae Won Kim</name>
    </author>
    <author>
      <name>Madeleine Udell</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01521v2</id>
    <updated>2018-08-09T16:33:56Z</updated>
    <published>2018-07-04T11:23:57Z</published>
    <title>Curiosity Driven Exploration of Learned Disentangled Goal Spaces</title>
    <summary>  Intrinsically motivated goal exploration processes enable agents to
autonomously sample goals to explore efficiently complex environments with
high-dimensional continuous actions. They have been applied successfully to
real world robots to discover repertoires of policies producing a wide
diversity of effects. Often these algorithms relied on engineered goal spaces
but it was recently shown that one can use deep representation learning
algorithms to learn an adequate goal space in simple environments. However, in
the case of more complex environments containing multiple objects or
distractors, an efficient exploration requires that the structure of the goal
space reflects the one of the environment. In this paper we show that using a
disentangled goal space leads to better exploration performances than an
entangled goal space. We further show that when the representation is
disentangled, one can leverage it by sampling goals that maximize learning
progress in a modular manner. Finally, we show that the measure of learning
progress, used to drive curiosity-driven exploration, can be used
simultaneously to discover abstract independently controllable features of the
environment.
</summary>
    <author>
      <name>Adrien Laversanne-Finot</name>
    </author>
    <author>
      <name>Alexandre Péré</name>
    </author>
    <author>
      <name>Pierre-Yves Oudeyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The code used in the experiments is available at
  https://github.com/flowersteam/Curiosity_Driven_Goal_Exploration</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01521v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01521v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03216v1</id>
    <updated>2018-08-09T16:11:31Z</updated>
    <published>2018-08-09T16:11:31Z</published>
    <title>Data-driven polynomial chaos expansion for machine learning regression</title>
    <summary>  We present a regression technique for data driven problems based on
polynomial chaos expansion (PCE). PCE is a popular technique in the field of
uncertainty quantification (UQ), where it is typically used to replace a
runnable but expensive computational model subject to random inputs with an
inexpensive-to-evaluate polynomial function. The metamodel obtained enables a
reliable estimation of the statistics of the output, provided that a suitable
probabilistic model of the input is available.
  In classical machine learning (ML) regression settings, however, the system
is only known through observations of its inputs and output, and the interest
lies in obtaining accurate pointwise predictions of the latter. Here, we show
that a PCE metamodel purely trained on data can yield pointwise predictions
whose accuracy is comparable to that of other ML regression models, such as
neural networks and support vector machines. The comparisons are performed on
benchmark datasets available from the literature. The methodology also enables
the quantification of the output uncertainties and is robust to noise.
Furthermore, it enjoys additional desirable properties, such as good
performance for small training sets and simplicity of construction, with only
little parameter tuning required. In the presence of statistically dependent
inputs, we investigate two ways to build the PCE, and show through simulations
that one approach is superior to the other in the stated settings.
</summary>
    <author>
      <name>E. Torre</name>
    </author>
    <author>
      <name>S. Marelli</name>
    </author>
    <author>
      <name>P. Embrechts</name>
    </author>
    <author>
      <name>B. Sudret</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10668v3</id>
    <updated>2018-08-09T15:05:34Z</updated>
    <published>2018-07-27T15:06:26Z</published>
    <title>On the overfly algorithm in deep learning of neural networks</title>
    <summary>  In this paper we investigate the supervised backpropagation training of
multilayer neural networks from a dynamical systems point of view. We discuss
some links with the qualitative theory of differential equations and introduce
the overfly algorithm to tackle the local minima problem. Our approach is based
on the existence of first integrals of the generalised gradient system with
build-in dissipation.
</summary>
    <author>
      <name>Alexei Tsygvintsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10668v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10668v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08968v3</id>
    <updated>2018-08-09T14:17:45Z</updated>
    <published>2017-12-24T21:00:10Z</published>
    <title>Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</title>
    <summary>  We consider the optimization problem associated with training simple ReLU
neural networks of the form $\mathbf{x}\mapsto
\sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\}$ with respect to the
squared loss. We provide a computer-assisted proof that even if the input
distribution is standard Gaussian, even if the dimension is arbitrarily large,
and even if the target values are generated by such a network, with orthonormal
parameter vectors, the problem can still have spurious local minima once $6\le
k\le 20$. By a concentration of measure argument, this implies that in high
input dimensions, \emph{nearly all} target networks of the relevant sizes lead
to spurious local minima. Moreover, we conduct experiments which show that the
probability of hitting such local minima is quite high, and increasing with the
network size. On the positive side, mild over-parameterization appears to
drastically reduce such local minima, indicating that an over-parameterization
assumption is necessary to get a positive result in this setting.
</summary>
    <author>
      <name>Itay Safran</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08968v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08968v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04119v2</id>
    <updated>2018-08-09T13:27:52Z</updated>
    <published>2018-07-11T13:31:35Z</published>
    <title>Exploiting statistical dependencies of time series with hierarchical
  correlation reconstruction</title>
    <summary>  While we are usually focused on forecasting future values of time series, it
is often valuable to additionally predict their entire probability
distributions, e.g. to evaluate risk, Monte Carlo simulations. On example of
time series of $\approx$ 30000 Dow Jones Industrial Averages, there will be
presented application of hierarchical correlation reconstruction for this
purpose: mean-square estimating polynomial as joint density for (current value,
context), where context is for example a few previous values. Then substituting
the currently observed context and normalizing density to 1, we get predicted
probability distribution for the current value. In contrast to standard machine
learning approaches like neural networks, optimal polynomial coefficients here
can be inexpensively directly calculated, have controllable accuracy, are
unique and independent, each has a specific cumulant-like interpretation, and
such approximation using can approach complete description of any real joint
distribution - providing a perfect tool to quantitatively describe and exploit
statistical dependencies in time series. There is also discussed application
for non-stationary time series: adapting coefficients to local statistical
behavior.
</summary>
    <author>
      <name>Jarek Duda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04119v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04119v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11258v2</id>
    <updated>2018-08-09T13:12:09Z</updated>
    <published>2018-06-29T03:32:29Z</published>
    <title>Hierarchical Dirichlet Process-based Open Set Recognition</title>
    <summary>  In this paper, we propose a novel hierarchical dirichlet process-based
classification framework for open set recognition (HDP-OSR) where new
categories' samples unseen in training appear during testing. Unlike the
existing methods which deal with this problem from the perspective of
discriminative model, we reconsider this problem from the perspective of
generative model. We model each known class data in training set as a group in
hierarchical dirichlet process (HDP) while the testing set as a whole is
treated in the same way, then co-clustering all the groups under the HDP
framework. Based on the properties of HDP, our HDP-OSR does not overly depend
on training samples and can achieve adaptive change as the data changes. More
precisely, HDP-OSR can automatically reserve space for unknown categories while
it can also discover new categories, meaning it naturally adapts to the open
set recognition scenario. Furthermore, treating the testing set as a whole
makes our framework take the correlations among the testing samples into
account whereas the existing methods obviously ignore this information.
Experimental results on a set of benchmark data sets indicate the validity of
our learning framework.
</summary>
    <author>
      <name>Chuanxing Geng</name>
    </author>
    <author>
      <name>Songcan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11258v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11258v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01975v2</id>
    <updated>2018-08-09T12:17:16Z</updated>
    <published>2018-08-06T16:12:04Z</published>
    <title>A Survey on Surrogate Approaches to Non-negative Matrix Factorization</title>
    <summary>  Motivated by applications in hyperspectral imaging we investigate methods for
approximating a high-dimensional non-negative matrix $\mathbf{\mathit{Y}}$ by a
product of two lower-dimensional, non-negative matrices $\mathbf{\mathit{K}}$
and $\mathbf{\mathit{X}}.$ This so-called non-negative matrix factorization is
based on defining suitable Tikhonov functionals, which combine a discrepancy
measure for $\mathbf{\mathit{Y}}\approx\mathbf{\mathit{KX}}$ with penalty terms
for enforcing additional properties of $\mathbf{\mathit{K}}$ and
$\mathbf{\mathit{X}}$. The minimization is based on alternating minimization
with respect to $\mathbf{\mathit{K}}$ or $\mathbf{\mathit{X}}$, where in each
iteration step one replaces the original Tikhonov functional by a locally
defined surrogate functional. The choice of surrogate functionals is crucial:
It should allow a comparatively simple minimization and simultaneously its
first order optimality condition should lead to multiplicative update rules,
which automatically preserve non-negativity of the iterates. We review the most
standard construction principles for surrogate functionals for Frobenius-norm
and Kullback-Leibler discrepancy measures. We extend the known surrogate
constructions by a general framework, which allows to add a large variety of
penalty terms. The paper finishes by deriving the corresponding alternating
minimization schemes explicitely and by applying these methods to MALDI imaging
data.
</summary>
    <author>
      <name>Pascal Fernsel</name>
    </author>
    <author>
      <name>Peter Maass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 6 figures. Submitted to the Vietnam Journal of Mathematics</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01975v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01975v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04189v2</id>
    <updated>2018-08-09T12:08:44Z</updated>
    <published>2018-03-12T11:07:58Z</published>
    <title>Noise2Noise: Learning Image Restoration without Clean Data</title>
    <summary>  We apply basic statistical reasoning to signal reconstruction by machine
learning -- learning to map corrupted observations to clean signals -- with a
simple and powerful conclusion: it is possible to learn to restore images by
only looking at corrupted examples, at performance at and sometimes exceeding
training using clean data, without explicit image priors or likelihood models
of the corruption. In practice, we show that a single model learns photographic
noise removal, denoising synthetic Monte Carlo images, and reconstruction of
undersampled MRI scans -- all corrupted by different processes -- based on
noisy data only.
</summary>
    <author>
      <name>Jaakko Lehtinen</name>
    </author>
    <author>
      <name>Jacob Munkberg</name>
    </author>
    <author>
      <name>Jon Hasselgren</name>
    </author>
    <author>
      <name>Samuli Laine</name>
    </author>
    <author>
      <name>Tero Karras</name>
    </author>
    <author>
      <name>Miika Aittala</name>
    </author>
    <author>
      <name>Timo Aila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final ICML 2018 version, supplemental pages included as appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03096v1</id>
    <updated>2018-08-09T11:27:11Z</updated>
    <published>2018-08-09T11:27:11Z</published>
    <title>On feature selection and evaluation of transportation mode prediction
  strategies</title>
    <summary>  Transportation modes prediction is a fundamental task for decision making in
smart cities and traffic management systems. Traffic policies designed based on
trajectory mining can save money and time for authorities and the public. It
may reduce the fuel consumption and commute time and moreover, may provide more
pleasant moments for residents and tourists. Since the number of features that
may be used to predict a user transportation mode can be substantial, finding a
subset of features that maximizes a performance measure is worth investigating.
In this work, we explore wrapper and information retrieval methods to find the
best subset of trajectory features. After finding the best classifier and the
best feature subset, our results were compared with two related papers that
applied deep learning methods and the results showed that our framework
achieved better performance. Furthermore, two types of cross-validation
approaches were investigated, and the performance results show that the random
cross-validation method provides optimistic results.
</summary>
    <author>
      <name>Mohammad Etemad</name>
    </author>
    <author>
      <name>Amilcar Soares Junior</name>
    </author>
    <author>
      <name>Stan Matwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1807.10876</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03030v1</id>
    <updated>2018-08-09T06:19:39Z</updated>
    <published>2018-08-09T06:19:39Z</published>
    <title>Policy Optimization as Wasserstein Gradient Flows</title>
    <summary>  Policy optimization is a core component of reinforcement learning (RL), and
most existing RL methods directly optimize parameters of a policy based on
maximizing the expected total reward, or its surrogate. Though often achieving
encouraging empirical success, its underlying mathematical principle on {\em
policy-distribution} optimization is unclear. We place policy optimization into
the space of probability measures, and interpret it as Wasserstein gradient
flows. On the probability-measure space, under specified circumstances, policy
optimization becomes a convex problem in terms of distribution optimization. To
make optimization feasible, we develop efficient algorithms by numerically
solving the corresponding discrete gradient flows. Our technique is applicable
to several RL settings, and is related to many state-of-the-art
policy-optimization algorithms. Empirical results verify the effectiveness of
our framework, often obtaining better performance compared to related
algorithms.
</summary>
    <author>
      <name>Ruiyi Zhang</name>
    </author>
    <author>
      <name>Changyou Chen</name>
    </author>
    <author>
      <name>Chunyuan Li</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICML 2018; Initial version on Deep Reinforcement Learning
  Symposium at NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03265v1</id>
    <updated>2018-08-09T04:08:46Z</updated>
    <published>2018-08-09T04:08:46Z</published>
    <title>A Hybrid Recommender System for Patient-Doctor Matchmaking in Primary
  Care</title>
    <summary>  We partner with a leading European healthcare provider and design a mechanism
to match patients with family doctors in primary care. We define the
matchmaking process for several distinct use cases given different levels of
available information about patients. Then, we adopt a hybrid recommender
system to present each patient a list of family doctor recommendations. In
particular, we model patient trust of family doctors using a large-scale
dataset of consultation histories, while accounting for the temporal dynamics
of their relationships. Our proposed approach shows higher predictive accuracy
than both a heuristic baseline and a collaborative filtering approach, and the
proposed trust measure further improves model performance.
</summary>
    <author>
      <name>Qiwei Han</name>
    </author>
    <author>
      <name>Mengxin Ji</name>
    </author>
    <author>
      <name>Inigo Martinez de Rituerto de Troya</name>
    </author>
    <author>
      <name>Manas Gaur</name>
    </author>
    <author>
      <name>Leid Zejnilovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted at DSAA 2018 as a full paper, Proc. of the 5th
  IEEE International Conference on Data Science and Advanced Analytics (DSAA),
  Turin, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06896v2</id>
    <updated>2018-08-09T02:52:27Z</updated>
    <published>2018-04-17T07:00:42Z</published>
    <title>A Multi-task Selected Learning Approach for Solving New Type 3D Bin
  Packing Problem</title>
    <summary>  This paper studies a new type of 3D bin packing problem (BPP), in which a
number of cuboid-shaped items must be put into a bin one by one orthogonally.
The objective is to find a way to place these items that can minimize the
surface area of the bin. This problem is based on the fact that there is no
fixed-sized bin in many real business scenarios and the cost of a bin is
proportional to its surface area. Based on previous research on 3D BPP, the
surface area is determined by the sequence, spatial locations and orientations
of items. It is a new NP-hard combinatorial optimization problem on
unfixed-sized bin packing, for which we propose a multi-task framework based on
Selected Learning, generating the sequence and orientations of items packed
into the bin simultaneously. During training steps, Selected Learning chooses
one of loss functions derived from Deep Reinforcement Learning and Supervised
Learning corresponding to the training procedure. Numerical results show that
the method proposed significantly outperforms Lego baselines by a substantial
gain of 7.52%. Moreover, we produce large scale 3D Bin Packing order data set
for studying bin packing problems and will release it to the research
community.
</summary>
    <author>
      <name>Haoyuan Hu</name>
    </author>
    <author>
      <name>Lu Duan</name>
    </author>
    <author>
      <name>Yu Gong</name>
    </author>
    <author>
      <name>Kenny Zhu</name>
    </author>
    <author>
      <name>Xiaodong Zhang</name>
    </author>
    <author>
      <name>Yinghui Xu</name>
    </author>
    <author>
      <name>Jiangwen Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1708.05930</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06896v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06896v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03001v1</id>
    <updated>2018-08-09T02:50:24Z</updated>
    <published>2018-08-09T02:50:24Z</published>
    <title>Compressed Sensing Using Binary Matrices of Nearly Optimal Dimensions</title>
    <summary>  In this paper, we study the problem of compressed sensing using binary
measurement matrices, and $\ell_1$-norm minimization (basis pursuit) as the
recovery algorithm. We derive new upper and lower bounds on the number of
measurements to achieve robust sparse recovery with binary matrices. We
establish sufficient conditions for a column-regular binary matrix to satisfy
the robust null space property (RNSP), and show that the sparsity bounds for
robust sparse recovery obtained using the RNSP are better by a factor of $(3
\sqrt{3})/2 \approx 2.6$ compared to the restricted isometry property (RIP).
Next we derive universal lower bounds on the number of measurements that any
binary matrix needs to have in order to satisfy the weaker sufficient condition
based on the RNSP, and show that bipartite graphs of girth six are optimal.
Then we display two classes of binary matrices, namely parity check matrices of
array codes, and Euler squares, that have girth six and are nearly optimal in
the sense of almost satisfying the lower bound. In principle randomly generated
Gaussian measurement matrices are "order-optimal." So we compare the phase
transition behavior of the basis pursuit formulation using binary array code
and Gaussian matrices, and show that (i) there is essentially no difference
between the phase transition boundaries in the two cases, and (ii) the CPU time
of basis pursuit with binary matrices is hundreds of times faster than with
Gaussian matrices, and the storage requirements are less. Therefore it is
suggested that binary matrices are a viable alternative to Gaussian matrices
for compressed sensing using basis pursuit.
</summary>
    <author>
      <name>Mahsa Lotfi</name>
    </author>
    <author>
      <name>Mathukumalli Vidyasagar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05464v1</id>
    <updated>2018-08-08T23:06:43Z</updated>
    <published>2018-08-08T23:06:43Z</published>
    <title>Transfer Learning for Brain-Computer Interfaces: An Euclidean Space Data
  Alignment Approach</title>
    <summary>  Almost all EEG-based brain-computer interfaces (BCIs) need some labeled
subject-specific data to calibrate a new subject, as neural responses are
different across subjects to even the same stimulus. So, a major challenge in
developing high-performance and user-friendly BCIs is to cope with such
individual differences so that the calibration can be reduced or even
completely eliminated. This paper focuses on the latter. More specifically, we
consider an offline application scenario, in which we have unlabeled EEG trials
from a new subject, and would like to accurately label them by leveraging
auxiliary labeled EEG trials from other subjects in the same task. To
accommodate the individual differences, we propose a novel unsupervised
approach to align the EEG trials from different subjects in the Euclidean space
to make them more consistent. It has three desirable properties: 1) the aligned
trial lie in the Euclidean space, which can be used by any Euclidean space
signal processing and machine learning approach; 2) it can be computed very
efficiently; and, 3) it does not need any labeled trials from the new subject.
Experiments on motor imagery and event-related potentials demonstrated the
effectiveness and efficiency of our approach.
</summary>
    <author>
      <name>He He</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04244v1</id>
    <updated>2018-08-08T22:39:46Z</updated>
    <published>2018-08-08T22:39:46Z</published>
    <title>Affect Estimation in 3D Space Using Multi-Task Active Learning for
  Regression</title>
    <summary>  Acquisition of labeled training samples for affective computing is usually
costly and time-consuming, as affects are intrinsically subjective, subtle and
uncertain, and hence multiple human assessors are needed to evaluate each
affective sample. Particularly, for affect estimation in the 3D space of
valence, arousal and dominance, each assessor has to perform the evaluations in
three dimensions, which makes the labeling problem even more challenging. Many
sophisticated machine learning approaches have been proposed to reduce the data
labeling requirement in various other domains, but so far few have considered
affective computing. This paper proposes two multi-task active learning for
regression approaches, which select the most beneficial samples to label, by
considering the three affect primitives simultaneously. Experimental results on
the VAM corpus demonstrated that our optimal sample selection approaches can
result in better estimation performance than random selection and several
traditional single-task active learning approaches. Thus, they can help
alleviate the data labeling problem in affective computing, i.e., better
estimation performance can be obtained from fewer labeling queries.
</summary>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06533v1</id>
    <updated>2018-08-08T22:39:13Z</updated>
    <published>2018-08-08T22:39:13Z</published>
    <title>Spatial Filtering for Brain Computer Interfaces: A Comparison between
  the Common Spatial Pattern and Its Variant</title>
    <summary>  The electroencephalogram (EEG) is the most popular form of input for brain
computer interfaces (BCIs). However, it can be easily contaminated by various
artifacts and noise, e.g., eye blink, muscle activities, powerline noise, etc.
Therefore, the EEG signals are often filtered both spatially and temporally to
increase the signal-to-noise ratio before they are fed into a machine learning
algorithm for recognition. This paper considers spatial filtering,
particularly, the common spatial pattern (CSP) filters for EEG classification.
In binary classification, CSP seeks a set of filters to maximize the variance
for one class while minimizing it for the other. We first introduce the
traditional solution, and then a new solution based on a slightly different
objective function. We performed comprehensive experiments on motor imagery to
compare the two approaches, and found that generally the traditional CSP
solution still gives better results. We also showed that adding regularization
to the covariance matrices can improve the final classification performance, no
matter which objective function is used.
</summary>
    <author>
      <name>He He</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02956v1</id>
    <updated>2018-08-08T22:29:45Z</updated>
    <published>2018-08-08T22:29:45Z</published>
    <title>Feature Dimensionality Reduction for Video Affect Classification: A
  Comparative Study</title>
    <summary>  Affective computing has become a very important research area in
human-machine interaction. However, affects are subjective, subtle, and
uncertain. So, it is very difficult to obtain a large number of labeled
training samples, compared with the number of possible features we could
extract. Thus, dimensionality reduction is critical in affective computing.
This paper presents our preliminary study on dimensionality reduction for
affect classification. Five popular dimensionality reduction approaches are
introduced and compared. Experiments on the DEAP dataset showed that no
approach can universally outperform others, and performing classification using
the raw features directly may not always be a bad choice.
</summary>
    <author>
      <name>Chenfeng Guo</name>
    </author>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04245v1</id>
    <updated>2018-08-08T22:29:19Z</updated>
    <published>2018-08-08T22:29:19Z</published>
    <title>Active Learning for Regression Using Greedy Sampling</title>
    <summary>  Regression problems are pervasive in real-world applications. Generally a
substantial amount of labeled samples are needed to build a regression model
with good generalization ability. However, many times it is relatively easy to
collect a large number of unlabeled samples, but time-consuming or expensive to
label them. Active learning for regression (ALR) is a methodology to reduce the
number of labeled samples, by selecting the most beneficial ones to label,
instead of random selection. This paper proposes two new ALR approaches based
on greedy sampling (GS). The first approach (GSy) selects new samples to
increase the diversity in the output space, and the second (iGS) selects new
samples to increase the diversity in both input and output spaces. Extensive
experiments on 12 UCI and CMU StatLib datasets from various domains, and on 15
subjects on EEG-based driver drowsiness estimation, verified their
effectiveness and robustness.
</summary>
    <author>
      <name>Dongrui Wu</name>
    </author>
    <author>
      <name>Chin-Teng Lin</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01069v2</id>
    <updated>2018-08-08T22:17:25Z</updated>
    <published>2018-07-03T10:25:26Z</published>
    <title>Adversarial Robustness Toolbox v0.3.0</title>
    <summary>  Adversarial examples have become an indisputable threat to the security of
modern AI systems based on deep neural networks (DNNs). The Adversarial
Robustness Toolbox (ART) is a Python library designed to support researchers
and developers in creating novel defence techniques, as well as in deploying
practical defences of real-world AI systems. Researchers can use ART to
benchmark novel defences against the state-of-the-art. For developers, the
library provides interfaces which support the composition of comprehensive
defence systems using individual methods as building blocks. The Adversarial
Robustness Toolbox supports machine learning models (and deep neural networks
(DNNs) specifically) implemented in any of the most popular deep learning
frameworks (TensorFlow, Keras, PyTorch and MXNet). Currently, the library is
primarily intended to improve the adversarial robustness of visual recognition
systems, however, future releases that will comprise adaptations to other data
modes (such as speech, text or time series) are envisioned. The ART source code
is released (https://github.com/IBM/adversarial-robustness-toolbox) under an
MIT license. The release includes code examples and extensive documentation
(http://adversarial-robustness-toolbox.readthedocs.io) to help researchers and
developers get quickly started.
</summary>
    <author>
      <name>Maria-Irina Nicolae</name>
    </author>
    <author>
      <name>Mathieu Sinn</name>
    </author>
    <author>
      <name>Minh Ngoc Tran</name>
    </author>
    <author>
      <name>Ambrish Rawat</name>
    </author>
    <author>
      <name>Martin Wistuba</name>
    </author>
    <author>
      <name>Valentina Zantedeschi</name>
    </author>
    <author>
      <name>Nathalie Baracaldo</name>
    </author>
    <author>
      <name>Bryant Chen</name>
    </author>
    <author>
      <name>Heiko Ludwig</name>
    </author>
    <author>
      <name>Ian M. Molloy</name>
    </author>
    <author>
      <name>Ben Edwards</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04339v2</id>
    <updated>2018-08-08T21:28:29Z</updated>
    <published>2018-01-12T22:13:48Z</published>
    <title>Estimating the Number of Connected Components in a Graph via Subgraph
  Sampling</title>
    <summary>  Learning properties of large graphs from samples has been an important
problem in statistical network analysis since the early work of Goodman
\cite{Goodman1949} and Frank \cite{Frank1978}. We revisit a problem formulated
by Frank \cite{Frank1978} of estimating the number of connected components in a
large graph based on the subgraph sampling model, in which we randomly sample a
subset of the vertices and observe the induced subgraph. The key question is
whether accurate estimation is achievable in the \emph{sublinear} regime where
only a vanishing fraction of the vertices are sampled. We show that it is
impossible if the parent graph is allowed to contain high-degree vertices or
long induced cycles. For the class of chordal graphs, where induced cycles of
length four or above are forbidden, we characterize the optimal sample
complexity within constant factors and construct linear-time estimators that
provably achieve these bounds. This significantly expands the scope of previous
results which have focused on unbiased estimators and special classes of graphs
such as forests or cliques.
  Both the construction and the analysis of the proposed methodology rely on
combinatorial properties of chordal graphs and identities of induced subgraph
counts. They, in turn, also play a key role in proving minimax lower bounds
based on construction of random instances of graphs with matching structures of
small subgraphs.
</summary>
    <author>
      <name>Jason M. Klusowski</name>
    </author>
    <author>
      <name>Yihong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04339v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04339v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62D05, 62C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01290v2</id>
    <updated>2018-08-08T21:27:08Z</updated>
    <published>2018-01-04T09:50:50Z</published>
    <title>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
  Learning with a Stochastic Actor</title>
    <summary>  Model-free deep reinforcement learning (RL) algorithms have been demonstrated
on a range of challenging decision making and control tasks. However, these
methods typically suffer from two major challenges: very high sample complexity
and brittle convergence properties, which necessitate meticulous hyperparameter
tuning. Both of these challenges severely limit the applicability of such
methods to complex, real-world domains. In this paper, we propose soft
actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum
entropy reinforcement learning framework. In this framework, the actor aims to
maximize expected reward while also maximizing entropy. That is, to succeed at
the task while acting as randomly as possible. Prior deep RL methods based on
this framework have been formulated as Q-learning methods. By combining
off-policy updates with a stable stochastic actor-critic formulation, our
method achieves state-of-the-art performance on a range of continuous control
benchmark tasks, outperforming prior on-policy and off-policy methods.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving very similar performance across
different random seeds.
</summary>
    <author>
      <name>Tuomas Haarnoja</name>
    </author>
    <author>
      <name>Aurick Zhou</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code:
  github.com/haarnoja/sac</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02240v2</id>
    <updated>2018-08-08T21:16:43Z</updated>
    <published>2018-08-07T07:49:25Z</published>
    <title>Speeding Up Distributed Gradient Descent by Utilizing Non-persistent
  Stragglers</title>
    <summary>  Distributed gradient descent (DGD) is an efficient way of implementing
gradient descent (GD), especially for large data sets, by dividing the
computation tasks into smaller subtasks and assigning to different computing
servers (CSs) to be executed in parallel. In standard parallel execution,
per-iteration waiting time is limited by the execution time of the straggling
servers. Coded DGD techniques have been introduced recently, which can tolerate
straggling servers via assigning redundant computation tasks to the CSs. In
most of the existing DGD schemes, either with coded computation or coded
communication, the non-straggling CSs transmit one message per iteration once
they complete all their assigned computation tasks. However, although the
straggling servers cannot complete all their assigned tasks, they are often
able to complete a certain portion of them. In this paper, we allow multiple
transmissions from each CS at each iteration in order to make sure a maximum
number of completed computations can be reported to the aggregating server
(AS), including the straggling servers. We numerically show that the average
completion time per iteration can be reduced significantly by slightly
increasing the communication load per server.
</summary>
    <author>
      <name>Emre Ozfatura</name>
    </author>
    <author>
      <name>Deniz Gunduz</name>
    </author>
    <author>
      <name>Sennur Ulukus</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02941v1</id>
    <updated>2018-08-08T21:14:07Z</updated>
    <published>2018-08-08T21:14:07Z</published>
    <title>On the Convergence of A Class of Adam-Type Algorithms for Non-Convex
  Optimization</title>
    <summary>  This paper studies a class of adaptive gradient based momentum algorithms
that update the search directions and learning rates simultaneously using past
gradients. This class, which we refer to as the "Adam-type", includes the
popular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their
popularity in training deep neural networks, the convergence of these
algorithms for solving nonconvex problems remains an open question. This paper
provides a set of mild sufficient conditions that guarantee the convergence for
the Adam-type methods. We prove that under our derived conditions, these
methods can achieve the convergence rate of order $O(\log{T}/\sqrt{T})$ for
nonconvex stochastic optimization. We show the conditions are essential in the
sense that violating them may make the algorithm diverge. Moreover, we propose
and analyze a class of (deterministic) incremental adaptive gradient
algorithms, which has the same $O(\log{T}/\sqrt{T})$ convergence rate. Our
study could also be extended to a broader class of adaptive gradient methods in
machine learning and optimization.
</summary>
    <author>
      <name>Xiangyi Chen</name>
    </author>
    <author>
      <name>Sijia Liu</name>
    </author>
    <author>
      <name>Ruoyu Sun</name>
    </author>
    <author>
      <name>Mingyi Hong</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02932v1</id>
    <updated>2018-08-08T20:40:15Z</updated>
    <published>2018-08-08T20:40:15Z</published>
    <title>Nonparametric Gaussian mixture models for the multi-armed contextual
  bandit</title>
    <summary>  The multi-armed bandit is a sequential allocation task where an agent must
learn a policy that maximizes long term payoff, where only the reward of the
played arm is observed at each iteration. In the stochastic setting, the reward
for each action is generated from an unknown distribution, which depends on a
given 'context', available at each interaction with the world. Thompson
sampling is a generative, interpretable multi-armed bandit algorithm that has
been shown both to perform well in practice, and to enjoy optimality properties
for certain reward functions. Nevertheless, Thompson sampling requires sampling
from parameter posteriors and calculation of expected rewards, which are
possible for a very limited choice of distributions. We here extend Thompson
sampling to more complex scenarios by adopting a very flexible set of reward
distributions: nonparametric Gaussian mixture models. The generative process of
Bayesian nonparametric mixtures naturally aligns with the Bayesian modeling of
multi-armed bandits. This allows for the implementation of an efficient and
flexible Thompson sampling algorithm: the nonparametric model autonomously
determines its complexity in an online fashion, as it observes new rewards for
the played arms. We show how the proposed method sequentially learns the
nonparametric mixture model that best approximates the true underlying reward
distribution. Our contribution is valuable for practical scenarios, as it
avoids stringent model specifications, and yet attains reduced regret.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03162v2</id>
    <updated>2018-08-08T20:20:27Z</updated>
    <published>2017-09-10T19:58:34Z</published>
    <title>Bayesian bandits: balancing the exploration-exploitation tradeoff via
  double sampling</title>
    <summary>  Reinforcement learning studies how to balance exploration and exploitation in
real-world systems, optimizing interactions with the world while simultaneously
learning how the world operates. One general class of algorithms for such
learning is the multi-armed bandit setting. Randomized probability matching,
based upon the Thompson sampling approach introduced in the 1930s, has recently
been shown to perform well and to enjoy provable optimality properties. It
permits generative, interpretable modeling in a Bayesian setting, where prior
knowledge is incorporated, and the computed posteriors naturally capture the
full state of knowledge. In this work, we harness the information contained in
the Bayesian posterior and estimate its sufficient statistics via sampling. In
several application domains, for example in health and medicine, each
interaction with the world can be expensive and invasive, whereas drawing
samples from the model is relatively inexpensive. Exploiting this viewpoint, we
develop a double sampling technique driven by the uncertainty in the learning
process: it favors exploitation when certain about the properties of each arm,
exploring otherwise. The proposed algorithm does not make any distributional
assumption and it is applicable to complex reward distributions, as long as
Bayesian posterior updates are computable. Utilizing the estimated posterior
sufficient statistics, double sampling autonomously balances the
exploration-exploitation tradeoff to make better informed decisions. We
empirically show its reduced cumulative regret when compared to
state-of-the-art alternatives in representative bandit settings.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03162v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03162v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03163v2</id>
    <updated>2018-08-08T20:16:40Z</updated>
    <published>2017-09-10T19:58:44Z</published>
    <title>Variational inference for the multi-armed contextual bandit</title>
    <summary>  In many biomedical, science, and engineering problems, one must sequentially
decide which action to take next so as to maximize rewards. One general class
of algorithms for optimizing interactions with the world, while simultaneously
learning how the world operates, is the multi-armed bandit setting and, in
particular, the contextual bandit case. In this setting, for each executed
action, one observes rewards that are dependent on a given 'context', available
at each interaction with the world. The Thompson sampling algorithm has
recently been shown to enjoy provable optimality properties for this set of
problems, and to perform well in real-world settings. It facilitates generative
and interpretable modeling of the problem at hand. Nevertheless, the design and
complexity of the model limit its application, since one must both sample from
the distributions modeled and calculate their expected rewards. We here show
how these limitations can be overcome using variational inference to
approximate complex models, applying to the reinforcement learning case
advances developed for the inference case in the machine learning community
over the past two decades. We consider contextual multi-armed bandit
applications where the true reward distribution is unknown and complex, which
we approximate with a mixture model whose parameters are inferred via
variational inference. We show how the proposed variational Thompson sampling
approach is accurate in approximating the true distribution, and attains
reduced regrets even with complex reward distributions. The proposed algorithm
is valuable for practical scenarios where restrictive modeling assumptions are
undesirable.
</summary>
    <author>
      <name>Iñigo Urteaga</name>
    </author>
    <author>
      <name>Chris H. Wiggins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The software used for this study is publicly available at
  https://github.com/iurteaga/bandits</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Twenty-First International Conference on
  Artificial Intelligence and Statistics, PMLR 84:698-706, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.03163v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03163v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05062v2</id>
    <updated>2018-08-08T19:36:06Z</updated>
    <published>2018-01-15T22:59:17Z</published>
    <title>Multi-Label Learning from Medical Plain Text with Convolutional Residual
  Models</title>
    <summary>  Predicting diagnoses from Electronic Health Records (EHRs) is an important
medical application of multi-label learning. We propose a convolutional
residual model for multi-label classification from doctor notes in EHR data. A
given patient may have multiple diagnoses, and therefore multi-label learning
is required. We employ a Convolutional Neural Network (CNN) to encode plain
text into a fixed-length sentence embedding vector. Since diagnoses are
typically correlated, a deep residual network is employed on top of the CNN
encoder, to capture label (diagnosis) dependencies and incorporate information
directly from the encoded sentence vector. A real EHR dataset is considered,
and we compare the proposed model with several well-known baselines, to predict
diagnoses based on doctor notes. Experimental results demonstrate the
superiority of the proposed convolutional residual model.
</summary>
    <author>
      <name>Xinyuan Zhang</name>
    </author>
    <author>
      <name>Ricardo Henao</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <author>
      <name>Yitong Li</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Healthcare 2018 spotlight paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05062v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05062v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09386v5</id>
    <updated>2018-08-08T18:58:11Z</updated>
    <published>2018-07-24T23:23:49Z</published>
    <title>On the Randomized Complexity of Minimizing a Convex Quadratic Function</title>
    <summary>  Minimizing a convex, quadratic objective is a fundamental problem in machine
learning and optimization. In this work, we study prove information-theoretic,
gradient query complexity lower bounds for minimizing convex quadratic
functions, which, unlike prior works, apply even for randomized algorithms.
Specifically, we construct a distribution over quadratic functions that
witnesses lower bounds which match those known for deterministic algorithms, up
to multiplicative constants. The distribution which witnesses our lower bound
is in fact quite benign: it is both closed form, and derived from classical
ensembles in random matrix theory. We believe that our construction constitutes
a plausible "average case" setting, and thus provides compelling evidence that
the worst case and average case complexity of convex-quadratic optimization are
essentially identical.
</summary>
    <author>
      <name>Max Simchowitz</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09386v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09386v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02822v1</id>
    <updated>2018-08-08T15:23:14Z</updated>
    <published>2018-08-08T15:23:14Z</published>
    <title>Backprop Evolution</title>
    <summary>  The back-propagation algorithm is the cornerstone of deep learning. Despite
its importance, few variations of the algorithm have been attempted. This work
presents an approach to discover new variations of the back-propagation
equation. We use a domain specific lan- guage to describe update equations as a
list of primitive functions. An evolution-based method is used to discover new
propagation rules that maximize the generalization per- formance after a few
epochs of training. We find several update equations that can train faster with
short training times than standard back-propagation, and perform similar as
standard back-propagation at convergence.
</summary>
    <author>
      <name>Maximilian Alber</name>
    </author>
    <author>
      <name>Irwan Bello</name>
    </author>
    <author>
      <name>Barret Zoph</name>
    </author>
    <author>
      <name>Pieter-Jan Kindermans</name>
    </author>
    <author>
      <name>Prajit Ramachandran</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02814v1</id>
    <updated>2018-08-08T15:15:29Z</updated>
    <published>2018-08-08T15:15:29Z</published>
    <title>Highly Accelerated Multishot EPI through Synergistic Combination of
  Machine Learning and Joint Reconstruction</title>
    <summary>  Purpose: To introduce a combined machine learning (ML) and physics-based
image reconstruction framework that enables navigator-free, highly accelerated
multishot echo planar imaging (msEPI), and demonstrate its application in
high-resolution structural imaging.
  Methods: Singleshot EPI is an efficient encoding technique, but does not lend
itself well to high-resolution imaging due to severe distortion artifacts and
blurring. While msEPI can mitigate these artifacts, high-quality msEPI has been
elusive because of phase mismatch arising from shot-to-shot physiological
variations which disrupt the combination of the multiple-shot data into a
single image. We employ Deep Learning to obtain an interim magnitude-valued
image with minimal artifacts, which permits estimation of image phase
variations due to shot-to-shot physiological changes. These variations are then
included in a Joint Virtual Coil Sensitivity Encoding (JVC-SENSE)
reconstruction to utilize data from all shots and improve upon the ML solution.
  Results: Our combined ML + physics approach enabled R=8-fold acceleration
from 2 EPI-shots while providing 1.8-fold error reduction compared to the
MUSSELS, a state-of-the-art reconstruction technique, which is also used as an
input to our ML network. Using 3 shots allowed us to push the acceleration to
R=10-fold, where we obtained a 1.7-fold error reduction over MUSSELS.
  Conclusion: Combination of ML and JVC-SENSE enabled navigator-free msEPI at
higher accelerations than previously possible while using fewer shots, with
reduced vulnerability to poor generalizability and poor acceptance of
end-to-end ML approaches.
</summary>
    <author>
      <name>Berkin Bilgic</name>
    </author>
    <author>
      <name>Itthi Chatnuntawech</name>
    </author>
    <author>
      <name>Mary Kate Manhard</name>
    </author>
    <author>
      <name>Qiyuan Tian</name>
    </author>
    <author>
      <name>Congyu Liao</name>
    </author>
    <author>
      <name>Stephen F. Cauley</name>
    </author>
    <author>
      <name>Susie Y. Huang</name>
    </author>
    <author>
      <name>Jonathan R. Polimeni</name>
    </author>
    <author>
      <name>Lawrence L. Wald</name>
    </author>
    <author>
      <name>Kawin Setsompop</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00520v3</id>
    <updated>2018-08-08T13:57:47Z</updated>
    <published>2017-04-03T10:40:15Z</published>
    <title>Efficient acquisition rules for model-based approximate Bayesian
  computation</title>
    <summary>  Approximate Bayesian computation (ABC) is a method for Bayesian inference
when the likelihood is unavailable but simulating from the model is possible.
However, many ABC algorithms require a large number of simulations, which can
be costly. To reduce the computational cost, Bayesian optimisation (BO) and
surrogate models such as Gaussian processes have been proposed. Bayesian
optimisation enables one to intelligently decide where to evaluate the model
next but common BO strategies are not designed for the goal of estimating the
posterior distribution. Our paper addresses this gap in the literature. We
propose to compute the uncertainty in the ABC posterior density, which is due
to a lack of simulations to estimate this quantity accurately, and define a
loss function that measures this uncertainty. We then propose to select the
next evaluation location to minimise the expected loss. Experiments show that
the proposed method often produces the most accurate approximations as compared
to common BO strategies.
</summary>
    <author>
      <name>Marko Järvenpää</name>
    </author>
    <author>
      <name>Michael U. Gutmann</name>
    </author>
    <author>
      <name>Arijus Pleska</name>
    </author>
    <author>
      <name>Aki Vehtari</name>
    </author>
    <author>
      <name>Pekka Marttinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00520v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00520v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10023v2</id>
    <updated>2018-08-08T13:44:56Z</updated>
    <published>2018-04-26T12:47:52Z</published>
    <title>Candidate Labeling for Crowd Learning</title>
    <summary>  Crowdsourcing has become very popular among the machine learning community as
a way to obtain labels that allow a ground truth to be estimated for a given
dataset. In most of the approaches that use crowdsourced labels, annotators are
asked to provide, for each presented instance, a single class label. Such a
request could be inefficient, that is, considering that the labelers may not be
experts, that way to proceed could fail to take real advantage of the knowledge
of the labelers. In this paper, the use of candidate labeling for crowd
learning is proposed, where the annotators may provide more than a single label
per instance to try not to miss the real label. The main hypothesis is that, by
allowing candidate labeling, knowledge can be extracted from the labelers more
efficiently by than in the standard crowd learning scenario. Empirical evidence
which supports that hypothesis is presented.
</summary>
    <author>
      <name>Iker Beñaran-Muñoz</name>
    </author>
    <author>
      <name>Jerónimo Hernández-González</name>
    </author>
    <author>
      <name>Aritz Pérez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, to be published</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.10023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01743v2</id>
    <updated>2018-08-08T09:54:20Z</updated>
    <published>2018-06-05T15:23:10Z</published>
    <title>A Machine Learning Framework for Stock Selection</title>
    <summary>  This paper demonstrates how to apply machine learning algorithms to
distinguish good stocks from the bad stocks. To this end, we construct 244
technical and fundamental features to characterize each stock, and label stocks
according to their ranking with respect to the return-to-volatility ratio.
Algorithms ranging from traditional statistical learning methods to recently
popular deep learning method, e.g. Logistic Regression (LR), Random Forest
(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the
classification task. Genetic Algorithm (GA) is also used to implement feature
selection. The effectiveness of the stock selection strategy is validated in
Chinese stock market in both statistical and practical aspects, showing that:
1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic
Algorithm picks a subset of 114 features and the prediction performances of all
models remain almost unchanged after the selection procedure, which suggests
some features are indeed redundant; 3) LR and DNN are radical models; RF is
risk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios
constructed by our models outperform market average in back tests.
</summary>
    <author>
      <name>XingYu Fu</name>
    </author>
    <author>
      <name>JinHong Du</name>
    </author>
    <author>
      <name>YiFeng Guo</name>
    </author>
    <author>
      <name>MingWen Liu</name>
    </author>
    <author>
      <name>Tao Dong</name>
    </author>
    <author>
      <name>XiuWen Duan</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01743v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01743v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11805v2</id>
    <updated>2018-08-08T09:29:37Z</updated>
    <published>2018-07-31T13:24:31Z</published>
    <title>Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning</title>
    <summary>  Monitoring of disasters is crucial for mitigating their effects on the
environment and human population, and can be facilitated by the use of unmanned
aerial vehicles (UAV), equipped with camera sensors that produce aerial photos
of the areas of interest. A modern technique for recognition of events based on
aerial photos is deep learning. In this paper, we present the state of the art
work related to the use of deep learning techniques for disaster
identification. We demonstrate the potential of this technique in identifying
disasters with high accuracy, by means of a relatively simple deep learning
model. Based on a dataset of 544 images (containing disaster images such as
fires, earthquakes, collapsed buildings, tsunami and flooding, as well as
non-disaster scenes), our results show an accuracy of 91% achieved, indicating
that deep learning, combined with UAV equipped with camera sensors, have the
potential to predict disasters with high accuracy.
</summary>
    <author>
      <name>Andreas Kamilaris</name>
    </author>
    <author>
      <name>Francesc X. Prenafeta-Boldú</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Disaster Management for Resilience and Public Safety Workshop, Proc.
  of EnviroInfo 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05407v2</id>
    <updated>2018-08-08T08:49:15Z</updated>
    <published>2018-03-14T17:09:27Z</published>
    <title>Averaging Weights Leads to Wider Optima and Better Generalization</title>
    <summary>  Deep neural networks are typically trained by optimizing a loss function with
an SGD variant, in conjunction with a decaying learning rate, until
convergence. We show that simple averaging of multiple points along the
trajectory of SGD, with a cyclical or constant learning rate, leads to better
generalization than conventional training. We also show that this Stochastic
Weight Averaging (SWA) procedure finds much broader optima than SGD, and
approximates the recent Fast Geometric Ensembling (FGE) approach with a single
model. Using SWA we achieve notable improvement in test accuracy over
conventional SGD training on a range of state-of-the-art residual networks,
PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and
ImageNet. In short, SWA is extremely easy to implement, improves
generalization, and has almost no computational overhead.
</summary>
    <author>
      <name>Pavel Izmailov</name>
    </author>
    <author>
      <name>Dmitrii Podoprikhin</name>
    </author>
    <author>
      <name>Timur Garipov</name>
    </author>
    <author>
      <name>Dmitry Vetrov</name>
    </author>
    <author>
      <name>Andrew Gordon Wilson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears at the Conference on Uncertainty in Artificial Intelligence
  (UAI), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05407v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05407v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08197v3</id>
    <updated>2018-08-08T08:46:31Z</updated>
    <published>2018-07-21T19:52:15Z</published>
    <title>On Numerical Estimation of Joint Probability Distribution from Lebesgue
  Integral Quadratures</title>
    <summary>  An important application of Lebesgue integral quadrature[1] is developed.
Given two random processes, $f(x)$ and $g(x)$, two generalized eigenvalue
problems can be formulated and solved. In addition to obtaining two Lebesgue
quadratures (for $f$ and $g$) from two eigenproblems, the projections of $f$--
and $g$-- eigenvectors on each other allow to build a joint distribution
estimator, the most general form of which is a density--matrix correlation. The
examples of the density--matrix correlation can be the value--correlation
$V_{f_i;g_j}$, similar to the regular correlation concept, and a new one, the
probability--correlation $P_{f_i;g_j}$. The theory is implemented numerically;
the software is available under the GPLv3 license.
</summary>
    <author>
      <name>Vladislav Gennadievich Malyshkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Grammar fixes. Density matrix relation added</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08197v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08197v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02668v1</id>
    <updated>2018-08-08T08:43:43Z</updated>
    <published>2018-08-08T08:43:43Z</published>
    <title>An Occam's Razor View on Learning Audiovisual Emotion Recognition with
  Small Training Sets</title>
    <summary>  This paper presents a light-weight and accurate deep neural model for
audiovisual emotion recognition. To design this model, the authors followed a
philosophy of simplicity, drastically limiting the number of parameters to
learn from the target datasets, always choosing the simplest earning methods:
i) transfer learning and low-dimensional space embedding allows to reduce the
dimensionality of the representations. ii) The isual temporal information is
handled by a simple score-per-frame selection process, averaged across time.
iii) A simple frame selection echanism is also proposed to weight the images of
a sequence. iv) The fusion of the different modalities is performed at
prediction level (late usion). We also highlight the inherent challenges of the
AFEW dataset and the difficulty of model selection with as few as 383
validation equences. The proposed real-time emotion classifier achieved a
state-of-the-art accuracy of 60.64 % on the test set of AFEW, and ranked 4th at
he Emotion in the Wild 2018 challenge.
</summary>
    <author>
      <name>Valentin Vielzeuf</name>
    </author>
    <author>
      <name>Corentin Kervadec</name>
    </author>
    <author>
      <name>Stéphane Pateux</name>
    </author>
    <author>
      <name>Alexis Lechervy</name>
    </author>
    <author>
      <name>Frédéric Jurie</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICMI (EmotiW) 2018, Oct 2018, Boulder, Colorado, United States</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.02668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02651v1</id>
    <updated>2018-08-08T08:01:18Z</updated>
    <published>2018-08-08T08:01:18Z</published>
    <title>Adversarial Geometry and Lighting using a Differentiable Renderer</title>
    <summary>  Many machine learning classifiers are vulnerable to adversarial attacks,
inputs with perturbations designed to intentionally trigger misclassification.
Modern adversarial methods either directly alter pixel colors, or "paint"
colors onto a 3D shapes. We propose novel adversarial attacks that directly
alter the geometry of 3D objects and/or manipulate the lighting in a virtual
scene. We leverage a novel differentiable renderer that is efficient to
evaluate and analytically differentiate. Our renderer generates images
realistic enough for correct classification by common pre-trained models, and
we use it to design physical adversarial examples that consistently fool these
models. We conduct qualitative and quantitate experiments to validate our
adversarial geometry and adversarial lighting attack capabilities.
</summary>
    <author>
      <name>Hsueh-Ti Derek Liu</name>
    </author>
    <author>
      <name>Michael Tao</name>
    </author>
    <author>
      <name>Chun-Liang Li</name>
    </author>
    <author>
      <name>Derek Nowrouzezahrai</name>
    </author>
    <author>
      <name>Alec Jacobson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00442v2</id>
    <updated>2018-08-08T06:47:30Z</updated>
    <published>2018-07-02T02:49:36Z</published>
    <title>Policy Optimization With Penalized Point Probability Distance: An
  Alternative To Proximal Policy Optimization</title>
    <summary>  This paper proposes a first order gradient reinforcement learning algorithm,
which can be seen as a variant for Trust Region Policy Optimization(TRPO). This
method, which we call policy optimization with penalized point probability
distance (POP3D), keeps almost all advantageous spheres of proximal policy
optimization (PPO) such as easy implementation, fast learning and high score
capability. In specific, a new surrogate objective without constraint is
proposed, where the point probability distance is applied to prevent update
step from growing too large while contributing to more exploration and
stability than Kullback-Leibler divergence. Conclusions can be drawn based on
Gym Atari and Mujoco experiments that POP3D is an alternative to PPO, because
it achieves state-of-the-art within 40 million frame steps on 49 Atari games
and competitive scores in continuous domain according to two common metrics:
final performance and fast learning ability. Moreover, we release the code on
github https://github.com/cxxgtxy/POP3D.git.
</summary>
    <author>
      <name>Xiangxiang Chu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review for AAAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02610v1</id>
    <updated>2018-08-08T03:10:24Z</updated>
    <published>2018-08-08T03:10:24Z</published>
    <title>L-Shapley and C-Shapley: Efficient Model Interpretation for Structured
  Data</title>
    <summary>  We study instancewise feature importance scoring as a method for model
interpretation. Any such method yields, for each predicted instance, a vector
of importance scores associated with the feature vector. Methods based on the
Shapley score have been proposed as a fair way of computing feature
attributions of this kind, but incur an exponential complexity in the number of
features. This combinatorial explosion arises from the definition of the
Shapley value and prevents these methods from being scalable to large data sets
and complex models. We focus on settings in which the data have a graph
structure, and the contribution of features to the target variable is
well-approximated by a graph-structured factorization. In such settings, we
develop two algorithms with linear complexity for instancewise feature
importance scoring. We establish the relationship of our methods to the Shapley
value and another closely related concept known as the Myerson value from
cooperative game theory. We demonstrate on both language and image data that
our algorithms compare favorably with other methods for model interpretation.
</summary>
    <author>
      <name>Jianbo Chen</name>
    </author>
    <author>
      <name>Le Song</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02602v1</id>
    <updated>2018-08-08T02:16:13Z</updated>
    <published>2018-08-08T02:16:13Z</published>
    <title>PIVETed-Granite: Computational Phenotypes through Constrained Tensor
  Factorization</title>
    <summary>  It has been recently shown that sparse, nonnegative tensor factorization of
multi-modal electronic health record data is a promising approach to
high-throughput computational phenotyping. However, such approaches typically
do not leverage available domain knowledge while extracting the phenotypes;
hence, some of the suggested phenotypes may not map well to clinical concepts
or may be very similar to other suggested phenotypes. To address these issues,
we present a novel, automatic approach called PIVETed-Granite that mines
existing biomedical literature (PubMed) to obtain cannot-link constraints that
are then used as side-information during a tensor-factorization based
computational phenotyping process. The resulting improvements are clearly
observed in experiments using a large dataset from VUMC to identify phenotypes
for hypertensive patients.
</summary>
    <author>
      <name>Jette Henderson</name>
    </author>
    <author>
      <name>Bradley A. Malin</name>
    </author>
    <author>
      <name>Joyce C. Ho</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05490v2</id>
    <updated>2018-08-08T02:08:15Z</updated>
    <published>2018-07-15T05:18:20Z</published>
    <title>Semi-Supervised Feature Learning for Off-Line Writer Identifications</title>
    <summary>  Conventional approaches used supervised learning to estimate off-line writer
identifications. In this study, we improved the off-line writer identifica-
tions by semi-supervised feature learning pipeline, which trained the extra
unla- beled data and the original labeled data simultaneously. In specific, we
proposed a weighted label smoothing regularization (WLSR) method, which
assigned the weighted uniform label distribution to the extra unlabeled data.
We regularized the convolutional neural network (CNN) baseline, which allows
learning more discriminative features to represent the properties of different
writing styles. Based on experiments on ICDAR2013, CVL and IAM benchmark
datasets, our results showed that semi-supervised feature learning improved the
baseline meas- urement and achieved better performance compared with existing
writer identifications approaches.
</summary>
    <author>
      <name>Shiming Chen</name>
    </author>
    <author>
      <name>Yisong Wang</name>
    </author>
    <author>
      <name>Chin-Teng Lin</name>
    </author>
    <author>
      <name>Zehong Cao</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05490v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05490v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09724v2</id>
    <updated>2018-08-08T01:56:30Z</updated>
    <published>2017-07-31T05:26:41Z</published>
    <title>Transfer Learning with Label Noise</title>
    <summary>  Transfer learning aims to improve learning in target domain by borrowing
knowledge from a related but different source domain. To reduce the
distribution shift between source and target domains, recent methods have
focused on exploring invariant representations that have similar distributions
across domains. However, when learning this invariant knowledge, existing
methods assume that the labels in source domain are uncontaminated, while in
reality, we often have access to source data with noisy labels. In this paper,
we first show how label noise adversely affect the learning of invariant
representations and the correcting of label shift in various transfer learning
scenarios. To reduce the adverse effects, we propose a novel Denoising
Conditional Invariant Component (DCIC) framework, which provably ensures (1)
extracting invariant representations given examples with noisy labels in source
domain and unlabeled examples in target domain; (2) estimating the label
distribution in target domain with no bias. Experimental results on both
synthetic and real-world data verify the effectiveness of the proposed method.
</summary>
    <author>
      <name>Xiyu Yu</name>
    </author>
    <author>
      <name>Tongliang Liu</name>
    </author>
    <author>
      <name>Mingming Gong</name>
    </author>
    <author>
      <name>Kun Zhang</name>
    </author>
    <author>
      <name>Kayhan Batmanghelich</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09724v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09724v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09535v3</id>
    <updated>2018-08-08T01:40:46Z</updated>
    <published>2017-11-27T04:52:05Z</published>
    <title>Learning with Biased Complementary Labels</title>
    <summary>  In this paper, we study the classification problem in which we have access to
easily obtainable surrogate for true labels, namely complementary labels, which
specify classes that observations do \textbf{not} belong to. Let $Y$ and
$\bar{Y}$ be the true and complementary labels, respectively. We first model
the annotation of complementary labels via transition probabilities
$P(\bar{Y}=i|Y=j), i\neq j\in\{1,\cdots,c\}$, where $c$ is the number of
classes. Previous methods implicitly assume that $P(\bar{Y}=i|Y=j), \forall
i\neq j$, are identical, which is not true in practice because humans are
biased toward their own experience. For example, as shown in Figure 1, if an
annotator is more familiar with monkeys than prairie dogs when providing
complementary labels for meerkats, she is more likely to employ "monkey" as a
complementary label. We therefore reason that the transition probabilities will
be different. In this paper, we propose a framework that contributes three main
innovations to learning with \textbf{biased} complementary labels: (1) It
estimates transition probabilities with no bias. (2) It provides a general
method to modify traditional loss functions and extends standard deep neural
network classifiers to learn with biased complementary labels. (3) It
theoretically ensures that the classifier learned with complementary labels
converges to the optimal one learned with true labels. Comprehensive
experiments on several benchmark datasets validate the superiority of our
method to current state-of-the-art methods.
</summary>
    <author>
      <name>Xiyu Yu</name>
    </author>
    <author>
      <name>Tongliang Liu</name>
    </author>
    <author>
      <name>Mingming Gong</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 Oral</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.09535v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09535v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00374v3</id>
    <updated>2018-08-07T23:18:45Z</updated>
    <published>2018-07-01T19:16:11Z</published>
    <title>Augmented Cyclic Adversarial Learning for Domain Adaptation</title>
    <summary>  Training a model to perform a task typically requires a large amount of data
from the domains in which the task will be applied. However, it is often the
case that data are abundant in some domains but scarce in others. Domain
adaptation deals with the challenge of adapting a model trained from a
data-rich source domain to perform well in a data-poor target domain. In
general, this requires learning plausible mappings between domains. CycleGAN is
a powerful framework that efficiently learns to map inputs from one domain to
another using adversarial training and a cycle-consistency constraint. However,
the conventional approach of enforcing cycle-consistency via reconstruction may
be overly restrictive in cases where one or more domains have limited training
data. In this paper, we propose an augmented cyclic adversarial learning model
that enforces the cycle-consistency constraint through an external task
specific model, which encourages the preservation of task-relevant content as
opposed to exact reconstruction. We explore digit classification with MNIST and
SVHN in a low-resource setting in supervised, semi and unsupervised situation.
In low-resource supervised setting, the results show that our approach improves
absolute performance by $14\%$ and $4\%$ when adapting SVHN to MNIST and vice
versa, respectively, which outperforms unsupervised domain adaptation methods
that require high-resource unlabeled target domain. Moreover, using only few
unsupervised target data, our approach can still outperforms many high-resource
unsupervised models. In speech domains, we also adopt a speech recognition
model from each domain as the task specific model. Our approach improves
absolute performance of speech recognition by $2\%$ for female speakers in the
TIMIT dataset, where the majority of training samples are from male voices.
</summary>
    <author>
      <name>Ehsan Hosseini-Asl</name>
    </author>
    <author>
      <name>Yingbo Zhou</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00374v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00374v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08873v3</id>
    <updated>2018-08-07T21:23:23Z</updated>
    <published>2017-10-24T16:30:29Z</published>
    <title>Robust Photometric Stereo via Dictionary Learning</title>
    <summary>  Photometric stereo is a method that seeks to reconstruct the normal vectors
of an object from a set of images of the object illuminated under different
light sources. While effective in some situations, classical photometric stereo
relies on a diffuse surface model that cannot handle objects with complex
reflectance patterns, and it is sensitive to non-idealities in the images. In
this work, we propose a novel approach to photometric stereo that relies on
dictionary learning to produce robust normal vector reconstructions.
Specifically, we develop two formulations for applying dictionary learning to
photometric stereo. We propose a model that applies dictionary learning to
regularize and reconstruct the normal vectors from the images under the classic
Lambertian reflectance model. We then generalize this model to explicitly model
non-Lambertian objects. We investigate both approaches through extensive
experimentation on synthetic and real benchmark datasets and observe
state-of-the-art performance compared to existing robust photometric stereo
methods.
</summary>
    <author>
      <name>Andrew J. Wagenmaker</name>
    </author>
    <author>
      <name>Brian E. Moore</name>
    </author>
    <author>
      <name>Raj Rao Nadakuditi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Transactions on Computational Imaging</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.08873v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08873v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02480v1</id>
    <updated>2018-08-07T21:23:21Z</updated>
    <published>2018-08-07T21:23:21Z</published>
    <title>Deep context: end-to-end contextual speech recognition</title>
    <summary>  In automatic speech recognition (ASR) what a user says depends on the
particular context she is in. Typically, this context is represented as a set
of word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E)
ASR sys- tem that utilizes such context. Our approach, which we re- fer to as
Contextual Listen, Attend and Spell (CLAS) jointly- optimizes the ASR
components along with embeddings of the context n-grams. During inference, the
CLAS system can be presented with context phrases which might contain out-of-
vocabulary (OOV) terms not seen during training. We com- pare our proposed
system to a more traditional contextualiza- tion approach, which performs
shallow-fusion between inde- pendently trained LAS and contextual n-gram models
during beam search. Across a number of tasks, we find that the pro- posed CLAS
system outperforms the baseline method by as much as 68% relative WER,
indicating the advantage of joint optimization over individually trained
components. Index Terms: speech recognition, sequence-to-sequence models,
listen attend and spell, LAS, attention, embedded speech recognition.
</summary>
    <author>
      <name>Golan Pundak</name>
    </author>
    <author>
      <name>Tara N. Sainath</name>
    </author>
    <author>
      <name>Rohit Prabhavalkar</name>
    </author>
    <author>
      <name>Anjuli Kannan</name>
    </author>
    <author>
      <name>Ding Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03972v3</id>
    <updated>2018-08-07T21:12:20Z</updated>
    <published>2018-06-06T19:21:09Z</published>
    <title>A Multi-task Deep Learning Architecture for Maritime Surveillance using
  AIS Data Streams</title>
    <summary>  In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular timesampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.
</summary>
    <author>
      <name>Duong Nguyen</name>
    </author>
    <author>
      <name>Rodolphe Vadaine</name>
    </author>
    <author>
      <name>Guillaume Hajduch</name>
    </author>
    <author>
      <name>René Garello</name>
    </author>
    <author>
      <name>Ronan Fablet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE DSAA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03972v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03972v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02513v1</id>
    <updated>2018-08-07T18:42:09Z</updated>
    <published>2018-08-07T18:42:09Z</published>
    <title>Rethinking Numerical Representations for Deep Neural Networks</title>
    <summary>  With ever-increasing computational demand for deep learning, it is critical
to investigate the implications of the numeric representation and precision of
DNN model weights and activations on computational efficiency. In this work, we
explore unconventional narrow-precision floating-point representations as it
relates to inference accuracy and efficiency to steer the improved design of
future DNN platforms. We show that inference using these custom numeric
representations on production-grade DNNs, including GoogLeNet and VGG, achieves
an average speedup of 7.6x with less than 1% degradation in inference accuracy
relative to a state-of-the-art baseline platform representing the most
sophisticated hardware using single-precision floating point. To facilitate the
use of such customized precision, we also present a novel technique that
drastically reduces the time required to derive the optimal precision
configuration.
</summary>
    <author>
      <name>Parker Hill</name>
    </author>
    <author>
      <name>Babak Zamirai</name>
    </author>
    <author>
      <name>Shengshuo Lu</name>
    </author>
    <author>
      <name>Yu-Wei Chao</name>
    </author>
    <author>
      <name>Michael Laurenzano</name>
    </author>
    <author>
      <name>Mehrzad Samadi</name>
    </author>
    <author>
      <name>Marios Papaefthymiou</name>
    </author>
    <author>
      <name>Scott Mahlke</name>
    </author>
    <author>
      <name>Thomas Wenisch</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <author>
      <name>Lingjia Tang</name>
    </author>
    <author>
      <name>Jason Mars</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02510v1</id>
    <updated>2018-08-07T18:37:13Z</updated>
    <published>2018-08-07T18:37:13Z</published>
    <title>Message Passing Graph Kernels</title>
    <summary>  Graph kernels have recently emerged as a promising approach for tackling the
graph similarity and learning tasks at the same time. In this paper, we propose
a general framework for designing graph kernels. The proposed framework
capitalizes on the well-known message passing scheme on graphs. The kernels
derived from the framework consist of two components. The first component is a
kernel between vertices, while the second component is a kernel between graphs.
The main idea behind the proposed framework is that the representations of the
vertices are implicitly updated using an iterative procedure. Then, these
representations serve as the building blocks of a kernel that compares pairs of
graphs. We derive four instances of the proposed framework, and show through
extensive experiments that these instances are competitive with
state-of-the-art methods in various tasks.
</summary>
    <author>
      <name>Giannis Nikolentzos</name>
    </author>
    <author>
      <name>Michalis Vazirgiannis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05136v5</id>
    <updated>2018-08-07T18:12:10Z</updated>
    <published>2017-11-14T15:02:47Z</published>
    <title>Deep Rewiring: Training very sparse deep networks</title>
    <summary>  Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
</summary>
    <author>
      <name>Guillaume Bellec</name>
    </author>
    <author>
      <name>David Kappel</name>
    </author>
    <author>
      <name>Wolfgang Maass</name>
    </author>
    <author>
      <name>Robert Legenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ICLR 2018. 10 pages (12 with references,
  24 with appendix), 4 Figures in the main text. Reviews are available at:
  https://openreview.net/forum?id=BJ_wN01C- . This recent version contains
  minor corrections in the appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05136v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05136v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02474v1</id>
    <updated>2018-08-07T17:48:40Z</updated>
    <published>2018-08-07T17:48:40Z</published>
    <title>Multi-Label Zero-Shot Learning with Transfer-Aware Label Embedding
  Projection</title>
    <summary>  Zero-shot learning transfers knowledge from seen classes to novel unseen
classes to reduce human labor of labelling data for building new classifiers.
Much effort on zero-shot learning however has focused on the standard
multi-class setting, the more challenging multi-label zero-shot problem has
received limited attention. In this paper we propose a transfer-aware embedding
projection approach to tackle multi-label zero-shot learning. The approach
projects the label embedding vectors into a low-dimensional space to induce
better inter-label relationships and explicitly facilitate information transfer
from seen labels to unseen labels, while simultaneously learning a max-margin
multi-label classifier with the projected label embeddings. Auxiliary
information can be conveniently incorporated to guide the label embedding
projection to further improve label relation structures for zero-shot knowledge
transfer. We conduct experiments for zero-shot multi-label image
classification. The results demonstrate the efficacy of the proposed approach.
</summary>
    <author>
      <name>Meng Ye</name>
    </author>
    <author>
      <name>Yuhong Guo</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06063v2</id>
    <updated>2018-08-07T17:30:23Z</updated>
    <published>2018-05-29T19:37:37Z</published>
    <title>Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet
  Process Switching Linear Dynamical Systems</title>
    <summary>  Using movement primitive libraries is an effective means to enable robots to
solve more complex tasks. In order to build these movement libraries, current
algorithms require a prior segmentation of the demonstration trajectories. A
promising approach is to model the trajectory as being generated by a set of
Switching Linear Dynamical Systems and inferring a meaningful segmentation by
inspecting the transition points characterized by the switching dynamics. With
respect to the learning, a nonparametric Bayesian approach is employed
utilizing a Gibbs sampler.
</summary>
    <author>
      <name>Maximilian Sieb</name>
    </author>
    <author>
      <name>Matthias Schultheis</name>
    </author>
    <author>
      <name>Sebastian Szelag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Premature upload</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06063v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06063v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02435v1</id>
    <updated>2018-08-07T15:59:17Z</updated>
    <published>2018-08-07T15:59:17Z</published>
    <title>Mixed Integer Linear Programming for Feature Selection in Support Vector
  Machine</title>
    <summary>  This work focuses on support vector machine (SVM) with feature selection. A
MILP formulation is proposed for the problem. The choice of suitable features
to construct the separating hyperplanes has been modelled in this formulation
by including a budget constraint that sets in advance a limit on the number of
features to be used in the classification process. We propose both an exact and
a heuristic procedure to solve this formulation in an efficient way. Finally,
the validation of the model is done by checking it with some well-known data
sets and comparing it with classical classification methods.
</summary>
    <author>
      <name>Martine Labbé</name>
    </author>
    <author>
      <name>Luisa I. Martínez-Merino</name>
    </author>
    <author>
      <name>Antonio M. Rodríguez-Chía</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02433v1</id>
    <updated>2018-08-07T15:59:08Z</updated>
    <published>2018-08-07T15:59:08Z</published>
    <title>Robust Implicit Backpropagation</title>
    <summary>  Arguably the biggest challenge in applying neural networks is tuning the
hyperparameters, in particular the learning rate. The sensitivity to the
learning rate is due to the reliance on backpropagation to train the network.
In this paper we present the first application of Implicit Stochastic Gradient
Descent (ISGD) to train neural networks, a method known in convex optimization
to be unconditionally stable and robust to the learning rate. Our key
contribution is a novel layer-wise approximation of ISGD which makes its
updates tractable for neural networks. Experiments show that our method is more
robust to high learning rates and generally outperforms standard
backpropagation on a variety of tasks.
</summary>
    <author>
      <name>Francois Fagan</name>
    </author>
    <author>
      <name>Garud Iyengar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08809v2</id>
    <updated>2018-08-07T15:45:48Z</updated>
    <published>2018-05-22T18:38:59Z</published>
    <title>Infinite-Task Learning with Vector-Valued RKHSs</title>
    <summary>  Machine learning has witnessed the tremendous success of solving tasks
depending on a hyperparameter. While multi-task learning is celebrated for its
capacity to solve jointly a finite number of tasks, learning a continuum of
tasks for various loss functions is still a challenge. A promising approach,
called Parametric Task Learning, has paved the way in the case of
piecewise-linear loss functions. We propose a generic approach, called
Infinite-Task Learning, to solve jointly a continuum of tasks via vector-valued
RKHSs. We provide generalization guarantees to the suggested scheme and
illustrate its efficiency in cost-sensitive classification, quantile regression
and density level set estimation.
</summary>
    <author>
      <name>Romain Brault</name>
    </author>
    <author>
      <name>Alex Lambert</name>
    </author>
    <author>
      <name>Zoltán Szabó</name>
    </author>
    <author>
      <name>Maxime Sangnier</name>
    </author>
    <author>
      <name>Florence d'Alché-Buc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 5 figures, 2 tables. Preprint NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46E22, 62G08, 65D15, 47B32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08273v2</id>
    <updated>2018-08-07T14:29:39Z</updated>
    <published>2018-05-21T20:01:52Z</published>
    <title>Multiple Causal Inference with Latent Confounding</title>
    <summary>  Causal inference from observational data requires assumptions. These
assumptions range from measuring confounders to identifying instruments.
Traditionally, these assumptions have focused on estimation in a single causal
problem. In this work, we develop techniques for causal estimation in causal
problems with multiple treatments. We develop two assumptions based on shared
confounding between treatments and independence of treatments given the
confounder. Together these assumptions lead to a confounder estimator
regularized by mutual information. For this estimator, we develop a tractable
lower bound. To fit the outcome model, we use the residual information in the
treatments given the confounder. We validate on simulations and an example from
clinical medicine.
</summary>
    <author>
      <name>Rajesh Ranganath</name>
    </author>
    <author>
      <name>Adler Perotte</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08273v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08273v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01010v2</id>
    <updated>2018-08-07T14:14:24Z</updated>
    <published>2018-06-04T08:42:09Z</published>
    <title>Meta Learner with Linear Nulling</title>
    <summary>  We propose a meta learning algorithm utilizing a linear transformer that
carries out null-space projection of neural network outputs. The main idea is
to construct a classification space such that the error signals during few-shot
training are zero-forced on that space. The final decision on a test sample is
obtained utilizing a null-space-projected distance measure between the network
output and label-dependent weights that have been trained in the initial meta
learning phase. Our meta learner achieves the best or near-best accuracies
among known methods in few-shot image classification tasks with Omniglot and
miniImageNet. In particular, our method shows stronger relative performance by
significant margins as the classification task becomes more complicated.
</summary>
    <author>
      <name>Sung Whan Yoon</name>
    </author>
    <author>
      <name>Jun Seo</name>
    </author>
    <author>
      <name>Jaekyun Moon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02341v1</id>
    <updated>2018-08-07T13:12:05Z</updated>
    <published>2018-08-07T13:12:05Z</published>
    <title>Optimal stopping via deeply boosted backward regression</title>
    <summary>  In this note we propose a new approach towards solving numerically optimal
stopping problems via boosted regression based Monte Carlo algorithms. The main
idea of the method is to boost standard linear regression algorithms in each
backward induction step by adding new basis functions based on previously
estimated continuation values. The proposed methodology is illustrated by
several numerical examples from finance.
</summary>
    <author>
      <name>Denis Belomestny</name>
    </author>
    <author>
      <name>John Schoenmakers</name>
    </author>
    <author>
      <name>Vladimir Spokoiny</name>
    </author>
    <author>
      <name>Yuri Tavyrikov</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91B28" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02316v1</id>
    <updated>2018-08-07T12:04:40Z</updated>
    <published>2018-08-07T12:04:40Z</published>
    <title>Modelling hidden structure of signals in group data analysis with
  modified (Lr, 1) and block-term decompositions</title>
    <summary>  This work is devoted to elaboration on the idea to use block term
decomposition for group data analysis and to raise the possibility of modelling
group activity with (Lr, 1) and Tucker blocks. A new generalization of block
tensor decomposition was considered in application to group data analysis.
Suggested approach was evaluated on multilabel classification task for a set of
images. This contribution also reports results of investigation on clustering
with proposed tensor models in comparison with known matrix models, namely
common orthogonal basis extraction and group independent component analysis.
</summary>
    <author>
      <name>Pavel Kharyuk</name>
    </author>
    <author>
      <name>Ivan Oseledets</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06096v3</id>
    <updated>2018-08-07T09:19:13Z</updated>
    <published>2017-12-17T12:15:08Z</published>
    <title>Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF
  Data using Deep Learning</title>
    <summary>  In portable, three dimensional, and ultra-fast ultrasound imaging systems,
there is an increasing demand for the reconstruction of high quality images
from a limited number of radio-frequency (RF) measurements due to receiver (Rx)
or transmit (Xmit) event sub-sampling. However, due to the presence of side
lobe artifacts from RF sub-sampling, the standard beamformer often produces
blurry images with less contrast, which are unsuitable for diagnostic purposes.
Existing compressed sensing approaches often require either hardware changes or
computationally expensive algorithms, but their quality improvements are
limited. To address this problem, here we propose a novel deep learning
approach that directly interpolates the missing RF data by utilizing redundancy
in the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF
data from a multi-line acquisition B-mode system confirm that the proposed
method can effectively reduce the data rate without sacrificing image quality.
</summary>
    <author>
      <name>Yeo Hun Yoon</name>
    </author>
    <author>
      <name>Shujaat Khan</name>
    </author>
    <author>
      <name>Jaeyoung Huh</name>
    </author>
    <author>
      <name>Jong Chul Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The title has been changed. This version will appear in IEEE Trans.
  on Medical Imaging</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06096v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06096v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02266v1</id>
    <updated>2018-08-07T09:01:05Z</updated>
    <published>2018-08-07T09:01:05Z</published>
    <title>Multi-Output Convolution Spectral Mixture for Gaussian Processes</title>
    <summary>  Multi-output Gaussian processes (MOGPs) are recently extended by using
spectral mixture kernel, which enables expressively pattern extrapolation with
a strong interpretation. In particular, Multi-Output Spectral Mixture kernel
(MOSM) is a recent, powerful state of the art method. However, MOSM cannot
reduce to the ordinary spectral mixture kernel (SM) when using a single
channel. Moreover, when the spectral density of different channels is either
very close or very far from each other in the frequency domain, MOSM generates
unreasonable scale effects on cross weights which produces an incorrect
description of the channel correlation structure. In this paper, we tackle
these drawbacks and introduce a principled multi-output convolution spectral
mixture kernel (MOCSM) framework. In our framework, we model channel
dependencies through convolution of time and phase delayed spectral mixtures
between different channels. Results of extensive experiments on synthetic and
real datasets demontrate the advantages of MOCSM and its state of the art
performance.
</summary>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Perry Groot</name>
    </author>
    <author>
      <name>Jinsong Chen</name>
    </author>
    <author>
      <name>Elena Marchiori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 26 figures. arXiv admin note: text overlap with
  arXiv:1808.01132</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01361v2</id>
    <updated>2018-08-07T08:32:58Z</updated>
    <published>2018-05-03T15:13:02Z</published>
    <title>Machine learning regression on hyperspectral data to estimate multiple
  water parameters</title>
    <summary>  In this paper, we present a regression framework involving several machine
learning models to estimate water parameters based on hyperspectral data.
Measurements from a multi-sensor field campaign, conducted on the River Elbe,
Germany, represent the benchmark dataset. It contains hyperspectral data and
the five water parameters chlorophyll a, green algae, diatoms, CDOM and
turbidity. We apply a PCA for the high-dimensional data as a possible
preprocessing step. Then, we evaluate the performance of the regression
framework with and without this preprocessing step. The regression results of
the framework clearly reveal the potential of estimating water parameters based
on hyperspectral data with machine learning. The proposed framework provides
the basis for further investigations, such as adapting the framework to
estimate water parameters of different inland waters.
</summary>
    <author>
      <name>Philipp M. Maier</name>
    </author>
    <author>
      <name>Sina Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been accepted to the IEEE WHISPERS 2018 conference. (C)
  2018 IEEE</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01361v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01361v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02237v1</id>
    <updated>2018-08-07T07:26:39Z</updated>
    <published>2018-08-07T07:26:39Z</published>
    <title>Inferring Molecular Pathology and micro-RNA Transcriptome from mRNA
  Profiles of Cancer Biopsies through Deep Multi-Task Learning</title>
    <summary>  Despite great advances, molecular cancer pathology is often limited to use a
small number of biomarkers rather than the whole transcriptome, partly due to
the computational challenges. Here, we introduce a novel architecture of DNNs
that is capable of simultaneous inference of various properties of biological
samples, through multi-task and transfer learning. We employed this
architecture on mRNA transcription profiles of 10787 clinical samples from 34
classes (one healthy and 33 different types of cancer) from 27 tissues. Our
system significantly outperforms prior works and classical machine learning
approaches in predicting tissue-of-origin, normal or disease state and cancer
type of each sample. Furthermore, it can predict miRNA transcription profile of
each sample, which enables performing miRNA expression research when only mRNA
transcriptome data are available. We also show this system is very robust
against noise and missing values. Collectively, our results highlight
applications of artificial intelligence in molecular cancer pathology and
oncological research.
</summary>
    <author>
      <name>Behrooz Azarkhalili</name>
    </author>
    <author>
      <name>Ali Saberi</name>
    </author>
    <author>
      <name>Hamidreza Chitsaz</name>
    </author>
    <author>
      <name>Ali Sharifi-Zarchi</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02213v1</id>
    <updated>2018-08-07T05:09:13Z</updated>
    <published>2018-08-07T05:09:13Z</published>
    <title>Importance of the Mathematical Foundations of Machine Learning Methods
  for Scientific and Engineering Applications</title>
    <summary>  There has been a lot of recent interest in adopting machine learning methods
for scientific and engineering applications. This has in large part been
inspired by recent successes and advances in the domains of Natural Language
Processing (NLP) and Image Classification (IC). However, scientific and
engineering problems have their own unique characteristics and requirements
raising new challenges for effective design and deployment of machine learning
approaches. There is a strong need for further mathematical developments on the
foundations of machine learning methods to increase the level of rigor of
employed methods and to ensure more reliable and interpretable results. Also as
reported in the recent literature on state-of-the-art results and indicated by
the No Free Lunch Theorems of statistical learning theory incorporating some
form of inductive bias and domain knowledge is essential to success.
Consequently, even for existing and widely used methods there is a strong need
for further mathematical work to facilitate ways to incorporate prior
scientific knowledge and related inductive biases into learning frameworks and
algorithms. We briefly discuss these topics and discuss some ideas proceeding
in this direction.
</summary>
    <author>
      <name>Paul J. Atzberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position Paper at SciML2018 Workshop, US Department of Energy,
  January 2018, (two-page limit)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04447v1</id>
    <updated>2018-08-07T05:09:11Z</updated>
    <published>2018-08-07T05:09:11Z</published>
    <title>Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological
  and Quantitative Magnetic Resonance Imaging</title>
    <summary>  Obtaining magnetic resonance images (MRI) with high resolution and generating
quantitative image-based biomarkers for assessing tissue biochemistry is
crucial in clinical and research applications. How- ever, acquiring
quantitative biomarkers requires high signal-to-noise ratio (SNR), which is at
odds with high-resolution in MRI, especially in a single rapid sequence. In
this paper, we demonstrate how super-resolution can be utilized to maintain
adequate SNR for accurate quantification of the T2 relaxation time biomarker,
while simultaneously generating high- resolution images. We compare the
efficacy of resolution enhancement using metrics such as peak SNR and
structural similarity. We assess accuracy of cartilage T2 relaxation times by
comparing against a standard reference method. Our evaluation suggests that SR
can successfully maintain high-resolution and generate accurate biomarkers for
accelerating MRI scans and enhancing the value of clinical and research MRI.
</summary>
    <author>
      <name>Akshay Chaudhari</name>
    </author>
    <author>
      <name>Zhongnan Fang</name>
    </author>
    <author>
      <name>Jin Hyung Lee</name>
    </author>
    <author>
      <name>Garry Gold</name>
    </author>
    <author>
      <name>Brian Hargreaves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the Machine Learning for Medical Image Reconstruction
  Workshop at MICCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03184v2</id>
    <updated>2018-08-07T03:58:01Z</updated>
    <published>2018-02-09T09:52:35Z</published>
    <title>Self-Bounded Prediction Suffix Tree via Approximate String Matching</title>
    <summary>  Prediction suffix trees (PST) provide an effective tool for sequence
modelling and prediction. Current prediction techniques for PSTs rely on exact
matching between the suffix of the current sequence and the previously observed
sequence. We present a provably correct algorithm for learning a PST with
approximate suffix matching by relaxing the exact matching condition. We then
present a self-bounded enhancement of our algorithm where the depth of suffix
tree grows automatically in response to the model performance on a training
sequence. Through experiments on synthetic datasets as well as three real-world
datasets, we show that the approximate matching PST results in better
predictive performance than the other variants of PST.
</summary>
    <author>
      <name>Dongwoo Kim</name>
    </author>
    <author>
      <name>Christian Walder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 35th International Conference on Machine Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07712v3</id>
    <updated>2018-08-07T03:04:11Z</updated>
    <published>2018-03-21T01:39:08Z</published>
    <title>Causal Inference on Discrete Data via Estimating Distance Correlations</title>
    <summary>  In this paper, we deal with the problem of inferring causal directions when
the data is on discrete domain. By considering the distribution of the cause
$P(X)$ and the conditional distribution mapping cause to effect $P(Y|X)$ as
independent random variables, we propose to infer the causal direction via
comparing the distance correlation between $P(X)$ and $P(Y|X)$ with the
distance correlation between $P(Y)$ and $P(X|Y)$. We infer "$X$ causes $Y$" if
the dependence coefficient between $P(X)$ and $P(Y|X)$ is smaller. Experiments
are performed to show the performance of the proposed method.
</summary>
    <author>
      <name>Furui Liu</name>
    </author>
    <author>
      <name>Laiwan Chan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00820</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00820" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, Vol. 28, No. 5, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.07712v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07712v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02180v1</id>
    <updated>2018-08-07T01:47:57Z</updated>
    <published>2018-08-07T01:47:57Z</published>
    <title>Instance-Dependent PU Learning by Bayesian Optimal Relabeling</title>
    <summary>  When learning from positive and unlabelled data, it is a strong assumption
that the positive observations are randomly sampled from the distribution of
$X$ conditional on $Y = 1$, where X stands for the feature and Y the label.
Most existing algorithms are optimally designed under the assumption. However,
for many real-world applications, the observed positive examples are dependent
on the conditional probability $P(Y = 1|X)$ and should be sampled biasedly. In
this paper, we assume that a positive example with a higher $P(Y = 1|X)$ is
more likely to be labelled and propose a probabilistic-gap based PU learning
algorithms. Specifically, by treating the unlabelled data as noisy negative
examples, we could automatically label a group positive and negative examples
whose labels are identical to the ones assigned by a Bayesian optimal
classifier with a consistency guarantee. The relabelled examples have a biased
domain, which is remedied by the kernel mean matching technique. The proposed
algorithm is model-free and thus do not have any parameters to tune.
Experimental results demonstrate that our method works well on both generated
and real-world datasets.
</summary>
    <author>
      <name>Fengxiang He</name>
    </author>
    <author>
      <name>Tongliang Liu</name>
    </author>
    <author>
      <name>Geoffrey I Webb</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09075v2</id>
    <updated>2018-08-07T01:44:58Z</updated>
    <published>2016-10-28T04:06:59Z</published>
    <title>Missing Data Imputation for Supervised Learning</title>
    <summary>  Missing data imputation can help improve the performance of prediction models
in situations where missing data hide useful information. This paper compares
methods for imputing missing categorical data for supervised classification
tasks. We experiment on two machine learning benchmark datasets with missing
categorical data, comparing classifiers trained on non-imputed (i.e., one-hot
encoded) or imputed data with different levels of additional missing-data
perturbation. We show imputation methods can increase predictive accuracy in
the presence of missing-data perturbation, which can actually improve
prediction accuracy by regularizing the classifier. We achieve the
state-of-the-art on the Adult dataset with missing-data perturbation and
k-nearest-neighbors (k-NN) imputation.
</summary>
    <author>
      <name>Jason Poulos</name>
    </author>
    <author>
      <name>Rafael Valle</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09075v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09075v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02169v1</id>
    <updated>2018-08-07T00:49:24Z</updated>
    <published>2018-08-07T00:49:24Z</published>
    <title>Fast Variance Reduction Method with Stochastic Batch Size</title>
    <summary>  In this paper we study a family of variance reduction methods with randomized
batch size---at each step, the algorithm first randomly chooses the batch size
and then selects a batch of samples to conduct a variance-reduced stochastic
update. We give the linear convergence rate for this framework for composite
functions, and show that the optimal strategy to achieve the optimal
convergence rate per data access is to always choose batch size of 1, which is
equivalent to the SAGA algorithm. However, due to the presence of cache/disk IO
effect in computer architecture, the number of data access cannot reflect the
running time because of 1) random memory access is much slower than sequential
access, 2) when data is too big to fit into memory, disk seeking takes even
longer time. After taking these into account, choosing batch size of $1$ is no
longer optimal, so we propose a new algorithm called SAGA++ and show how to
calculate the optimal average batch size theoretically. Our algorithm
outperforms SAGA and other existing batched and stochastic solvers on real
datasets. In addition, we also conduct a precise analysis to compare different
update rules for variance reduction methods, showing that SAGA++ converges
faster than SVRG in theory.
</summary>
    <author>
      <name>Xuanqing Liu</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01550v2</id>
    <updated>2018-08-07T00:47:43Z</updated>
    <published>2018-08-05T02:01:31Z</published>
    <title>Designing Adaptive Neural Networks for Energy-Constrained Image
  Classification</title>
    <summary>  As convolutional neural networks (CNNs) enable state-of-the-art computer
vision applications, their high energy consumption has emerged as a key
impediment to their deployment on embedded and mobile devices. Towards
efficient image classification under hardware constraints, prior work has
proposed adaptive CNNs, i.e., systems of networks with different accuracy and
computation characteristics, where a selection scheme adaptively selects the
network to be evaluated for each input image. While previous efforts have
investigated different network selection schemes, we find that they do not
necessarily result in energy savings when deployed on mobile systems. The key
limitation of existing methods is that they learn only how data should be
processed among the CNNs and not the network architectures, with each network
being treated as a blackbox.
  To address this limitation, we pursue a more powerful design paradigm where
the architecture settings of the CNNs are treated as hyper-parameters to be
globally optimized. We cast the design of adaptive CNNs as a hyper-parameter
optimization problem with respect to energy, accuracy, and communication
constraints imposed by the mobile device. To efficiently solve this problem, we
adapt Bayesian optimization to the properties of the design space, reaching
near-optimal configurations in few tens of function evaluations. Our method
reduces the energy consumed for image classification on a mobile device by up
to 6x, compared to the best previously published work that uses CNNs as
blackboxes. Finally, we evaluate two image classification practices, i.e.,
classifying all images locally versus over the cloud under energy and
communication constraints.
</summary>
    <author>
      <name>Dimitrios Stamoulis</name>
    </author>
    <author>
      <name>Ting-Wu Chin</name>
    </author>
    <author>
      <name>Anand Krishnan Prakash</name>
    </author>
    <author>
      <name>Haocheng Fang</name>
    </author>
    <author>
      <name>Sribhuvan Sajja</name>
    </author>
    <author>
      <name>Mitchell Bognar</name>
    </author>
    <author>
      <name>Diana Marculescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This conference paper will appear in the proceedings of ICCAD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01550v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01550v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02766v1</id>
    <updated>2018-08-06T22:15:20Z</updated>
    <published>2018-08-06T22:15:20Z</published>
    <title>Improved survival of cancer patients admitted to the ICU between 2002
  and 2011 at a U.S. teaching hospital</title>
    <summary>  Over the past decades, both critical care and cancer care have improved
substantially. Due to increased cancer-specific survival, we hypothesized that
both the number of cancer patients admitted to the ICU and overall survival
have increased since the millennium change. MIMIC-III, a freely accessible
critical care database of Beth Israel Deaconess Medical Center, Boston, USA was
used to retrospectively study trends and outcomes of cancer patients admitted
to the ICU between 2002 and 2011. Multiple logistic regression analysis was
performed to adjust for confounders of 28-day and 1-year mortality.
  Out of 41,468 unique ICU admissions, 1,100 hemato-oncologic, 3,953 oncologic
and 49 patients with both a hematological and solid malignancy were analyzed.
Hematological patients had higher critical illness scores than non-cancer
patients, while oncologic patients had similar APACHE-III and SOFA-scores
compared to non-cancer patients. In the univariate analysis, cancer was
strongly associated with mortality (OR= 2.74, 95%CI: 2.56, 2.94). Over the
10-year study period, 28-day mortality of cancer patients decreased by 30%.
This trend persisted after adjustment for covariates, with cancer patients
having significantly higher mortality (OR=2.63, 95%CI: 2.38, 2.88). Between
2002 and 2011, both the adjusted odds of 28-day mortality and the adjusted odds
of 1-year mortality for cancer patients decreased by 6% (95%CI: 4%, 9%). Having
cancer was the strongest single predictor of 1-year mortality in the
multivariate model (OR=4.47, 95%CI: 4.11, 4.84).
</summary>
    <author>
      <name>Chris Sauer</name>
    </author>
    <author>
      <name>Jinghui Dong</name>
    </author>
    <author>
      <name>Leo Celi</name>
    </author>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02123v1</id>
    <updated>2018-08-06T21:27:05Z</updated>
    <published>2018-08-06T21:27:05Z</published>
    <title>Structure Learning for Relational Logistic Regression: An Ensemble
  Approach</title>
    <summary>  We consider the problem of learning Relational Logistic Regression (RLR).
Unlike standard logistic regression, the features of RLRs are first-order
formulae with associated weight vectors instead of scalar weights. We turn the
problem of learning RLR to learning these vector-weighted formulae and develop
a learning algorithm based on the recently successful functional-gradient
boosting methods for probabilistic logic models. We derive the functional
gradients and show how weights can be learned simultaneously in an efficient
manner. Our empirical evaluation on standard and novel data sets demonstrates
the superiority of our approach over other methods for learning RLR.
</summary>
    <author>
      <name>Nandini Ramanan</name>
    </author>
    <author>
      <name>Gautam Kunapuli</name>
    </author>
    <author>
      <name>Tushar Khot</name>
    </author>
    <author>
      <name>Bahare Fatemi</name>
    </author>
    <author>
      <name>Seyed Mehran Kazemi</name>
    </author>
    <author>
      <name>David Poole</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <author>
      <name>Sriraam Natarajan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02026v1</id>
    <updated>2018-08-06T21:21:48Z</updated>
    <published>2018-08-06T21:21:48Z</published>
    <title>Active Learning based on Data Uncertainty and Model Sensitivity</title>
    <summary>  Robots can rapidly acquire new skills from demonstrations. However, during
generalisation of skills or transitioning across fundamentally different
skills, it is unclear whether the robot has the necessary knowledge to perform
the task. Failing to detect missing information often leads to abrupt movements
or to collisions with the environment. Active learning can quantify the
uncertainty of performing the task and, in general, locate regions of missing
information. We introduce a novel algorithm for active learning and demonstrate
its utility for generating smooth trajectories. Our approach is based on deep
generative models and metric learning in latent spaces. It relies on the
Jacobian of the likelihood to detect non-smooth transitions in the latent
space, i.e., transitions that lead to abrupt changes in the movement of the
robot. When non-smooth transitions are detected, our algorithm asks for an
additional demonstration from that specific region. The newly acquired
knowledge modifies the data manifold and allows for learning a latent
representation for generating smooth movements. We demonstrate the efficacy of
our approach on generalising elementary skills, transitioning across different
skills, and implicitly avoiding collisions with the environment. For our
experiments, we use a simulated pendulum where we observe its motion from
images and a 7-DoF anthropomorphic arm.
</summary>
    <author>
      <name>Nutan Chen</name>
    </author>
    <author>
      <name>Alexej Klushyn</name>
    </author>
    <author>
      <name>Alexandros Paraschos</name>
    </author>
    <author>
      <name>Djalel Benbouzid</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published on 2018 IEEE/RSJ International Conference on Intelligent
  Robots and System</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02113v1</id>
    <updated>2018-08-06T21:05:55Z</updated>
    <published>2018-08-06T21:05:55Z</published>
    <title>Paying Attention to Attention: Highlighting Influential Samples in
  Sequential Analysis</title>
    <summary>  In (Yang et al. 2016), a hierarchical attention network (HAN) is created for
document classification. The attention layer can be used to visualize text
influential in classifying the document, thereby explaining the model's
prediction. We successfully applied HAN to a sequential analysis task in the
form of real-time monitoring of turn taking in conversations. However, we
discovered instances where the attention weights were uniform at the stopping
point (indicating all turns were equivalently influential to the classifier),
preventing meaningful visualization for real-time human review or classifier
improvement. We observed that attention weights for turns fluctuated as the
conversations progressed, indicating turns had varying influence based on
conversation state. Leveraging this observation, we develop a method to create
more informative real-time visuals (as confirmed by human reviewers) in cases
of uniform attention weights using the changes in turn importance as a
conversation progresses over time.
</summary>
    <author>
      <name>Cynthia Freeman</name>
    </author>
    <author>
      <name>Jonathan Merriman</name>
    </author>
    <author>
      <name>Abhinav Aggarwal</name>
    </author>
    <author>
      <name>Ian Beaver</name>
    </author>
    <author>
      <name>Abdullah Mueen</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02093v1</id>
    <updated>2018-08-06T20:10:27Z</updated>
    <published>2018-08-06T20:10:27Z</published>
    <title>Learning to Share and Hide Intentions using Information Regularization</title>
    <summary>  Learning to cooperate with friends and compete with foes is a key component
of multi-agent reinforcement learning. Typically to do so, one requires access
to either a model of or interaction with the other agent(s). Here we show how
to learn effective strategies for cooperation and competition in an asymmetric
information game with no such model or interaction. Our approach is to
encourage an agent to reveal or hide their intentions using an
information-theoretic regularizer. We consider both the mutual information
between goal and action given state, as well as the mutual information between
goal and state. We show how to stochastically optimize these regularizers in a
way that is easy to integrate with policy gradient reinforcement learning.
Finally, we demonstrate that cooperative (competitive) policies learned with
our approach lead to more (less) reward for a second agent in two simple
asymmetric information games.
</summary>
    <author>
      <name>DJ Strouse</name>
    </author>
    <author>
      <name>Max Kleiman-Weiner</name>
    </author>
    <author>
      <name>Josh Tenenbaum</name>
    </author>
    <author>
      <name>Matt Botvinick</name>
    </author>
    <author>
      <name>David Schwab</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04207v2</id>
    <updated>2018-08-06T20:02:49Z</updated>
    <published>2018-06-11T19:24:59Z</published>
    <title>Swarming for Faster Convergence in Stochastic Optimization</title>
    <summary>  We study a distributed framework for stochastic optimization which is
inspired by models of collective motion found in nature (e.g., swarming) with
mild communication requirements. Specifically, we analyze a scheme in which
each one of $N &gt; 1$ independent threads, implements in a distributed and
unsynchronized fashion, a stochastic gradient-descent algorithm which is
perturbed by a swarming potential. Assuming the overhead caused by
synchronization is not negligible, we show the swarming-based approach exhibits
better performance than a centralized algorithm (based upon the average of $N$
observations) in terms of (real-time) convergence speed. We also derive an
error bound that is monotone decreasing in network size and connectivity. We
characterize the scheme's finite-time performances for both convex and
non-convex objective functions.
</summary>
    <author>
      <name>Shi Pu</name>
    </author>
    <author>
      <name>Alfredo Garcia</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04207v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04207v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05096v2</id>
    <updated>2018-08-06T19:36:59Z</updated>
    <published>2018-06-13T14:58:49Z</published>
    <title>Introducing user-prescribed constraints in Markov chains for nonlinear
  dimensionality reduction</title>
    <summary>  Stochastic kernel based dimensionality reduction approaches have become
popular in the last decade. The central component of many of these methods is a
symmetric kernel that quantifies the vicinity between pairs of data points and
a kernel-induced Markov chain on the data. Typically, the Markov chain is fully
specified by the kernel through row normalization. However, in many cases, it
is desirable to impose user-specified stationary-state and dynamical
constraints on the Markov chain. Unfortunately, no systematic framework exists
to impose such user-defined constraints. Here, we introduce a path entropy
maximization based approach to derive the transition probabilities of Markov
chains using a kernel and additional user-specified constraints. We illustrate
the usefulness of these Markov chains with examples.
</summary>
    <author>
      <name>Purushottam D. Dixit</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05096v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05096v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02078v1</id>
    <updated>2018-08-06T19:28:26Z</updated>
    <published>2018-08-06T19:28:26Z</published>
    <title>Unbiased Implicit Variational Inference</title>
    <summary>  We develop unbiased implicit variational inference (UIVI), a method that
expands the applicability of variational inference by defining an expressive
variational family. UIVI considers an implicit variational distribution
obtained in a hierarchical manner using a simple reparameterizable distribution
whose variational parameters are defined by arbitrarily flexible deep neural
networks. Unlike previous works, UIVI directly optimizes the evidence lower
bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on
several models, including Bayesian multinomial logistic regression and
variational autoencoders, and show that UIVI achieves both tighter ELBO and
better predictive performance than existing approaches at a similar
computational cost.
</summary>
    <author>
      <name>Michalis K. Titsias</name>
    </author>
    <author>
      <name>Francisco J. R. Ruiz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02061v1</id>
    <updated>2018-08-06T18:36:12Z</updated>
    <published>2018-08-06T18:36:12Z</published>
    <title>Semblance: A Rank-Based Kernel on Probability Spaces for Niche Detection</title>
    <summary>  Kernel methods provide a principled approach for detecting nonlinear
relations using well understood linear algorithms. In exploratory data analyses
when the underlying structure of the data's probability space is unclear, the
choice of kernel is often arbitrary. Here, we present a novel kernel,
Semblance, on a probability feature space. The advantage of Semblance lies in
its distribution free formulation and its ability to detect niche features by
placing greater emphasis on similarity between observation pairs that fall at
the tail ends of a distribution, as opposed to those that fall towards the
mean. We prove that Semblance is a valid Mercer kernel and illustrate its
applicability through simulations and real world examples.
</summary>
    <author>
      <name>Divyansh Agarwal</name>
    </author>
    <author>
      <name>Nancy Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Figures, 4 Supplementary Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01990v1</id>
    <updated>2018-08-06T16:51:36Z</updated>
    <published>2018-08-06T16:51:36Z</published>
    <title>Hashing with Binary Matrix Pursuit</title>
    <summary>  We propose theoretical and empirical improvements for two-stage hashing
methods. We first provide a theoretical analysis on the quality of the binary
codes and show that, under mild assumptions, a residual learning scheme can
construct binary codes that fit any neighborhood structure with arbitrary
accuracy. Secondly, we show that with high-capacity hash functions such as
CNNs, binary code inference can be greatly simplified for many standard
neighborhood definitions, yielding smaller optimization problems and more
robust codes. Incorporating our findings, we propose a novel two-stage hashing
method that significantly outperforms previous hashing studies on widely used
image retrieval benchmarks.
</summary>
    <author>
      <name>Fatih Cakir</name>
    </author>
    <author>
      <name>Kun He</name>
    </author>
    <author>
      <name>Stan Sclaroff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures. In Proceedings of European Conference on
  Computer Vision (ECCV), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01976v1</id>
    <updated>2018-08-06T16:13:43Z</updated>
    <published>2018-08-06T16:13:43Z</published>
    <title>Adversarial Vision Challenge</title>
    <summary>  The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate
measurable progress towards robust machine vision models and more generally
applicable adversarial attacks. This document is an updated version of our
competition proposal that was accepted in the competition track of 32nd
Conference on Neural Information Processing Systems (NIPS 2018).
</summary>
    <author>
      <name>Wieland Brendel</name>
    </author>
    <author>
      <name>Jonas Rauber</name>
    </author>
    <author>
      <name>Alexey Kurakin</name>
    </author>
    <author>
      <name>Nicolas Papernot</name>
    </author>
    <author>
      <name>Behar Veliqi</name>
    </author>
    <author>
      <name>Marcel Salathé</name>
    </author>
    <author>
      <name>Sharada P. Mohanty</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://www.crowdai.org/challenges/adversarial-vision-challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01974v1</id>
    <updated>2018-08-06T16:06:43Z</updated>
    <published>2018-08-06T16:06:43Z</published>
    <title>A Survey on Deep Transfer Learning</title>
    <summary>  As a new classification platform, deep learning has recently received
increasing attention from researchers and has been successfully applied to many
domains. In some domains, like bioinformatics and robotics, it is very
difficult to construct a large-scale well-annotated dataset due to the expense
of data acquisition and costly annotation, which limits its development.
Transfer learning relaxes the hypothesis that the training data must be
independent and identically distributed (i.i.d.) with the test data, which
motivates us to use transfer learning to solve the problem of insufficient
training data. This survey focuses on reviewing the current researches of
transfer learning by using deep neural network and its applications. We defined
deep transfer learning, category and review the recent research works based on
the techniques used in deep transfer learning.
</summary>
    <author>
      <name>Chuanqi Tan</name>
    </author>
    <author>
      <name>Fuchun Sun</name>
    </author>
    <author>
      <name>Tao Kong</name>
    </author>
    <author>
      <name>Wenchang Zhang</name>
    </author>
    <author>
      <name>Chao Yang</name>
    </author>
    <author>
      <name>Chunfang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 27th International Conference on Artificial Neural Networks
  (ICANN 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01960v1</id>
    <updated>2018-08-06T15:22:13Z</updated>
    <published>2018-08-06T15:22:13Z</published>
    <title>Distributional Multivariate Policy Evaluation and Exploration with the
  Bellman GAN</title>
    <summary>  The recently proposed distributional approach to reinforcement learning
(DiRL) is centered on learning the distribution of the reward-to-go, often
referred to as the value distribution. In this work, we show that the
distributional Bellman equation, which drives DiRL methods, is equivalent to a
generative adversarial network (GAN) model. In this formulation, DiRL can be
seen as learning a deep generative model of the value distribution, driven by
the discrepancy between the distribution of the current value, and the
distribution of the sum of current reward and next value. We use this insight
to propose a GAN-based approach to DiRL, which leverages the strengths of GANs
in learning distributions of high-dimensional data. In particular, we show that
our GAN approach can be used for DiRL with multivariate rewards, an important
setting which cannot be tackled with prior methods. The multivariate setting
also allows us to unify learning the distribution of values and state
transitions, and we exploit this idea to devise a novel exploration method that
is driven by the discrepancy in estimating both values and states.
</summary>
    <author>
      <name>Dror Freirich</name>
    </author>
    <author>
      <name>Ron Meir</name>
    </author>
    <author>
      <name>Aviv Tamar</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02375v2</id>
    <updated>2018-08-06T15:12:45Z</updated>
    <published>2018-06-01T03:57:56Z</published>
    <title>Understanding Batch Normalization</title>
    <summary>  Batch normalization is a ubiquitous deep learning technique that normalizes
activations in intermediate layers. It is associated with improved accuracy and
faster learning, but despite its enormous success there is little consensus
regarding why it works. We aim to rectify this and take an empirical approach
to understanding batch normalization. Our primary observation is that the
higher learning rates that batch normalization enables have a regularizing
effect that dramatically improves generalization of normalized networks, which
is both demonstrated empirically and motivated theoretically. We show how
activations become large and how the convolutional channels become increasingly
ill-behaved for layers deep in unnormalized networks, and how this results in
larger input-independent gradients. Beyond just gradient scaling, we
demonstrate how the learning rate in unnormalized networks is further limited
by the magnitude of activations growing exponentially with network depth for
large parameter updates, a problem batch normalization trivially avoids.
Motivated by recent results in random matrix theory, we argue that
ill-conditioning of the activations is due to fluctuations in random
initialization, shedding new light on classical initialization schemes and
their consequences.
</summary>
    <author>
      <name>Johan Bjorck</name>
    </author>
    <author>
      <name>Carla Gomes</name>
    </author>
    <author>
      <name>Bart Selman</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02375v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02375v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01944v1</id>
    <updated>2018-08-06T14:51:33Z</updated>
    <published>2018-08-06T14:51:33Z</published>
    <title>V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial
  Segmentation</title>
    <summary>  Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder
that causes changes in the anatomy of the atria. A better characterization of
these changes is desirable for the definition of clinical biomarkers, and thus
there is a need of its fully automatic segmentation from clinical images. In
this work we present an architecture based in 3D-convolution kernels, a
Volumetric Fully Convolution Neural Network (V-FCNN), able to segment the
entire volume in one-shot, and consequently integrate the implicit spatial
redundancy present in high resolution images. A loss function based on the
mixture of both Mean Square Error (MSE) and Dice Loss (DL) is used, in an
attempt to combine the ability to capture the bulk shape and the reduction of
local errors products by over segmentation. Results demonstrate a reasonable
performance in the middle region of the atria, and the impact of the challenges
of capturing the variability of the pulmonary veins or the identification of
the valve plane that separates the atria to the ventricle.
</summary>
    <author>
      <name>Nicoló Savioli</name>
    </author>
    <author>
      <name>Giovanni Montana</name>
    </author>
    <author>
      <name>Pablo Lamata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, In Proceedings of MICCAI 2018 Atrial Segmentation
  Challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01916v1</id>
    <updated>2018-08-06T14:00:40Z</updated>
    <published>2018-08-06T14:00:40Z</published>
    <title>Residual Memory Networks: Feed-forward approach to learn long temporal
  dependencies</title>
    <summary>  Training deep recurrent neural network (RNN) architectures is complicated due
to the increased network complexity. This disrupts the learning of higher order
abstracts using deep RNN. In case of feed-forward networks training deep
structures is simple and faster while learning long-term temporal information
is not possible. In this paper we propose a residual memory neural network
(RMN) architecture to model short-time dependencies using deep feed-forward
layers having residual and time delayed connections. The residual connection
paves way to construct deeper networks by enabling unhindered flow of gradients
and the time delay units capture temporal information with shared weights. The
number of layers in RMN signifies both the hierarchical processing depth and
temporal depth. The computational complexity in training RMN is significantly
less when compared to deep recurrent networks. RMN is further extended as
bi-directional RMN (BRMN) to capture both past and future information.
Experimental analysis is done on AMI corpus to substantiate the capability of
RMN in learning long-term information and hierarchical information. Recognition
performance of RMN trained with 300 hours of Switchboard corpus is compared
with various state-of-the-art LVCSR systems. The results indicate that RMN and
BRMN gains 6 % and 3.8 % relative improvement over LSTM and BLSTM networks.
</summary>
    <author>
      <name>Murali Karthick Baskar</name>
    </author>
    <author>
      <name>Martin Karafiat</name>
    </author>
    <author>
      <name>Lukas Burget</name>
    </author>
    <author>
      <name>Karel Vesely</name>
    </author>
    <author>
      <name>Frantisek Grezl</name>
    </author>
    <author>
      <name>Jan Honza Cernocky</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01857v1</id>
    <updated>2018-08-06T12:54:04Z</updated>
    <published>2018-08-06T12:54:04Z</published>
    <title>Statistical Windows in Testing for the Initial Distribution of a
  Reversible Markov Chain</title>
    <summary>  We study the problem of hypothesis testing between two discrete
distributions, where we only have access to samples after the action of a known
reversible Markov chain, playing the role of noise. We derive
instance-dependent minimax rates for the sample complexity of this problem, and
show how its dependence in time is related to the spectral properties of the
Markov chain. We show that there exists a wide statistical window, in terms of
sample complexity for hypothesis testing between different pairs of initial
distributions. We illustrate these results in several concrete examples.
</summary>
    <author>
      <name>Quentin Berthet</name>
    </author>
    <author>
      <name>Varun Kanade</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62C20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01842v1</id>
    <updated>2018-08-06T12:23:42Z</updated>
    <published>2018-08-06T12:23:42Z</published>
    <title>Beyond $1/2$-Approximation for Submodular Maximization on Massive Data
  Streams</title>
    <summary>  Many tasks in machine learning and data mining, such as data diversification,
non-parametric learning, kernel machines, clustering etc., require extracting a
small but representative summary from a massive dataset. Often, such problems
can be posed as maximizing a submodular set function subject to a cardinality
constraint. We consider this question in the streaming setting, where elements
arrive over time at a fast pace and thus we need to design an efficient,
low-memory algorithm. One such method, proposed by Badanidiyuru et al. (2014),
always finds a $0.5$-approximate solution. Can this approximation factor be
improved? We answer this question affirmatively by designing a new algorithm
SALSA for streaming submodular maximization. It is the first low-memory,
single-pass algorithm that improves the factor $0.5$, under the natural
assumption that elements arrive in a random order. We also show that this
assumption is necessary, i.e., that there is no such algorithm with better than
$0.5$-approximation when elements arrive in arbitrary order. Our experiments
demonstrate that SALSA significantly outperforms the state of the art in
applications related to exemplar-based clustering, social graph analysis, and
recommender systems.
</summary>
    <author>
      <name>Ashkan Norouzi-Fard</name>
    </author>
    <author>
      <name>Jakub Tarnawski</name>
    </author>
    <author>
      <name>Slobodan Mitrović</name>
    </author>
    <author>
      <name>Amir Zandieh</name>
    </author>
    <author>
      <name>Aida Mousavifar</name>
    </author>
    <author>
      <name>Ola Svensson</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10206v3</id>
    <updated>2018-08-06T11:40:15Z</updated>
    <published>2018-06-26T20:39:13Z</published>
    <title>Deep Feature Factorization For Concept Discovery</title>
    <summary>  We propose Deep Feature Factorization (DFF), a method capable of localizing
similar semantic concepts within an image or a set of images. We use DFF to
gain insight into a deep convolutional neural network's learned features, where
we detect hierarchical cluster structures in feature space. This is visualized
as heat maps, which highlight semantically matching regions across a set of
images, revealing what the network `perceives' as similar. DFF can also be used
to perform co-segmentation and co-localization, and we report state-of-the-art
results on these tasks.
</summary>
    <author>
      <name>Edo Collins</name>
    </author>
    <author>
      <name>Radhakrishna Achanta</name>
    </author>
    <author>
      <name>Sabine Süsstrunk</name>
    </author>
    <link href="http://arxiv.org/abs/1806.10206v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10206v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03915v2</id>
    <updated>2018-08-06T11:14:31Z</updated>
    <published>2018-07-11T01:13:13Z</published>
    <title>Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment
  Analysis</title>
    <summary>  Multimodal machine learning is a core research area spanning the language,
visual and acoustic modalities. The central challenge in multimodal learning
involves learning representations that can process and relate information from
multiple modalities. In this paper, we propose two methods for unsupervised
learning of joint multimodal representations using sequence to sequence
(Seq2Seq) methods: a \textit{Seq2Seq Modality Translation Model} and a
\textit{Hierarchical Seq2Seq Modality Translation Model}. We also explore
multiple different variations on the multimodal inputs and outputs of these
seq2seq models. Our experiments on multimodal sentiment analysis using the
CMU-MOSI dataset indicate that our methods learn informative multimodal
representations that outperform the baselines and achieve improved performance
on multimodal sentiment analysis, specifically in the Bimodal case where our
model is able to improve F1 Score by twelve points. We also discuss future
directions for multimodal Seq2Seq methods.
</summary>
    <author>
      <name>Hai Pham</name>
    </author>
    <author>
      <name>Thomas Manzini</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages of content, 11 pages total, 2 figures. Published as a
  workshop paper at ACL 2018, Proceedings of Grand Challenge and Workshop on
  Human Multimodal Language (Challenge-HML). 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03915v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03915v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01813v1</id>
    <updated>2018-08-06T10:34:40Z</updated>
    <published>2018-08-06T10:34:40Z</published>
    <title>Regret Bounds for Reinforcement Learning via Markov Chain Concentration</title>
    <summary>  We give a simple optimistic algorithm for which it is easy to derive regret
bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ after $T$ steps in uniformly
ergodic MDPs with $S$ states, $A$ actions, and mixing time parameter $t_{\rm
mix}$. These bounds are the first regret bounds in the general, non-episodic
setting with an optimal dependence on all given parameters. They could only be
improved by using an alternative mixing time parameter.
</summary>
    <author>
      <name>Ronald Ortner</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11582v2</id>
    <updated>2018-08-06T10:17:55Z</updated>
    <published>2018-07-27T11:10:03Z</published>
    <title>A Hierarchical Approach to Neural Context-Aware Modeling</title>
    <summary>  We present a new recurrent neural network topology to enhance
state-of-the-art machine learning systems by incorporating a broader context.
Our approach overcomes recent limitations with extended narratives through a
multi-layered computational approach to generate an abstract context
representation. Therefore, the developed system captures the narrative on
word-level, sentence-level, and context-level. Through the hierarchical set-up,
our proposed model summarizes the most salient information on each level and
creates an abstract representation of the extended context. We subsequently use
this representation to enhance neural language processing systems on the task
of semantic error detection. To show the potential of the newly introduced
topology, we compare the approach against a context-agnostic set-up including a
standard neural language model and a supervised binary classification network.
The performance measures on the error detection task show the advantage of the
hierarchical context-aware topologies, improving the baseline by 12.75%
relative for unsupervised models and 20.37% relative for supervised models.
</summary>
    <author>
      <name>Patrick Huber</name>
    </author>
    <author>
      <name>Jan Niehues</name>
    </author>
    <author>
      <name>Alex Waibel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11582v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11582v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06823v2</id>
    <updated>2018-08-06T09:48:28Z</updated>
    <published>2018-02-19T19:39:59Z</published>
    <title>Entropy-Isomap: Manifold Learning for High-dimensional Dynamic Processes</title>
    <summary>  Scientific and engineering processes deliver massive high-dimensional data
sets that are generated as non-linear transformations of an initial state and
few process parameters. Mapping such data to a low-dimensional manifold
facilitates better understanding of the underlying processes, and enables their
optimization. In this paper, we first show that off-the-shelf non-linear
spectral dimensionality reduction methods, e.g., Isomap, fail for such data,
primarily due to the presence of strong temporal correlations. Then, we propose
a novel method, Entropy-Isomap, to address the issue. The proposed method is
successfully applied to large data describing a fabrication process of organic
materials. The resulting low-dimensional representation correctly captures
process control variables, allows for low-dimensional visualization of the
material morphology evolution, and provides key insights to improve the
process.
</summary>
    <author>
      <name>Frank Schoeneman</name>
    </author>
    <author>
      <name>Varun Chandola</name>
    </author>
    <author>
      <name>Nils Napp</name>
    </author>
    <author>
      <name>Olga Wodo</name>
    </author>
    <author>
      <name>Jaroslaw Zola</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06823v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06823v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07888v4</id>
    <updated>2018-08-06T08:42:35Z</updated>
    <published>2017-04-25T19:52:52Z</published>
    <title>Stochastic Optimization from Distributed, Streaming Data in Rate-limited
  Networks</title>
    <summary>  Motivated by machine learning applications in networks of sensors,
internet-of-things (IoT) devices, and autonomous agents, we propose techniques
for distributed stochastic convex learning from high-rate data streams. The
setup involves a network of nodes---each one of which has a stream of data
arriving at a constant rate---that solve a stochastic convex optimization
problem by collaborating with each other over rate-limited communication links.
To this end, we present and analyze two algorithms---termed distributed
stochastic approximation mirror descent (D-SAMD) and accelerated distributed
stochastic approximation mirror descent (AD-SAMD)---that are based on two
stochastic variants of mirror descent and in which nodes collaborate via
approximate averaging of the local, noisy subgradients using distributed
consensus. Our main contributions are (i) bounds on the convergence rates of
D-SAMD and AD-SAMD in terms of the number of nodes, network topology, and ratio
of the data streaming and communication rates, and (ii) sufficient conditions
for order-optimum convergence of these algorithms. In particular, we show that
for sufficiently well-connected networks, distributed learning schemes can
obtain order-optimum convergence even if the communications rate is small.
Further we find that the use of accelerated methods significantly enlarges the
regime in which order-optimum convergence is achieved; this is in contrast to
the centralized setting, where accelerated methods usually offer only a modest
improvement. Finally, we demonstrate the effectiveness of the proposed
algorithms using numerical experiments.
</summary>
    <author>
      <name>Matthew Nokleby</name>
    </author>
    <author>
      <name>Waheed U. Bajwa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 6 figures; Accepted for publication in IEEE Transactions on
  Signal and Information Processing over Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07888v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07888v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01753v1</id>
    <updated>2018-08-06T07:26:44Z</updated>
    <published>2018-08-06T07:26:44Z</published>
    <title>Gray-box Adversarial Training</title>
    <summary>  Adversarial samples are perturbed inputs crafted to mislead the machine
learning systems. A training mechanism, called adversarial training, which
presents adversarial samples along with clean samples has been introduced to
learn robust models. In order to scale adversarial training for large datasets,
these perturbations can only be crafted using fast and simple methods (e.g.,
gradient ascent). However, it is shown that adversarial training converges to a
degenerate minimum, where the model appears to be robust by generating weaker
adversaries. As a result, the models are vulnerable to simple black-box
attacks. In this paper we, (i) demonstrate the shortcomings of existing
evaluation policy, (ii) introduce novel variants of white-box and black-box
attacks, dubbed gray-box adversarial attacks" based on which we propose novel
evaluation method to assess the robustness of the learned models, and (iii)
propose a novel variant of adversarial training, named Graybox Adversarial
Training" that uses intermediate versions of the models to seed the
adversaries. Experimental evaluation demonstrates that the models trained using
our method exhibit better robustness compared to both undefended and
adversarially trained model
</summary>
    <author>
      <name>Vivek B. S.</name>
    </author>
    <author>
      <name>Konda Reddy Mopuri</name>
    </author>
    <author>
      <name>R. Venkatesh Babu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01743v1</id>
    <updated>2018-08-06T06:28:35Z</updated>
    <published>2018-08-06T06:28:35Z</published>
    <title>NIMFA: A Python Library for Nonnegative Matrix Factorization</title>
    <summary>  NIMFA is an open-source Python library that provides a unified interface to
nonnegative matrix factorization algorithms. It includes implementations of
state-of-the-art factorization methods, initialization approaches, and quality
scoring. It supports both dense and sparse matrix representation. NIMFA's
component-based implementation and hierarchical design should help the users to
employ already implemented techniques or design and code new strategies for
matrix factorization tasks.
</summary>
    <author>
      <name>Marinka Zitnik</name>
    </author>
    <author>
      <name>Blaz Zupan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 13 (2012) 849-853</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.01743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01739v1</id>
    <updated>2018-08-06T06:02:15Z</updated>
    <published>2018-08-06T06:02:15Z</published>
    <title>Concentration bounds for empirical conditional value-at-risk: The
  unbounded case</title>
    <summary>  In several real-world applications involving decision making under
uncertainty, the traditional expected value objective may not be suitable, as
it may be necessary to control losses in the case of a rare but extreme event.
Conditional Value-at-Risk (CVaR) is a popular risk measure for modeling the
aforementioned objective. We consider the problem of estimating CVaR from
i.i.d. samples of an unbounded random variable, which is either sub-Gaussian or
sub-exponential. We derive a novel one-sided concentration bound for a natural
sample-based CVaR estimator in this setting. Our bound relies on a
concentration result for a quantile-based estimator for Value-at-Risk (VaR),
which may be of independent interest.
</summary>
    <author>
      <name>Ravi Kumar Kolla</name>
    </author>
    <author>
      <name>Prashanth L. A.</name>
    </author>
    <author>
      <name>Sanjay P. Bhat</name>
    </author>
    <author>
      <name>Krishna Jagannathan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11414v2</id>
    <updated>2018-08-06T01:41:19Z</updated>
    <published>2018-07-30T16:06:26Z</published>
    <title>Faster Convergence &amp; Generalization in DNNs</title>
    <summary>  Deep neural networks have gained tremendous popularity in last few years.
They have been applied for the task of classification in almost every domain.
Despite the success, deep networks can be incredibly slow to train for even
moderate sized models on sufficiently large datasets. Additionally, these
networks require large amounts of data to be able to generalize. The importance
of speeding up convergence, and generalization in deep networks can not be
overstated. In this work, we develop an optimization algorithm based on
generalized-optimal updates derived from minibatches that lead to faster
convergence. Towards the end, we demonstrate on two benchmark datasets that the
proposed method achieves two orders of magnitude speed up over traditional
back-propagation, and is more robust to noise/over-fitting.
</summary>
    <author>
      <name>Gaurav Singh</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11414v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11414v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03679v1</id>
    <updated>2018-08-06T01:15:51Z</updated>
    <published>2018-08-06T01:15:51Z</published>
    <title>Machine Learning Promoting Extreme Simplification of Spectroscopy
  Equipment</title>
    <summary>  The spectroscopy measurement is one of main pathways for exploring and
understanding the nature. Today, it seems that racing artificial intelligence
will remould its styles. The algorithms contained in huge neural networks are
capable of substituting many of expensive and complex components of spectrum
instruments. In this work, we presented a smart machine learning strategy on
the measurement of absorbance curves, and also initially verified that an
exceedingly-simplified equipment is sufficient to meet the needs for this
strategy. Further, with its simplicity, the setup is expected to infiltrate
into many scientific areas in versatile forms.
</summary>
    <author>
      <name>Jianchao Lee</name>
    </author>
    <author>
      <name>Qiannan Duan</name>
    </author>
    <author>
      <name>Sifan Bi</name>
    </author>
    <author>
      <name>Ruen Luo</name>
    </author>
    <author>
      <name>Yachao Lian</name>
    </author>
    <author>
      <name>Hanqiang Liu</name>
    </author>
    <author>
      <name>Ruixing Tian</name>
    </author>
    <author>
      <name>Jiayuan Chen</name>
    </author>
    <author>
      <name>Guodong Ma</name>
    </author>
    <author>
      <name>Jinhong Gao</name>
    </author>
    <author>
      <name>Zhaoyi Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06167v5</id>
    <updated>2018-08-06T00:22:53Z</updated>
    <published>2018-02-17T01:04:53Z</published>
    <title>CapsuleGAN: Generative Adversarial Capsule Network</title>
    <summary>  We present Generative Adversarial Capsule Network (CapsuleGAN), a framework
that uses capsule networks (CapsNets) instead of the standard convolutional
neural networks (CNNs) as discriminators within the generative adversarial
network (GAN) setting, while modeling image data. We provide guidelines for
designing CapsNet discriminators and the updated GAN objective function, which
incorporates the CapsNet margin loss, for training CapsuleGAN models. We show
that CapsuleGAN outperforms convolutional-GAN at modeling image data
distribution on MNIST and CIFAR-10 datasets, evaluated on the generative
adversarial metric and at semi-supervised image classification.
</summary>
    <author>
      <name>Ayush Jaiswal</name>
    </author>
    <author>
      <name>Wael AbdAlmageed</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Premkumar Natarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV Workshop on Brain Driven Computer Vision (BDCV) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06167v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06167v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01687v1</id>
    <updated>2018-08-05T21:28:10Z</updated>
    <published>2018-08-05T21:28:10Z</published>
    <title>Hybrid Subspace Learning for High-Dimensional Data</title>
    <summary>  The high-dimensional data setting, in which p &gt;&gt; n, is a challenging
statistical paradigm that appears in many real-world problems. In this setting,
learning a compact, low-dimensional representation of the data can
substantially help distinguish signal from noise. One way to achieve this goal
is to perform subspace learning to estimate a small set of latent features that
capture the majority of the variance in the original data. Most existing
subspace learning models, such as PCA, assume that the data can be fully
represented by its embedding in one or more latent subspaces. However, in this
work, we argue that this assumption is not suitable for many high-dimensional
datasets; often only some variables can easily be projected to a
low-dimensional space. We propose a hybrid dimensionality reduction technique
in which some features are mapped to a low-dimensional subspace while others
remain in the original space. Our model leads to more accurate estimation of
the latent space and lower reconstruction error. We present a simple
optimization procedure for the resulting biconvex problem and show synthetic
data results that demonstrate the advantages of our approach over existing
methods. Finally, we demonstrate the effectiveness of this method for
extracting meaningful features from both gene expression and video background
subtraction datasets.
</summary>
    <author>
      <name>Micol Marchetti-Bowick</name>
    </author>
    <author>
      <name>Benjamin J. Lengerich</name>
    </author>
    <author>
      <name>Ankur P. Parikh</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06919v2</id>
    <updated>2018-08-05T21:09:36Z</updated>
    <published>2018-07-18T13:28:59Z</published>
    <title>Backplay: "Man muss immer umkehren"</title>
    <summary>  A long-standing problem in model free reinforcement learning (RL) is that it
requires a large number of trials to learn a good policy, especially in
environments with sparse rewards. We explore a method to increase the sample
efficiency of RL when we have access to demonstrations. Our approach, which we
call Backplay, uses a single demonstration to construct a curriculum for a
given task. Rather than starting each training episode in the environment's
fixed initial state, we start the agent near the end of the demonstration and
move the starting point backwards during the course of training until we reach
the initial state. We perform experiments in a competitive four player game
(Pommerman) and a path-finding maze game. We find that this weak form of
guidance provides significant gains in sample complexity with a stark advantage
in sparse reward environments. In some cases, standard RL did not yield any
improvement while Backplay reached success rates greater than 50% and
generalized to unseen initial conditions in the same amount of training time.
Additionally, we see that agents trained via Backplay can learn policies
superior to those of the original demonstration.
</summary>
    <author>
      <name>Cinjon Resnick</name>
    </author>
    <author>
      <name>Roberta Raileanu</name>
    </author>
    <author>
      <name>Sanyam Kapoor</name>
    </author>
    <author>
      <name>Alex Peysakhovich</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06919v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06919v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01684v1</id>
    <updated>2018-08-05T21:06:05Z</updated>
    <published>2018-08-05T21:06:05Z</published>
    <title>Missing Value Imputation Based on Deep Generative Models</title>
    <summary>  Missing values widely exist in many real-world datasets, which hinders the
performing of advanced data analytics. Properly filling these missing values is
crucial but challenging, especially when the missing rate is high. Many
approaches have been proposed for missing value imputation (MVI), but they are
mostly heuristics-based, lacking a principled foundation and do not perform
satisfactorily in practice. In this paper, we propose a probabilistic framework
based on deep generative models for MVI. Under this framework, imputing the
missing entries amounts to seeking a fixed-point solution between two
conditional distributions defined on the missing entries and latent variables
respectively. These distributions are parameterized by deep neural networks
(DNNs) which possess high approximation power and can capture the nonlinear
relationships between missing entries and the observed values. The learning of
weight parameters of DNNs is performed by maximizing an approximation of the
log-likelihood of observed values. We conducted extensive evaluation on 13
datasets and compared with 11 baselines methods, where our methods largely
outperforms the baselines.
</summary>
    <author>
      <name>Hongbao Zhang</name>
    </author>
    <author>
      <name>Pengtao Xie</name>
    </author>
    <author>
      <name>Eric Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01664v1</id>
    <updated>2018-08-05T18:06:37Z</updated>
    <published>2018-08-05T18:06:37Z</published>
    <title>Structured Adversarial Attack: Towards General Implementation and Better
  Interpretability</title>
    <summary>  When generating adversarial examples to attack deep neural networks (DNNs),
$\ell_p$ norm of the added perturbation is usually used to measure the
similarity between original image and adversarial example. However, such
adversarial attacks may fail to capture key infomation hidden in the input.
This work develops a more general attack model i.e., the structured attack that
explores group sparsity in adversarial perturbations by sliding a mask through
images aiming for extracting key structures. An ADMM (alternating direction
method of multipliers)-based framework is proposed that can split the original
problem into a sequence of analytically solvable subproblems and can be
generalized to implement other state-of-the-art attacks. Strong group sparsity
is achieved in adversarial perturbations even with the same level of distortion
in terms of $\ell_p$ norm as the state-of-the-art attacks. Extensive
experimental results on MNIST, CIFAR-10 and ImageNet show that our attack could
be much stronger (in terms of smaller $\ell_0$ distortion) than the existing
ones, and its better interpretability from group sparse structures aids in
uncovering the origins of adversarial examples.
</summary>
    <author>
      <name>Kaidi Xu</name>
    </author>
    <author>
      <name>Sijia Liu</name>
    </author>
    <author>
      <name>Pu Zhao</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Huan Zhang</name>
    </author>
    <author>
      <name>Deniz Erdogmus</name>
    </author>
    <author>
      <name>Yanzhi Wang</name>
    </author>
    <author>
      <name>Xue Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01642v1</id>
    <updated>2018-08-05T16:19:56Z</updated>
    <published>2018-08-05T16:19:56Z</published>
    <title>Multi-Objective Cognitive Model: a supervised approach for multi-subject
  fMRI analysis</title>
    <summary>  In order to decode the human brain, Multivariate Pattern (MVP) classification
generates cognitive models by using functional Magnetic Resonance Imaging
(fMRI) datasets. As a standard pipeline in the MVP analysis, brain patterns in
multi-subject fMRI dataset must be mapped to a shared space and then a
classification model is generated by employing the mapped patterns. However,
the MVP models may not provide stable performance on a new fMRI dataset because
the standard pipeline uses disjoint steps for generating these models. Indeed,
each step in the pipeline includes an objective function with independent
optimization approach, where the best solution of each step may not be optimum
for the next steps. For tackling the mentioned issue, this paper introduces the
Multi-Objective Cognitive Model (MOCM) that utilizes an integrated objective
function for MVP analysis rather than just using those disjoint steps. For
solving the integrated problem, we proposed a customized multi-objective
optimization approach, where all possible solutions are firstly generated, and
then our method ranks and selects the robust solutions as the final results.
Empirical studies confirm that the proposed method can generate superior
performance in comparison with other techniques.
</summary>
    <author>
      <name>Muhammad Yousefnezhad</name>
    </author>
    <author>
      <name>Daoqiang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroinformatics, Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00701v3</id>
    <updated>2018-08-05T15:49:20Z</updated>
    <published>2018-06-02T20:31:30Z</published>
    <title>On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional
  Neural Networks</title>
    <summary>  Parsimonious representations in data modeling are ubiquitous and central for
processing information. Motivated by the recent Multi-Layer Convolutional
Sparse Coding (ML-CSC) model, we herein generalize the traditional Basis
Pursuit regression problem to a multi-layer setting, introducing similar sparse
enforcing penalties at different representation layers in a symbiotic relation
between synthesis and analysis sparse priors. We propose and analyze different
iterative algorithms to solve this new problem in practice. We prove that the
presented multi-layer Iterative Soft Thresholding (ML-ISTA) and multi-layer
Fast ISTA (ML-FISTA) converge to the global optimum of our multi-layer
formulation at a rate of $\mathcal{O}(1/k)$ and $\mathcal{O}(1/k^2)$,
respectively and independently of the number of layers. We further show how
these algorithms effectively implement particular recurrent neural networks
that generalize feed-forward architectures without any increase in the number
of parameters. We present different architectures that result from unfolding
the iterations of the proposed multi-layer pursuit algorithms, providing a
principled way to construct deep recurrent CNNs from feed-forward ones. We
demonstrate the emerging constructions by training them in an end-to-end
manner, consistently improving the performance of classical networks without
introducing extra filters or parameters.
</summary>
    <author>
      <name>Jeremias Sulam</name>
    </author>
    <author>
      <name>Aviad Aberdam</name>
    </author>
    <author>
      <name>Michael Elad</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00701v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00701v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01614v1</id>
    <updated>2018-08-05T13:40:22Z</updated>
    <published>2018-08-05T13:40:22Z</published>
    <title>Using Machine Learning Safely in Automotive Software: An Assessment and
  Adaption of Software Process Requirements in ISO 26262</title>
    <summary>  The use of machine learning (ML) is on the rise in many sectors of software
development, and automotive software development is no different. In
particular, Advanced Driver Assistance Systems (ADAS) and Automated Driving
Systems (ADS) are two areas where ML plays a significant role. In automotive
development, safety is a critical objective, and the emergence of standards
such as ISO 26262 has helped focus industry practices to address safety in a
systematic and consistent way. Unfortunately, these standards were not designed
to accommodate technologies such as ML or the type of functionality that is
provided by an ADS and this has created a conflict between the need to innovate
and the need to improve safety. In this report, we take steps to address this
conflict by doing a detailed assessment and adaption of ISO 26262 for ML,
specifically in the context of supervised learning. First we analyze the key
factors that are the source of the conflict. Then we assess each software
development process requirement (Part 6 of ISO 26262) for applicability to ML.
Where there are gaps, we propose new requirements to address the gaps. Finally
we discuss the application of this adapted and extended variant of Part 6 to ML
development scenarios.
</summary>
    <author>
      <name>Rick Salay</name>
    </author>
    <author>
      <name>Krzysztof Czarnecki</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03154v3</id>
    <updated>2018-08-05T10:05:13Z</updated>
    <published>2018-04-09T18:00:08Z</published>
    <title>Cauchy noise loss for stochastic optimization of random matrix models
  via free deterministic equivalents</title>
    <summary>  For random matrix models, the parameter estimation based on the traditional
likelihood is not straightforward in particular when there is only one sample
matrix. We introduce a new parameter optimization method of random matrix
models which works even in such a case not based on the traditional likelihood,
instead based on the spectral distribution. We use the spectral distribution
perturbed by Cauchy noises because the free deterministic equivalent, which is
a tool in free probability theory, allows us to approximate it by a smooth and
accessible density function.
  Moreover, we study an asymptotic property of a determination gap, which has a
similar role as the generalization gap. In addition, we propose a new
dimensionality recovery method for the signal-plus-noise model, and
experimentally demonstrate that it recovers the rank of the signal part even if
the rank is not low. It is a simultaneous rank selection and parameter
estimation procedure.
</summary>
    <author>
      <name>Tomohiro Hayase</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 13 figures, v3: minor correction. Submitted. Our simulation
  code is available at https://github.com/ThayaFluss/cnl</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03154v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03154v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04326v3</id>
    <updated>2018-08-05T07:53:09Z</updated>
    <published>2018-06-12T04:21:53Z</published>
    <title>Differentiable Compositional Kernel Learning for Gaussian Processes</title>
    <summary>  The generalization properties of Gaussian processes depend heavily on the
choice of kernel, and this choice remains a dark art. We present the Neural
Kernel Network (NKN), a flexible family of kernels represented by a neural
network. The NKN architecture is based on the composition rules for kernels, so
that each unit of the network corresponds to a valid kernel. It can compactly
approximate compositional kernel structures such as those used by the Automatic
Statistician (Lloyd et al., 2014), but because the architecture is
differentiable, it is end-to-end trainable with gradient-based optimization. We
show that the NKN is universal for the class of stationary kernels. Empirically
we demonstrate pattern discovery and extrapolation abilities of NKN on several
tasks that depend crucially on identifying the underlying structure, including
time series and texture extrapolation, as well as Bayesian optimization.
</summary>
    <author>
      <name>Shengyang Sun</name>
    </author>
    <author>
      <name>Guodong Zhang</name>
    </author>
    <author>
      <name>Chaoqi Wang</name>
    </author>
    <author>
      <name>Wenyuan Zeng</name>
    </author>
    <author>
      <name>Jiaman Li</name>
    </author>
    <author>
      <name>Roger Grosse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018; update proof</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01574v1</id>
    <updated>2018-08-05T07:45:06Z</updated>
    <published>2018-08-05T07:45:06Z</published>
    <title>Autoencoder Based Sample Selection for Self-Taught Learning</title>
    <summary>  Self-taught learning is a technique that uses a large number of unlabeled
data as source samples to improve the task performance on target samples.
Compared with other transfer learning techniques, self-taught learning can be
applied to a broader set of scenarios due to the loose restrictions on source
data. However, knowledge transferred from source samples that are not
sufficiently related to the target domain may negatively influence the target
learner, which is referred to as negative transfer. In this paper, we propose a
metric for the relevance between a source sample and target samples. To be more
specific, both source and target samples are reconstructed through a
single-layer autoencoder with a linear relationship between source samples and
target samples simultaneously enforced. An l_{2,1}-norm sparsity constraint is
imposed on the transformation matrix to identify source samples relevant to the
target domain. Source domain samples that are deemed relevant are assigned
pseudo-labels reflecting their relevance to target domain samples, and are
combined with target samples in order to provide an expanded training set for
classifier training. Local data structures are also preserved during source
sample selection through spectral graph analysis. Promising results in
extensive experiments show the advantages of the proposed approach.
</summary>
    <author>
      <name>Siwei Feng</name>
    </author>
    <author>
      <name>Marco F. Duarte</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01535v1</id>
    <updated>2018-08-04T21:10:03Z</updated>
    <published>2018-08-04T21:10:03Z</published>
    <title>Triplet Network with Attention for Speaker Diarization</title>
    <summary>  In automatic speech processing systems, speaker diarization is a crucial
front-end component to separate segments from different speakers. Inspired by
the recent success of deep neural networks (DNNs) in semantic inferencing,
triplet loss-based architectures have been successfully used for this problem.
However, existing work utilizes conventional i-vectors as the input
representation and builds simple fully connected networks for metric learning,
thus not fully leveraging the modeling power of DNN architectures. This paper
investigates the importance of learning effective representations from the
sequences directly in metric learning pipelines for speaker diarization. More
specifically, we propose to employ attention models to learn embeddings and the
metric jointly in an end-to-end fashion. Experiments are conducted on the
CALLHOME conversational speech corpus. The diarization results demonstrate
that, besides providing a unified model, the proposed approach achieves
improved performance when compared against existing approaches.
</summary>
    <author>
      <name>Huan Song</name>
    </author>
    <author>
      <name>Megan Willi</name>
    </author>
    <author>
      <name>Jayaraman J. Thiagarajan</name>
    </author>
    <author>
      <name>Visar Berisha</name>
    </author>
    <author>
      <name>Andreas Spanias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01531v1</id>
    <updated>2018-08-04T20:47:44Z</updated>
    <published>2018-08-04T20:47:44Z</published>
    <title>Global Convergence to the Equilibrium of GANs using Variational
  Inequalities</title>
    <summary>  In optimization, the negative gradient of a function denotes the direction of
steepest descent. Furthermore, traveling in any direction orthogonal to the
gradient maintains the value of the function. In this work, we show that these
orthogonal directions that are ignored by gradient descent can be critical in
equilibrium problems. Equilibrium problems have drawn heightened attention in
machine learning due to the emergence of the Generative Adversarial Network
(GAN). We use the framework of Variational Inequalities to analyze popular
training algorithms for a fundamental GAN variant: the Wasserstein
Linear-Quadratic GAN. We show that the steepest descent direction causes
divergence from the equilibrium, and guaranteed convergence to the equilibrium
is achieved through following a particular orthogonal direction. We call this
successful technique Crossing-the-Curl, named for its mathematical derivation
as well as its intuition: identify the game's axis of rotation and move
"across" space in the direction towards smaller "curling".
</summary>
    <author>
      <name>Ian Gemp</name>
    </author>
    <author>
      <name>Sridhar Mahadevan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07756v4</id>
    <updated>2018-08-04T20:38:14Z</updated>
    <published>2018-02-21T19:09:17Z</published>
    <title>Determining the best classifier for predicting the value of a boolean
  field on a blood donor database using genetic algorithms</title>
    <summary>  Motivation: Thanks to digitization, we often have access to large databases,
consisting of various fields of information, ranging from numbers to texts and
even boolean values. Such databases lend themselves especially well to machine
learning, classification and big data analysis tasks. We are able to train
classifiers, using already existing data and use them for predicting the values
of a certain field, given that we have information regarding the other fields.
Most specifically, in this study, we look at the Electronic Health Records
(EHRs) that are compiled by hospitals. These EHRs are convenient means of
accessing data of individual patients, but there processing as a whole still
remains a task. However, EHRs that are composed of coherent, well-tabulated
structures lend themselves quite well to the application to machine language,
via the usage of classifiers. In this study, we look at a Blood Transfusion
Service Center Data Set (Data taken from the Blood Transfusion Service Center
in Hsin-Chu City in Taiwan). We used scikit-learn machine learning in python.
From Support Vector Machines(SVM), we use Support Vector Classification(SVC),
from the linear model we import Perceptron. We also used the
K.neighborsclassifier and the decision tree classifiers. Furthermore, we use
the TPOT library to find an optimized pipeline using genetic algorithms. Using
the above classifiers, we score each one of them using k fold cross-validation.
  Contact: ritabratamaiti@hiretrex.com GitHub Repository:
https://github.com/ritabratamaiti/Blooddonorprediction
</summary>
    <author>
      <name>Ritabrata Maiti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1336304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1336304" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GitHub Repository here:
  https://github.com/ritabratamaiti/Blooddonorprediction</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07756v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07756v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01527v1</id>
    <updated>2018-08-04T20:13:35Z</updated>
    <published>2018-08-04T20:13:35Z</published>
    <title>Deep Reinforcement One-Shot Learning for Artificially Intelligent
  Classification Systems</title>
    <summary>  In recent years there has been a sharp rise in networking applications, in
which significant events need to be classified but only a few training
instances are available. These are known as cases of one-shot learning.
Examples include analyzing network traffic under zero-day attacks, and computer
vision tasks by sensor networks deployed in the field. To handle this
challenging task, organizations often use human analysts to classify events
under high uncertainty. Existing algorithms use a threshold-based mechanism to
decide whether to classify an object automatically or send it to an analyst for
deeper inspection. However, this approach leads to a significant waste of
resources since it does not take the practical temporal constraints of system
resources into account. Our contribution is threefold. First, we develop a
novel Deep Reinforcement One-shot Learning (DeROL) framework to address this
challenge. The basic idea of the DeROL algorithm is to train a deep-Q network
to obtain a policy which is oblivious to the unseen classes in the testing
data. Then, in real-time, DeROL maps the current state of the one-shot learning
process to operational actions based on the trained deep-Q network, to maximize
the objective function. Second, we develop the first open-source software for
practical artificially intelligent one-shot classification systems with limited
resources for the benefit of researchers in related fields. Third, we present
an extensive experimental study using the OMNIGLOT dataset for computer vision
tasks and the UNSW-NB15 dataset for intrusion detection tasks that demonstrates
the versatility and efficiency of the DeROL framework.
</summary>
    <author>
      <name>Anton Puzanov</name>
    </author>
    <author>
      <name>Kobi Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01524v1</id>
    <updated>2018-08-04T19:34:39Z</updated>
    <published>2018-08-04T19:34:39Z</published>
    <title>Learning disentangled representation from 12-lead electrograms:
  application in localizing the origin of Ventricular Tachycardia</title>
    <summary>  The increasing availability of electrocardiogram (ECG) data has motivated the
use of data-driven models for automating various clinical tasks based on ECG
data. The development of subject-specific models are limited by the cost and
difficulty of obtaining sufficient training data for each individual. The
alternative of population model, however, faces challenges caused by the
significant inter-subject variations within the ECG data. We address this
challenge by investigating for the first time the problem of learning
representations for clinically-informative variables while disentangling other
factors of variations within the ECG data. In this work, we present a
conditional variational autoencoder (VAE) to extract the subject-specific
adjustment to the ECG data, conditioned on task-specific representations
learned from a deterministic encoder. To encourage the representation for
inter-subject variations to be independent from the task-specific
representation, maximum mean discrepancy is used to match all the moments
between the distributions learned by the VAE conditioning on the code from the
deterministic encoder. The learning of the task-specific representation is
regularized by a weak supervision in the form of contrastive regularization. We
apply the proposed method to a novel yet important clinical task of classifying
the origin of ventricular tachycardia (VT) into pre-defined segments,
demonstrating the efficacy of the proposed method against the standard VAE.
</summary>
    <author>
      <name>Prashnna K Gyawali</name>
    </author>
    <author>
      <name>B. Milan Horacek</name>
    </author>
    <author>
      <name>John L. Sapp</name>
    </author>
    <author>
      <name>Linwei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01517v1</id>
    <updated>2018-08-04T18:26:24Z</updated>
    <published>2018-08-04T18:26:24Z</published>
    <title>DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging</title>
    <summary>  DELIMIT is a framework extension for deep learning in diffusion imaging,
which extends the basic framework PyTorch towards spherical signals. Based on
several novel layers, deep learning can be applied to spherical diffusion
imaging data in a very convenient way. First, two spherical harmonic
interpolation layers are added to the extension, which allow to transform the
signal from spherical surface space into the spherical harmonic space, and vice
versa. In addition, a local spherical convolution layer is introduced that adds
the possibility to include gradient neighborhood information within the
network. Furthermore, these extensions can also be utilized for the
preprocessing of diffusion signals.
</summary>
    <author>
      <name>Simon Koppers</name>
    </author>
    <author>
      <name>Dorit Merhof</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02017v1</id>
    <updated>2018-08-04T17:50:41Z</updated>
    <published>2018-08-04T17:50:41Z</published>
    <title>Withholding aggressive treatments may not accelerate time to death among
  dying ICU patients</title>
    <summary>  Critically ill patients may die despite aggressive treatment. In this study,
we examine trends in the application of two such treatments over a decade, as
well as the impact of these trends on survival durations in patients who die
within a month of ICU admission. We considered observational data available
from the MIMIC-III open-access ICU database, collected from June 2001 to
October 2012: These data comprise almost 60,000 hospital admissions for a total
of 38,645 unique adults.
  We explored two hypotheses: (i) administration of aggressive treatment during
the ICU stay immediately preceding end-of-life would decrease over the study
time period and (ii) time-to-death from ICU admission would also decrease due
to the decrease in aggressive treatment administration. Tests for significant
trends were performed and a p-value threshold of 0.05 was used to assess
statistical significance. We found that aggressive treatments in this
population were employed with decreasing frequency over the study period
duration, and also that reducing aggressive treatments for such patients may
not result in shorter times to death. The latter finding has implications for
end of life discussions that involve the possible use or non-use of such
treatments in those patients with very poor prognosis.
</summary>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <author>
      <name>Peter Clardy</name>
    </author>
    <author>
      <name>Leo Anthony Celi</name>
    </author>
    <author>
      <name>David J. Stone</name>
    </author>
    <author>
      <name>Robert S. Rudin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04876v2</id>
    <updated>2018-08-04T15:16:31Z</updated>
    <published>2018-02-13T22:15:10Z</published>
    <title>Uncertainty Quantification for Online Learning and Stochastic
  Approximation via Hierarchical Incremental Gradient Descent</title>
    <summary>  Stochastic gradient descent (SGD) is an immensely popular approach for online
learning in settings where data arrives in a stream or data sizes are very
large. However, despite an ever- increasing volume of work on SGD, much less is
known about the statistical inferential properties of SGD-based predictions.
Taking a fully inferential viewpoint, this paper introduces a novel procedure
termed HiGrad to conduct statistical inference for online learning, without
incurring additional computational cost compared with SGD. The HiGrad procedure
begins by performing SGD updates for a while and then splits the single thread
into several threads, and this procedure hierarchically operates in this
fashion along each thread. With predictions provided by multiple threads in
place, a t-based confidence interval is constructed by decorrelating
predictions using covariance structures given by a Donsker-style extension of
the Ruppert--Polyak averaging scheme, which is a technical contribution of
independent interest. Under certain regularity conditions, the HiGrad
confidence interval is shown to attain asymptotically exact coverage
probability. Finally, the performance of HiGrad is evaluated through extensive
simulation studies and a real data example. An R package higrad has been
developed to implement the method.
</summary>
    <author>
      <name>Weijie J. Su</name>
    </author>
    <author>
      <name>Yuancheng Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Changed the title and polished writing. For more details, please
  visit the HiGrad webpage http://stat.wharton.upenn.edu/~suw/higrad</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.04876v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04876v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00089v2</id>
    <updated>2018-08-04T13:37:51Z</updated>
    <published>2018-04-30T20:34:16Z</published>
    <title>Concolic Testing for Deep Neural Networks</title>
    <summary>  Concolic testing combines program execution and symbolic analysis to explore
the execution paths of a software program. This paper presents the first
concolic testing approach for Deep Neural Networks (DNNs). More specifically,
we formalise coverage criteria for DNNs that have been studied in the
literature, and then develop a coherent method for performing concolic testing
to increase test coverage. Our experimental results show the effectiveness of
the concolic testing approach in both achieving high coverage and finding
adversarial examples.
</summary>
    <author>
      <name>Youcheng Sun</name>
    </author>
    <author>
      <name>Min Wu</name>
    </author>
    <author>
      <name>Wenjie Ruan</name>
    </author>
    <author>
      <name>Xiaowei Huang</name>
    </author>
    <author>
      <name>Marta Kwiatkowska</name>
    </author>
    <author>
      <name>Daniel Kroening</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00089v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00089v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03258v1</id>
    <updated>2018-08-04T13:37:51Z</updated>
    <published>2018-08-04T13:37:51Z</published>
    <title>Application of Bounded Total Variation Denoising in Urban Traffic
  Analysis</title>
    <summary>  While it is believed that denoising is not always necessary in many big data
applications, we show in this paper that denoising is helpful in urban traffic
analysis by applying the method of bounded total variation denoising to the
urban road traffic prediction and clustering problem. We propose two
easy-to-implement methods to estimate the noise strength parameter in the
denoising algorithm, and apply the denoising algorithm to GPS-based traffic
data from Beijing taxi system. For the traffic prediction problem, we combine
neural network and history matching method for roads randomly chosen from an
urban area of Beijing. Numerical experiments show that the predicting accuracy
is improved significantly by applying the proposed bounded total variation
denoising algorithm. We also test the algorithm on clustering problem, where a
recently developed clustering analysis method is applied to more than one
hundred urban road segments in Beijing based on their velocity profiles. Better
clustering result is obtained after denoising.
</summary>
    <author>
      <name>Shanshan Tang</name>
    </author>
    <author>
      <name>Haijun Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02771v4</id>
    <updated>2018-08-04T09:19:51Z</updated>
    <published>2017-04-10T09:20:47Z</published>
    <title>Group Importance Sampling for Particle Filtering and MCMC</title>
    <summary>  Bayesian methods and their implementations by means of sophisticated Monte
Carlo techniques have become very popular in signal processing over the last
years. Importance Sampling (IS) is a well-known Monte Carlo technique that
approximates integrals involving a posterior distribution by means of weighted
samples. In this work, we study the assignation of a single weighted sample
which compresses the information contained in a population of weighted samples.
Part of the theory that we present as Group Importance Sampling (GIS) has been
employed implicitly in different works in the literature. The provided analysis
yields several theoretical and practical consequences. For instance, we discuss
the application of GIS into the Sequential Importance Resampling framework and
show that Independent Multiple Try Metropolis schemes can be interpreted as a
standard Metropolis-Hastings algorithm, following the GIS approach. We also
introduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.
The first one, named Group Metropolis Sampling method, produces a Markov chain
of sets of weighted samples. All these sets are then employed for obtaining a
unique global estimator. The second one is the Distributed Particle
Metropolis-Hastings technique, where different parallel particle filters are
jointly used to drive an MCMC algorithm. Different resampled trajectories are
compared and then tested with a proper acceptance probability. The novel
schemes are tested in different numerical experiments such as learning the
hyperparameters of Gaussian Processes, two localization problems in a wireless
sensor network (with synthetic and real data) and the tracking of vegetation
parameters given satellite observations, where they are compared with several
benchmark Monte Carlo techniques. Three illustrative Matlab demos are also
provided.
</summary>
    <author>
      <name>L. Martino</name>
    </author>
    <author>
      <name>V. Elvira</name>
    </author>
    <author>
      <name>G. Camps-Valls</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Digital Signal Processing. Related Matlab demos are
  provided at https://github.com/lukafree/GIS.git</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02771v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02771v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03708v2</id>
    <updated>2018-08-04T05:56:19Z</updated>
    <published>2018-07-10T15:24:36Z</published>
    <title>Generalized deterministic policy gradient algorithms</title>
    <summary>  We study a setting of reinforcement learning, where the state transition is a
convex combination of a stochastic continuous function and a deterministic
function. Such a setting include as a special case the stochastic state
transition setting, namely the setting of deterministic policy gradient (DPG).
We firstly give a simple example to illustrate that the deterministic policy
gradient may not exist under deterministic state transitions, and introduce a
theoretical technique to prove the existence of the policy gradient in this
generalized setting. Using this technique, we prove that the deterministic
policy gradient indeed exists for a certain set of discount factors, and
further prove two conditions that guarantee the existence for all discount
factors. We then derive a closed form of the policy gradient whenever exists.
Interestingly, the form of the policy gradient in such setting is equivalent to
that in DPG. Furthermore, to overcome the challenge of high sample complexity
of DPG in this setting, we propose the Generalized Deterministic Policy
Gradient (GDPG) algorithm. The main innovation of the algorithm is to optimize
a weighted objective of the original Markov decision process (MDP) and an
augmented MDP that simplifies the original one, and serves as its lower bound.
To solve the augmented MDP, we make use of the model-based methods which enable
fast convergence. We finally conduct extensive experiments comparing GDPG with
state-of-the-art methods on several standard benchmarks. Results demonstrate
that GDPG substantially outperforms other baselines in terms of both
convergence and long-term rewards.
</summary>
    <author>
      <name>Qingpeng Cai</name>
    </author>
    <author>
      <name>Ling Pan</name>
    </author>
    <author>
      <name>Pingzhong Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03708v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03708v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01410v1</id>
    <updated>2018-08-04T02:21:07Z</updated>
    <published>2018-08-04T02:21:07Z</published>
    <title>Predicting Expressive Speaking Style From Text In End-To-End Speech
  Synthesis</title>
    <summary>  Global Style Tokens (GSTs) are a recently-proposed method to learn latent
disentangled representations of high-dimensional data. GSTs can be used within
Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to
uncover expressive factors of variation in speaking style. In this work, we
introduce the Text-Predicted Global Style Token (TP-GST) architecture, which
treats GST combination weights or style embeddings as "virtual" speaking style
labels within Tacotron. TP-GST learns to predict stylistic renderings from text
alone, requiring neither explicit labels during training nor auxiliary inputs
for inference. We show that, when trained on a dataset of expressive speech,
our system generates audio with more pitch and energy variation than two
state-of-the-art baseline models. We further demonstrate that TP-GSTs can
synthesize speech with background noise removed, and corroborate these analyses
with positive results on human-rated listener preference audiobook tasks.
Finally, we demonstrate that multi-speaker TP-GST models successfully factorize
speaker identity and speaking style. We provide a website with audio samples
for each of our findings.
</summary>
    <author>
      <name>Daisy Stanton</name>
    </author>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>RJ Skerry-Ryan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01400v1</id>
    <updated>2018-08-04T01:26:07Z</updated>
    <published>2018-08-04T01:26:07Z</published>
    <title>code2seq: Generating Sequences from Structured Representations of Code</title>
    <summary>  The ability to generate natural language sequences from source code snippets
can be used for code summarization, documentation, and retrieval.
Sequence-to-sequence (seq2seq) models, adopted from neural machine translation
(NMT), have achieved state-of-the-art performance on these tasks by treating
source code as a sequence of tokens. We present ${\rm {\scriptsize CODE2SEQ}}$:
an alternative approach that leverages the syntactic structure of programming
languages to better encode source code. Our model represents a code snippet as
the set of paths in its abstract syntax tree (AST) and uses attention to select
the relevant paths during decoding, much like contemporary NMT models. We
demonstrate the effectiveness of our approach for two tasks, two programming
languages, and four datasets of up to 16M examples. Our model significantly
outperforms previous models that were specifically designed for programming
languages, as well as general state-of-the-art NMT models.
</summary>
    <author>
      <name>Uri Alon</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <author>
      <name>Eran Yahav</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06288v3</id>
    <updated>2018-08-04T01:25:57Z</updated>
    <published>2016-03-20T22:58:43Z</published>
    <title>Multi-fidelity Gaussian Process Bandit Optimisation</title>
    <summary>  In many scientific and engineering applications, we are tasked with the
maximisation of an expensive to evaluate black box function $f$. Traditional
settings for this problem assume just the availability of this single function.
However, in many cases, cheap approximations to $f$ may be obtainable. For
example, the expensive real world behaviour of a robot can be approximated by a
cheap computer simulation. We can use these approximations to eliminate low
function value regions cheaply and use the expensive evaluations of $f$ in a
small but promising region and speedily identify the optimum. We formalise this
task as a \emph{multi-fidelity} bandit problem where the target function and
its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a
novel method based on upper confidence bound techniques. In our theoretical
analysis we demonstrate that it exhibits precisely the above behaviour, and
achieves better regret than strategies which ignore multi-fidelity information.
Empirically, MF-GP-UCB outperforms such naive strategies and other
multi-fidelity methods on several synthetic and real experiments.
</summary>
    <author>
      <name>Kirthevasan Kandasamy</name>
    </author>
    <author>
      <name>Gautam Dasarathy</name>
    </author>
    <author>
      <name>Junier B. Oliva</name>
    </author>
    <author>
      <name>Jeff Schneider</name>
    </author>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version appeared at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06288v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06288v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09045v3</id>
    <updated>2018-08-04T01:13:03Z</updated>
    <published>2018-05-23T10:43:56Z</published>
    <title>When Simple Exploration is Sample Efficient: Identifying Sufficient
  Conditions for Random Exploration to Yield PAC RL Algorithms</title>
    <summary>  Efficient exploration is one of the key challenges for reinforcement learning
(RL) algorithms. Most traditional sample efficiency bounds require strategic
exploration. Recently many deep RL algorithms with simple heuristic exploration
strategies that have few formal guarantees, achieve surprising success in many
domains. These results pose an important question about understanding these
exploration strategies such as $e$-greedy, as well as understanding what
characterize the difficulty of exploration in MDPs. In this work we propose
problem specific sample complexity bounds of $Q$ learning with random walk
exploration that rely on several structural properties. We also link our
theoretical results to some empirical benchmark domains, to illustrate if our
bound gives polynomial sample complexity in these domains and how that is
related with the empirical performance.
</summary>
    <author>
      <name>Yao Liu</name>
    </author>
    <author>
      <name>Emma Brunskill</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09045v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09045v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01701v4</id>
    <updated>2018-08-03T22:51:24Z</updated>
    <published>2017-04-06T04:02:35Z</published>
    <title>Learning Certifiably Optimal Rule Lists for Categorical Data</title>
    <summary>  We present the design and implementation of a custom discrete optimization
technique for building rule lists over a categorical feature space. Our
algorithm produces rule lists with optimal training performance, according to
the regularized empirical risk, with a certificate of optimality. By leveraging
algorithmic bounds, efficient data structures, and computational reuse, we
achieve several orders of magnitude speedup in time and a massive reduction of
memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is
possible to construct optimal sparse rule lists that are approximately as
accurate as the COMPAS proprietary risk prediction tool on data from Broward
County, Florida, but that are completely interpretable. This framework is a
novel alternative to CART and other decision tree methods for interpretable
modeling.
</summary>
    <author>
      <name>Elaine Angelino</name>
    </author>
    <author>
      <name>Nicholas Larus-Stone</name>
    </author>
    <author>
      <name>Daniel Alabi</name>
    </author>
    <author>
      <name>Margo Seltzer</name>
    </author>
    <author>
      <name>Cynthia Rudin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this work appeared in KDD '17 as "Learning
  Certifiably Optimal Rule Lists"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 18(234):1-78, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.01701v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01701v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04706v7</id>
    <updated>2018-08-03T22:13:06Z</updated>
    <published>2016-04-16T07:26:58Z</published>
    <title>DS-MLR: Exploiting Double Separability for Scaling up Distributed
  Multinomial Logistic Regression</title>
    <summary>  Scaling multinomial logistic regression to datasets with very large number of
data points and classes is challenging. This is primarily because one needs to
compute the log-partition function on every data point. This makes distributing
the computation hard. In this paper, we present a distributed stochastic
gradient descent based optimization method (DS-MLR) for scaling up multinomial
logistic regression problems to massive scale datasets without hitting any
storage constraints on the data and model parameters. Our algorithm exploits
double-separability, an attractive property that allows us to achieve both data
as well as model parallelism simultaneously. In addition, we introduce a
non-blocking and asynchronous variant of our algorithm that avoids
bulk-synchronization. We demonstrate the versatility of DS-MLR to various
scenarios in data and model parallelism, through an extensive empirical study
using several real-world datasets. In particular, we demonstrate the
scalability of DS-MLR by solving an extreme multi-class classification problem
on the Reddit dataset (159 GB data, 358 GB parameters) where, to the best of
our knowledge, no other existing methods apply.
</summary>
    <author>
      <name>Parameswaran Raman</name>
    </author>
    <author>
      <name>Sriram Srinivasan</name>
    </author>
    <author>
      <name>Shin Matsushima</name>
    </author>
    <author>
      <name>Xinhua Zhang</name>
    </author>
    <author>
      <name>Hyokun Yun</name>
    </author>
    <author>
      <name>S. V. N. Vishwanathan</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04706v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04706v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09499v9</id>
    <updated>2018-08-03T22:05:49Z</updated>
    <published>2016-05-31T05:10:51Z</published>
    <title>Extreme Stochastic Variational Inference: Distributed and Asynchronous</title>
    <summary>  Stochastic variational inference (SVI), the state-of-the-art algorithm for
scaling variational inference to large-datasets, is inherently serial.
Moreover, it requires the parameters to fit in the memory of a single
processor; this is problematic when the number of parameters is in billions. In
this paper, we propose extreme stochastic variational inference (ESVI), an
asynchronous and lock-free algorithm to perform variational inference for
mixture models on massive real world datasets. ESVI overcomes the limitations
of SVI by requiring that each processor only access a subset of the data and a
subset of the parameters, thus providing data and model parallelism
simultaneously. We demonstrate the effectiveness of ESVI by running Latent
Dirichlet Allocation (LDA) on UMBC-3B, a dataset that has a vocabulary of 3
million and a token size of 3 billion. In our experiments, we found that ESVI
not only outperforms VI and SVI in wallclock-time, but also achieves a better
quality solution. In addition, we propose a strategy to speed up computation
and save memory when fitting large number of topics.
</summary>
    <author>
      <name>Jiong Zhang</name>
    </author>
    <author>
      <name>Parameswaran Raman</name>
    </author>
    <author>
      <name>Shihao Ji</name>
    </author>
    <author>
      <name>Hsiang-Fu Yu</name>
    </author>
    <author>
      <name>S. V. N. Vishwanathan</name>
    </author>
    <author>
      <name>Inderjit S. Dhillon</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09499v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09499v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01345v1</id>
    <updated>2018-08-03T20:22:57Z</updated>
    <published>2018-08-03T20:22:57Z</published>
    <title>Multi-objective optimization to explicitly account for model complexity
  when learning Bayesian Networks</title>
    <summary>  Bayesian Networks have been widely used in the last decades in many fields,
to describe statistical dependencies among random variables. In general,
learning the structure of such models is a problem with considerable
theoretical interest that still poses many challenges. On the one hand, this is
a well-known NP-complete problem, which is practically hardened by the huge
search space of possible solutions. On the other hand, the phenomenon of
I-equivalence, i.e., different graphical structures underpinning the same set
of statistical dependencies, may lead to multimodal fitness landscapes further
hindering maximum likelihood approaches to solve the task. Despite all these
difficulties, greedy search methods based on a likelihood score coupled with a
regularization term to account for model complexity, have been shown to be
surprisingly effective in practice. In this paper, we consider the formulation
of the task of learning the structure of Bayesian Networks as an optimization
problem based on a likelihood score. Nevertheless, our approach do not adjust
this score by means of any of the complexity terms proposed in the literature;
instead, it accounts directly for the complexity of the discovered solutions by
exploiting a multi-objective optimization procedure. To this extent, we adopt
NSGA-II and define the first objective function to be the likelihood of a
solution and the second to be the number of selected arcs. We thoroughly
analyze the behavior of our method on a wide set of simulated data, and we
discuss the performance considering the goodness of the inferred solutions both
in terms of their objective functions and with respect to the retrieved
structure. Our results show that NSGA-II can converge to solutions
characterized by better likelihood and less arcs than classic approaches,
although paradoxically frequently characterized by a lower similarity to the
target network.
</summary>
    <author>
      <name>Paolo Cazzaniga</name>
    </author>
    <author>
      <name>Marco S. Nobile</name>
    </author>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08676v2</id>
    <updated>2018-08-03T20:16:37Z</updated>
    <published>2017-04-27T17:40:22Z</published>
    <title>Learning the structure of Bayesian Networks: A quantitative assessment
  of the effect of different algorithmic schemes</title>
    <summary>  One of the most challenging tasks when adopting Bayesian Networks (BNs) is
the one of learning their structure from data. This task is complicated by the
huge search space of possible solutions, and by the fact that the problem is
NP-hard. Hence, full enumeration of all the possible solutions is not always
feasible and approximations are often required. However, to the best of our
knowledge, a quantitative analysis of the performance and characteristics of
the different heuristics to solve this problem has never been done before.
  For this reason, in this work, we provide a detailed comparison of many
different state-of-the-arts methods for structural learning on simulated data
considering both BNs with discrete and continuous variables, and with different
rates of noise in the data. In particular, we investigate the performance of
different widespread scores and algorithmic approaches proposed for the
inference and the statistical pitfalls within them.
</summary>
    <author>
      <name>Stefano Beretta</name>
    </author>
    <author>
      <name>Mauro Castelli</name>
    </author>
    <author>
      <name>Ivo Goncalves</name>
    </author>
    <author>
      <name>Roberto Henriques</name>
    </author>
    <author>
      <name>Daniele Ramazzotti</name>
    </author>
    <link href="http://arxiv.org/abs/1704.08676v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08676v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02334v1</id>
    <updated>2018-08-03T19:10:39Z</updated>
    <published>2018-08-03T19:10:39Z</published>
    <title>A novel topology design approach using an integrated deep learning
  network architecture</title>
    <summary>  Topology design optimization offers tremendous opportunity in design and
manufacturing freedoms by designing and producing a part from the ground-up
without a meaningful initial design as required by conventional shape design
optimization approaches. Ideally, with adequate problem statements, to
formulate and solve the topology design problem using a standard topology
optimization process, such as SIMP (Simplified Isotropic Material with
Penalization) is possible. In reality, an estimated over thousands of design
iterations is often required for just a few design variables, the conventional
optimization approach is in general impractical or computationally unachievable
for real world applications significantly diluting the development of the
topology optimization technology. There is, therefore, a need for a different
approach that will be able to optimize the initial design topology effectively
and rapidly. Therefore, this work presents a new topology design procedure to
generate optimal structures using an integrated Generative Adversarial Networks
(GANs) and convolutional neural network architecture.
</summary>
    <author>
      <name>Sharad Rawat</name>
    </author>
    <author>
      <name>M. H. Herman Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 Figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06567v2</id>
    <updated>2018-08-03T17:41:19Z</updated>
    <published>2018-03-17T20:13:28Z</published>
    <title>A Dual Approach to Scalable Verification of Deep Networks</title>
    <summary>  This paper addresses the problem of formally verifying desirable properties
of neural networks, i.e., obtaining provable guarantees that neural networks
satisfy specifications relating their inputs and outputs (robustness to bounded
norm adversarial perturbations, for example). Most previous work on this topic
was limited in its applicability by the size of the network, network
architecture and the complexity of properties to be verified. In contrast, our
framework applies to a general class of activation functions and specifications
on neural network inputs and outputs. We formulate verification as an
optimization problem (seeking to find the largest violation of the
specification) and solve a Lagrangian relaxation of the optimization problem to
obtain an upper bound on the worst case violation of the specification being
verified. Our approach is anytime i.e. it can be stopped at any time and a
valid bound on the maximum violation can be obtained. We develop specialized
verification algorithms with provable tightness guarantees under special
assumptions and demonstrate the practical significance of our general
verification approach on a variety of verification tasks.
</summary>
    <author>
      <name> Krishnamurthy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dj</arxiv:affiliation>
    </author>
    <author>
      <name> Dvijotham</name>
    </author>
    <author>
      <name>Robert Stanforth</name>
    </author>
    <author>
      <name>Sven Gowal</name>
    </author>
    <author>
      <name>Timothy Mann</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06567v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06567v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10454v2</id>
    <updated>2018-08-03T16:57:47Z</updated>
    <published>2018-07-27T06:50:43Z</published>
    <title>From Adversarial Training to Generative Adversarial Networks</title>
    <summary>  In this paper, we are interested in two seemingly different concepts:
\textit{adversarial training} and \textit{generative adversarial networks
(GANs)}. Particularly, how these techniques help to improve each other. To this
end, we analyze the limitation of adversarial training as the defense method,
starting from questioning how well the robustness of a model can generalize.
Then, we successfully improve the generalizability via data augmentation by the
"fake" images sampled from generative adversarial networks. After that, we are
surprised to see that the resulting robust classifier leads to a better
generator, for free. We intuitively explain this interesting phenomenon and
leave the theoretical analysis for future work. Motivated by these
observations, we propose a system that combines generator, discriminator, and
adversarial attacker in a single network. After end-to-end training and fine
tuning, our method can simultaneously improve the robustness of classifiers,
measured by accuracy under strong adversarial attacks; and the quality of
generators, evaluated both aesthetically and quantitatively. In terms of the
classifier, we achieve better robustness than the state-of-the-art adversarial
training algorithm proposed in (Madry etla., 2017), while our generator
achieves competitive performance compared with SN-GAN (Miyato and Koyama,
2018). Source code is publicly available online at
\url{https://github.com/anonymous}.
</summary>
    <author>
      <name>Xuanqing Liu</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2018 submission, under review. v2: More experiments on comparing
  inception score, release code and some minor fixes</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10454v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10454v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09318v2</id>
    <updated>2018-08-03T16:00:26Z</updated>
    <published>2018-03-25T19:24:52Z</published>
    <title>Data-driven Discovery of Closure Models</title>
    <summary>  Derivation of reduced order representations of dynamical systems requires the
modeling of the truncated dynamics on the retained dynamics. In its most
general form, this so-called closure model has to account for memory effects.
In this work, we present a framework of operator inference to extract the
governing dynamics of closure from data in a compact, non-Markovian form. We
employ sparse polynomial regression and artificial neural networks to extract
the underlying operator. For a special class of non-linear systems,
observability of the closure in terms of the resolved dynamics is analyzed and
theoretical results are presented on the compactness of the memory. The
proposed framework is evaluated on examples consisting of linear to nonlinear
systems with and without chaotic dynamics, with an emphasis on predictive
performance on unseen data.
</summary>
    <author>
      <name>Shaowu Pan</name>
    </author>
    <author>
      <name>Karthik Duraisamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09318v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09318v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="70G60, 76F20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04446v1</id>
    <updated>2018-08-03T14:32:02Z</updated>
    <published>2018-08-03T14:32:02Z</published>
    <title>Visual Reasoning with Multi-hop Feature Modulation</title>
    <summary>  Recent breakthroughs in computer vision and natural language processing have
spurred interest in challenging multi-modal tasks such as visual
question-answering and visual dialogue. For such tasks, one successful approach
is to condition image-based convolutional network computation on language via
Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and
shifting. We propose to generate the parameters of FiLM layers going up the
hierarchy of a convolutional network in a multi-hop fashion rather than all at
once, as in prior work. By alternating between attending to the language input
and generating FiLM layer parameters, this approach is better able to scale to
settings with longer input sequences such as dialogue. We demonstrate that
multi-hop FiLM generation achieves state-of-the-art for the short input
sequence task ReferIt --- on-par with single-hop FiLM generation --- while also
significantly outperforming prior state-of-the-art and single-hop FiLM
generation on the GuessWhat?! visual dialogue task.
</summary>
    <author>
      <name>Florian Strub</name>
    </author>
    <author>
      <name>Mathieu Seurin</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Harm de Vries</name>
    </author>
    <author>
      <name>Jérémie Mary</name>
    </author>
    <author>
      <name>Philippe Preux</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Olivier Pietquin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc of ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02303v3</id>
    <updated>2018-08-03T13:44:20Z</updated>
    <published>2018-07-06T08:14:27Z</published>
    <title>A survey on policy search algorithms for learning robot controllers in a
  handful of trials</title>
    <summary>  Most policy search algorithms require thousands of training episodes to find
an effective policy, which is often infeasible with a physical robot. This
survey article focuses on the extreme other end of the spectrum: how can a
robot adapt with only a handful of trials (a dozen) and a few minutes? By
analogy with the word "big-data", we refer to this challenge as "micro-data
reinforcement learning". We show that a first strategy is to leverage prior
knowledge on the policy structure (e.g., dynamic movement primitives), on the
policy parameters (e.g., demonstrations), or on the dynamics (e.g.,
simulators). A second strategy is to create data-driven surrogate models of the
expected reward (e.g., Bayesian optimization) or the dynamical model (e.g.,
model-based policy search), so that the policy optimizer queries the model
instead of the real system. Overall, all successful micro-data algorithms
combine these two strategies by varying the kind of model and prior knowledge.
The current scientific challenges essentially revolve around scaling up to
complex robots (e.g., humanoids), designing generic priors, and optimizing the
computing time.
</summary>
    <author>
      <name>Konstantinos Chatzilygeroudis</name>
    </author>
    <author>
      <name>Vassilis Vassiliades</name>
    </author>
    <author>
      <name>Freek Stulp</name>
    </author>
    <author>
      <name>Sylvain Calinon</name>
    </author>
    <author>
      <name>Jean-Baptiste Mouret</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02303v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02303v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01181v1</id>
    <updated>2018-08-03T13:25:39Z</updated>
    <published>2018-08-03T13:25:39Z</published>
    <title>Robust Spectral Filtering and Anomaly Detection</title>
    <summary>  We consider a setting, where the output of a linear dynamical system (LDS)
is, with an unknown but fixed probability, replaced by noise. There, we present
a robust method for the prediction of the outputs of the LDS and identification
of the samples of noise, and prove guarantees on its statistical performance.
One application lies in anomaly detection: the samples of noise, unlikely to
have been generated by the dynamics, can be flagged to operators of the system
for further study.
</summary>
    <author>
      <name>Jakub Marecek</name>
    </author>
    <author>
      <name>Tigran Tchrakian</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03581v6</id>
    <updated>2018-08-03T13:07:04Z</updated>
    <published>2017-04-12T01:02:27Z</published>
    <title>Polya Urn Latent Dirichlet Allocation: a doubly sparse massively
  parallel sampler</title>
    <summary>  Latent Dirichlet Allocation (LDA) is a topic model widely used in natural
language processing and machine learning. Most approaches to training the model
rely on iterative algorithms, which makes it difficult to run LDA on big
corpora that are best analyzed in parallel and distributed computational
environments. Indeed, current approaches to parallel inference either don't
converge to the correct posterior or require storage of large dense matrices in
memory. We present a novel sampler that overcomes both problems, and we show
that this sampler is faster, both empirically and theoretically, than previous
Gibbs samplers for LDA. We do so by employing a novel P\'olya-urn-based
approximation in the sparse partially collapsed sampler for LDA. We prove that
the approximation error vanishes with data size, making our algorithm
asymptotically exact, a property of importance for large-scale topic models. In
addition, we show, via an explicit example, that -- contrary to popular belief
in the topic modeling literature -- partially collapsed samplers can be more
efficient than fully collapsed samplers. We conclude by comparing the
performance of our algorithm with that of other approaches on well-known
corpora.
</summary>
    <author>
      <name>Alexander Terenin</name>
    </author>
    <author>
      <name>Måns Magnusson</name>
    </author>
    <author>
      <name>Leif Jonsson</name>
    </author>
    <author>
      <name>David Draper</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03581v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03581v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01174v1</id>
    <updated>2018-08-03T12:57:12Z</updated>
    <published>2018-08-03T12:57:12Z</published>
    <title>Generalization Error in Deep Learning</title>
    <summary>  Deep learning models have lately shown great performance in various fields
such as computer vision, speech recognition, speech translation, and natural
language processing. However, alongside their state-of-the-art performance, it
is still generally unclear what is the source of their generalization ability.
Thus, an important question is what makes deep neural networks able to
generalize well from the training set to new data. In this article, we provide
an overview of the existing theory and bounds for the characterization of the
generalization error of deep neural networks, combining both classical and more
recent theoretical and empirical results.
</summary>
    <author>
      <name>Daniel Jakubovitz</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Miguel R. D. Rodrigues</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01623v2</id>
    <updated>2018-08-03T12:50:59Z</updated>
    <published>2018-07-04T14:50:00Z</published>
    <title>Modeling outcomes of soccer matches</title>
    <summary>  We compare various extensions of the Bradley-Terry model and a hierarchical
Poisson log-linear model in terms of their performance in predicting the
outcome of soccer matches (win, draw, or loss). The parameters of the
Bradley-Terry extensions are estimated by maximizing the log-likelihood, or an
appropriately penalized version of it, while the posterior densities of the
parameters of the hierarchical Poisson log-linear model are approximated using
integrated nested Laplace approximations. The prediction performance of the
various modeling approaches is assessed using a novel, context-specific
framework for temporal validation that is found to deliver accurate estimates
of the test error. The direct modeling of outcomes via the various
Bradley-Terry extensions and the modeling of match scores using the
hierarchical Poisson log-linear model demonstrate similar behavior in terms of
predictive performance.
</summary>
    <author>
      <name>Alkeos Tsokos</name>
    </author>
    <author>
      <name>Santhosh Narayanan</name>
    </author>
    <author>
      <name>Ioannis Kosmidis</name>
    </author>
    <author>
      <name>Gianluca Baio</name>
    </author>
    <author>
      <name>Mihai Cucuringu</name>
    </author>
    <author>
      <name>Gavin Whitaker</name>
    </author>
    <author>
      <name>Franz J. Király</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01623v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01623v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07987v2</id>
    <updated>2018-08-03T11:27:28Z</updated>
    <published>2018-07-20T18:20:34Z</published>
    <title>Deep Learning</title>
    <summary>  Deep learning (DL) is a high dimensional data reduction technique for
constructing high-dimensional predictors in input-output models. DL is a form
of machine learning that uses hierarchical layers of latent features. In this
article, we review the state-of-the-art of deep learning from a modeling and
algorithmic perspective. We provide a list of successful areas of applications
in Artificial Intelligence (AI), Image Processing, Robotics and Automation.
Deep learning is predictive in its nature rather then inferential and can be
viewed as a black-box methodology for high-dimensional function estimation.
</summary>
    <author>
      <name>Nicholas G. Polson</name>
    </author>
    <author>
      <name>Vadim O. Sokolov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1602.06561</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01918v2</id>
    <updated>2018-08-03T11:05:06Z</updated>
    <published>2018-06-05T20:05:51Z</published>
    <title>A Framework for the construction of upper bounds on the number of affine
  linear regions of ReLU feed-forward neural networks</title>
    <summary>  In this work we present a new framework to derive upper bounds on the number
regions of feed-forward neural nets with ReLU activation functions. We derive
all existing such bounds as special cases, however in a different
representation in terms of matrices. This provides new insight and allows a
more detailed analysis of the corresponding bounds. In particular, we provide a
Jordan-like decomposition for the involved matrices and present new tighter
results for an asymptotic setting. Moreover, new even stronger bounds may be
obtained from our framework.
</summary>
    <author>
      <name>Peter Hinz</name>
    </author>
    <author>
      <name>Sara van de Geer</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01918v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01918v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01145v1</id>
    <updated>2018-08-03T10:24:32Z</updated>
    <published>2018-08-03T10:24:32Z</published>
    <title>Hoeffding Trees with nmin adaptation</title>
    <summary>  Machine learning software accounts for a significant amount of energy
consumed in data centers. These algorithms are usually optimized towards
predictive performance, i.e. accuracy, and scalability. This is the case of
data stream mining algorithms. Although these algorithms are adaptive to the
incoming data, they have fixed parameters from the beginning of the execution.
We have observed that having fixed parameters lead to unnecessary computations,
thus making the algorithm energy inefficient. In this paper we present the nmin
adaptation method for Hoeffding trees. This method adapts the value of the nmin
parameter, which significantly affects the energy consumption of the algorithm.
The method reduces unnecessary computations and memory accesses, thus reducing
the energy, while the accuracy is only marginally affected. We experimentally
compared VFDT (Very Fast Decision Tree, the first Hoeffding tree algorithm) and
CVFDT (Concept-adapting VFDT) with the VFDT-nmin (VFDT with nmin adaptation).
The results show that VFDT-nmin consumes up to 27% less energy than the
standard VFDT, and up to 92% less energy than CVFDT, trading off a few percent
of accuracy in a few datasets.
</summary>
    <author>
      <name>Eva García-Martín</name>
    </author>
    <author>
      <name>Niklas Lavesson</name>
    </author>
    <author>
      <name>Håkan Grahn</name>
    </author>
    <author>
      <name>Emiliano Casalicchio</name>
    </author>
    <author>
      <name>Veselka Boeva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at: The 5th IEEE International Conference on Data Science
  and Advanced Analytics (DSAA 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01132v1</id>
    <updated>2018-08-03T09:42:45Z</updated>
    <published>2018-08-03T09:42:45Z</published>
    <title>Generalized Spectral Mixture Kernels for Multi-Task Gaussian Processes</title>
    <summary>  Multi-Task Gaussian processes (MTGPs) have shown a significant progress both
in expressiveness and interpretation of the relatedness between different
tasks: from linear combinations of independent single-output Gaussian processes
(GPs), through the direct modeling of the cross-covariances such as spectral
mixture kernels with phase shift, to the design of multivariate covariance
functions based on spectral mixture kernels which model delays among tasks in
addition to phase differences, and which provide a parametric interpretation of
the relatedness across tasks. In this paper we further extend expressiveness
and interpretability of MTGPs models and introduce a new family of kernels
capable to model nonlinear correlations between tasks as well as dependencies
between spectral mixtures, including time and phase delay. Specifically, we use
generalized convolution spectral mixture kernels for modeling dependencies at
spectral mixture level, and coupling coregionalization for discovering task
level correlations. The proposed kernels for MTGP are validated on artificial
data and compared with existing MTGPs methods on three real-world experiments.
Results indicate the benefits of our more expressive representation with
respect to performance and interpretability.
</summary>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Perry Groot</name>
    </author>
    <author>
      <name>Jinsong Chen</name>
    </author>
    <author>
      <name>Elena Marchiori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 33 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01128v1</id>
    <updated>2018-08-03T09:34:20Z</updated>
    <published>2018-08-03T09:34:20Z</published>
    <title>PHI Scrubber: A Deep Learning Approach</title>
    <summary>  Confidentiality of patient information is an essential part of Electronic
Health Record System. Patient information, if exposed, can cause a serious
damage to the privacy of individuals receiving healthcare. Hence it is
important to remove such details from physician notes. A system is proposed
which consists of a deep learning model where a de-convolutional neural network
and bi-directional LSTM-CNN is used along with regular expressions to recognize
and eliminate the individually identifiable information. This information is
then removed from a medical practitioner's data which further allows the fair
usage of such information among researchers and in clinical trials.
</summary>
    <author>
      <name>Abhai Kollara Dilip</name>
    </author>
    <author>
      <name>Kamal Raj K</name>
    </author>
    <author>
      <name>Malaikannan Sankarasubbu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01126v1</id>
    <updated>2018-08-03T09:22:46Z</updated>
    <published>2018-08-03T09:22:46Z</published>
    <title>Information-Theoretic Scoring Rules to Learn Additive Bayesian Network
  Applied to Epidemiology</title>
    <summary>  Bayesian network modelling is a well adapted approach to study messy and
highly correlated datasets which are very common in, e.g., systems
epidemiology. A popular approach to learn a Bayesian network from an
observational datasets is to identify the maximum a posteriori network in a
search-and-score approach. Many scores have been proposed both Bayesian or
frequentist based. In an applied perspective, a suitable approach would allow
multiple distributions for the data and is robust enough to run autonomously. A
promising framework to compute scores are generalized linear models. Indeed,
there exists fast algorithms for estimation and many tailored solutions to
common epidemiological issues. The purpose of this paper is to present an R
package abn that has an implementation of multiple frequentist scores and some
realistic simulations that show its usability and performance. It includes
features to deal efficiently with data separation and adjustment which are very
common in systems epidemiology.
</summary>
    <author>
      <name>Gilles Kratzer</name>
    </author>
    <author>
      <name>Reinhard Furrer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04097v3</id>
    <updated>2018-08-03T08:58:17Z</updated>
    <published>2018-04-11T17:07:43Z</published>
    <title>End-to-end Deep Learning of Optical Fiber Communications</title>
    <summary>  In this paper, we implement an optical fiber communication system as an
end-to-end deep neural network, including the complete chain of transmitter,
channel model, and receiver. This approach enables the optimization of the
transceiver in a single end-to-end process. We illustrate the benefits of this
method by applying it to intensity modulation/direct detection (IM/DD) systems
and show that we can achieve bit error rates below the 6.7\% hard-decision
forward error correction (HD-FEC) threshold. We model all componentry of the
transmitter and receiver, as well as the fiber channel, and apply deep learning
to find transmitter and receiver configurations minimizing the symbol error
rate. We propose and verify in simulations a training method that yields robust
and flexible transceivers that allow---without reconfiguration---reliable
transmission over a large range of link dispersions. The results from
end-to-end deep learning are successfully verified for the first time in an
experiment. In particular, we achieve information rates of 42\,Gb/s below the
HD-FEC threshold at distances beyond 40\,km. We find that our results
outperform conventional IM/DD solutions based on 2 and 4 level pulse amplitude
modulation (PAM2/PAM4) with feedforward equalization (FFE) at the receiver. Our
study is the first step towards end-to-end deep learning-based optimization of
optical fiber communication systems.
</summary>
    <author>
      <name>Boris Karanov</name>
    </author>
    <author>
      <name>Mathieu Chagnon</name>
    </author>
    <author>
      <name>Félix Thouin</name>
    </author>
    <author>
      <name>Tobias A. Eriksson</name>
    </author>
    <author>
      <name>Henning Bülow</name>
    </author>
    <author>
      <name>Domaniç Lavery</name>
    </author>
    <author>
      <name>Polina Bayvel</name>
    </author>
    <author>
      <name>Laurent Schmalen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JLT.2018.2865109</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JLT.2018.2865109" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE/OSA Journal of Lightwave Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04097v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04097v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01095v1</id>
    <updated>2018-08-03T06:02:46Z</updated>
    <published>2018-08-03T06:02:46Z</published>
    <title>Helix: Accelerating Human-in-the-loop Machine Learning</title>
    <summary>  Data application developers and data scientists spend an inordinate amount of
time iterating on machine learning (ML) workflows -- by modifying the data
pre-processing, model training, and post-processing steps -- via
trial-and-error to achieve the desired model performance. Existing work on
accelerating machine learning focuses on speeding up one-shot execution of
workflows, failing to address the incremental and dynamic nature of typical ML
development. We propose Helix, a declarative machine learning system that
accelerates iterative development by optimizing workflow execution end-to-end
and across iterations. Helix minimizes the runtime per iteration via program
analysis and intelligent reuse of previous results, which are selectively
materialized -- trading off the cost of materialization for potential future
benefits -- to speed up future iterations. Additionally, Helix offers a
graphical interface to visualize workflow DAGs and compare versions to
facilitate iterative development. Through two ML applications, in
classification and in structured prediction, attendees will experience the
succinctness of Helix programming interface and the speed and ease of iterative
development using Helix. In our evaluations, Helix achieved up to an order of
magnitude reduction in cumulative run time compared to state-of-the-art machine
learning tools.
</summary>
    <author>
      <name>Doris Xin</name>
    </author>
    <author>
      <name>Litian Ma</name>
    </author>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Stephen Macke</name>
    </author>
    <author>
      <name>Shuchen Song</name>
    </author>
    <author>
      <name>Aditya Parameswaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14778/3229863.3236234</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14778/3229863.3236234" rel="related"/>
    <link href="http://arxiv.org/abs/1808.01095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02128v2</id>
    <updated>2018-08-03T05:19:25Z</updated>
    <published>2018-07-05T18:05:18Z</published>
    <title>Adaptive Path-Integral Autoencoder: Representation Learning and Planning
  for Dynamical Systems</title>
    <summary>  We present a representation learning algorithm that learns a low-dimensional
latent dynamical system from high-dimensional sequential raw data, e.g., video.
The framework builds upon recent advances in amortized inference methods that
use both an inference network and a refinement procedure to output samples from
a variational distribution given an observation sequence, and takes advantage
of the duality between control and inference to approximately solve the
intractable inference problem using the path integral control approach. The
learned dynamical model can be used to predict and plan the future states; we
also present the efficient planning method that exploits the learned
low-dimensional latent dynamics. Numerical experiments show that the proposed
path-integral control based variational inference method leads to tighter lower
bounds in statistical model learning of sequential data. The supplementary
video can be found at https://youtu.be/NLM0GQ0o4jM
</summary>
    <author>
      <name>Jung-Su Ha</name>
    </author>
    <author>
      <name>Young-Jin Park</name>
    </author>
    <author>
      <name>Hyeok-Joo Chae</name>
    </author>
    <author>
      <name>Soon-Seo Park</name>
    </author>
    <author>
      <name>Han-Lim Choi</name>
    </author>
    <link href="http://arxiv.org/abs/1807.02128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00525v2</id>
    <updated>2018-08-03T02:59:12Z</updated>
    <published>2018-07-30T14:29:27Z</published>
    <title>The impact of imbalanced training data on machine learning for author
  name disambiguation</title>
    <summary>  In supervised machine learning for author name disambiguation, negative
training data are often dominantly larger than positive training data. This
paper examines how the ratios of negative to positive training data can affect
the performance of machine learning algorithms to disambiguate author names in
bibliographic records. On multiple labeled datasets, three classifiers -
Logistic Regression, Na\"ive Bayes, and Random Forest - are trained through
representative features such as coauthor names, and title words extracted from
the same training data but with various positive-negative training data ratios.
Results show that increasing negative training data can improve disambiguation
performance but with a few percent of performance gains and sometimes degrade
it. Logistic Regression and Na\"ive Bayes learn optimal disambiguation models
even with a base ratio (1:1) of positive and negative training data. Also, the
performance improvement by Random Forest tends to quickly saturate roughly
after 1:10 ~ 1:15. These findings imply that contrary to the common practice
using all training data, name disambiguation algorithms can be trained using
part of negative training data without degrading much disambiguation
performance while increasing computational efficiency. This study calls for
more attention from author name disambiguation scholars to methods for machine
learning from imbalanced data.
</summary>
    <author>
      <name>Jinseok Kim</name>
    </author>
    <author>
      <name>Jenna Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11192-018-2865-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11192-018-2865-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures, and 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Kim, J. &amp; Kim, J. (2018). The impact of imbalanced training data
  on machine learning for author name disambiguation. Scientometrics</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00525v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00525v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01056v1</id>
    <updated>2018-08-03T01:01:15Z</updated>
    <published>2018-08-03T01:01:15Z</published>
    <title>Robust Regression for Automatic Fusion Plasma Analysis based on
  Generative Modeling</title>
    <summary>  The first step to realize automatic experimental data analysis for fusion
plasma experiments is fitting noisy data of temperature and density spatial
profiles, which are obtained routinely. However, it has been difficult to
construct algorithms that fit all the data without over- and under-fitting. In
this paper, we show that this difficulty originates from the lack of knowledge
of the probability distribution that the measurement data follow. We
demonstrate the use of a machine learning technique to estimate the data
distribution and to construct an optimal generative model. We show that the
fitting algorithm based on the generative modeling outperforms classical
heuristic methods in terms of the stability as well as the accuracy.
</summary>
    <author>
      <name>Keisuke Fujii</name>
    </author>
    <author>
      <name>Chihiro Suzuki</name>
    </author>
    <author>
      <name>Masahiro Hasuo</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01048v1</id>
    <updated>2018-08-02T23:30:50Z</updated>
    <published>2018-08-02T23:30:50Z</published>
    <title>Variational Information Bottleneck on Vector Quantized Autoencoders</title>
    <summary>  In this paper, we provide an information-theoretic interpretation of the
Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss
function of the original VQ-VAE can be derived from the variational
deterministic information bottleneck (VDIB) principle. On the other hand, the
VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as
an approximation to the variational information bottleneck(VIB) principle.
</summary>
    <author>
      <name>Hanwei Wu</name>
    </author>
    <author>
      <name>Markus Flierl</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05271v2</id>
    <updated>2018-08-02T21:24:35Z</updated>
    <published>2018-04-14T20:21:48Z</published>
    <title>Adaptive Federated Learning in Resource Constrained Edge Computing
  Systems</title>
    <summary>  Emerging technologies and applications including Internet of Things (IoT),
social networking, and crowd-sourcing generate large amounts of data at the
network edge. Machine learning models are often built from the collected data,
to enable the detection, classification, and prediction of future events. Due
to bandwidth, storage, and privacy concerns, it is often impractical to send
all the data to a centralized location. In this paper, we consider the problem
of learning model parameters from data distributed across multiple edge nodes,
without sending raw data to a centralized place. Our focus is on a generic
class of machine learning models that are trained using gradient-descent based
approaches. We analyze the convergence bound of distributed gradient descent
from a theoretical point of view, based on which we propose a control algorithm
that determines the best trade-off between local update and global parameter
aggregation to minimize the loss function under a given resource budget. The
performance of the proposed algorithm is evaluated via extensive experiments
with real datasets, both on a networked prototype system and in a larger-scale
simulated environment. The experimentation results show that our proposed
approach performs near to the optimum with various machine learning models and
different data distributions.
</summary>
    <author>
      <name>Shiqiang Wang</name>
    </author>
    <author>
      <name>Tiffany Tuor</name>
    </author>
    <author>
      <name>Theodoros Salonidis</name>
    </author>
    <author>
      <name>Kin K. Leung</name>
    </author>
    <author>
      <name>Christian Makaya</name>
    </author>
    <author>
      <name>Ting He</name>
    </author>
    <author>
      <name>Kevin Chan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The current version includes a new convergence bound that is more
  general than the bound in the previous version. The control algorithm and
  experimentation results in the current version are new. The new control
  algorithm can guarantee convergence to zero optimality gap as the resource
  budget goes to infinity. The experiments are conducted on larger datasets and
  more results are included</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03437v2</id>
    <updated>2018-08-02T21:03:29Z</updated>
    <published>2018-01-10T16:13:15Z</published>
    <title>Approximation beats concentration? An approximation view on inference
  with smooth radial kernels</title>
    <summary>  Positive definite kernels and their associated Reproducing Kernel Hilbert
Spaces provide a mathematically compelling and practically competitive
framework for learning from data.
  In this paper we take the approximation theory point of view to explore
various aspects of smooth kernels related to their inferential properties. We
analyze eigenvalue decay of kernels operators and matrices, properties of
eigenfunctions/eigenvectors and "Fourier" coefficients of functions in the
kernel space restricted to a discrete set of data points. We also investigate
the fitting capacity of kernels, giving explicit bounds on the fat shattering
dimension of the balls in Reproducing Kernel Hilbert spaces. Interestingly, the
same properties that make kernels very effective approximators for functions in
their "native" kernel space, also limit their capacity to represent arbitrary
functions. We discuss various implications, including those for gradient
descent type methods.
  It is important to note that most of our bounds are measure independent.
Moreover, at least in moderate dimension, the bounds for eigenvalues are much
tighter than the bounds which can be obtained from the usual matrix
concentration results. For example, we see that the eigenvalues of kernel
matrices show nearly exponential decay with constants depending only on the
kernel and the domain. We call this "approximation beats concentration"
phenomenon as even when the data are sampled from a probability distribution,
some of their aspects are better understood in terms of approximation theory.
</summary>
    <author>
      <name>Mikhail Belkin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Computational Learning Theory (COLT) 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.03437v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03437v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07824v2</id>
    <updated>2018-08-02T18:20:17Z</updated>
    <published>2018-04-20T20:56:33Z</published>
    <title>Autotune: A Derivative-free Optimization Framework for Hyperparameter
  Tuning</title>
    <summary>  Machine learning applications often require hyperparameter tuning. The
hyperparameters usually drive both the efficiency of the model training process
and the resulting model quality. For hyperparameter tuning, machine learning
algorithms are complex black-boxes. This creates a class of challenging
optimization problems, whose objective functions tend to be nonsmooth,
discontinuous, unpredictably varying in computational expense, and include
continuous, categorical, and/or integer variables. Further, function
evaluations can fail for a variety of reasons including numerical difficulties
or hardware failures. Additionally, not all hyperparameter value combinations
are compatible, which creates so called hidden constraints. Robust and
efficient optimization algorithms are needed for hyperparameter tuning. In this
paper we present an automated parallel derivative-free optimization framework
called \textbf{Autotune}, which combines a number of specialized sampling and
search methods that are very effective in tuning machine learning models
despite these challenges. Autotune provides significantly improved models over
using default hyperparameter settings with minimal user interaction on
real-world applications. Given the inherent expense of training numerous
candidate models, we demonstrate the effectiveness of Autotune's search methods
and the efficient distributed and parallel paradigms for training and tuning
models, and also discuss the resource trade-offs associated with the ability to
both distribute the training process and parallelize the tuning process.
</summary>
    <author>
      <name>Patrick Koch</name>
    </author>
    <author>
      <name>Oleg Golovidov</name>
    </author>
    <author>
      <name>Steven Gardner</name>
    </author>
    <author>
      <name>Brett Wujek</name>
    </author>
    <author>
      <name>Joshua Griffin</name>
    </author>
    <author>
      <name>Yan Xu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3219837</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3219837" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 9 figures, accept by KDD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07824v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07824v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00973v1</id>
    <updated>2018-08-02T18:00:08Z</updated>
    <published>2018-08-02T18:00:08Z</published>
    <title>Likelihood-free inference with an improved cross-entropy estimator</title>
    <summary>  We extend recent work (Brehmer, et. al., 2018) that use neural networks as
surrogate models for likelihood-free inference. As in the previous work, we
exploit the fact that the joint likelihood ratio and joint score, conditioned
on both observed and latent variables, can often be extracted from an implicit
generative model or simulator to augment the training data for these surrogate
models. We show how this augmented training data can be used to provide a new
cross-entropy estimator, which provides improved sample efficiency compared to
previous loss functions exploiting this augmented training data.
</summary>
    <author>
      <name>Markus Stoye</name>
    </author>
    <author>
      <name>Johann Brehmer</name>
    </author>
    <author>
      <name>Gilles Louppe</name>
    </author>
    <author>
      <name>Juan Pavez</name>
    </author>
    <author>
      <name>Kyle Cranmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06055v2</id>
    <updated>2018-08-02T17:53:43Z</updated>
    <published>2018-06-15T17:45:58Z</published>
    <title>Classification with Fairness Constraints: A Meta-Algorithm with Provable
  Guarantees</title>
    <summary>  Developing classification algorithms that are fair with respect to sensitive
attributes of the data has become an important problem due to the growing
deployment of classification algorithms in various social contexts. Several
recent works have focused on fairness with respect to a specific metric,
modeled the corresponding fair classification problem as a constrained
optimization problem, and developed tailored algorithms to solve them. Despite
this, there still remain important metrics for which we do not have fair
classifiers and many of the aforementioned algorithms do not come with
theoretical guarantees; perhaps because the resulting optimization problem is
non-convex. The main contribution of this paper is a new meta-algorithm for
classification that takes as input a large class of fairness constraints, with
respect to multiple non-disjoint sensitive attributes, and which comes with
provable guarantees. This is achieved by first developing a meta-algorithm for
a large family of classification problems with convex constraints, and then
showing that classification problems with general types of fairness constraints
can be reduced to those in this family. We present empirical results that show
that our algorithm can achieve near-perfect fairness with respect to various
fairness metrics, and that the loss in accuracy due to the imposed fairness
constraints is often small. Overall, this work unifies several prior works on
fair classification, presents a practical algorithm with theoretical
guarantees, and can handle fairness metrics that were previously not possible.
</summary>
    <author>
      <name>L. Elisa Celis</name>
    </author>
    <author>
      <name>Lingxiao Huang</name>
    </author>
    <author>
      <name>Vijay Keswani</name>
    </author>
    <author>
      <name>Nisheeth K. Vishnoi</name>
    </author>
    <link href="http://arxiv.org/abs/1806.06055v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06055v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00935v1</id>
    <updated>2018-08-02T17:41:56Z</updated>
    <published>2018-08-02T17:41:56Z</published>
    <title>Inferring Parameters Through Inverse Multiobjective Optimization</title>
    <summary>  Given a set of human's decisions that are observed, inverse optimization has
been developed and utilized to infer the underlying decision making problem.
The majority of existing studies assumes that the decision making problem is
with a single objective function, and attributes data divergence to noises,
errors or bounded rationality, which, however, could lead to a corrupted
inference when decisions are tradeoffs among multiple criteria. In this paper,
we take a data-driven approach and design a more sophisticated inverse
optimization formulation to explicitly infer parameters of a multiobjective
decision making problem from noisy observations. This framework, together with
our mathematical analyses and advanced algorithm developments, demonstrates a
strong capacity in estimating critical parameters, decoupling "interpretable"
components from noises or errors, deriving the denoised \emph{optimal}
decisions, and ensuring statistical significance. In particular, for the whole
decision maker population, if suitable conditions hold, we will be able to
understand the overall diversity and the distribution of their preferences over
multiple criteria, which is important when a precise inference on every single
decision maker is practically unnecessary or infeasible. Numerical results on a
large number of experiments are reported to confirm the effectiveness of our
unique inverse optimization model and the computational efficacy of the
developed algorithms.
</summary>
    <author>
      <name>Chaosheng Dong</name>
    </author>
    <author>
      <name>Bo Zeng</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00934v1</id>
    <updated>2018-08-02T17:41:15Z</updated>
    <published>2018-08-02T17:41:15Z</published>
    <title>Streaming Kernel PCA with $\tilde{O}(\sqrt{n})$ Random Features</title>
    <summary>  We study the statistical and computational aspects of kernel principal
component analysis using random Fourier features and show that under mild
assumptions, $O(\sqrt{n} \log n)$ features suffices to achieve
$O(1/\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient
streaming algorithm based on classical Oja's algorithm that achieves this rate.
</summary>
    <author>
      <name>Enayat Ullah</name>
    </author>
    <author>
      <name>Poorya Mianjy</name>
    </author>
    <author>
      <name>Teodor V. Marinov</name>
    </author>
    <author>
      <name>Raman Arora</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00878v1</id>
    <updated>2018-08-02T16:00:32Z</updated>
    <published>2018-08-02T16:00:32Z</published>
    <title>Supervised classification for object identification in urban areas using
  satellite imagery</title>
    <summary>  This paper presents a useful method to achieve classification in satellite
imagery. The approach is based on pixel level study employing various features
such as correlation, homogeneity, energy and contrast. In this study gray-scale
images are used for training the classification model. For supervised
classification, two classification techniques are employed namely the Support
Vector Machine (SVM) and the Naive Bayes. With textural features used for
gray-scale images, Naive Bayes performs better with an overall accuracy of 76%
compared to 68% achieved by SVM. The computational time is evaluated while
performing the experiment with two different window sizes i.e., 50x50 and
70x70. The required computational time on a single image is found to be 27
seconds for a window size of 70x70 and 45 seconds for a window size of 50x50.
</summary>
    <author>
      <name>Hazrat Ali</name>
    </author>
    <author>
      <name>Adnan Ali Awan</name>
    </author>
    <author>
      <name>Sanaullah Khan</name>
    </author>
    <author>
      <name>Omer Shafique</name>
    </author>
    <author>
      <name>Atiq ur Rahman</name>
    </author>
    <author>
      <name>Shahid Khan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICOMET.2018.8346383</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICOMET.2018.8346383" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2018 International Conference on Computing, Mathematics and
  Engineering Technologies (iCoMET)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">H. Ali et al., 2018 International Conference on Computing,
  Mathematics and Engineering Technologies (iCoMET), Sukkur, 2018, pp. 1-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00845v1</id>
    <updated>2018-08-02T14:58:51Z</updated>
    <published>2018-08-02T14:58:51Z</published>
    <title>RGB Video Based Tennis Action Recognition Using a Deep Weighted Long
  Short-Term Memory</title>
    <summary>  Action recognition has attracted increasing attention from RGB input in
computer vision partially due to potential applications on somatic simulation
and statistics of sport such as virtual tennis game and tennis techniques and
tactics analysis by video. Recently, deep learning based methods have achieved
promising performance for action recognition. In this paper, we propose
weighted Long Short-Term Memory adopted with convolutional neural network
representations for three dimensional tennis shots recognition. First, the
local two-dimensional convolutional neural network spatial representations are
extracted from each video frame individually using a pre-trained Inception
network. Then, a weighted Long Short-Term Memory decoder is introduced to take
the output state at time t and the historical embedding feature at time t-1 to
generate feature vector using a score weighting scheme. Finally, we use the
adopted CNN and weighted LSTM to map the original visual features into a vector
space to generate the spatial-temporal semantical description of visual
sequences and classify the action video content. Experiments on the benchmark
demonstrate that our method using only simple raw RGB video can achieve better
performance than the state-of-the-art baselines for tennis shot recognition.
</summary>
    <author>
      <name>Jiaxin Cai</name>
    </author>
    <author>
      <name>Xin Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00831v1</id>
    <updated>2018-08-02T14:32:58Z</updated>
    <published>2018-08-02T14:32:58Z</published>
    <title>Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes</title>
    <summary>  We present an approximate Bayesian inference approach for estimating the
intensity of a inhomogeneous Poisson process, where the intensity function is
modelled using a Gaussian process (GP) prior via a sigmoid link function.
Augmenting the model using a latent marked Poisson process and P\'olya--Gamma
random variables we obtain a representation of the likelihood which is
conjugate to the GP prior. We approximate the posterior using a free--form mean
field approximation together with the framework of sparse GPs. Furthermore, as
alternative approximation we suggest a sparse Laplace approximation of the
posterior, for which an efficient expectation--maximisation algorithm is
derived to find the posterior's mode. Results of both algorithms compare well
with exact inference obtained by a Markov Chain Monte Carlo sampler and
standard variational Gauss approach, while being one order of magnitude faster.
</summary>
    <author>
      <name>Christian Donner</name>
    </author>
    <author>
      <name>Manfred Opper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages; 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00818v1</id>
    <updated>2018-08-02T14:02:50Z</updated>
    <published>2018-08-02T14:02:50Z</published>
    <title>Dirichlet Mixture Model based VQ Performance Prediction for Line
  Spectral Frequency</title>
    <summary>  In this paper, we continue our previous work on the Dirichlet mixture model
(DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF
parameters are transformed into the $\Delta$LSF domain and the underlying
distribution of the $\Delta$LSF parameters are modelled by a DMM with finite
number of mixture components. The quantization distortion, in terms of the mean
squared error (MSE), is calculated with the high rate theory. The mapping
relation between the perceptually motivated log spectral distortion (LSD) and
the MSE is empirically approximated by a polynomial. With this mapping
function, the minimum required bit rate for transparent coding of the LSF is
estimated.
</summary>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00961v1</id>
    <updated>2018-08-02T13:52:01Z</updated>
    <published>2018-08-02T13:52:01Z</published>
    <title>Impacts of Weather Conditions on District Heat System</title>
    <summary>  Using artificial neural network for the prediction of heat demand has
attracted more and more attention. Weather conditions, such as ambient
temperature, wind speed and direct solar irradiance, have been identified as
key input parameters. In order to further improve the model accuracy, it is of
great importance to understand the influence of different parameters. Based on
an Elman neural network (ENN), this paper investigates the impact of direct
solar irradiance and wind speed on predicting the heat demand of a district
heating network. Results show that including wind speed can generally result in
a lower overall mean absolute percentage error (MAPE) (6.43%) than including
direct solar irradiance (6.47%); while including direct solar irradiance can
achieve a lower maximum absolute deviation (71.8%) than including wind speed
(81.53%). In addition, even though including both wind speed and direct solar
irradiance shows the best overall performance (MAPE=6.35%).
</summary>
    <author>
      <name>Jiyang Xie</name>
    </author>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <author>
      <name>Jun Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01359v1</id>
    <updated>2018-08-02T13:50:29Z</updated>
    <published>2018-08-02T13:50:29Z</published>
    <title>Deep Neural Network for Analysis of DNA Methylation Data</title>
    <summary>  Many researches demonstrated that the DNA methylation, which occurs in the
context of a CpG, has strong correlation with diseases, including cancer. There
is a strong interest in analyzing the DNA methylation data to find how to
distinguish different subtypes of the tumor. However, the conventional
statistical methods are not suitable for analyzing the highly dimensional DNA
methylation data with bounded support. In order to explicitly capture the
properties of the data, we design a deep neural network, which composes of
several stacked binary restricted Boltzmann machines, to learn the low
dimensional deep features of the DNA methylation data. Experiments show these
features perform best in breast cancer DNA methylation data cluster analysis,
comparing with some state-of-the-art methods.
</summary>
    <author>
      <name>Hong Yu</name>
    </author>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Techinical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00814v1</id>
    <updated>2018-08-02T13:48:48Z</updated>
    <published>2018-08-02T13:48:48Z</published>
    <title>Classification of EEG Signal based on non-Gaussian Neutral Vector</title>
    <summary>  In the design of brain-computer interface systems, classification of
Electroencephalogram (EEG) signals is the essential part and a challenging
task. Recently, as the marginalized discrete wavelet transform (mDWT)
representations can reveal features related to the transient nature of the EEG
signals, the mDWT coefficients have been frequently used in EEG signal
classification. In our previous work, we have proposed a super-Dirichlet
distribution-based classifier, which utilized the nonnegative and sum-to-one
properties of the mDWT coefficients. The proposed classifier performed better
than the state-of-the-art support vector machine-based classifier. In this
paper, we further study the neutrality of the mDWT coefficients. Assuming the
mDWT vector coefficients to be a neutral vector, we transform them non-linearly
into a set of independent scalar coefficients. Feature selection strategy is
proposed on the transformed feature domain. Experimental results show that the
feature selection strategy helps improving the classification accuracy.
</summary>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00959v1</id>
    <updated>2018-08-02T13:45:37Z</updated>
    <published>2018-08-02T13:45:37Z</published>
    <title>Histogram Transform-based Speaker Identification</title>
    <summary>  A novel text-independent speaker identification (SI) method is proposed. This
method uses the Mel-frequency Cepstral coefficients (MFCCs) and the dynamic
information among adjacent frames as feature sets to capture speaker's
characteristics. In order to utilize dynamic information, we design super-MFCCs
features by cascading three neighboring MFCCs frames together. The probability
density function (PDF) of these super-MFCCs features is estimated by the
recently proposed histogram transform~(HT) method, which generates more
training data by random transforms to realize the histogram PDF estimation and
recedes the commonly occurred discontinuity problem in multivariate histograms
computing. Compared to the conventional PDF estimation methods, such as
Gaussian mixture models, the HT model shows promising improvement in the SI
performance.
</summary>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <author>
      <name>Hong Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00803v1</id>
    <updated>2018-08-02T13:31:23Z</updated>
    <published>2018-08-02T13:31:23Z</published>
    <title>Mobile big data analysis with machine learning</title>
    <summary>  This paper investigates to identify the requirement and the development of
machine learning-based mobile big data analysis through discussing the insights
of challenges in the mobile big data (MBD). Furthermore, it reviews the
state-of-the-art applications of data analysis in the area of MBD. Firstly, we
introduce the development of MBD. Secondly, the frequently adopted methods of
data analysis are reviewed. Three typical applications of MBD analysis, namely
wireless channel modeling, human online and offline behavior analysis, and
speech recognition in the internet of vehicles, are introduced respectively.
Finally, we summarize the main challenges and future development directions of
mobile big data analysis.
</summary>
    <author>
      <name>Jiyang Xie</name>
    </author>
    <author>
      <name>Zeyu Song</name>
    </author>
    <author>
      <name>Yupeng Li</name>
    </author>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 0.1</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04497v2</id>
    <updated>2018-08-02T13:27:12Z</updated>
    <published>2018-02-14T13:00:05Z</published>
    <title>Automated software vulnerability detection with machine learning</title>
    <summary>  Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.
</summary>
    <author>
      <name>Jacob A. Harer</name>
    </author>
    <author>
      <name>Louis Y. Kim</name>
    </author>
    <author>
      <name>Rebecca L. Russell</name>
    </author>
    <author>
      <name>Onur Ozdemir</name>
    </author>
    <author>
      <name>Leonard R. Kosta</name>
    </author>
    <author>
      <name>Akshay Rangamani</name>
    </author>
    <author>
      <name>Lei H. Hamilton</name>
    </author>
    <author>
      <name>Gabriel I. Centeno</name>
    </author>
    <author>
      <name>Jonathan R. Key</name>
    </author>
    <author>
      <name>Paul M. Ellingwood</name>
    </author>
    <author>
      <name>Erik Antelman</name>
    </author>
    <author>
      <name>Alan Mackay</name>
    </author>
    <author>
      <name>Marc W. McConley</name>
    </author>
    <author>
      <name>Jeffrey M. Opper</name>
    </author>
    <author>
      <name>Peter Chin</name>
    </author>
    <author>
      <name>Tomo Lazovich</name>
    </author>
    <link href="http://arxiv.org/abs/1803.04497v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04497v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00783v1</id>
    <updated>2018-08-02T12:44:09Z</updated>
    <published>2018-08-02T12:44:09Z</published>
    <title>The Quest for the Golden Activation Function</title>
    <summary>  Deep Neural Networks have been shown to be beneficial for a variety of tasks,
in particular allowing for end-to-end learning and reducing the requirement for
manual design decisions. However, still many parameters have to be chosen in
advance, also raising the need to optimize them. One important, but often
ignored system parameter is the selection of a proper activation function.
Thus, in this paper we target to demonstrate the importance of activation
functions in general and show that for different tasks different activation
functions might be meaningful. To avoid the manual design or selection of
activation functions, we build on the idea of genetic algorithms to learn the
best activation function for a given task. In addition, we introduce two new
activation functions, ELiSH and HardELiSH, which can easily be incorporated in
our framework. In this way, we demonstrate for three different image
classification benchmarks that different activation functions are learned, also
showing improved results compared to typically used baselines.
</summary>
    <author>
      <name>Mina Basirat</name>
    </author>
    <author>
      <name>Peter M. Roth</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08157v4</id>
    <updated>2018-08-02T12:33:18Z</updated>
    <published>2017-08-28T00:37:38Z</published>
    <title>Characteristic and Universal Tensor Product Kernels</title>
    <summary>  Maximum mean discrepancy (MMD), also called energy distance or N-distance in
statistics and Hilbert-Schmidt independence criterion (HSIC), specifically
distance covariance in statistics, are among the most popular and successful
approaches to quantify the difference and independence of random variables,
respectively. Thanks to their kernel-based foundations, MMD and HSIC are
applicable on a wide variety of domains. Despite their tremendous success,
quite little is known about when HSIC characterizes independence and when MMD
with tensor product kernel can discriminate probability distributions. In this
paper, we answer these questions by studying various notions of characteristic
property of the tensor product kernel.
</summary>
    <author>
      <name>Zoltan Szabo</name>
    </author>
    <author>
      <name>Bharath K. Sriperumbudur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">final version appeared in JMLR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 18(233):1-29, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.08157v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08157v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46E22, 94A15, 62G10, 47B32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; H.1.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11811v3</id>
    <updated>2018-08-02T11:51:44Z</updated>
    <published>2018-05-30T05:23:20Z</published>
    <title>Stochastic Zeroth-order Optimization via Variance Reduction method</title>
    <summary>  Derivative-free optimization has become an important technique used in
machine learning for optimizing black-box models. To conduct updates without
explicitly computing gradient, most current approaches iteratively sample a
random search direction from Gaussian distribution and compute the estimated
gradient along that direction. However, due to the variance in the search
direction, the convergence rates and query complexities of existing methods
suffer from a factor of $d$, where $d$ is the problem dimension. In this paper,
we introduce a novel Stochastic Zeroth-order method with Variance Reduction
under Gaussian smoothing (SZVR-G) and establish the complexity for optimizing
non-convex problems. With variance reduction on both sample space and search
space, the complexity of our algorithm is sublinear to $d$ and is strictly
better than current approaches, in both smooth and non-smooth cases. Moreover,
we extend the proposed method to the mini-batch version. Our experimental
results demonstrate the superior performance of the proposed method over
existing derivative-free optimization techniques. Furthermore, we successfully
apply our method to conduct a universal black-box attack to deep neural
networks and present some interesting results.
</summary>
    <author>
      <name>Liu Liu</name>
    </author>
    <author>
      <name>Minhao Cheng</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11811v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11811v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06400v2</id>
    <updated>2018-08-02T10:07:57Z</updated>
    <published>2017-05-18T02:50:40Z</published>
    <title>Learning a bidirectional mapping between human whole-body motion and
  natural language using deep recurrent neural networks</title>
    <summary>  Linking human whole-body motion and natural language is of great interest for
the generation of semantic representations of observed human behaviors as well
as for the generation of robot behaviors based on natural language input. While
there has been a large body of research in this area, most approaches that
exist today require a symbolic representation of motions (e.g. in the form of
motion primitives), which have to be defined a-priori or require complex
segmentation algorithms. In contrast, recent advances in the field of neural
networks and especially deep learning have demonstrated that sub-symbolic
representations that can be learned end-to-end usually outperform more
traditional approaches, for applications such as machine translation. In this
paper we propose a generative model that learns a bidirectional mapping between
human whole-body motion and natural language using deep recurrent neural
networks (RNNs) and sequence-to-sequence learning. Our approach does not
require any segmentation or manual feature engineering and learns a distributed
representation, which is shared for all motions and descriptions. We evaluate
our approach on 2,846 human whole-body motions and 6,187 natural language
descriptions thereof from the KIT Motion-Language Dataset. Our results clearly
demonstrate the effectiveness of the proposed model: We show that our model
generates a wide variety of realistic motions only from descriptions thereof in
form of a single sentence. Conversely, our model is also capable of generating
correct and detailed natural language descriptions from human motions.
</summary>
    <author>
      <name>Matthias Plappert</name>
    </author>
    <author>
      <name>Christian Mandery</name>
    </author>
    <author>
      <name>Tamim Asfour</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.robot.2018.07.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.robot.2018.07.006" rel="related"/>
    <link href="http://arxiv.org/abs/1705.06400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00741v1</id>
    <updated>2018-08-02T09:57:29Z</updated>
    <published>2018-08-02T09:57:29Z</published>
    <title>Online Aggregation of Unbounded Losses Using Shifting Experts with
  Confidence</title>
    <summary>  We develop the setting of sequential prediction based on shifting experts and
on a "smooth" version of the method of specialized experts. To aggregate
experts predictions, we use the AdaHedge algorithm, which is a version of the
Hedge algorithm with adaptive learning rate, and extend it by the
meta-algorithm Fixed Share. Due to this, we combine the advantages of both
algorithms: (1) we use the shifting regret which is a more optimal
characteristic of the algorithm; (2) regret bounds are valid in the case of
signed unbounded losses of the experts. Also, (3) we incorporate in this scheme
a "smooth" version of the method of specialized experts which allows us to make
more flexible and accurate predictions. All results are obtained in the
adversarial setting -- no assumptions are made about the nature of data source.
We present results of numerical experiments for short-term forecasting of
electricity consumption based on a real data.
</summary>
    <author>
      <name>Vladimir V'yugin</name>
    </author>
    <author>
      <name>Vladimir Trunov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10994-018-5751-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10994-018-5751-z" rel="related"/>
    <link href="http://arxiv.org/abs/1808.00741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03987v2</id>
    <updated>2018-08-02T09:48:48Z</updated>
    <published>2018-02-12T11:39:14Z</published>
    <title>Latent Variable Time-varying Network Inference</title>
    <summary>  In many applications of finance, biology and sociology, complex systems
involve entities interacting with each other. These processes have the
peculiarity of evolving over time and of comprising latent factors, which
influence the system without being explicitly measured. In this work we present
latent variable time-varying graphical lasso (LTGL), a method for multivariate
time-series graphical modelling that considers the influence of hidden or
unmeasurable factors. The estimation of the contribution of the latent factors
is embedded in the model which produces both sparse and low-rank components for
each time point. In particular, the first component represents the connectivity
structure of observable variables of the system, while the second represents
the influence of hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both components,
providing an accurate evolutionary pattern of the system. We derive a tractable
optimisation algorithm based on alternating direction method of multipliers,
and develop a scalable and efficient implementation which exploits proximity
operators in closed form. LTGL is extensively validated on synthetic data,
achieving optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art methods for
graphical inference. We conclude with the application of LTGL to real case
studies, from biology and finance, to illustrate how our method can be
successfully employed to gain insights on multivariate time-series data.
</summary>
    <author>
      <name>Federico Tomasi</name>
    </author>
    <author>
      <name>Veronica Tozzo</name>
    </author>
    <author>
      <name>Saverio Salzo</name>
    </author>
    <author>
      <name>Alessandro Verri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3219819.3220121</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3219819.3220121" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 24th ACM SIGKDD International Conference on
  Knowledge Discovery &amp; Data Mining (KDD 2018). ACM, New York, NY, USA,
  2338-2346</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.03987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00723v1</id>
    <updated>2018-08-02T09:22:39Z</updated>
    <published>2018-08-02T09:22:39Z</published>
    <title>High-dimensional regression in practice: an empirical study of
  finite-sample prediction, variable selection and ranking</title>
    <summary>  Penalized likelihood methods are widely used for high-dimensional regression.
Although many methods have been proposed and the associated theory is now
well-developed, the relative efficacy of different methods in finite-sample
settings, as encountered in practice, remains incompletely understood. There is
therefore a need for empirical investigations in this area that can offer
practical insight and guidance to users of these methods. In this paper we
present a large-scale comparison of penalized regression methods. We
distinguish between three related goals: prediction, variable selection and
variable ranking. Our results span more than 1,800 data-generating scenarios,
allowing us to systematically consider the influence of various factors (sample
size, dimensionality, sparsity, signal strength and multicollinearity). We
consider several widely-used methods (Lasso, Elastic Net, Ridge Regression,
SCAD, the Dantzig Selector as well as Stability Selection). We find
considerable variation in performance between methods, with results dependent
on details of the data-generating scenario and the specific goal. Our results
support a `no panacea' view, with no unambiguous winner across all scenarios,
even in this restricted setting where all data align well with the assumptions
underlying the methods. Lasso is well-behaved, performing competitively in many
scenarios, while SCAD is highly variable. Substantial benefits from a
Ridge-penalty are only seen in the most challenging scenarios with strong
multi-collinearity. The results are supported by semi-synthetic analyzes using
gene expression data from cancer samples. Our empirical results complement
existing theory and provide a resource to compare methods across a range of
scenarios and metrics.
</summary>
    <author>
      <name>Fan Wang</name>
    </author>
    <author>
      <name>Sach Mukherjee</name>
    </author>
    <author>
      <name>Sylvia Richardson</name>
    </author>
    <author>
      <name>Steven M. Hill</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11130v3</id>
    <updated>2018-08-02T08:29:20Z</updated>
    <published>2018-04-30T11:41:48Z</published>
    <title>Clustering Meets Implicit Generative Models</title>
    <summary>  Clustering is a cornerstone of unsupervised learning which can be thought as
disentangling the multiple generative mechanisms underlying the data. In this
paper we introduce an algorithmic framework to train mixtures of implicit
generative models which we instantiate for variational autoencoders. Relying on
an additional set of discriminators, we propose a competitive procedure in
which the models only need to approximate the portion of the data distribution
from which they can produce realistic samples. As a byproduct, each model is
simpler to train, and a clustering interpretation arises naturally from the
partitioning of the training points among the models. We empirically show that
our approach splits the training distribution in a reasonable way and increases
the quality of the generated samples.
</summary>
    <author>
      <name>Francesco Locatello</name>
    </author>
    <author>
      <name>Damien Vincent</name>
    </author>
    <author>
      <name>Ilya Tolstikhin</name>
    </author>
    <author>
      <name>Gunnar Rätsch</name>
    </author>
    <author>
      <name>Sylvain Gelly</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <link href="http://arxiv.org/abs/1804.11130v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11130v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01199v1</id>
    <updated>2018-08-02T08:26:19Z</updated>
    <published>2018-08-02T08:26:19Z</published>
    <title>Generation Meets Recommendation: Proposing Novel Items for Groups of
  Users</title>
    <summary>  Consider a movie studio aiming to produce a set of new movies for summer
release: What types of movies it should produce? Who would the movies appeal
to? How many movies should it make? Similar issues are encountered by a variety
of organizations, e.g., mobile-phone manufacturers and online magazines, who
have to create new (non-existent) items to satisfy groups of users with
different preferences. In this paper, we present a joint problem formalization
of these interrelated issues, and propose generative methods that address these
questions simultaneously. Specifically, we leverage the latent space obtained
by training a deep generative model---the Variational Autoencoder (VAE)---via a
loss function that incorporates both rating performance and item reconstruction
terms. We then apply a greedy search algorithm that utilizes this learned
latent space to jointly obtain K plausible new items, and user groups that
would find the items appealing. An evaluation of our methods on a synthetic
dataset indicates that our approach is able to generate novel items similar to
highly-desirable unobserved items. As case studies on real-world data, we
applied our method on the MART abstract art and Movielens Tag Genome dataset,
which resulted in promising results: small and diverse sets of novel items.
</summary>
    <author>
      <name>Vinh Vo Thanh</name>
    </author>
    <author>
      <name>Harold Soh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures, Accepted to Recsys'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04854v3</id>
    <updated>2018-08-02T08:21:25Z</updated>
    <published>2018-06-13T05:45:22Z</published>
    <title>Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam</title>
    <summary>  Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization.
</summary>
    <author>
      <name>Mohammad Emtiyaz Khan</name>
    </author>
    <author>
      <name>Didrik Nielsen</name>
    </author>
    <author>
      <name>Voot Tangkaratt</name>
    </author>
    <author>
      <name>Wu Lin</name>
    </author>
    <author>
      <name>Yarin Gal</name>
    </author>
    <author>
      <name>Akash Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera ready version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Thirty-fifth International Conference on Machine Learning, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.04854v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04854v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04489v2</id>
    <updated>2018-08-02T07:59:27Z</updated>
    <published>2018-07-12T09:15:46Z</published>
    <title>Fast yet Simple Natural-Gradient Descent for Variational Inference in
  Complex Models</title>
    <summary>  Bayesian inference plays an important role in advancing machine learning, but
faces computational challenges when applied to complex models such as deep
neural networks. Variational inference circumvents these challenges by
formulating Bayesian inference as an optimization problem and solving it using
gradient-based optimization. In this paper, we argue in favor of
natural-gradient approaches which, unlike their gradient-based counterparts,
can improve convergence by exploiting the information geometry of the
solutions. We show how to derive fast yet simple natural-gradient updates by
using a duality associated with exponential-family distributions. An attractive
feature of these methods is that, by using natural-gradients, they are able to
extract accurate local approximations for individual model components. We
summarize recent results for Bayesian deep learning showing the superiority of
natural-gradient approaches over their gradient counterparts.
</summary>
    <author>
      <name>Mohammad Emtiyaz Khan</name>
    </author>
    <author>
      <name>Didrik Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera-ready version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Information Theory and Its Applications
  (ISITA), 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.04489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02357v1</id>
    <updated>2018-08-02T07:40:17Z</updated>
    <published>2018-08-02T07:40:17Z</published>
    <title>Acoustic Scene Classification: A Competition Review</title>
    <summary>  In this paper we study the problem of acoustic scene classification, i.e.,
categorization of audio sequences into mutually exclusive classes based on
their spectral content. We describe the methods and results discovered during a
competition organized in the context of a graduate machine learning course;
both by the students and external participants. We identify the most suitable
methods and study the impact of each by performing an ablation study of the
mixture of approaches. We also compare the results with a neural network
baseline, and show the improvement over that. Finally, we discuss the impact of
using a competition as a part of a university course, and justify its
importance in the curriculum based on student feedback.
</summary>
    <author>
      <name>Shayan Gharib</name>
    </author>
    <author>
      <name>Honain Derrar</name>
    </author>
    <author>
      <name>Daisuke Niizumi</name>
    </author>
    <author>
      <name>Tuukka Senttula</name>
    </author>
    <author>
      <name>Janne Tommola</name>
    </author>
    <author>
      <name>Toni Heittola</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <author>
      <name>Heikki Huttunen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been accepted in IEEE International Workshop on Machine
  Learning for Signal Processing (MLSP 2018). Copyright may be transferred
  without notice, after which this version may no longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00668v1</id>
    <updated>2018-08-02T04:58:49Z</updated>
    <published>2018-08-02T04:58:49Z</published>
    <title>On the achievability of blind source separation for high-dimensional
  nonlinear source mixtures</title>
    <summary>  For many years, a combination of principal component analysis (PCA) and
independent component analysis (ICA) has been used as a blind source separation
(BSS) technique to separate hidden sources of natural data. However, it is
unclear why these linear methods work well because most real-world data involve
nonlinear mixtures of sources. We show that a cascade of PCA and ICA can solve
this nonlinear BSS problem accurately as the variety of input signals
increases. Specifically, we present two theorems that guarantee asymptotically
zero-error BSS when sources are mixed by a feedforward network with two
processing layers. Our first theorem analytically quantifies the performance of
an optimal linear encoder that reconstructs independent sources. Zero-error is
asymptotically reached when the number of sources is large and the numbers of
inputs and nonlinear bases are large relative to the number of sources. The
next question involves finding an optimal linear encoder without observing the
underlying sources. Our second theorem guarantees that PCA can reliably extract
all the subspace represented by the optimal linear encoder, so that a
subsequent application of ICA can separate all sources. Thereby, for almost all
nonlinear generative processes with sufficient variety, the cascade of PCA and
ICA performs asymptotically zero-error BSS in an unsupervised manner. We
analytically and numerically validate the theorems. These results highlight the
utility of linear BSS techniques for accurately recovering nonlinearly mixed
sources when observations are sufficiently diverse. We also discuss a possible
biological BSS implementation.
</summary>
    <author>
      <name>Takuya Isomura</name>
    </author>
    <author>
      <name>Taro Toyoizumi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00665v1</id>
    <updated>2018-08-02T04:25:30Z</updated>
    <published>2018-08-02T04:25:30Z</published>
    <title>Investigating accuracy of pitch-accent annotations in neural
  network-based speech synthesis and denoising effects</title>
    <summary>  We investigated the impact of noisy linguistic features on the performance of
a Japanese speech synthesis system based on neural network that uses WaveNet
vocoder. We compared an ideal system that uses manually corrected linguistic
features including phoneme and prosodic information in training and test sets
against a few other systems that use corrupted linguistic features. Both
subjective and objective results demonstrate that corrupted linguistic
features, especially those in the test set, affected the ideal system's
performance significantly in a statistical sense due to a mismatched condition
between the training and test sets. Interestingly, while an utterance-level
Turing test showed that listeners had a difficult time differentiating
synthetic speech from natural speech, it further indicated that adding noise to
the linguistic features in the training set can partially reduce the effect of
the mismatch, regularize the model, and help the system perform better when
linguistic features of the test set are noisy.
</summary>
    <author>
      <name>Hieu-Thi Luong</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <author>
      <name>Nobuyuki Nishizawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00629v1</id>
    <updated>2018-08-02T01:54:26Z</updated>
    <published>2018-08-02T01:54:26Z</published>
    <title>Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models
  Using LIME</title>
    <summary>  We present a heuristic based algorithm to induce non-monotonic logic programs
that would explain the behavior of XGBoost trained classifiers. We use the LIME
technique to locally select the most important features contributing to the
classification decision. Then, in order to explain the model's global behavior,
we propose the UFOLD algorithm ---a heuristic-based ILP algorithm capable of
learning non-monotonic logic programs--- that we apply to a transformed dataset
produced by LIME. Our experiments with UCI standard benchmarks suggest a
significant improvement in terms of the classification evaluation metrics.
Meanwhile, the number of induced rules dramatically decreases compared ALEPH, a
state-of-the-art ILP system. While the proposed approach is agnostic to the
choice of ILP algorithm, our experiments suggest that the UFOLD algorithm
almost always outperforms ALEPH once incorporated in this approach.
</summary>
    <author>
      <name>Farhad Shakerin</name>
    </author>
    <author>
      <name>Gopal Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00628v1</id>
    <updated>2018-08-02T01:54:15Z</updated>
    <published>2018-08-02T01:54:15Z</published>
    <title>Fusion Subspace Clustering: Full and Incomplete Data</title>
    <summary>  Modern inference and learning often hinge on identifying low-dimensional
structures that approximate large scale data. Subspace clustering achieves this
through a union of linear subspaces. However, in contemporary applications data
is increasingly often incomplete, rendering standard (full-data) methods
inapplicable. On the other hand, existing incomplete-data methods present major
drawbacks, like lifting an already high-dimensional problem, or requiring a
super polynomial number of samples. Motivated by this, we introduce a new
subspace clustering algorithm inspired by fusion penalties. The main idea is to
permanently assign each datum to a subspace of its own, and minimize the
distance between the subspaces of all data, so that subspaces of the same
cluster get fused together. Our approach is entirely new to both, full and
missing data, and unlike other methods, it directly allows noise, it requires
no liftings, it allows low, high, and even full-rank data, it approaches
optimal (information-theoretic) sampling rates, and it does not rely on other
methods such as low-rank matrix completion to handle missing data. Furthermore,
our extensive experiments on both real and synthetic data show that our
approach performs comparably to the state-of-the-art with complete data, and
dramatically better if data is missing.
</summary>
    <author>
      <name>Daniel L. Pimentel-Alarcón</name>
    </author>
    <author>
      <name>Usman Mahmood</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00616v1</id>
    <updated>2018-08-02T01:09:24Z</updated>
    <published>2018-08-02T01:09:24Z</published>
    <title>Mixture Matrix Completion</title>
    <summary>  Completing a data matrix X has become an ubiquitous problem in modern data
science, with applications in recommender systems, computer vision, and
networks inference, to name a few. One typical assumption is that X is
low-rank. A more general model assumes that each column of X corresponds to one
of several low-rank matrices. This paper generalizes these models to what we
call mixture matrix completion (MMC): the case where each entry of X
corresponds to one of several low-rank matrices. MMC is a more accurate model
for recommender systems, and brings more flexibility to other completion and
clustering problems. We make four fundamental contributions about this new
model. First, we show that MMC is theoretically possible (well-posed). Second,
we give its precise information-theoretic identifiability conditions. Third, we
derive the sample complexity of MMC. Finally, we give a practical algorithm for
MMC with performance comparable to the state-of-the-art for simpler related
problems, both on synthetic and real data.
</summary>
    <author>
      <name>Daniel L. Pimentel-Alarcón</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00601v1</id>
    <updated>2018-08-01T23:56:28Z</updated>
    <published>2018-08-01T23:56:28Z</published>
    <title>Classification of Building Information Model (BIM) Structures with Deep
  Learning</title>
    <summary>  In this work we study an application of machine learning to the construction
industry and we use classical and modern machine learning methods to categorize
images of building designs into three classes: Apartment building, Industrial
building or Other. No real images are used, but only images extracted from
Building Information Model (BIM) software, as these are used by the
construction industry to store building designs. For this task, we compared
four different methods: the first is based on classical machine learning, where
Histogram of Oriented Gradients (HOG) was used for feature extraction and a
Support Vector Machine (SVM) for classification; the other three methods are
based on deep learning, covering common pre-trained networks as well as ones
designed from scratch. To validate the accuracy of the models, a database of
240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and
above 89% for the neural networks.
</summary>
    <author>
      <name>Francesco Lomio</name>
    </author>
    <author>
      <name>Ricardo Farinha</name>
    </author>
    <author>
      <name>Mauri Laasonen</name>
    </author>
    <author>
      <name>Heikki Huttunen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05597v2</id>
    <updated>2018-08-01T23:41:26Z</updated>
    <published>2017-11-15T14:46:27Z</published>
    <title>Advances in Variational Inference</title>
    <summary>  Many modern unsupervised or semi-supervised machine learning algorithms rely
on Bayesian probabilistic models. These models are usually intractable and thus
require approximate inference. Variational inference (VI) lets us approximate a
high-dimensional Bayesian posterior with a simpler variational distribution by
solving an optimization problem. This approach has been successfully used in
various models and large-scale applications. In this review, we give an
overview of recent trends in variational inference. We first introduce standard
mean field variational inference, then review recent advances focusing on the
following aspects: (a) scalable VI, which includes stochastic approximations,
(b) generic VI, which extends the applicability of VI to a large class of
otherwise intractable models, such as non-conjugate models, (c) accurate VI,
which includes variational models beyond the mean field approximation or with
atypical divergences, and (d) amortized VI, which implements the inference over
local latent variables with inference networks. Finally, we provide a summary
of promising future research directions.
</summary>
    <author>
      <name>Cheng Zhang</name>
    </author>
    <author>
      <name>Judith Butepage</name>
    </author>
    <author>
      <name>Hedvig Kjellstrom</name>
    </author>
    <author>
      <name>Stephan Mandt</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05597v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05597v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00590v1</id>
    <updated>2018-08-01T22:45:48Z</updated>
    <published>2018-08-01T22:45:48Z</published>
    <title>MLCapsule: Guarded Offline Deployment of Machine Learning as a Service</title>
    <summary>  With the widespread use of machine learning (ML) techniques, ML as a service
has become increasingly popular. In this setting, an ML model resides on a
server and users can query the model with their data via an API. However, if
the user's input is sensitive, sending it to the server is not an option.
Equally, the service provider does not want to share the model by sending it to
the client for protecting its intellectual property and pay-per-query business
model. In this paper, we propose MLCapsule, a guarded offline deployment of
machine learning as a service. MLCapsule executes the machine learning model
locally on the user's client and therefore the data never leaves the client.
Meanwhile, MLCapsule offers the service provider the same level of control and
security of its model as the commonly used server-side execution. In addition,
MLCapsule is applicable to offline applications that require local execution.
Beyond protecting against direct model access, we demonstrate that MLCapsule
allows for implementing defenses against advanced attacks on machine learning
models such as model stealing/reverse engineering and membership inference.
</summary>
    <author>
      <name>Lucjan Hanzlik</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Kathrin Grosse</name>
    </author>
    <author>
      <name>Ahmed Salem</name>
    </author>
    <author>
      <name>Max Augustin</name>
    </author>
    <author>
      <name>Michael Backes</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00563v1</id>
    <updated>2018-08-01T21:00:50Z</updated>
    <published>2018-08-01T21:00:50Z</published>
    <title>Data Augmentation for Robust Keyword Spotting under Playback
  Interference</title>
    <summary>  Accurate on-device keyword spotting (KWS) with low false accept and false
reject rate is crucial to customer experience for far-field voice control of
conversational agents. It is particularly challenging to maintain low false
reject rate in real world conditions where there is (a) ambient noise from
external sources such as TV, household appliances, or other speech that is not
directed at the device (b) imperfect cancellation of the audio playback from
the device, resulting in residual echo, after being processed by the Acoustic
Echo Cancellation (AEC) system. In this paper, we propose a data augmentation
strategy to improve keyword spotting performance under these challenging
conditions. The training set audio is artificially corrupted by mixing in music
and TV/movie audio, at different signal to interference ratios. Our results
show that we get around 30-45% relative reduction in false reject rates, at a
range of false alarm rates, under audio playback from such devices.
</summary>
    <author>
      <name>Anirudh Raju</name>
    </author>
    <author>
      <name>Sankaran Panchapagesan</name>
    </author>
    <author>
      <name>Xing Liu</name>
    </author>
    <author>
      <name>Arindam Mandal</name>
    </author>
    <author>
      <name>Nikko Strom</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00560v1</id>
    <updated>2018-08-01T20:55:54Z</updated>
    <published>2018-08-01T20:55:54Z</published>
    <title>Spectral Mixture Kernels with Time and Phase Delay Dependencies</title>
    <summary>  Spectral Mixture (SM) kernels form a powerful class of kernels for Gaussian
processes, capable to discover patterns, extrapolate, and model negative
co-variances. In SM kernels, spectral mixture components are linearly combined
to construct a final flexible kernel. As a consequence SM kernels does not
explicitly model correlations between components and dependencies related to
time and phase delays between components, because only the auto-convolution of
base components are used. To address these drawbacks we introduce Generalized
Convolution Spectral Mixture (GCSM) kernels. We incorporate time and phase
delay into the base spectral mixture and use cross-convolution between a base
component and the complex conjugate of another base component to construct a
complex-valued and positive definite kernel representing correlations between
base components. In this way the total number of components in GCSM becomes
quadratic. We perform a thorough comparative experimental analysis of GCSM on
synthetic and real-life datasets. Results indicate the beneficial effect of the
extra features of GCSM. This is illustrated in the problem of forecasting the
long range trend of a river flow to monitor environment evolution, where GCSM
is capable of discovering correlated patterns that SM cannot and improving
patterns recognition ability of SM.
</summary>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Perry Groot</name>
    </author>
    <author>
      <name>Jinsong Chen</name>
    </author>
    <author>
      <name>Elena Marchiori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00529v1</id>
    <updated>2018-08-01T19:41:04Z</updated>
    <published>2018-08-01T19:41:04Z</published>
    <title>Open Category Detection with PAC Guarantees</title>
    <summary>  Open category detection is the problem of detecting "alien" test instances
that belong to categories or classes that were not present in the training
data. In many applications, reliably detecting such aliens is central to
ensuring the safety and accuracy of test set predictions. Unfortunately, there
are no algorithms that provide theoretical guarantees on their ability to
detect aliens under general assumptions. Further, while there are algorithms
for open category detection, there are few empirical results that directly
report alien detection rates. Thus, there are significant theoretical and
empirical gaps in our understanding of open category detection. In this paper,
we take a step toward addressing this gap by studying a simple, but
practically-relevant variant of open category detection. In our setting, we are
provided with a "clean" training set that contains only the target categories
of interest and an unlabeled "contaminated" training set that contains a
fraction $\alpha$ of alien examples. Under the assumption that we know an upper
bound on $\alpha$, we develop an algorithm with PAC-style guarantees on the
alien detection rate, while aiming to minimize false alarms. Empirical results
on synthetic and standard benchmark datasets demonstrate the regimes in which
the algorithm can be effective and provide a baseline for further advancements.
</summary>
    <author>
      <name>Si Liu</name>
    </author>
    <author>
      <name>Risheek Garrepalli</name>
    </author>
    <author>
      <name>Thomas G. Dietterich</name>
    </author>
    <author>
      <name>Alan Fern</name>
    </author>
    <author>
      <name>Dan Hendrycks</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00523v1</id>
    <updated>2018-08-01T19:28:03Z</updated>
    <published>2018-08-01T19:28:03Z</published>
    <title>Mod-DeepESN: Modular Deep Echo State Network</title>
    <summary>  Neuro-inspired recurrent neural network algorithms, such as echo state
networks, are computationally lightweight and thereby map well onto untethered
devices. The baseline echo state network algorithms are shown to be efficient
in solving small-scale spatio-temporal problems. However, they underperform for
complex tasks that are characterized by multi-scale structures. In this
research, an intrinsic plasticity-infused modular deep echo state network
architecture is proposed to solve complex and multiple timescale temporal
tasks. It outperforms state-of-the-art for time series prediction tasks.
</summary>
    <author>
      <name>Zachariah Carmichael</name>
    </author>
    <author>
      <name>Humza Syed</name>
    </author>
    <author>
      <name>Stuart Burtner</name>
    </author>
    <author>
      <name>Dhireesha Kudithipudi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, CCN 2018 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00516v1</id>
    <updated>2018-08-01T19:12:22Z</updated>
    <published>2018-08-01T19:12:22Z</published>
    <title>A Learning-Based Framework for Two-Dimensional Vehicle Maneuver
  Prediction over V2V Networks</title>
    <summary>  Situational awareness in vehicular networks could be substantially improved
utilizing reliable trajectory prediction methods. More precise situational
awareness, in turn, results in notably better performance of critical safety
applications, such as Forward Collision Warning (FCW), as well as comfort
applications like Cooperative Adaptive Cruise Control (CACC). Therefore,
vehicle trajectory prediction problem needs to be deeply investigated in order
to come up with an end to end framework with enough precision required by the
safety applications' controllers. This problem has been tackled in the
literature using different methods. However, machine learning, which is a
promising and emerging field with remarkable potential for time series
prediction, has not been explored enough for this purpose. In this paper, a
two-layer neural network-based system is developed which predicts the future
values of vehicle parameters, such as velocity, acceleration, and yaw rate, in
the first layer and then predicts the two-dimensional, i.e. longitudinal and
lateral, trajectory points based on the first layer's outputs. The performance
of the proposed framework has been evaluated in realistic cut-in scenarios from
Safety Pilot Model Deployment (SPMD) dataset and the results show a noticeable
improvement in the prediction accuracy in comparison with the kinematics model
which is the dominant employed model by the automotive industry. Both ideal and
nonideal communication circumstances have been investigated for our system
evaluation. For non-ideal case, an estimation step is included in the framework
before the parameter prediction block to handle the drawbacks of packet drops
or sensor failures and reconstruct the time series of vehicle parameters at a
desirable frequency.
</summary>
    <author>
      <name>Hossein Nourkhiz Mahjoub</name>
    </author>
    <author>
      <name>Amin Tahmasbi-Sarvestani</name>
    </author>
    <author>
      <name>Hadi Kazemi</name>
    </author>
    <author>
      <name>Yaser P. Fallah</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DASC-PICom-DataCom-CyberSciTec.2017.39</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DASC-PICom-DataCom-CyberSciTec.2017.39" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE Cyber Science and Technology Congress(CyberSciTech),
  Orlando, FL, 2017, pp. 156-163</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00342v6</id>
    <updated>2018-08-01T18:40:19Z</updated>
    <published>2017-11-01T13:42:54Z</published>
    <title>Orthogonal Machine Learning: Power and Limitations</title>
    <summary>  Double machine learning provides $\sqrt{n}$-consistent estimates of
parameters of interest even when high-dimensional or nonparametric nuisance
parameters are estimated at an $n^{-1/4}$ rate. The key is to employ
Neyman-orthogonal moment equations which are first-order insensitive to
perturbations in the nuisance parameters. We show that the $n^{-1/4}$
requirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order
notion of orthogonality that grants robustness to more complex or
higher-dimensional nuisance parameters. In the partially linear regression
setting popular in causal inference, we show that we can construct second-order
orthogonal moments if and only if the treatment residual is not normally
distributed. Our proof relies on Stein's lemma and may be of independent
interest. We conclude by demonstrating the robustness benefits of an explicit
doubly-orthogonal estimation procedure for treatment effect.
</summary>
    <author>
      <name>Lester Mackey</name>
    </author>
    <author>
      <name>Vasilis Syrgkanis</name>
    </author>
    <author>
      <name>Ilias Zadik</name>
    </author>
    <link href="http://arxiv.org/abs/1711.00342v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00342v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00496v1</id>
    <updated>2018-08-01T18:28:12Z</updated>
    <published>2018-08-01T18:28:12Z</published>
    <title>SlimNets: An Exploration of Deep Model Compression and Acceleration</title>
    <summary>  Deep neural networks have achieved increasingly accurate results on a wide
variety of complex tasks. However, much of this improvement is due to the
growing use and availability of computational resources (e.g use of GPUs, more
layers, more parameters, etc). Most state-of-the-art deep networks, despite
performing well, over-parameterize approximate functions and take a significant
amount of time to train. With increased focus on deploying deep neural networks
on resource constrained devices like smart phones, there has been a push to
evaluate why these models are so resource hungry and how they can be made more
efficient. This work evaluates and compares three distinct methods for deep
model compression and acceleration: weight pruning, low rank factorization, and
knowledge distillation. Comparisons on VGG nets trained on CIFAR10 show that
each of the models on their own are effective, but that the true power lies in
combining them. We show that by combining pruning and knowledge distillation
methods we can create a compressed network 85 times smaller than the original,
all while retaining 96% of the original model's accuracy.
</summary>
    <author>
      <name>Ini Oguntola</name>
    </author>
    <author>
      <name>Subby Olubeko</name>
    </author>
    <author>
      <name>Christopher Sweeney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in 2018 IEEE High Performance Extreme Computing
  Conference (HPEC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00490v1</id>
    <updated>2018-08-01T18:16:57Z</updated>
    <published>2018-08-01T18:16:57Z</published>
    <title>Deep Reinforcement Learning for Distributed Dynamic Power Allocation in
  Wireless Networks</title>
    <summary>  This work demonstrates the potential of deep reinforcement learning
techniques for transmit power control in emerging and future wireless networks.
Various techniques have been proposed in the literature to find near-optimal
power allocations, often by solving a challenging optimization problem. Most of
these algorithms are not scalable to large networks in real-world scenarios
because of their computational complexity and instantaneous cross-cell channel
state information (CSI) requirement. In this paper, a model-free distributed
dynamic power allocation scheme is developed based on deep reinforcement
learning. Each transmitter collects CSI and quality of service (QoS)
information from several neighbors and adapts its own transmit power
accordingly. The objective is to maximize a weighted sum-rate utility function,
which can be particularized to achieve maximum sum-rate or proportionally fair
scheduling (with weights that are changing over time). Both random variations
and delays in the CSI are inherently addressed using deep Q-learning. For a
typical network architecture, the proposed algorithm is shown to achieve
near-optimal power allocation in real time based on delayed CSI measurements
available to the agents. This work indicates that deep reinforcement learning
based radio resource management can be very fast and deliver highly competitive
performance, especially in practical scenarios where the system model is
inaccurate and CSI delay is non-negligible.
</summary>
    <author>
      <name>Yasar Sinan Nasir</name>
    </author>
    <author>
      <name>Dongning Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 6 figures, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00441v1</id>
    <updated>2018-08-01T17:41:23Z</updated>
    <published>2018-08-01T17:41:23Z</published>
    <title>Matrix completion and extrapolation via kernel regression</title>
    <summary>  Matrix completion and extrapolation (MCEX) are dealt with here over
reproducing kernel Hilbert spaces (RKHSs) in order to account for prior
information present in the available data. Aiming at a faster and
low-complexity solver, the task is formulated as a kernel ridge regression. The
resultant MCEX algorithm can also afford online implementation, while the class
of kernel functions also encompasses several existing approaches to MC with
prior information. Numerical tests on synthetic and real datasets show that the
novel approach performs faster than widespread methods such as alternating
least squares (ALS) or stochastic gradient descent (SGD), and that the recovery
error is reduced, especially when dealing with noisy data.
</summary>
    <author>
      <name>Pere Giménez-Febrer</name>
    </author>
    <author>
      <name>Alba Pagès-Zamora</name>
    </author>
    <author>
      <name>Georgios B. Giannakis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07445v3</id>
    <updated>2018-08-01T17:25:51Z</updated>
    <published>2018-05-18T21:11:58Z</published>
    <title>DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors</title>
    <summary>  Boltzmann machines are powerful distributions that have been shown to be an
effective prior over binary latent variables in variational autoencoders
(VAEs). However, previous methods for training discrete VAEs have used the
evidence lower bound and not the tighter importance-weighted bound. We propose
two approaches for relaxing Boltzmann machines to continuous distributions that
permit training with importance-weighted bounds. These relaxations are based on
generalized overlapping transformations and the Gaussian integral trick.
Experiments on the MNIST and OMNIGLOT datasets show that these relaxations
outperform previous discrete VAEs with Boltzmann priors. An implementation
which reproduces these results is available at
https://github.com/QuadrantAI/dvae .
</summary>
    <author>
      <name>Arash Vahdat</name>
    </author>
    <author>
      <name>Evgeny Andriyash</name>
    </author>
    <author>
      <name>William G. Macready</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added a new figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07445v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07445v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00423v1</id>
    <updated>2018-08-01T17:04:48Z</updated>
    <published>2018-08-01T17:04:48Z</published>
    <title>Seq2Seq and Multi-Task Learning for joint intent and content extraction
  for domain specific interpreters</title>
    <summary>  This study evaluates the performances of an LSTM network for detecting and
extracting the intent and content of com- mands for a financial chatbot. It
presents two techniques, sequence to sequence learning and Multi-Task Learning,
which might improve on the previous task.
</summary>
    <author>
      <name>Marc Velay</name>
    </author>
    <author>
      <name>Fabrice Daniel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00418v1</id>
    <updated>2018-08-01T17:00:43Z</updated>
    <published>2018-08-01T17:00:43Z</published>
    <title>Stock Chart Pattern recognition with Deep Learning</title>
    <summary>  This study evaluates the performances of CNN and LSTM for recognizing common
charts patterns in a stock historical data. It presents two common patterns,
the method used to build the training set, the neural networks architectures
and the accuracies obtained.
</summary>
    <author>
      <name>Marc Velay</name>
    </author>
    <author>
      <name>Fabrice Daniel</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00408v1</id>
    <updated>2018-08-01T16:33:20Z</updated>
    <published>2018-08-01T16:33:20Z</published>
    <title>Geometry of energy landscapes and the optimizability of deep neural
  networks</title>
    <summary>  Deep neural networks are workhorse models in machine learning with multiple
layers of non-linear functions composed in series. Their loss function is
highly non-convex, yet empirically even gradient descent minimisation is
sufficient to arrive at accurate and predictive models. It is hitherto unknown
why are deep neural networks easily optimizable. We analyze the energy
landscape of a spin glass model of deep neural networks using random matrix
theory and algebraic geometry. We analytically show that the multilayered
structure holds the key to optimizability: Fixing the number of parameters and
increasing network depth, the number of stationary points in the loss function
decreases, minima become more clustered in parameter space, and the tradeoff
between the depth and width of minima becomes less severe. Our analytical
results are numerically verified through comparison with neural networks
trained on a set of classical benchmark datasets. Our model uncovers generic
design principles of machine learning models.
</summary>
    <author>
      <name>Simon Becker</name>
    </author>
    <author>
      <name>Yao Zhang</name>
    </author>
    <author>
      <name>Alpha A. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02125v2</id>
    <updated>2018-08-01T16:22:55Z</updated>
    <published>2018-07-05T18:00:34Z</published>
    <title>Scalable Gaussian Processes with Grid-Structured Eigenfunctions
  (GP-GRIEF)</title>
    <summary>  We introduce a kernel approximation strategy that enables computation of the
Gaussian process log marginal likelihood and all hyperparameter derivatives in
$\mathcal{O}(p)$ time. Our GRIEF kernel consists of $p$ eigenfunctions found
using a Nystrom approximation from a dense Cartesian product grid of inducing
points. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor
products, computational complexity of the training procedure can be practically
independent of the number of inducing points. This allows us to use arbitrarily
many inducing points to achieve a globally accurate kernel approximation, even
in high-dimensional problems. The fast likelihood evaluation enables type-I or
II Bayesian inference on large-scale datasets. We benchmark our algorithms on
real-world problems with up to two-million training points and $10^{33}$
inducing points.
</summary>
    <author>
      <name>Trefor W. Evans</name>
    </author>
    <author>
      <name>Prasanth B. Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the proceedings of the International Conference on Machine
  Learning (ICML), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02125v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02125v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00387v1</id>
    <updated>2018-08-01T15:50:23Z</updated>
    <published>2018-08-01T15:50:23Z</published>
    <title>Just Interpolate: Kernel "Ridgeless" Regression Can Generalize</title>
    <summary>  In the absence of explicit regularization, Kernel "Ridgeless" Regression with
nonlinear kernels has the potential to fit the training data perfectly. It has
been observed empirically, however, that such interpolated solutions can still
generalize well on test data. We isolate a phenomenon of implicit
regularization for minimum-norm interpolated solutions which is due to a
combination of high dimensionality of the input data, curvature of the kernel
function, and favorable geometric properties of the data such as an eigenvalue
decay of the empirical covariance and kernel matrices. In addition to deriving
a data-dependent upper bound on the out-of-sample error, we present
experimental evidence suggesting that the phenomenon occurs in the MNIST
dataset.
</summary>
    <author>
      <name>Tengyuan Liang</name>
    </author>
    <author>
      <name>Alexander Rakhlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00380v1</id>
    <updated>2018-08-01T15:38:08Z</updated>
    <published>2018-08-01T15:38:08Z</published>
    <title>A Differentially Private Kernel Two-Sample Test</title>
    <summary>  Kernel two-sample testing is a useful statistical tool in determining whether
data samples arise from different distributions without imposing any parametric
assumptions on those distributions. However, raw data samples can expose
sensitive information about individuals who participate in scientific studies,
which makes the current tests vulnerable to privacy breaches. Hence, we design
a new framework for kernel two-sample testing conforming to differential
privacy constraints, in order to guarantee the privacy of subjects in the data.
Unlike existing differentially private parametric tests that simply add noise
to data, kernel-based testing imposes a challenge due to a complex dependence
of test statistics on the raw data, as these statistics correspond to
estimators of distances between representations of probability measures in
Hilbert spaces. Our approach considers finite dimensional approximations to
those representations. As a result, a simple chi-squared test is obtained,
where a test statistic depends on a mean and covariance of empirical
differences between the samples, which we perturb for a privacy guarantee. We
investigate the utility of our framework in two realistic settings and conclude
that our method requires only a relatively modest increase in sample size to
achieve a similar level of power to the non-private tests in both settings.
</summary>
    <author>
      <name>Anant Raj</name>
    </author>
    <author>
      <name>Ho Chung Leon Law</name>
    </author>
    <author>
      <name>Dino Sejdinovic</name>
    </author>
    <author>
      <name>Mijung Park</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09571v2</id>
    <updated>2018-08-01T15:28:05Z</updated>
    <published>2018-06-25T17:12:53Z</published>
    <title>Asymptotic Properties of Recursive Maximum Likelihood Estimation in
  Non-Linear State-Space Models</title>
    <summary>  Using stochastic gradient search and the optimal filter derivative, it is
possible to perform recursive (i.e., online) maximum likelihood estimation in a
non-linear state-space model. As the optimal filter and its derivative are
analytically intractable for such a model, they need to be approximated
numerically. In [Poyiadjis, Doucet and Singh, Biometrika 2018], a recursive
maximum likelihood algorithm based on a particle approximation to the optimal
filter derivative has been proposed and studied through numerical simulations.
Here, this algorithm and its asymptotic behavior are analyzed theoretically. We
show that the algorithm accurately estimates maxima to the underlying (average)
log-likelihood when the number of particles is sufficiently large. We also
derive (relatively) tight bounds on the estimation error. The obtained results
hold under (relatively) mild conditions and cover several classes of non-linear
state-space models met in practice.
</summary>
    <author>
      <name>Vladislav Z. B. Tadic</name>
    </author>
    <author>
      <name>Arnaud Doucet</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09571v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09571v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00361v1</id>
    <updated>2018-08-01T15:13:28Z</updated>
    <published>2018-08-01T15:13:28Z</published>
    <title>Structured Differential Learning for Automatic Threshold Setting</title>
    <summary>  We introduce a technique that can automatically tune the parameters of a
rule-based computer vision system comprised of thresholds, combinational logic,
and time constants. This lets us retain the flexibility and perspicacity of a
conventionally structured system while allowing us to perform approximate
gradient descent using labeled data. While this is only a heuristic procedure,
as far as we are aware there is no other efficient technique for tuning such
systems. We describe the components of the system and the associated supervised
learning mechanism. We also demonstrate the utility of the algorithm by
comparing its performance versus hand tuning for an automotive headlight
controller. Despite having over 100 parameters, the method is able to
profitably adjust the system values given just the desired output for a number
of videos.
</summary>
    <author>
      <name>Jonathan Connell</name>
    </author>
    <author>
      <name>Benjamin Herta</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01179v4</id>
    <updated>2018-08-01T15:11:22Z</updated>
    <published>2017-09-04T22:00:16Z</published>
    <title>Continuous-Time Flows for Efficient Inference and Density Estimation</title>
    <summary>  Two fundamental problems in unsupervised learning are efficient inference for
latent-variable models and robust density estimation based on large amounts of
unlabeled data. Algorithms for the two tasks, such as normalizing flows and
generative adversarial networks (GANs), are often developed independently. In
this paper, we propose the concept of {\em continuous-time flows} (CTFs), a
family of diffusion-based methods that are able to asymptotically approach a
target distribution. Distinct from normalizing flows and GANs, CTFs can be
adopted to achieve the above two goals in one framework, with theoretical
guarantees. Our framework includes distilling knowledge from a CTF for
efficient inference, and learning an explicit energy-based distribution with
CTFs for density estimation. Both tasks rely on a new technique for
distribution matching within amortized learning. Experiments on various tasks
demonstrate promising performance of the proposed CTF framework, compared to
related techniques.
</summary>
    <author>
      <name>Changyou Chen</name>
    </author>
    <author>
      <name>Chunyuan Li</name>
    </author>
    <author>
      <name>Liqun Chen</name>
    </author>
    <author>
      <name>Wenlin Wang</name>
    </author>
    <author>
      <name>Yunchen Pu</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018 (fixed a reference)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01179v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01179v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09518v3</id>
    <updated>2018-08-01T14:20:53Z</updated>
    <published>2018-03-26T11:36:24Z</published>
    <title>Fréchet ChemNet Distance: A metric for generative models for molecules
  in drug discovery</title>
    <summary>  The new wave of successful generative models in machine learning has
increased the interest in deep learning driven de novo drug design. However,
assessing the performance of such generative models is notoriously difficult.
Metrics that are typically used to assess the performance of such generative
models are the percentage of chemically valid molecules or the similarity to
real molecules in terms of particular descriptors, such as the partition
coefficient (logP) or druglikeness. However, method comparison is difficult
because of the inconsistent use of evaluation metrics, the necessity for
multiple metrics, and the fact that some of these measures can easily be
tricked by simple rule-based systems. We propose a novel distance measure
between two sets of molecules, called Fr\'echet ChemNet distance (FCD), that
can be used as an evaluation metric for generative models. The FCD is similar
to a recently established performance metric for comparing image generation
methods, the Fr\'echet Inception Distance (FID). Whereas the FID uses one of
the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a
deep neural network called ChemNet, which was trained to predict drug
activities. Thus, the FCD metric takes into account chemically and biologically
relevant information about molecules, and also measures the diversity of the
set via the distribution of generated molecules. The FCD's advantage over
previous metrics is that it can detect if generated molecules are a) diverse
and have similar b) chemical and c) biological properties as real molecules. We
further provide an easy-to-use implementation that only requires the SMILES
representation of the generated molecules as input to calculate the FCD.
Implementations are available at: https://www.github.com/bioinf-jku/FCD
</summary>
    <author>
      <name>Kristina Preuer</name>
    </author>
    <author>
      <name>Philipp Renz</name>
    </author>
    <author>
      <name>Thomas Unterthiner</name>
    </author>
    <author>
      <name>Sepp Hochreiter</name>
    </author>
    <author>
      <name>Günter Klambauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Implementations are available at:
  https://www.github.com/bioinf-jku/FCD</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09518v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09518v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00309v1</id>
    <updated>2018-08-01T13:13:46Z</updated>
    <published>2018-08-01T13:13:46Z</published>
    <title>Model-order selection in statistical shape models</title>
    <summary>  Statistical shape models enhance machine learning algorithms providing prior
information about deformation. A Point Distribution Model (PDM) is a popular
landmark-based statistical shape model for segmentation. It requires choosing a
model order, which determines how much of the variation seen in the training
data is accounted for by the PDM. A good choice of the model order depends on
the number of training samples and the noise level in the training data set.
Yet the most common approach for choosing the model order simply keeps a
predetermined percentage of the total shape variation. In this paper, we
present a technique for choosing the model order based on information-theoretic
criteria, and we show empirical evidence that the model order chosen by this
technique provides a good trade-off between over- and underfitting.
</summary>
    <author>
      <name>Alma Eguizabal</name>
    </author>
    <author>
      <name>Peter J. Schreier</name>
    </author>
    <author>
      <name>David Ramírez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in 2018 IEEE International Workshop on Machine Learning for
  Signal Processing, Sept.\ 17--20, 2018, Aalborg, Denmark</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11908v2</id>
    <updated>2018-08-01T10:00:45Z</updated>
    <published>2018-05-30T11:42:44Z</published>
    <title>Who Learns Better Bayesian Network Structures: Constraint-Based,
  Score-based or Hybrid Algorithms?</title>
    <summary>  The literature groups algorithms to learn the structure of Bayesian networks
from data in three separate classes: constraint-based algorithms, which use
conditional independence tests to learn the dependence structure of the data;
score-based algorithms, which use goodness-of-fit scores as objective functions
to maximise; and hybrid algorithms that combine both approaches. Famously,
Cowell (2001) showed that algorithms in the first two classes learn the same
structures when the topological ordering of the network is known and we use
entropy to assess conditional independence and goodness of fit.
  In this paper we address the complementary question: how do these classes of
algorithms perform outside of the assumptions above? We approach this question
by recognising that structure learning is defined by the combination of a
statistical criterion and an algorithm that determines how the criterion is
applied to the data. Removing the confounding effect of different choices for
the statistical criterion, we find using both simulated and real-world data
that constraint-based algorithms do not appear to be more efficient or more
sensitive to errors than score-based algorithms; and that hybrid algorithms are
not faster or more accurate than constraint-based algorithms. This suggests
that commonly held beliefs on structure learning in the literature are strongly
influenced by the choice of particular statistical criteria rather than just
properties of the algorithms themselves.
</summary>
    <author>
      <name>Marco Scutari</name>
    </author>
    <author>
      <name>Catharina Elisabeth Graafland</name>
    </author>
    <author>
      <name>José Manuel Gutiérrez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11908v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11908v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05678v2</id>
    <updated>2018-08-01T09:27:23Z</updated>
    <published>2016-12-16T23:05:31Z</published>
    <title>Causal Discovery as Semi-Supervised Learning</title>
    <summary>  We frame causal discovery as a semi-supervised machine learning task. The
idea is to allow direct learning of a causal graph by treating indicators of
causal influence between variables as "labels". Available data on the variables
of interest are used to provide features for the labelling task. Background
knowledge or any available interventional data provide labels on some edges in
the graph and the remaining edges are treated as unlabelled. To illustrate the
key ideas, we develop a distance-based approach (based on simple bivariate
histograms) within a semi-supervised manifold regularization framework. We
present empirical results on three different biological datasets (including
data where causal effects can be verified by experimental intervention), which
demonstrate the efficacy and highly general nature of the approach as well as
its simplicity from a user's point of view.
</summary>
    <author>
      <name>Chris. J. Oates</name>
    </author>
    <author>
      <name>Steven M. Hill</name>
    </author>
    <author>
      <name>Duncan A. Blythe</name>
    </author>
    <author>
      <name>Sach Mukherjee</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05678v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05678v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00232v1</id>
    <updated>2018-08-01T08:56:32Z</updated>
    <published>2018-08-01T08:56:32Z</published>
    <title>Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error
  Reduction via Surrogate Policy</title>
    <summary>  When learning from a batch of logged bandit feedback, the discrepancy between
the policy to be learned and the off-policy training data imposes statistical
and computational challenges. Unlike classical supervised learning and online
learning settings, in batch contextual bandit learning, one only has access to
a collection of logged feedback from the actions taken by a historical policy,
and expect to learn a policy that takes good actions in possibly unseen
contexts. Such a batch learning setting is ubiquitous in online and interactive
systems, such as ad platforms and recommendation systems. Existing approaches
based on inverse propensity weights, such as Inverse Propensity Scoring (IPS)
and Policy Optimizer for Exponential Models (POEM), enjoy unbiasedness but
often suffer from large mean squared error. In this work, we introduce a new
approach named Maximum Likelihood Inverse Propensity Scoring (MLIPS) for batch
learning from logged bandit feedback. Instead of using the given historical
policy as the proposal in inverse propensity weights, we estimate a maximum
likelihood surrogate policy based on the logged action-context pairs, and then
use this surrogate policy as the proposal. We prove that MLIPS is
asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared
error than IPS. Such an error reduction phenomenon is somewhat surprising as
the estimated surrogate policy is less accurate than the given historical
policy. Results on multi-label classification problems and a large- scale ad
placement dataset demonstrate the empirical effectiveness of MLIPS.
Furthermore, the proposed surrogate policy technique is complementary to
existing error reduction techniques, and when combined, is able to consistently
boost the performance of several widely used approaches.
</summary>
    <author>
      <name>Yuan Xie</name>
    </author>
    <author>
      <name>Boyi Liu</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Zhaoran Wang</name>
    </author>
    <author>
      <name>Yuan Zhou</name>
    </author>
    <author>
      <name>Jian Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00212v1</id>
    <updated>2018-08-01T08:00:33Z</updated>
    <published>2018-08-01T08:00:33Z</published>
    <title>Model selection by minimum description length: Lower-bound sample sizes
  for the Fisher information approximation</title>
    <summary>  The Fisher information approximation (FIA) is an implementation of the
minimum description length principle for model selection. Unlike information
criteria such as AIC or BIC, it has the advantage of taking the functional form
of a model into account. Unfortunately, FIA can be misleading in finite
samples, resulting in an inversion of the correct rank order of complexity
terms for competing models in the worst case. As a remedy, we propose a
lower-bound $N'$ for the sample size that suffices to preclude such errors. We
illustrate the approach using three examples from the family of multinomial
processing tree models.
</summary>
    <author>
      <name>Daniel W. Heck</name>
    </author>
    <author>
      <name>Morten Moshagen</name>
    </author>
    <author>
      <name>Edgar Erdfelder</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jmp.2014.06.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jmp.2014.06.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Mathematical Psychology (2014) 60, 29-34</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00209v1</id>
    <updated>2018-08-01T07:48:26Z</updated>
    <published>2018-08-01T07:48:26Z</published>
    <title>Binarized Convolutional Neural Networks for Efficient Inference on GPUs</title>
    <summary>  Convolutional neural networks have recently achieved significant
breakthroughs in various image classification tasks. However, they are
computationally expensive,which can make their feasible mplementation on
embedded and low-power devices difficult. In this paper convolutional neural
network binarization is implemented on GPU-based platforms for real-time
inference on resource constrained devices. In binarized networks, all weights
and intermediate computations between layers are quantized to +1 and -1,
allowing multiplications and additions to be replaced with bit-wise operations
between 32-bit words. This representation completely eliminates the need for
floating point multiplications and additions and decreases both the
computational load and the memory footprint compared to a full-precision
network implemented in floating point, making it well-suited for
resource-constrained environments. We compare the performance of our
implementation with an equivalent floating point implementation on one desktop
and two embedded GPU platforms. Our implementation achieves a maximum speed up
of 7. 4X with only 4.4% loss in accuracy compared to a reference
implementation.
</summary>
    <author>
      <name>Mir Khan</name>
    </author>
    <author>
      <name>Heikki Huttunen</name>
    </author>
    <author>
      <name>Jani Boutellier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE EUSIPCO 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00200v1</id>
    <updated>2018-08-01T07:14:57Z</updated>
    <published>2018-08-01T07:14:57Z</published>
    <title>Anomaly Detection via Minimum Likelihood Generative Adversarial Networks</title>
    <summary>  Anomaly detection aims to detect abnormal events by a model of normality. It
plays an important role in many domains such as network intrusion detection,
criminal activity identity and so on. With the rapidly growing size of
accessible training data and high computation capacities, deep learning based
anomaly detection has become more and more popular. In this paper, a new
domain-based anomaly detection method based on generative adversarial networks
(GAN) is proposed. Minimum likelihood regularization is proposed to make the
generator produce more anomalies and prevent it from converging to normal data
distribution. Proper ensemble of anomaly scores is shown to improve the
stability of discriminator effectively. The proposed method has achieved
significant improvement than other anomaly detection methods on Cifar10 and UCI
datasets.
</summary>
    <author>
      <name>Chu Wang</name>
    </author>
    <author>
      <name>Yan-Ming Zhang</name>
    </author>
    <author>
      <name>Cheng-Lin Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00198v1</id>
    <updated>2018-08-01T07:13:16Z</updated>
    <published>2018-08-01T07:13:16Z</published>
    <title>Towards Machine Learning on data from Professional Cyclists</title>
    <summary>  Professional sports are developing towards increasingly scientific training
methods with increasing amounts of data being collected from laboratory tests,
training sessions and competitions. In cycling, it is standard to equip
bicycles with small computers recording data from sensors such as power-meters,
in addition to heart-rate, speed, altitude etc. Recently, machine learning
techniques have provided huge success in a wide variety of areas where large
amounts of data (big data) is available. In this paper, we perform a pilot
experiment on machine learning to model physical response in elite cyclists. As
a first experiment, we show that it is possible to train a LSTM machine
learning algorithm to predict the heart-rate response of a cyclist during a
training session. This work is a promising first step towards developing more
elaborate models based on big data and machine learning to capture performance
aspects of athletes.
</summary>
    <author>
      <name>Agrin Hilmkil</name>
    </author>
    <author>
      <name>Oscar Ivarsson</name>
    </author>
    <author>
      <name>Moa Johansson</name>
    </author>
    <author>
      <name>Dan Kuylenstierna</name>
    </author>
    <author>
      <name>Teun van Erp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the 12th World Congress on Performance Analysis of
  Sports, Opatija, Croatia, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00197v1</id>
    <updated>2018-08-01T07:07:15Z</updated>
    <published>2018-08-01T07:07:15Z</published>
    <title>MaxMin Linear Initialization for Fuzzy C-Means</title>
    <summary>  Clustering is an extensive research area in data science. The aim of
clustering is to discover groups and to identify interesting patterns in
datasets. Crisp (hard) clustering considers that each data point belongs to one
and only one cluster. However, it is inadequate as some data points may belong
to several clusters, as is the case in text categorization. Thus, we need more
flexible clustering. Fuzzy clustering methods, where each data point can belong
to several clusters, are an interesting alternative. Yet, seeding iterative
fuzzy algorithms to achieve high quality clustering is an issue. In this paper,
we propose a new linear and efficient initialization algorithm MaxMin Linear to
deal with this problem. Then, we validate our theoretical results through
extensive experiments on a variety of numerical real-world and artificial
datasets. We also test several validity indices, including a new validity index
that we propose, Transformed Standardized Fuzzy Difference (TSFD).
</summary>
    <author>
      <name>Aybükë Oztürk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC, ArAr</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Lallich</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ERIC</arxiv:affiliation>
    </author>
    <author>
      <name>Sylvie Yona Waksman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ArAr</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IBaI. 14th International Conference on Machine Learning and Data
  Mining (MLDM 2018), Jul 2018, New York, United States. Springer, Lecture
  Notes in Artificial Intelligence, 10934-10935, 2018, Machine Learning and
  Data Mining in Pattern Recognition. http://www.mldm.de</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00196v1</id>
    <updated>2018-08-01T07:04:08Z</updated>
    <published>2018-08-01T07:04:08Z</published>
    <title>Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of
  Machine Learning Models</title>
    <summary>  Interpretation and diagnosis of machine learning models have gained renewed
interest in recent years with breakthroughs in new approaches. We present
Manifold, a framework that utilizes visual analysis techniques to support
interpretation, debugging, and comparison of machine learning models in a more
transparent and interactive manner. Conventional techniques usually focus on
visualizing the internal logic of a specific model type (i.e., deep neural
networks), lacking the ability to extend to a more complex scenario where
different model types are integrated. To this end, Manifold is designed as a
generic framework that does not rely on or access the internal logic of the
model and solely observes the input (i.e., instances or features) and the
output (i.e., the predicted result and probability distribution). We describe
the workflow of Manifold as an iterative process consisting of three major
phases that are commonly involved in the model development and diagnosis
process: inspection (hypothesis), explanation (reasoning), and refinement
(verification). The visual components supporting these tasks include a
scatterplot-based visual summary that overviews the models' outcome and a
customizable tabular view that reveals feature discrimination. We demonstrate
current applications of the framework on the classification and regression
tasks and discuss other potential machine learning use scenarios where Manifold
can be applied.
</summary>
    <author>
      <name>Jiawei Zhang</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Piero Molino</name>
    </author>
    <author>
      <name>Lezhi Li</name>
    </author>
    <author>
      <name>David S. Ebert</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10311v2</id>
    <updated>2018-08-01T07:03:41Z</updated>
    <published>2018-02-28T09:02:26Z</published>
    <title>Fast Maximum Likelihood estimation via Equilibrium Expectation for Large
  Network Data</title>
    <summary>  A major line of contemporary research on complex networks is based on the
development of statistical models that specify the local motifs associated with
macro-structural properties observed in actual networks. This statistical
approach becomes increasingly problematic as network size increases. In the
context of current research on efficient estimation of models for large network
data sets, we propose a fast algorithm for maximum likelihood estimation (MLE)
that afords a signifcant increase in the size of networks amenable to direct
empirical analysis. The algorithm we propose in this paper relies on properties
of Markov chains at equilibrium, and for this reason it is called equilibrium
expectation (EE). We demonstrate the performance of the EE algorithm in the
context of exponential random graphmodels (ERGMs) a family of statistical
models commonly used in empirical research based on network data observed at a
single period in time. Thus far, the lack of efcient computational strategies
has limited the empirical scope of ERGMs to relatively small networks with a
few thousand nodes. The approach we propose allows a dramatic increase in the
size of networks that may be analyzed using ERGMs. This is illustrated in an
analysis of several biological networks and one social network with 104,103
nodes
</summary>
    <author>
      <name>Maksym Byshkin</name>
    </author>
    <author>
      <name>Alex Stivala</name>
    </author>
    <author>
      <name>Antonietta Mira</name>
    </author>
    <author>
      <name>Garry Robins</name>
    </author>
    <author>
      <name>Alessandro Lomi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41598-018-29725-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41598-018-29725-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports | (2018) 8:11509
  https://www.nature.com/articles/s41598-018-29725-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.10311v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10311v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00142v1</id>
    <updated>2018-08-01T02:28:59Z</updated>
    <published>2018-08-01T02:28:59Z</published>
    <title>Sleep-wake classification via quantifying heart rate variability by
  convolutional neural network</title>
    <summary>  Fluctuations in heart rate are intimately tied to changes in the
physiological state of the organism. We examine and exploit this relationship
by classifying a human subject's wake/sleep status using his instantaneous
heart rate (IHR) series. We use a convolutional neural network (CNN) to build
features from the IHR series extracted from a whole-night electrocardiogram
(ECG) and predict every 30 seconds whether the subject is awake or asleep. Our
training database consists of 56 normal subjects, and we consider three
different databases for validation; one is private, and two are public with
different races and apnea severities. On our private database of 27 subjects,
our accuracy, sensitivity, specificity, and AUC values for predicting the wake
stage are 83.1%, 52.4%, 89.4%, and 0.83, respectively. Validation performance
is similar on our two public databases. When we use the photoplethysmography
instead of the ECG to obtain the IHR series, the performance is also
comparable. A robustness check is carried out to confirm the obtained
performance statistics. This result advocates for an effective and scalable
method for recognizing changes in physiological state using non-invasive heart
rate monitoring. The CNN model adaptively quantifies IHR fluctuation as well as
its location in time and is suitable for differentiating between the wake and
sleep stages.
</summary>
    <author>
      <name>John Malik</name>
    </author>
    <author>
      <name>Yu-Lun Lo</name>
    </author>
    <author>
      <name>Hau-tieng Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1361-6579/aad5a9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1361-6579/aad5a9" rel="related"/>
    <link href="http://arxiv.org/abs/1808.00142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00131v1</id>
    <updated>2018-08-01T01:30:22Z</updated>
    <published>2018-08-01T01:30:22Z</published>
    <title>A Theory of Dichotomous Valuation with Applications to Variable
  Selection</title>
    <summary>  An econometric or statistical model may undergo a marginal gain when a new
variable is admitted, and a marginal loss if an existing variable is removed.
The value of a variable to the model is quantified by its expected marginal
gain and marginal loss. Assuming the equality of opportunity, we derive a few
formulas which evaluate the overall performance in potential modeling
scenarios. However, the value is not symmetric to marginal gain and marginal
loss; thus, we introduce an unbiased solution. Simulation studies show that our
new approaches significantly outperform a few practice-used variable selection
methods.
</summary>
    <author>
      <name>Xingwei Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages, 4 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03931v2</id>
    <updated>2018-08-01T01:10:58Z</updated>
    <published>2018-07-11T02:24:20Z</published>
    <title>A Hierarchical Bayesian Linear Regression Model with Local Features for
  Stochastic Dynamics Approximation</title>
    <summary>  One of the challenges in model-based control of stochastic dynamical systems
is that the state transition dynamics are involved, and it is not easy or
efficient to make good-quality predictions of the states. Moreover, there are
not many representational models for the majority of autonomous systems, as it
is not easy to build a compact model that captures the entire dynamical
subtleties and uncertainties. In this work, we present a hierarchical Bayesian
linear regression model with local features to learn the dynamics of a
micro-robotic system as well as two simpler examples, consisting of a
stochastic mass-spring damper and a stochastic double inverted pendulum on a
cart. The model is hierarchical since we assume non-stationary priors for the
model parameters. These non-stationary priors make the model more flexible by
imposing priors on the priors of the model. To solve the maximum likelihood
(ML) problem for this hierarchical model, we use the variational expectation
maximization (EM) algorithm, and enhance the procedure by introducing hidden
target variables. The algorithm yields parsimonious model structures, and
consistently provides fast and accurate predictions for all our examples
involving large training and test sets. This demonstrates the effectiveness of
the method in learning stochastic dynamics, which makes it suitable for future
use in a paradigm, such as model-based reinforcement learning, to compute
optimal control policies in real time.
</summary>
    <author>
      <name>Behnoosh Parsa</name>
    </author>
    <author>
      <name>Keshav Rajasekaran</name>
    </author>
    <author>
      <name>Franziska Meier</name>
    </author>
    <author>
      <name>Ashis G. Banerjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00673v2</id>
    <updated>2018-08-01T00:44:31Z</updated>
    <published>2017-12-02T22:26:12Z</published>
    <title>Towards Robust Neural Networks via Random Self-ensemble</title>
    <summary>  Recent studies have revealed the vulnerability of deep neural networks: A
small adversarial perturbation that is imperceptible to human can easily make a
well-trained deep neural network misclassify. This makes it unsafe to apply
neural networks in security-critical applications. In this paper, we propose a
new defense algorithm called Random Self-Ensemble (RSE) by combining two
important concepts: {\bf randomness} and {\bf ensemble}. To protect a targeted
model, RSE adds random noise layers to the neural network to prevent the strong
gradient-based attacks, and ensembles the prediction over random noises to
stabilize the performance. We show that our algorithm is equivalent to ensemble
an infinite number of noisy models $f_\epsilon$ without any additional memory
overhead, and the proposed training procedure based on noisy stochastic
gradient descent can ensure the ensemble model has a good predictive
capability. Our algorithm significantly outperforms previous defense techniques
on real data sets. For instance, on CIFAR-10 with VGG network (which has 92\%
accuracy without any attack), under the strong C\&amp;W attack within a certain
distortion tolerance, the accuracy of unprotected model drops to less than
10\%, the best previous defense technique has $48\%$ accuracy, while our method
still has $86\%$ prediction accuracy under the same level of attack. Finally,
our method is simple and easy to integrate into any neural network.
</summary>
    <author>
      <name>Xuanqing Liu</name>
    </author>
    <author>
      <name>Minhao Cheng</name>
    </author>
    <author>
      <name>Huan Zhang</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 camera ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00111v1</id>
    <updated>2018-07-31T23:37:21Z</updated>
    <published>2018-07-31T23:37:21Z</published>
    <title>Probability Calibration Trees</title>
    <summary>  Obtaining accurate and well calibrated probability estimates from classifiers
is useful in many applications, for example, when minimising the expected cost
of classifications. Existing methods of calibrating probability estimates are
applied globally, ignoring the potential for improvements by applying a more
fine-grained model. We propose probability calibration trees, a modification of
logistic model trees that identifies regions of the input space in which
different probability calibration models are learned to improve performance. We
compare probability calibration trees to two widely used calibration
methods---isotonic regression and Platt scaling---and show that our method
results in lower root mean squared error on average than both methods, for
estimates produced by a variety of base learners.
</summary>
    <author>
      <name>Tim Leathart</name>
    </author>
    <author>
      <name>Eibe Frank</name>
    </author>
    <author>
      <name>Geoffrey Holmes</name>
    </author>
    <author>
      <name>Bernhard Pfahringer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 9th Asian Conference on Machine Learning</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Leathart, T., Frank, E., Holmes, G., &amp; Pfahringer, B. (2017).
  Probability calibration trees. In Proceedings of the 9th Asian Conference on
  Machine Learning (pp. 145-160)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00098v1</id>
    <updated>2018-07-31T22:41:20Z</updated>
    <published>2018-07-31T22:41:20Z</published>
    <title>Universal Approximation with Quadratic Deep Networks</title>
    <summary>  Recently, deep learning has been playing a central role in machine learning
research and applications. Since AlexNet, increasingly more advanced networks
have achieved state-of-the-art performance in computer vision, speech
recognition, language processing, game playing, medical imaging, and so on. In
our previous studies, we proposed quadratic/second-order neurons and deep
quadratic neural networks. In a quadratic neuron, the inner product of a vector
of data and the corresponding weights in a conventional neuron is replaced with
a quadratic function. The resultant second-order neuron enjoys an enhanced
expressive capability over the conventional neuron. However, how quadratic
neurons improve the expressing capability of a deep quadratic network has not
been studied up to now, preferably in relation to that of a conventional neural
network. In this paper, we ask three basic questions regarding the expressive
capability of a quadratic network: (1) for the one-hidden-layer network
structure, is there any function that a quadratic network can approximate much
more efficiently than a conventional network? (2) for the same multi-layer
network structure, is there any function that can be expressed by a quadratic
network but cannot be expressed with conventional neurons in the same
structure? (3) Does a quadratic network give a new insight into universal
approximation? Our main contributions are the three theorems shedding light
upon these three questions and demonstrating the merits of a quadratic network
in terms of expressive efficiency, unique capability, and compact architecture
respectively.
</summary>
    <author>
      <name>Fenglei Fan</name>
    </author>
    <author>
      <name>Ge Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00087v1</id>
    <updated>2018-07-31T22:13:39Z</updated>
    <published>2018-07-31T22:13:39Z</published>
    <title>Subsampled Rényi Differential Privacy and Analytical Moments
  Accountant</title>
    <summary>  We study the problem of subsampling in differential privacy (DP), a question
that is the centerpiece behind many successful differentially private machine
learning algorithms. Specifically, we provide a tight upper bound on the
R\'enyi Differential Privacy (RDP) (Mironov, 2017) parameters for algorithms
that: (1) subsample the dataset, and then (2) apply a randomized mechanism M to
the subsample, in terms of the RDP parameters of M and the subsampling
probability parameter. This result generalizes the classic subsampling-based
"privacy amplification" property of $(\epsilon,\delta)$-differential privacy
that applies to only one fixed pair of $(\epsilon,\delta)$, to a stronger
version that exploits properties of each specific randomized algorithm and
satisfies an entire family of $(\epsilon(\delta),\delta)$-differential privacy
for all $\delta\in [0,1]$. Our experiments confirm the advantage of using our
techniques over keeping track of $(\epsilon,\delta)$ directly, especially in
the setting where we need to compose many rounds of data access.
</summary>
    <author>
      <name>Yu-Xiang Wang</name>
    </author>
    <author>
      <name>Borja Balle</name>
    </author>
    <author>
      <name>Shiva Kasiviswanathan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00079v1</id>
    <updated>2018-07-31T21:52:08Z</updated>
    <published>2018-07-31T21:52:08Z</published>
    <title>Cutting Down Training Memory by Re-fowarding</title>
    <summary>  Deep Neutral Networks(DNN) require huge GPU memory when training on modern
image/video databases. Unfortunately, the GPU memory is always finite, which
limits the image resolution, batch size, and learning rate that could be tuned
for better performances. In this paper, we propose a novel approach, called
Re-forwarding, that substantially reduces memory usage in training. Our
approach only saves the tensors at a subset of layers during the first forward,
and conduct extra local forwards (the Re-forwarding process) to compute the
missing tensors needed during backward. The total memory cost becomes the sum
of (1) the cost at the subset of layers and (2) the maximum cost of the
re-forwarding processes. We propose theories and algorithms that achieve the
optimal memory solutions for DNNs with either linear or arbitrary optimization
graphs. Experiments show that Re-forwarding cut down huge amount of training
memory on all popular DNNs such as Alexnet, VGG net, ResNet, Densenet and
Inception net.
</summary>
    <author>
      <name>Jianwei Feng</name>
    </author>
    <author>
      <name>Dong Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06464v2</id>
    <updated>2018-07-31T21:36:26Z</updated>
    <published>2018-06-17T23:29:19Z</published>
    <title>Learning Policy Representations in Multiagent Systems</title>
    <summary>  Modeling agent behavior is central to understanding the emergence of complex
phenomena in multiagent systems. Prior work in agent modeling has largely been
task-specific and driven by hand-engineering domain-specific prior knowledge.
We propose a general learning framework for modeling agent behavior in any
multiagent system using only a handful of interaction data. Our framework casts
agent modeling as a representation learning problem. Consequently, we construct
a novel objective inspired by imitation learning and agent identification and
design an algorithm for unsupervised learning of representations of agent
policies. We demonstrate empirically the utility of the proposed framework in
(i) a challenging high-dimensional competitive environment for continuous
control and (ii) a cooperative environment for communication, on supervised
predictive tasks, unsupervised clustering, and policy optimization using deep
reinforcement learning.
</summary>
    <author>
      <name>Aditya Grover</name>
    </author>
    <author>
      <name>Maruan Al-Shedivat</name>
    </author>
    <author>
      <name>Jayesh K. Gupta</name>
    </author>
    <author>
      <name>Yura Burda</name>
    </author>
    <author>
      <name>Harrison Edwards</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06464v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06464v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01442v2</id>
    <updated>2018-07-31T21:30:28Z</updated>
    <published>2018-07-04T03:57:21Z</published>
    <title>Modeling Sparse Deviations for Compressed Sensing using Generative
  Models</title>
    <summary>  In compressed sensing, a small number of linear measurements can be used to
reconstruct an unknown signal. Existing approaches leverage assumptions on the
structure of these signals, such as sparsity or the availability of a
generative model. A domain-specific generative model can provide a stronger
prior and thus allow for recovery with far fewer measurements. However, unlike
sparsity-based approaches, existing methods based on generative models
guarantee exact recovery only over their support, which is typically only a
small subset of the space on which the signals are defined. We propose
Sparse-Gen, a framework that allows for sparse deviations from the support set,
thereby achieving the best of both worlds by using a domain specific prior and
allowing reconstruction over the full space of signals. Theoretically, our
framework provides a new class of signals that can be acquired using compressed
sensing, reducing classic sparse vector recovery to a special case and avoiding
the restrictive support due to a generative model prior. Empirically, we
observe consistent improvements in reconstruction accuracy over competing
approaches, especially in the more practical setting of transfer compressed
sensing where a generative model for a data-rich, source domain aids sensing on
a data-scarce, target domain.
</summary>
    <author>
      <name>Manik Dhar</name>
    </author>
    <author>
      <name>Aditya Grover</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00076v1</id>
    <updated>2018-07-31T21:15:54Z</updated>
    <published>2018-07-31T21:15:54Z</published>
    <title>News Session-Based Recommendations using Deep Neural Networks</title>
    <summary>  News recommender systems are aimed to personalize users experiences and help
them to discover relevant articles from a large and dynamic search space.
Therefore, news domain is a challenging scenario for recommendations, due to
its sparse user profiling, fast growing number of items, accelerated item's
value decay, and users preferences dynamic shift. Some promising results have
been recently achieved by the usage of Deep Learning techniques on Recommender
Systems, specially for item's feature extraction and for session-based
recommendations with Recurrent Neural Networks. In this paper, its presented a
Deep Learning architecture for Session-Based recommendations of News articles.
This architecture is composed of two modules, the first responsible to learn
news articles representations, based on their text and metadata, and the second
module aimed to provide session-based recommendations using Recurrent Neural
Networks. The recommendation task addressed in this work is next-item
prediction for user sessions: "what is the next most likely article a user
might read in a session?" User session context is leveraged by the architecture
to provide additional information in such extreme cold-start scenario of news
recommendation. Users' behavior and item features are both merged in an hybrid
recommendation approach. A temporal offline evaluation method is also proposed
as a complementary contribution, for a more realistic evaluation of such task,
considering dynamic factors that affect global readership interests like
popularity, recency, and seasonality.
</summary>
    <author>
      <name>Gabriel de Souza P. Moreira</name>
    </author>
    <author>
      <name>Felipe Ferreira</name>
    </author>
    <author>
      <name>Adilson Marques da Cunha</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00068v1</id>
    <updated>2018-07-31T20:38:19Z</updated>
    <published>2018-07-31T20:38:19Z</published>
    <title>A Fuzzy-Rough based Binary Shuffled Frog Leaping Algorithm for Feature
  Selection</title>
    <summary>  Feature selection and attribute reduction are crucial problems, and widely
used techniques in the field of machine learning, data mining and pattern
recognition to overcome the well-known phenomenon of the Curse of
Dimensionality, by either selecting a subset of features or removing unrelated
ones. This paper presents a new feature selection method that efficiently
carries out attribute reduction, thereby selecting the most informative
features of a dataset. It consists of two components: 1) a measure for feature
subset evaluation, and 2) a search strategy. For the evaluation measure, we
have employed the fuzzy-rough dependency degree (FRFDD) in the lower
approximation-based fuzzy-rough feature selection (L-FRFS) due to its
effectiveness in feature selection. As for the search strategy, a new version
of a binary shuffled frog leaping algorithm is proposed (B-SFLA). The new
feature selection method is obtained by hybridizing the B-SFLA with the FRDD.
Non-parametric statistical tests are conducted to compare the proposed approach
with several existing methods over twenty two datasets, including nine high
dimensional and large ones, from the UCI repository. The experimental results
demonstrate that the B-SFLA approach significantly outperforms other
metaheuristic methods in terms of the number of selected features and the
classification accuracy.
</summary>
    <author>
      <name>Javad Rahimipour Anaraki</name>
    </author>
    <author>
      <name>Saeed Samet</name>
    </author>
    <author>
      <name>Mahdi Eftekhari</name>
    </author>
    <author>
      <name>Chang Wook Ahn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 11 Tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.2; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00036v1</id>
    <updated>2018-07-31T19:26:59Z</updated>
    <published>2018-07-31T19:26:59Z</published>
    <title>Scalable Multi-Task Gaussian Process Tensor Regression for Normative
  Modeling of Structured Variation in Neuroimaging Data</title>
    <summary>  Most brain disorders are very heterogeneous in terms of their underlying
biology and developing analysis methods to model such heterogeneity is a major
challenge. A promising approach is to use probabilistic regression methods to
estimate normative models of brain function using (f)MRI data then use these to
map variation across individuals in clinical populations (e.g., via anomaly
detection). To fully capture individual differences, it is crucial to
statistically model the patterns of correlation across different brain regions
and individuals. However, this is very challenging for neuroimaging data
because of high-dimensionality and highly structured patterns of correlation
across multiple axes. Here, we propose a general and flexible multi-task
learning framework to address this problem. Our model uses a tensor-variate
Gaussian process in a Bayesian mixed-effects model and makes use of Kronecker
algebra and a low-rank approximation to scale efficiently to multi-way
neuroimaging data at the whole brain level. On a publicly available clinical
fMRI dataset, we show that our computationally affordable approach
substantially improves detection sensitivity over both a mass-univariate
normative model and a classifier that --unlike our approach-- has full access
to the clinical labels.
</summary>
    <author>
      <name>Seyed Mostafa Kia</name>
    </author>
    <author>
      <name>Christian F. Beckmann</name>
    </author>
    <author>
      <name>Andre F. Marquand</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00033v1</id>
    <updated>2018-07-31T19:14:39Z</updated>
    <published>2018-07-31T19:14:39Z</published>
    <title>Techniques for Interpretable Machine Learning</title>
    <summary>  Interpretable machine learning tackles the important problem that humans
cannot understand the behaviors of complex machine learning models and how
these classifiers arrive at a particular decision. Although many approaches
have been proposed, a comprehensive understanding of the achievements and
challenges is still lacking. This paper provides a survey covering existing
techniques and methods to increase the interpretability of machine learning
models and also discusses the crucial issues to consider in future work such as
interpretation design principles and evaluation metrics in order to push
forward the area of interpretable machine learning.
</summary>
    <author>
      <name>Mengnan Du</name>
    </author>
    <author>
      <name>Ninghao Liu</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00020v1</id>
    <updated>2018-07-31T18:34:56Z</updated>
    <published>2018-07-31T18:34:56Z</published>
    <title>Online Adaptative Curriculum Learning for GANs</title>
    <summary>  Generative Adversarial Networks (GANs) can successfully learn a probability
distribution and produce realistic samples. However, open questions such as
sufficient convergence conditions and mode collapse still persist. In this
paper, we build on existing work in the area by proposing a novel framework for
training the generator against an ensemble of discriminator networks, which can
be seen as a one-student/multiple-teachers setting. We formalize this problem
within the non-stationary Multi-Armed Bandit (MAB) framework, where we evaluate
the capability of a bandit algorithm to select discriminators for providing the
generator with feedback during learning. To this end, we propose a reward
function which reflects the amount of knowledge learned by the generator and
dynamically selects the optimal discriminator network. Finally, we connect our
algorithm to stochastic optimization methods and show that existing methods
using multiple discriminators in literature can be recovered from our
parametric model. Experimental results based on the Fr\'echet Inception
Distance (FID) demonstrates faster convergence than existing baselines and show
that our method learns a curriculum.
</summary>
    <author>
      <name>Thang Doan</name>
    </author>
    <author>
      <name>Joao Monteiro</name>
    </author>
    <author>
      <name>Isabela Albuquerque</name>
    </author>
    <author>
      <name>Bogdan Mazoure</name>
    </author>
    <author>
      <name>Audrey Durand</name>
    </author>
    <author>
      <name>Joelle Pineau</name>
    </author>
    <author>
      <name>R Devon Hjelm</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06317v2</id>
    <updated>2018-07-31T18:21:15Z</updated>
    <published>2018-06-17T00:42:09Z</published>
    <title>Laplacian Smoothing Gradient Descent</title>
    <summary>  We propose a very simple modification of gradient descent and stochastic
gradient descent. We show that when applied to a variety of machine learning
models including softmax regression, convolutional neural nets, generative
adversarial nets, and deep reinforcement learning, this very simple surrogate
can dramatically reduce the variance and improve the accuracy of the
generalization. The new algorithm, (which depends on one nonnegative parameter)
when applied to non-convex minimization, tends to avoid sharp local minima.
Instead it seeks somewhat flatter local (and often global) minima. The method
only involves preconditioning the gradient by the inverse of a tri-diagonal
matrix that is positive definite. The motivation comes from the theory of
Hamilton-Jacobi partial differential equations. This theory demonstrates that
the new algorithm is almost the same as doing gradient descent on a new
function which (a) has the same global minima as the original function and (b)
is "more convex". Again, the programming effort in doing this is minimal, in
cost, complexity and effort. We implement our algorithm into both PyTorch and
Tensorflow platforms, which will be made publicly available.
</summary>
    <author>
      <name>Stanley Osher</name>
    </author>
    <author>
      <name>Bao Wang</name>
    </author>
    <author>
      <name>Penghang Yin</name>
    </author>
    <author>
      <name>Xiyang Luo</name>
    </author>
    <author>
      <name>Minh Pham</name>
    </author>
    <author>
      <name>Alex Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65-06" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05591v2</id>
    <updated>2018-07-31T18:18:05Z</updated>
    <published>2018-03-15T05:09:51Z</published>
    <title>On the insufficiency of existing momentum schemes for Stochastic
  Optimization</title>
    <summary>  Momentum based stochastic gradient methods such as heavy ball (HB) and
Nesterov's accelerated gradient descent (NAG) method are widely used in
practice for training deep networks and other supervised learning models, as
they often provide significant improvements over stochastic gradient descent
(SGD). Rigorously speaking, "fast gradient" methods have provable improvements
over gradient descent only for the deterministic case, where the gradients are
exact. In the stochastic case, the popular explanations for their wide
applicability is that when these fast gradient methods are applied in the
stochastic case, they partially mimic their exact gradient counterparts,
resulting in some practical gain. This work provides a counterpoint to this
belief by proving that there exist simple problem instances where these methods
cannot outperform SGD despite the best setting of its parameters. These
negative problem instances are, in an informal sense, generic; they do not look
like carefully constructed pathological instances. These results suggest (along
with empirical evidence) that HB or NAG's practical performance gains are a
by-product of mini-batching.
  Furthermore, this work provides a viable (and provable) alternative, which,
on the same set of problem instances, significantly improves over HB, NAG, and
SGD's performance. This algorithm, referred to as Accelerated Stochastic
Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based
on a relatively less popular variant of Nesterov's Acceleration. Extensive
empirical results in this paper show that ASGD has performance gains over HB,
NAG, and SGD.
</summary>
    <author>
      <name>Rahul Kidambi</name>
    </author>
    <author>
      <name>Praneeth Netrapalli</name>
    </author>
    <author>
      <name>Prateek Jain</name>
    </author>
    <author>
      <name>Sham M. Kakade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 10 figures. Updated acknowledgements. Appeared as an oral
  presentation at International Conference on Learning Representations (ICLR),
  2018. Code implementing the ASGD method can be found at
  https://github.com/rahulkidambi/AccSGD</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05591v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05591v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08227v2</id>
    <updated>2018-07-31T18:11:32Z</updated>
    <published>2017-04-26T17:30:27Z</published>
    <title>Accelerating Stochastic Gradient Descent For Least Squares Regression</title>
    <summary>  There is widespread sentiment that it is not possible to effectively utilize
fast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy
ball) for the purposes of stochastic optimization due to their instability and
error accumulation, a notion made precise in d'Aspremont 2008 and Devolder,
Glineur, and Nesterov 2014. This work considers these issues for the special
case of stochastic approximation for the least squares regression problem, and
our main result refutes the conventional wisdom by showing that acceleration
can be made robust to statistical errors. In particular, this work introduces
an accelerated stochastic gradient method that provably achieves the minimax
optimal statistical risk faster than stochastic gradient descent. Critical to
the analysis is a sharp characterization of accelerated stochastic gradient
descent as a stochastic process. We hope this characterization gives insights
towards the broader question of designing simple and effective accelerated
stochastic methods for more general convex and non-convex optimization
problems.
</summary>
    <author>
      <name>Prateek Jain</name>
    </author>
    <author>
      <name>Sham M. Kakade</name>
    </author>
    <author>
      <name>Rahul Kidambi</name>
    </author>
    <author>
      <name>Praneeth Netrapalli</name>
    </author>
    <author>
      <name>Aaron Sidford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages, 3 figures, 1 table; updated acknowledgements, minor title
  change. Paper appeared in the proceedings of the Conference on Learning
  Theory (COLT), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08227v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08227v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03774v4</id>
    <updated>2018-07-31T17:50:00Z</updated>
    <published>2016-10-12T16:30:11Z</published>
    <title>Parallelizing Stochastic Gradient Descent for Least Squares Regression:
  mini-batching, averaging, and model misspecification</title>
    <summary>  This work characterizes the benefits of averaging schemes widely used in
conjunction with stochastic gradient descent (SGD). In particular, this work
provides a sharp analysis of: (1) mini-batching, a method of averaging many
samples of a stochastic gradient to both reduce the variance of the stochastic
gradient estimate and for parallelizing SGD and (2) tail-averaging, a method
involving averaging the final few iterates of SGD to decrease the variance in
SGD's final iterate. This work presents non-asymptotic excess risk bounds for
these schemes for the stochastic approximation problem of least squares
regression.
  Furthermore, this work establishes a precise problem-dependent extent to
which mini-batch SGD yields provable near-linear parallelization speedups over
SGD with batch size one. This allows for understanding learning rate versus
batch size tradeoffs for the final iterate of an SGD method. These results are
then utilized in providing a highly parallelizable SGD method that obtains the
minimax risk with nearly the same number of serial updates as batch gradient
descent, improving significantly over existing SGD methods. A non-asymptotic
analysis of communication efficient parallelization schemes such as
model-averaging/parameter mixing methods is then provided.
  Finally, this work sheds light on some fundamental differences in SGD's
behavior when dealing with agnostic noise in the (non-realizable) least squares
regression problem. In particular, the work shows that the stepsizes that
ensure minimax risk for the agnostic case must be a function of the noise
properties.
  This paper builds on the operator view of analyzing SGD methods, introduced
by Defossez and Bach (2015), followed by developing a novel analysis in
bounding these operators to characterize the excess risk. These techniques are
of broader interest in analyzing computational aspects of stochastic
approximation.
</summary>
    <author>
      <name>Prateek Jain</name>
    </author>
    <author>
      <name>Sham M. Kakade</name>
    </author>
    <author>
      <name>Rahul Kidambi</name>
    </author>
    <author>
      <name>Praneeth Netrapalli</name>
    </author>
    <author>
      <name>Aaron Sidford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages. Published in the Journal of Machine Learning Research
  (JMLR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03774v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03774v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03752v3</id>
    <updated>2018-07-31T17:23:02Z</updated>
    <published>2018-02-11T15:34:20Z</published>
    <title>Supervised classification of Dermatological diseases by Deep learning</title>
    <summary>  This paper introduces a deep-learning based efficient classifier for common
dermatological conditions, aimed at people without easy access to skin
specialists. We report approximately 80% accuracy, in a situation where primary
care doctors have attained 57% success rate, according to recent literature.
The rationale of its design is centered on deploying and updating it on
handheld devices in near future. Dermatological diseases are common in every
population and have a wide spectrum in severity. With a shortage of
dermatological expertise being observed in several countries, machine learning
solutions can augment medical services and advise regarding existence of common
diseases. The paper implements supervised classification of nine distinct
conditions which have high occurrence in East Asian countries. Our current
attempt establishes that deep learning based techniques are viable avenues for
preliminary information to aid patients.
</summary>
    <author>
      <name>Sourav Mishra</name>
    </author>
    <author>
      <name>Toshihiko Yamasaki</name>
    </author>
    <author>
      <name>Hideaki Imaizumi</name>
    </author>
    <link href="http://arxiv.org/abs/1802.03752v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03752v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02464v3</id>
    <updated>2018-07-31T16:55:11Z</updated>
    <published>2018-04-06T21:43:13Z</published>
    <title>Differentiable plasticity: training plastic neural networks with
  backpropagation</title>
    <summary>  How can we build agents that keep learning from experience, quickly and
efficiently, after their initial training? Here we take inspiration from the
main mechanism of learning in biological brains: synaptic plasticity, carefully
tuned by evolution to produce efficient lifelong learning. We show that
plasticity, just like connection weights, can be optimized by gradient descent
in large (millions of parameters) recurrent networks with Hebbian plastic
connections. First, recurrent plastic networks with more than two million
parameters can be trained to memorize and reconstruct sets of novel,
high-dimensional 1000+ pixels natural images not seen during training.
Crucially, traditional non-plastic recurrent networks fail to solve this task.
Furthermore, trained plastic networks can also solve generic meta-learning
tasks such as the Omniglot task, with competitive results and little parameter
overhead. Finally, in reinforcement learning settings, plastic networks
outperform a non-plastic equivalent in a maze exploration task. We conclude
that differentiable plasticity may provide a powerful novel approach to the
learning-to-learn problem.
</summary>
    <author>
      <name>Thomas Miconi</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <author>
      <name>Kenneth O. Stanley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at ICML 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 35th International Conference on Machine
  Learning (ICML2018), Stockholm, Sweden, PMLR 80, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.02464v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02464v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01050v3</id>
    <updated>2018-07-31T16:53:19Z</updated>
    <published>2018-04-03T16:04:22Z</published>
    <title>Training VAEs Under Structured Residuals</title>
    <summary>  Variational auto-encoders (VAEs) are a popular and powerful deep generative
model. Previous works on VAEs have assumed a factorized likelihood model,
whereby the output uncertainty of each pixel is assumed to be independent. This
approximation is clearly limited as demonstrated by observing a residual image
from a VAE reconstruction, which often possess a high level of structure. This
paper demonstrates a novel scheme to incorporate a structured Gaussian
likelihood prediction network within the VAE that allows the residual
correlations to be modeled. Our novel architecture, with minimal increase in
complexity, incorporates the covariance matrix prediction within the VAE. We
also propose a new mechanism for allowing structured uncertainty on color
images. Furthermore, we provide a scheme for effectively training this model,
and include some suggestions for improving performance in terms of efficiency
or modeling longer range correlations.
</summary>
    <author>
      <name>Garoe Dorta</name>
    </author>
    <author>
      <name>Sara Vicente</name>
    </author>
    <author>
      <name>Lourdes Agapito</name>
    </author>
    <author>
      <name>Neill D. F. Campbell</name>
    </author>
    <author>
      <name>Ivor Simpson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Simplified training methodology, added more results</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01050v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01050v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10222v2</id>
    <updated>2018-07-31T16:26:22Z</updated>
    <published>2018-05-25T16:06:09Z</published>
    <title>Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic
  Optimization</title>
    <summary>  We suggest a general oracle-based framework that captures different parallel
stochastic optimization settings described by a dependency graph, and derive
generic lower bounds in terms of this graph. We then use the framework and
derive lower bounds for several specific parallel optimization settings,
including delayed updates and parallel processing with intermittent
communication. We highlight gaps between lower and upper bounds on the oracle
complexity, and cases where the "natural" algorithms are not known to be
optimal.
</summary>
    <author>
      <name>Blake Woodworth</name>
    </author>
    <author>
      <name>Jialei Wang</name>
    </author>
    <author>
      <name>Brendan McMahan</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10222v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10222v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07336v2</id>
    <updated>2018-07-31T16:10:52Z</updated>
    <published>2018-06-19T16:43:44Z</published>
    <title>Neural Code Comprehension: A Learnable Representation of Code Semantics</title>
    <summary>  With the recent success of embeddings in natural language processing,
research has been conducted into applying similar methods to code analysis.
Most works attempt to process the code directly or use a syntactic tree
representation, treating it like sentences written in a natural language.
However, none of the existing methods are sufficient to comprehend program
semantics robustly, due to structural features such as function calls,
branching, and interchangeable order of statements. In this paper, we propose a
novel processing technique to learn code semantics, and apply it to a variety
of program analysis tasks. In particular, we stipulate that a robust
distributional hypothesis of code applies to both human- and machine-generated
programs. Following this hypothesis, we define an embedding space, inst2vec,
based on an Intermediate Representation (IR) of the code that is independent of
the source programming language. We provide a novel definition of contextual
flow for this IR, leveraging both the underlying data- and control-flow of the
program. We then analyze the embeddings qualitatively using analogies and
clustering, and evaluate the learned representation on three different
high-level tasks. We show that with a single RNN architecture and pre-trained
fixed embeddings, inst2vec outperforms specialized approaches for performance
prediction (compute device mapping, optimal thread coarsening); and algorithm
classification from raw code (104 classes), where we set a new
state-of-the-art.
</summary>
    <author>
      <name>Tal Ben-Nun</name>
    </author>
    <author>
      <name>Alice Shoshana Jakobovits</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07336v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07336v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11880v1</id>
    <updated>2018-07-31T15:51:08Z</updated>
    <published>2018-07-31T15:51:08Z</published>
    <title>Stochastic Gradient Descent with Biased but Consistent Gradient
  Estimators</title>
    <summary>  Stochastic gradient descent (SGD), which dates back to the 1950s, is one of
the most popular and effective approaches for performing stochastic
optimization. Research on SGD resurged recently in machine learning for
optimizing convex loss functions as well as training nonconvex deep neural
networks. The theory assumes that one can easily compute an unbiased gradient
estimator, which is usually the case due to the sample average nature of
empirical risk minimization. There exist, however, many scenarios (e.g., graph
learning) where an unbiased estimator may be as expensive to compute as the
full gradient, because training examples are interconnected. In a recent work,
Chen et al. (2018) proposed using a consistent gradient estimator as an
economic alternative. Encouraged by empirical success, we show, in a general
setting, that consistent estimators result in the same convergence behavior as
do unbiased ones. Our analysis covers strongly convex, convex, and nonconvex
objectives. This work opens several new research directions, including the
development of more efficient SGD updates with consistent estimators and the
design of efficient training algorithms for large-scale graphs.
</summary>
    <author>
      <name>Jie Chen</name>
    </author>
    <author>
      <name>Ronny Luss</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11876v1</id>
    <updated>2018-07-31T15:39:37Z</updated>
    <published>2018-07-31T15:39:37Z</published>
    <title>Predicting Solution Summaries to Integer Linear Programs under Imperfect
  Information with Machine Learning</title>
    <summary>  The paper provides a methodological contribution at the intersection of
machine learning and operations research. Namely, we propose a methodology to
quickly predict solution summaries (i.e., solution descriptions at a given
level of detail) to discrete stochastic optimization problems. We approximate
the solutions based on supervised learning and the training dataset consists of
a large number of deterministic problems that have been solved independently
and offline. Uncertainty regarding a missing subset of the inputs is addressed
through sampling and aggregation methods.
  Our motivating application concerns booking decisions of intermodal
containers on double-stack trains. Under perfect information, this is the
so-called load planning problem and it can be formulated by means of integer
linear programming. However, the formulation cannot be used for the application
at hand because of the restricted computational budget and unknown container
weights. The results show that standard deep learning algorithms allow one to
predict descriptions of solutions with high accuracy in very short time
(milliseconds or less).
</summary>
    <author>
      <name>Eric Larsen</name>
    </author>
    <author>
      <name>Sébastien Lachapelle</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Emma Frejinger</name>
    </author>
    <author>
      <name>Simon Lacoste-Julien</name>
    </author>
    <author>
      <name>Andrea Lodi</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09048v2</id>
    <updated>2018-07-31T15:10:28Z</updated>
    <published>2018-06-23T22:03:10Z</published>
    <title>A classification point-of-view about conditional Kendall's tau</title>
    <summary>  We show how the problem of estimating conditional Kendall's tau can be
rewritten as a classification task. Conditional Kendall's tau is a conditional
dependence parameter that is a characteristic of a given pair of random
variables. The goal is to predict whether the pair is concordant (value of $1$)
or discordant (value of $-1$) conditionally on some covariates. We prove the
consistency and the asymptotic normality of a family of penalized approximate
maximum likelihood estimators, including the equivalent of the logit and probit
regressions in our framework. Then, we detail specific algorithms adapting
usual machine learning techniques, including nearest neighbors, decision trees,
random forests and neural networks, to the setting of the estimation of
conditional Kendall's tau. A small simulation study compares their finite
sample properties. Finally, we apply all these estimators to a dataset of
European stock indices.
</summary>
    <author>
      <name>Alexis Derumigny</name>
    </author>
    <author>
      <name>Jean-David Fermanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09048v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09048v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02612v2</id>
    <updated>2018-07-31T14:54:24Z</updated>
    <published>2018-06-07T11:11:13Z</published>
    <title>Dimensionality-Driven Learning with Noisy Labels</title>
    <summary>  Datasets with significant proportions of noisy (incorrect) class labels
present challenges for training accurate Deep Neural Networks (DNNs). We
propose a new perspective for understanding DNN generalization for such
datasets, by investigating the dimensionality of the deep representation
subspace of training samples. We show that from a dimensionality perspective,
DNNs exhibit quite distinctive learning styles when trained with clean labels
versus when trained with a proportion of noisy labels. Based on this finding,
we develop a new dimensionality-driven learning strategy, which monitors the
dimensionality of subspaces during training and adapts the loss function
accordingly. We empirically demonstrate that our approach is highly tolerant to
significant proportions of noisy labels, and can effectively learn
low-dimensional local subspaces that capture the data distribution.
</summary>
    <author>
      <name>Xingjun Ma</name>
    </author>
    <author>
      <name>Yisen Wang</name>
    </author>
    <author>
      <name>Michael E. Houle</name>
    </author>
    <author>
      <name>Shuo Zhou</name>
    </author>
    <author>
      <name>Sarah M. Erfani</name>
    </author>
    <author>
      <name>Shu-Tao Xia</name>
    </author>
    <author>
      <name>Sudanthi Wijewickrema</name>
    </author>
    <author>
      <name>James Bailey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the International Conference on Machine Learning
  (ICML), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02612v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02612v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11836v1</id>
    <updated>2018-07-31T14:21:32Z</updated>
    <published>2018-07-31T14:21:32Z</published>
    <title>Inferring the ground truth through crowdsourcing</title>
    <summary>  Universally valid ground truth is almost impossible to obtain or would come
at a very high cost. For supervised learning without universally valid ground
truth, a recommended approach is applying crowdsourcing: Gathering a large data
set annotated by multiple individuals of varying possibly expertise levels and
inferring the ground truth data to be used as labels to train the classifier.
Nevertheless, due to the sensitivity of the problem at hand (e.g. mitosis
detection in breast cancer histology images), the obtained data needs
verification and proper assessment before being used for classifier training.
Even in the context of organic computing systems, an indisputable ground truth
might not always exist. Therefore, it should be inferred through the
aggregation and verification of the local knowledge of each autonomous agent.
</summary>
    <author>
      <name>Jean Pierre Char</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, Intelligent Systems seminar SS18</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00004v1</id>
    <updated>2018-07-31T14:16:54Z</updated>
    <published>2018-07-31T14:16:54Z</published>
    <title>Graph-Based Recommendation System</title>
    <summary>  In this work, we study recommendation systems modelled as contextual
multi-armed bandit (MAB) problems. We propose a graph-based recommendation
system that learns and exploits the geometry of the user space to create
meaningful clusters in the user domain. This reduces the dimensionality of the
recommendation problem while preserving the accuracy of MAB. We then study the
effect of graph sparsity and clusters size on the MAB performance and provide
exhaustive simulation results both in synthetic and in real-case datasets.
Simulation results show improvements with respect to state-of-the-art MAB
algorithms.
</summary>
    <author>
      <name>Kaige Yang</name>
    </author>
    <author>
      <name>Laura Toni</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11824v1</id>
    <updated>2018-07-31T14:04:33Z</updated>
    <published>2018-07-31T14:04:33Z</published>
    <title>t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data</title>
    <summary>  Modern datasets and models are notoriously difficult to explore and analyze
due to their inherent high dimensionality and massive numbers of samples.
Existing visualization methods which employ dimensionality reduction to two or
three dimensions are often inefficient and/or ineffective for these datasets.
This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of
t-distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and
models. t-SNE-CUDA significantly outperforms current implementations with
50-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for
the first time, visualization of the neural network activations on the entire
ImageNet dataset - a feat that was previously computationally intractable. We
also demonstrate visualization performance in the NLP domain by visualizing the
GloVe embedding vectors. From these visualizations, we can draw interesting
conclusions about using the L2 metric in these embedding spaces. t-SNE-CUDA is
publicly available athttps://github.com/CannyLab/tsne-cuda
</summary>
    <author>
      <name>David M. Chan</name>
    </author>
    <author>
      <name>Roshan Rao</name>
    </author>
    <author>
      <name>Forrest Huang</name>
    </author>
    <author>
      <name>John F. Canny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in HPML 2018 High Performance Machine Learning Workshop
  (Accepted, 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05975v2</id>
    <updated>2018-07-31T13:48:56Z</updated>
    <published>2018-06-13T21:20:43Z</published>
    <title>Structured Variational Learning of Bayesian Neural Networks with
  Horseshoe Priors</title>
    <summary>  Bayesian Neural Networks (BNNs) have recently received increasing attention
for their ability to provide well-calibrated posterior uncertainties. However,
model selection---even choosing the number of nodes---remains an open question.
Recent work has proposed the use of a horseshoe prior over node pre-activations
of a Bayesian neural network, which effectively turns off nodes that do not
help explain the data. In this work, we propose several modeling and inference
advances that consistently improve the compactness of the model learned while
maintaining predictive performance, especially in smaller-sample settings
including reinforcement learning.
</summary>
    <author>
      <name>Soumya Ghosh</name>
    </author>
    <author>
      <name>Jiayu Yao</name>
    </author>
    <author>
      <name>Finale Doshi-Velez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018. v2 -- Minor edits and fixes typos. arXiv admin note: text
  overlap with arXiv:1705.10388</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05975v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05975v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11809v1</id>
    <updated>2018-07-31T13:30:03Z</updated>
    <published>2018-07-31T13:30:03Z</published>
    <title>Deep learning in agriculture: A survey</title>
    <summary>  Deep learning constitutes a recent, modern technique for image processing and
data analysis, with promising results and large potential. As deep learning has
been successfully applied in various domains, it has recently entered also the
domain of agriculture. In this paper, we perform a survey of 40 research
efforts that employ deep learning techniques, applied to various agricultural
and food production challenges. We examine the particular agricultural problems
under study, the specific models and frameworks employed, the sources, nature
and pre-processing of data used, and the overall performance achieved according
to the metrics used at each work under study. Moreover, we study comparisons of
deep learning with other existing popular techniques, in respect to differences
in classification or regression performance. Our findings indicate that deep
learning provides high accuracy, outperforming existing commonly used image
processing techniques.
</summary>
    <author>
      <name>Andreas Kamilaris</name>
    </author>
    <author>
      <name>Francesc X. Prenafeta-Boldu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.compag.2018.02.016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.compag.2018.02.016" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers and Electronics in Agriculture International Journal,
  2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.11809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11790v1</id>
    <updated>2018-07-31T12:42:46Z</updated>
    <published>2018-07-31T12:42:46Z</published>
    <title>Practical Constrained Optimization of Auction Mechanisms in E-Commerce
  Sponsored Search Advertising</title>
    <summary>  Sponsored search in E-commerce platforms such as Amazon, Taobao and Tmall
provides sellers an effective way to reach potential buyers with most relevant
purpose. In this paper, we study the auction mechanism optimization problem in
sponsored search on Alibaba's mobile E-commerce platform. Besides generating
revenue, we are supposed to maintain an efficient marketplace with plenty of
quality users, guarantee a reasonable return on investment (ROI) for
advertisers, and meanwhile, facilitate a pleasant shopping experience for the
users. These requirements essentially pose a constrained optimization problem.
Directly optimizing over auction parameters yields a discontinuous, non-convex
problem that denies effective solutions. One of our major contribution is a
practical convex optimization formulation of the original problem. We devise a
novel re-parametrization of auction mechanism with discrete sets of
representative instances. To construct the optimization problem, we build an
auction simulation system which estimates the resulted business indicators of
the selected parameters by replaying the auctions recorded from real online
requests. We summarized the experiments on real search traffics to analyze the
effects of fidelity of auction simulation, the efficacy under various
constraint targets and the influence of regularization. The experiment results
show that with proper entropy regularization, we are able to maximize revenue
while constraining other business indicators within given ranges.
</summary>
    <author>
      <name>Gang Bai</name>
    </author>
    <author>
      <name>Zhihui Xie</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00468v2</id>
    <updated>2018-07-31T12:08:59Z</updated>
    <published>2018-07-02T05:29:57Z</published>
    <title>Automated Directed Fairness Testing</title>
    <summary>  Fairness is a critical trait in decision making. As machine-learning models
are increasingly being used in sensitive application domains (e.g. education
and employment) for decision making, it is crucial that the decisions computed
by such models are free of unintended bias. But how can we automatically
validate the fairness of arbitrary machine-learning models? For a given
machine-learning model and a set of sensitive input parameters, our AEQUITAS
approach automatically discovers discriminatory inputs that highlight fairness
violation. At the core of AEQUITAS are three novel strategies to employ
probabilistic search over the input space with the objective of uncovering
fairness violation. Our AEQUITAS approach leverages inherent robustness
property in common machine-learning models to design and implement scalable
test generation methodologies. An appealing feature of our generated test
inputs is that they can be systematically added to the training set of the
underlying model and improve its fairness. To this end, we design a fully
automated module that guarantees to improve the fairness of the underlying
model.
  We implemented AEQUITAS and we have evaluated it on six state-of-the-art
classifiers, including a classifier that was designed with fairness
constraints. We show that AEQUITAS effectively generates inputs to uncover
fairness violation in all the subject classifiers and systematically improves
the fairness of the respective models using the generated test inputs. In our
evaluation, AEQUITAS generates up to 70% discriminatory inputs (w.r.t. the
total number of inputs generated) and leverages these inputs to improve the
fairness up to 94%.
</summary>
    <author>
      <name>Sakshi Udeshi</name>
    </author>
    <author>
      <name>Pryanshu Arora</name>
    </author>
    <author>
      <name>Sudipta Chattopadhyay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3238147.3238165</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3238147.3238165" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 2018 33rd ACM/IEEE International Conference on
  Automated Software Engineering (ASE 18), September 3-7, 2018, Montpellier,
  France</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Automated Directed Fairness Testing. In Proceedings of the 2018
  33rd ACM/IEEE International Conference on Automated Software Engineering (ASE
  18), September 3-7, 2018, Montpellier, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.00468v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00468v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03121v3</id>
    <updated>2018-07-31T11:58:24Z</updated>
    <published>2018-06-08T12:40:04Z</published>
    <title>Machine Learning CICY Threefolds</title>
    <summary>  The latest techniques from Neural Networks and Support Vector Machines (SVM)
are used to investigate geometric properties of Complete Intersection
Calabi-Yau (CICY) threefolds, a class of manifolds that facilitate string model
building. An advanced neural network classifier and SVM are employed to (1)
learn Hodge numbers and report a remarkable improvement over previous efforts,
(2) query for favourability, and (3) predict discrete symmetries, a highly
imbalanced problem to which both Synthetic Minority Oversampling Technique
(SMOTE) and permutations of the CICY matrix are used to decrease the class
imbalance and improve performance. In each case study, we employ a genetic
algorithm to optimise the hyperparameters of the neural network. We demonstrate
that our approach provides quick diagnostic tools capable of shortlisting
quasi-realistic string models based on compactification over smooth CICYs and
further supports the paradigm that classes of problems in algebraic geometry
can be machine learned.
</summary>
    <author>
      <name>Kieran Bull</name>
    </author>
    <author>
      <name>Yang-Hui He</name>
    </author>
    <author>
      <name>Vishnu Jejjala</name>
    </author>
    <author>
      <name>Challenger Mishra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physletb.2018.08.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physletb.2018.08.008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 figures. V2: Added new results of using permutations of
  the CICY matrix to reduce the class imbalance when predicting discrete
  symmetries. V3: Corrected typing errors in table 5</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03121v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03121v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00564v1</id>
    <updated>2018-07-31T10:38:29Z</updated>
    <published>2018-07-31T10:38:29Z</published>
    <title>Towards fully automated protein structure elucidation with NMR
  spectroscopy</title>
    <summary>  Nuclear magnetic resonance (NMR) spectroscopy is one of the leading
techniques for protein studies. The method features a number of properties,
allowing to explain macromolecular interactions mechanistically and resolve
structures with atomic resolution. However, due to laborious data analysis, a
full potential of NMR spectroscopy remains unexploited. Here we present an
approach aiming at automation of two major bottlenecks in the analysis
pipeline, namely, peak picking and chemical shift assignment. Our approach
combines deep learning, non-parametric models and combinatorial optimization,
and is able to detect signals of interest in a multidimensional NMR data with
high accuracy and match them with atoms in medium-length protein sequences,
which is a preliminary step to solve protein spatial structure.
</summary>
    <author>
      <name>Piotr Klukowski</name>
    </author>
    <author>
      <name>Adam Gonczarek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Workshop on Biomedical Informatics with
  Optimization and Machine Learning. ICML 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.00564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04458v2</id>
    <updated>2018-07-31T10:31:14Z</updated>
    <published>2018-06-12T12:16:54Z</published>
    <title>Sparse Stochastic Zeroth-Order Optimization with an Application to
  Bandit Structured Prediction</title>
    <summary>  Stochastic zeroth-order (SZO), or gradient-free, optimization allows to
optimize arbitrary functions by relying only on function evaluations under
parameter perturbations, however, the iteration complexity of SZO methods
suffers a factor proportional to the dimensionality of the perturbed function.
We show that in scenarios with natural sparsity patterns as in structured
prediction applications, this factor can be reduced to the expected number of
active features over input-output pairs. We give a general proof that applies
sparse SZO optimization to Lipschitz-continuous, nonconvex, stochastic
objectives, and present an experimental evaluation on linear bandit structured
prediction tasks with sparse word-based feature representations that confirm
our theoretical results.
</summary>
    <author>
      <name>Artem Sokolov</name>
    </author>
    <author>
      <name>Julian Hitschler</name>
    </author>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <link href="http://arxiv.org/abs/1806.04458v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04458v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10478v2</id>
    <updated>2018-07-31T10:02:41Z</updated>
    <published>2018-07-27T08:02:45Z</published>
    <title>Interpreting RNN behaviour via excitable network attractors</title>
    <summary>  Machine learning has become a basic tool in scientific research and for the
development of technologies with significant impact on society. In fact, such
methods allow to discover regularities in data and make predictions without
explicit knowledge of the rules governing the system under analysis. However, a
price must be paid for exploiting such a modeling flexibility: machine learning
methods are usually black-box, meaning that it is difficult to fully understand
what the machine is doing and how. This poses constraints on the applicability
of such methods, neglecting the possibility to gather novel scientific insights
from experimental data. Our research aims to open the black-box of recurrent
neural networks, an important family of neural networks suitable to process
sequential data. Here, we propose a novel methodology that allows to provide a
mechanistic interpretation of their behaviour when used to solve computational
tasks. The methodology is based on mathematical constructs called excitable
network attractors, which are models represented as networks in phase space
composed by stable attractors and excitable connections between them. As the
behaviour of recurrent neural networks depends on training and inputs driving
the autonomous system, we introduce an algorithm to extract network attractors
directly from a trajectory generated by the neural network while solving tasks.
Simulations conducted on a controlled benchmark highlight the relevance of the
proposed methodology for interpreting the behaviour of recurrent neural
networks on tasks that involve learning a finite number of stable states.
</summary>
    <author>
      <name>Andrea Ceni</name>
    </author>
    <author>
      <name>Peter Ashwin</name>
    </author>
    <author>
      <name>Lorenzo Livi</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10478v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10478v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01244v7</id>
    <updated>2018-07-31T09:44:24Z</updated>
    <published>2017-11-03T17:14:14Z</published>
    <title>Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory</title>
    <summary>  In meta-learning an agent extracts knowledge from observed tasks, aiming to
facilitate learning of novel future tasks. Under the assumption that future
tasks are 'related' to previous tasks, the accumulated knowledge should be
learned in a way which captures the common structure across learned tasks,
while allowing the learner sufficient flexibility to adapt to novel aspects of
new tasks. We present a framework for meta-learning that is based on
generalization error bounds, allowing us to extend various PAC-Bayes bounds to
meta-learning. Learning takes place through the construction of a distribution
over hypotheses based on the observed tasks, and its utilization for learning a
new task. Thus, prior knowledge is incorporated through setting an
experience-dependent prior for novel tasks. We develop a gradient-based
algorithm which minimizes an objective function derived from the bounds and
demonstrate its effectiveness numerically with deep neural networks. In
addition to establishing the improved performance available through
meta-learning, we demonstrate the intuitive way by which prior information is
manifested at different levels of the network.
</summary>
    <author>
      <name>Ron Amit</name>
    </author>
    <author>
      <name>Ron Meir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01244v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01244v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11393v2</id>
    <updated>2018-07-31T09:35:01Z</updated>
    <published>2018-07-30T15:13:49Z</published>
    <title>Making Classifier Chains Resilient to Class Imbalance</title>
    <summary>  Class imbalance is an intrinsic characteristic of multi-label data. Most of
the labels in multi-label data sets are associated with a small number of
training examples, much smaller compared to the size of the data set. Class
imbalance poses a key challenge that plagues most multi-label learning methods.
Ensemble of Classifier Chains (ECC), one of the most prominent multi-label
learning methods, is no exception to this rule, as each of the binary models it
builds is trained from all positive and negative examples of a label. To make
ECC resilient to class imbalance, we first couple it with random undersampling.
We then present two extensions of this basic approach, where we build a varying
number of binary models per label and construct chains of different sizes, in
order to improve the exploitation of majority examples with approximately the
same computational budget. Experimental results on 16 multi-label datasets
demonstrate the effectiveness of the proposed approaches in a variety of
evaluation metrics.
</summary>
    <author>
      <name>Bin Liu</name>
    </author>
    <author>
      <name>Grigorios Tsoumakas</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11393v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11393v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11718v1</id>
    <updated>2018-07-31T09:33:58Z</updated>
    <published>2018-07-31T09:33:58Z</published>
    <title>Using Feature Grouping as a Stochastic Regularizer for High-Dimensional
  Noisy Data</title>
    <summary>  The use of complex models --with many parameters-- is challenging with
high-dimensional small-sample problems: indeed, they face rapid overfitting.
Such situations are common when data collection is expensive, as in
neuroscience, biology, or geology. Dedicated regularization can be crafted to
tame overfit, typically via structured penalties. But rich penalties require
mathematical expertise and entail large computational costs. Stochastic
regularizers such as dropout are easier to implement: they prevent overfitting
by random perturbations. Used inside a stochastic optimizer, they come with
little additional cost. We propose a structured stochastic regularization that
relies on feature grouping. Using a fast clustering algorithm, we define a
family of groups of features that capture feature covariations. We then
randomly select these groups inside a stochastic gradient descent loop. This
procedure acts as a structured regularizer for high-dimensional correlated data
without additional computational cost and it has a denoising effect. We
demonstrate the performance of our approach for logistic regression both on a
sample-limited face image dataset with varying additive noise and on a typical
high-dimensional learning problem, brain image classification.
</summary>
    <author>
      <name>Sergul Aydore</name>
    </author>
    <author>
      <name>Bertrand Thirion</name>
    </author>
    <author>
      <name>Olivier Grisel</name>
    </author>
    <author>
      <name>Gael Varoquaux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05748v2</id>
    <updated>2018-07-31T09:25:36Z</updated>
    <published>2018-07-16T09:21:38Z</published>
    <title>Learning Stochastic Differential Equations With Gaussian Processes
  Without Gradient Matching</title>
    <summary>  We introduce a novel paradigm for learning non-parametric drift and diffusion
functions for stochastic differential equation (SDE). The proposed model learns
to simulate path distributions that match observations with non-uniform time
increments and arbitrary sparseness, which is in contrast with gradient
matching that does not optimize simulated responses. We formulate sensitivity
equations for learning and demonstrate that our general stochastic distribution
optimisation leads to robust and efficient learning of SDE systems.
</summary>
    <author>
      <name>Cagatay Yildiz</name>
    </author>
    <author>
      <name>Markus Heinonen</name>
    </author>
    <author>
      <name>Jukka Intosalmi</name>
    </author>
    <author>
      <name>Henrik Mannerström</name>
    </author>
    <author>
      <name>Harri Lähdesmäki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The accepted version of the paper to be presented in 2018 IEEE
  International Workshop on Machine Learning for Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05748v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05748v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11698v1</id>
    <updated>2018-07-31T08:16:08Z</updated>
    <published>2018-07-31T08:16:08Z</published>
    <title>Rank and Rate: Multi-task Learning for Recommender Systems</title>
    <summary>  The two main tasks in the Recommender Systems domain are the ranking and
rating prediction tasks. The rating prediction task aims at predicting to what
extent a user would like any given item, which would enable to recommend the
items with the highest predicted scores. The ranking task on the other hand
directly aims at recommending the most valuable items for the user. Several
previous approaches proposed learning user and item representations to optimize
both tasks simultaneously in a multi-task framework. In this work we propose a
novel multi-task framework that exploits the fact that a user does a two-phase
decision process - first decides to interact with an item (ranking task) and
only afterward to rate it (rating prediction task). We evaluated our framework
on two benchmark datasets, on two different configurations and showed its
superiority over state-of-the-art methods.
</summary>
    <author>
      <name>Guy Hadash</name>
    </author>
    <author>
      <name>Oren Sar Shalom</name>
    </author>
    <author>
      <name>Rita Osadchy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240323.3240406</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240323.3240406" rel="related"/>
    <link href="http://arxiv.org/abs/1807.11698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11697v1</id>
    <updated>2018-07-31T08:08:40Z</updated>
    <published>2018-07-31T08:08:40Z</published>
    <title>Multimodal Deep Domain Adaptation</title>
    <summary>  Typically a classifier trained on a given dataset (source domain) does not
performs well if it is tested on data acquired in a different setting (target
domain). This is the problem that domain adaptation (DA) tries to overcome and,
while it is a well explored topic in computer vision, it is largely ignored in
robotic vision where usually visual classification methods are trained and
tested in the same domain. Robots should be able to deal with unknown
environments, recognize objects and use them in the correct way, so it is
important to explore the domain adaptation scenario also in this context. The
goal of the project is to define a benchmark and a protocol for multi-modal
domain adaptation that is valuable for the robot vision community. With this
purpose some of the state-of-the-art DA methods are selected: Deep Adaptation
Network (DAN), Domain Adversarial Training of Neural Network (DANN), Automatic
Domain Alignment Layers (AutoDIAL) and Adversarial Discriminative Domain
Adaptation (ADDA). Evaluations have been done using different data types: RGB
only, depth only and RGB-D over the following datasets, designed for the
robotic community: RGB-D Object Dataset (ROD), Web Object Dataset (WOD),
Autonomous Robot Indoor Dataset (ARID), Big Berkeley Instance Recognition
Dataset (BigBIRD) and Active Vision Dataset. Although progresses have been made
on the formulation of effective adaptation algorithms and more realistic object
datasets are available, the results obtained show that, training a sufficiently
good object classifier, especially in the domain adaptation scenario, is still
an unsolved problem. Also the best way to combine depth with RGB informations
to improve the performance is a point that needs to be investigated more.
</summary>
    <author>
      <name>Silvia Bucci</name>
    </author>
    <author>
      <name>Mohammad Reza Loghmani</name>
    </author>
    <author>
      <name>Barbara Caputo</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11694v1</id>
    <updated>2018-07-31T07:49:59Z</updated>
    <published>2018-07-31T07:49:59Z</published>
    <title>Spectrum concentration in deep residual learning: a free probability
  appproach</title>
    <summary>  We revisit the initialization of deep residual networks (ResNets) by
introducing a novel analytical tool in free probability to the community of
deep learning. This tool deals with non-Hermitian random matrices, rather than
their conventional Hermitian counterparts in the literature. As a consequence,
this new tool enables us to evaluate the singular value spectrum of the
input-output Jacobian of a fully- connected deep ResNet for both linear and
nonlinear cases. With the powerful tool of free probability, we conduct an
asymptotic analysis of the spectrum on the single-layer case, and then extend
this analysis to the multi-layer case of an arbitrary number of layers. In
particular, we propose to rescale the classical random initialization by the
number of residual units, so that the spectrum has the order of $O(1)$, when
compared with the large width and depth of the network. We empirically
demonstrate that the proposed initialization scheme learns at a speed of orders
of magnitudes faster than the classical ones, and thus attests a strong
practical relevance of this investigation.
</summary>
    <author>
      <name>Zenan Ling</name>
    </author>
    <author>
      <name>Robert C. Qiu</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10733v2</id>
    <updated>2018-07-31T07:22:55Z</updated>
    <published>2017-11-29T09:04:27Z</published>
    <title>DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching
  Problems</title>
    <summary>  In this work we study convex relaxations of quadratic optimisation problems
over permutation matrices. While existing semidefinite programming approaches
can achieve remarkably tight relaxations, they have the strong disadvantage
that they lift the original $n {\times} n$-dimensional variable to an $n^2
{\times} n^2$-dimensional variable, which limits their practical applicability.
In contrast, here we present a lifting-free convex relaxation that is provably
at least as tight as existing (lifting-free) convex relaxations. We demonstrate
experimentally that our approach is superior to existing convex and non-convex
methods for various problems, including image arrangement and multi-graph
matching.
</summary>
    <author>
      <name>Florian Bernard</name>
    </author>
    <author>
      <name>Christian Theobalt</name>
    </author>
    <author>
      <name>Michael Moeller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at CVPR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10733v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10733v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07754v2</id>
    <updated>2018-07-31T07:15:22Z</updated>
    <published>2018-07-20T09:35:25Z</published>
    <title>Learning the effect of latent variables in Gaussian Graphical models
  with unobserved variables</title>
    <summary>  The edge structure of the graph defining an undirected graphical model
describes precisely the structure of dependence between the variables in the
graph. In many applications, the dependence structure is unknown and it is
desirable to learn it from data, often because it is a preliminary step to be
able to ascertain causal effects. This problem, known as structure learning, is
hard in general, but for Gaussian graphical models it is slightly easier
because the structure of the graph is given by the sparsity pattern of the
precision matrix of the joint distribution, and because independence coincides
with decorrelation. A major difficulty too often ignored in structure learning
is the fact that if some variables are not observed, the marginal dependence
graph over the observed variables will possibly be significantly more complex
and no longer reflect the direct dependencies that are potentially associated
with causal effects. In this work, we consider a family of latent variable
Gaussian graphical models in which the graph of the joint distribution between
observed and unobserved variables is sparse, and the unobserved variables are
conditionally independent given the others. Prior work was able to recover the
connectivity between observed variables, but could only identify the subspace
spanned by unobserved variables, whereas we propose a convex optimization
formulation based on structured matrix sparsity to estimate the complete
connectivity of the complete graph including unobserved variables, given the
knowledge of the number of missing variables, and a priori knowledge of their
level of connectivity. Our formulation is supported by a theoretical result of
identifiability of the latent dependence structure for sparse graphs in the
infinite data limit. We propose an algorithm leveraging recent active set
methods, which performs well in the experiments on synthetic data.
</summary>
    <author>
      <name>Marina Vinyes</name>
    </author>
    <author>
      <name>Guillaume Obozinski</name>
    </author>
    <link href="http://arxiv.org/abs/1807.07754v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07754v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11682v1</id>
    <updated>2018-07-31T06:54:28Z</updated>
    <published>2018-07-31T06:54:28Z</published>
    <title>Deep Belief Networks Based Feature Generation and Regression for
  Predicting Wind Power</title>
    <summary>  Wind energy forecasting helps to manage power production, and hence, reduces
energy cost. Deep Neural Networks (DNN) mimics hierarchical learning in the
human brain and thus possesses hierarchical, distributed, and multi-task
learning capabilities. Based on aforementioned characteristics, we report Deep
Belief Network (DBN) based forecast engine for wind power prediction because of
its good generalization and unsupervised pre-training attributes. The proposed
DBN-WP forecast engine, which exhibits stochastic feature generation
capabilities and is composed of multiple Restricted Boltzmann Machines,
generates suitable features for wind power prediction using atmospheric
properties as input. DBN-WP, due to its unsupervised pre-training of RBM layers
and generalization capabilities, is able to learn the fluctuations in the
meteorological properties and thus is able to perform effective mapping of the
wind power. In the deep network, a regression layer is appended at the end to
predict sort-term wind power. It is experimentally shown that the deep learning
and unsupervised pre-training capabilities of DBN based model has comparable
and in some cases better results than hybrid and complex learning techniques
proposed for wind power prediction. The proposed prediction system based on
DBN, achieves mean values of RMSE, MAE and SDE as 0.124, 0.083 and 0.122,
respectively. Statistical analysis of several independent executions of the
proposed DBN-WP wind power prediction system demonstrates the stability of the
system. The proposed DBN-WP architecture is easy to implement and offers
generalization as regards the change in location of the wind farm is concerned.
</summary>
    <author>
      <name>Asifullah Khan</name>
    </author>
    <author>
      <name>Aneela Zameer</name>
    </author>
    <author>
      <name>Tauseef Jamal</name>
    </author>
    <author>
      <name>Ahmad Raza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages:31 Figure:11 Table:5</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11679v1</id>
    <updated>2018-07-31T06:38:54Z</updated>
    <published>2018-07-31T06:38:54Z</published>
    <title>Wasserstein GAN and Waveform Loss-based Acoustic Model Training for
  Multi-speaker Text-to-Speech Synthesis Systems Using a WaveNet Vocoder</title>
    <summary>  Recent neural networks such as WaveNet and sampleRNN that learn directly from
speech waveform samples have achieved very high-quality synthetic speech in
terms of both naturalness and speaker similarity even in multi-speaker
text-to-speech synthesis systems. Such neural networks are being used as an
alternative to vocoders and hence they are often called neural vocoders. The
neural vocoder uses acoustic features as local condition parameters, and these
parameters need to be accurately predicted by another acoustic model. However,
it is not yet clear how to train this acoustic model, which is problematic
because the final quality of synthetic speech is significantly affected by the
performance of the acoustic model. Significant degradation happens, especially
when predicted acoustic features have mismatched characteristics compared to
natural ones. In order to reduce the mismatched characteristics between natural
and generated acoustic features, we propose frameworks that incorporate either
a conditional generative adversarial network (GAN) or its variant, Wasserstein
GAN with gradient penalty (WGAN-GP), into multi-speaker speech synthesis that
uses the WaveNet vocoder. We also extend the GAN frameworks and use the
discretized mixture logistic loss of a well-trained WaveNet in addition to mean
squared error and adversarial losses as parts of objective functions.
Experimental results show that acoustic models trained using the WGAN-GP
framework using back-propagated discretized-mixture-of-logistics (DML) loss
achieves the highest subjective evaluation scores in terms of both quality and
speaker similarity.
</summary>
    <author>
      <name>Yi Zhao</name>
    </author>
    <author>
      <name>Shinji Takaki</name>
    </author>
    <author>
      <name>Hieu-Thi Luong</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <author>
      <name>Daisuke Saito</name>
    </author>
    <author>
      <name>Nobuaki Minematsu</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02329v4</id>
    <updated>2018-07-31T04:31:27Z</updated>
    <published>2017-07-10T02:29:17Z</published>
    <title>Deep Q-Learning for Self-Organizing Networks Fault Management and Radio
  Performance Improvement</title>
    <summary>  We propose an algorithm to automate fault management in an outdoor cellular
network using deep reinforcement learning (RL) against wireless impairments.
This algorithm enables the cellular network cluster to self-heal by allowing RL
to learn how to improve the downlink signal to interference plus noise ratio
and spectral efficiency through exploration and exploitation of various alarm
corrective actions. The main contributions of this paper are to 1) introduce a
deep RL-based fault handling algorithm which self-organizing networks can
implement in a polynomial runtime and 2) show that this fault management method
can improve the radio link performance in a realistic network setup. Simulation
results show that our proposed learns an action sequence to clear alarms and
improve the performance in the cellular cluster better than existing
algorithms, even against the randomness of the network fault occurrences and
user movements.
</summary>
    <author>
      <name>Faris B. Mismar</name>
    </author>
    <author>
      <name>Brian L. Evans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, accepted to 52nd Annual Asilomar Conference on
  Signals, Systems, and Computers on July 8, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02329v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02329v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11655v1</id>
    <updated>2018-07-31T04:18:26Z</updated>
    <published>2018-07-31T04:18:26Z</published>
    <title>Security and Privacy Issues in Deep Learning</title>
    <summary>  With the development of machine learning, expectations for artificial
intelligence (AI) technology are increasing day by day. In particular, deep
learning has shown enriched performance results in a variety of fields. There
are many applications that are closely related to our daily life, such as
making significant decisions in application area based on predictions or
classifications, in which a deep learning (DL) model could be relevant. Hence,
if a DL model causes mispredictions or misclassifications due to malicious
external influences, it can cause very large difficulties in real life.
Moreover, training deep learning models involves relying on an enormous amount
of data and the training data often includes sensitive information. Therefore,
deep learning models should not expose the privacy of such data. In this paper,
we reviewed the threats and developed defense methods on the security of the
models and the data privacy under the notion of SPAI: Secure and Private AI. We
also discuss current challenges and open issues.
</summary>
    <author>
      <name>Ho Bae</name>
    </author>
    <author>
      <name>Jaehee Jang</name>
    </author>
    <author>
      <name>Dahuin Jung</name>
    </author>
    <author>
      <name>Hyemi Jang</name>
    </author>
    <author>
      <name>Heonseok Ha</name>
    </author>
    <author>
      <name>Sungroh Yoon</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11648v1</id>
    <updated>2018-07-31T03:27:34Z</updated>
    <published>2018-07-31T03:27:34Z</published>
    <title>Composable Core-sets for Determinant Maximization Problems via Spectral
  Spanners</title>
    <summary>  We study a spectral generalization of classical combinatorial graph spanners
to the spectral setting. Given a set of vectors $V\subseteq \Re^d$, we say a
set $U\subseteq V$ is an $\alpha$-spectral spanner if for all $v\in V$ there is
a probability distribution $\mu_v$ supported on $U$ such that $$vv^\intercal
\preceq \alpha\cdot\mathbb{E}_{u\sim\mu_v} uu^\intercal.$$ We show that any set
$V$ has an $\tilde{O}(d)$-spectral spanner of size $\tilde{O}(d)$ and this
bound is almost optimal in the worst case.
  We use spectral spanners to study composable core-sets for spectral problems.
We show that for many objective functions one can use a spectral spanner,
independent of the underlying functions, as a core-set and obtain almost
optimal composable core-sets. For example, for the determinant maximization
problem we obtain an $\tilde{O}(k)^k$-composable core-set and we show that this
is almost optimal in the worst case.
  Our algorithm is a spectral analogue of the classical greedy algorithm for
finding (combinatorial) spanners in graphs. We expect that our spanners find
many other applications in distributed or parallel models of computation. Our
proof is spectral. As a side result of our techniques, we show that the rank of
diagonally dominant lower-triangular matrices are robust under `small
perturbations' which could be of independent interests.
</summary>
    <author>
      <name>Piotr Indyk</name>
    </author>
    <author>
      <name>Sepideh Mahabadi</name>
    </author>
    <author>
      <name>Shayan Oveis Gharan</name>
    </author>
    <author>
      <name>Alireza Rezaei</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04228v1</id>
    <updated>2018-07-31T02:33:34Z</updated>
    <published>2018-07-31T02:33:34Z</published>
    <title>DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human
  Activity Recognition</title>
    <summary>  Deep Convolutional Neural Networks (DCNNs) are currently popular in human
activity recognition applications. However, in the face of modern artificial
intelligence sensor-based games, many research achievements cannot be
practically applied on portable devices. DCNNs are typically resource-intensive
and too large to be deployed on portable devices, thus this limits the
practical application of complex activity detection. In addition, since
portable devices do not possess high-performance Graphic Processing Units
(GPUs), there is hardly any improvement in Action Game (ACT) experience.
Besides, in order to deal with multi-sensor collaboration, all previous human
activity recognition models typically treated the representations from
different sensor signal sources equally. However, distinct types of activities
should adopt different fusion strategies. In this paper, a novel scheme is
proposed. This scheme is used to train 2-bit Convolutional Neural Networks with
weights and activations constrained to {-0.5,0,0.5}. It takes into account the
correlation between different sensor signal sources and the activity types.
This model, which we refer to as DFTerNet, aims at producing a more reliable
inference and better trade-offs for practical applications. Our basic idea is
to exploit quantization of weights and activations directly in pre-trained
filter banks and adopt dynamic fusion strategies for different activity types.
Experiments demonstrate that by using dynamic fusion strategy can exceed the
baseline model performance by up to ~5% on activity recognition like
OPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we
were able to achieve performances closer to that of full-precision counterpart.
These results were also verified using the UniMiB-SHAR dataset. In addition,
the proposed method can achieve ~9x acceleration on CPUs and ~11x memory
saving.
</summary>
    <author>
      <name>Zhan Yang</name>
    </author>
    <author>
      <name>Osolo Ian Raymond</name>
    </author>
    <author>
      <name>ChengYuan Zhang</name>
    </author>
    <author>
      <name>Ying Wan</name>
    </author>
    <author>
      <name>Jun Long</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 5 figures, 6 tables, submitted to IEEE Access</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11632v1</id>
    <updated>2018-07-31T02:29:41Z</updated>
    <published>2018-07-31T02:29:41Z</published>
    <title>Scaling and bias codes for modeling speaker-adaptive DNN-based speech
  synthesis systems</title>
    <summary>  Most neural-network based speaker-adaptive acoustic models for speech
synthesis can be categorized into either layer-based or input-code approaches.
Although both approaches have their own pros and cons, most existing works on
speaker adaptation focus on improving one or the other. In this paper, after we
first systematically overview the common principles of neural-network based
speaker-adaptive models, we show that these approaches can be represented in a
unified framework and can be generalized further. More specifically, we
introduce the use of scaling and bias codes as generalized means for
speaker-adaptive transformation. By utilizing these codes, we can create a more
efficient factorized speaker-adaptive model and capture advantages of both
approaches while reducing their disadvantages. The experiments show that the
proposed method can improve the performance of speaker adaptation compared with
speaker adaptation based on the conventional input code.
</summary>
    <author>
      <name>Hieu-Thi Luong</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to 2018 IEEE Workshop on Spoken Language Technology (SLT),
  Athens, Greece</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11620v1</id>
    <updated>2018-07-31T01:10:04Z</updated>
    <published>2018-07-31T01:10:04Z</published>
    <title>K-medoids Clustering of Data Sequences with Composite Distributions</title>
    <summary>  This paper studies clustering of data sequences using the k-medoids
algorithm. All the data sequences are assumed to be generated from
\emph{unknown} continuous distributions, which form clusters with each cluster
containing a composite set of closely located distributions (based on a certain
distance metric between distributions). The maximum intra-cluster distance is
assumed to be smaller than the minimum inter-cluster distance, and both values
are assumed to be known. The goal is to group the data sequences together if
their underlying generative distributions (which are unknown) belong to one
cluster. Distribution distance metrics based k-medoids algorithms are proposed
for known and unknown number of distribution clusters. Upper bounds on the
error probability and convergence results in the large sample regime are also
provided. It is shown that the error probability decays exponentially fast as
the number of samples in each data sequence goes to infinity. The error
exponent has a simple form regardless of the distance metric applied when
certain conditions are satisfied. In particular, the error exponent is
characterized when either the Kolmogrov-Smirnov distance or the maximum mean
discrepancy are used as the distance metric. Simulation results are provided to
validate the analysis.
</summary>
    <author>
      <name>Tiexing Wang</name>
    </author>
    <author>
      <name>Qunwei Li</name>
    </author>
    <author>
      <name>Donald J. Bucci</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <author>
      <name>Biao Chen</name>
    </author>
    <author>
      <name>Pramod K. Varshney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 two-column pages, submitted to IEEE Transactions on Signal
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11158v2</id>
    <updated>2018-07-31T01:01:55Z</updated>
    <published>2018-07-30T03:27:04Z</published>
    <title>Robust Student Network Learning</title>
    <summary>  Deep neural networks bring in impressive accuracy in various applications,
but the success often relies on the heavy network architecture. Taking
well-trained heavy networks as teachers, classical teacher-student learning
paradigm aims to learn a student network that is lightweight yet accurate. In
this way, a portable student network with significantly fewer parameters can
achieve a considerable accuracy which is comparable to that of teacher network.
However, beyond accuracy, robustness of the learned student network against
perturbation is also essential for practical uses. Existing teacher-student
learning frameworks mainly focus on accuracy and compression ratios, but ignore
the robustness. In this paper, we make the student network produce more
confident predictions with the help of the teacher network, and analyze the
lower bound of the perturbation that will destroy the confidence of the student
network. Two important objectives regarding prediction scores and gradients of
examples are developed to maximize this lower bound, so as to enhance the
robustness of the student network without sacrificing the performance.
Experiments on benchmark datasets demonstrate the efficiency of the proposed
approach to learn robust student networks which have satisfying accuracy and
compact sizes.
</summary>
    <author>
      <name>Tianyu Guo</name>
    </author>
    <author>
      <name>Chang Xu</name>
    </author>
    <author>
      <name>Shiyi He</name>
    </author>
    <author>
      <name>Boxin Shi</name>
    </author>
    <author>
      <name>Chao Xu</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11158v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11158v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01610v2</id>
    <updated>2018-07-30T22:46:24Z</updated>
    <published>2018-02-05T19:22:34Z</published>
    <title>Fast and accurate approximation of the full conditional for gamma shape
  parameters</title>
    <summary>  The gamma distribution arises frequently in Bayesian models, but there is not
an easy-to-use conjugate prior for the shape parameter of a gamma. This
inconvenience is usually dealt with by using either Metropolis-Hastings moves,
rejection sampling methods, or numerical integration. However, in models with a
large number of shape parameters, these existing methods are slower or more
complicated than one would like, making them burdensome in practice. It turns
out that the full conditional distribution of the gamma shape parameter is well
approximated by a gamma distribution, even for small sample sizes, when the
prior on the shape parameter is also a gamma distribution. This article
introduces a quick and easy algorithm for finding a gamma distribution that
approximates the full conditional distribution of the shape parameter. We
empirically demonstrate the speed and accuracy of the approximation across a
wide range of conditions. If exactness is required, the approximation can be
used as a proposal distribution for Metropolis-Hastings.
</summary>
    <author>
      <name>Jeffrey W. Miller</name>
    </author>
    <link href="http://arxiv.org/abs/1802.01610v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01610v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03269v5</id>
    <updated>2018-07-30T22:03:53Z</updated>
    <published>2017-07-10T02:30:01Z</published>
    <title>Q-Learning Algorithm for VoLTE Closed-Loop Power Control in Indoor Small
  Cells</title>
    <summary>  We propose a reinforcement learning (RL) based closed loop power control
algorithm for the downlink of the voice over LTE (VoLTE) radio bearer for an
indoor environment served by small cells. The main contributions of our paper
are to 1) use RL to solve performance tuning problems in an indoor cellular
network for voice bearers and 2) show that our derived lower bound loss in
effective SINR is sufficient for VoLTE power control purposes in practical
cellular networks. In our simulation, the proposed RL-based power control
algorithm significantly improves both voice retainability and mean opinion
score compared to current industry standards. The improvement is due to
maintaining an effective downlink signal to interference plus noise ratio
against adverse network operational issues and faults.
</summary>
    <author>
      <name>Faris B. Mismar</name>
    </author>
    <author>
      <name>Brian L. Evans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, accepted to 52nd Annual Asilomar Conference on
  Signals, Systems, and Computers on July 5, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.03269v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03269v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08263v2</id>
    <updated>2018-07-30T21:57:21Z</updated>
    <published>2017-06-26T07:45:55Z</published>
    <title>Efficient Manifold and Subspace Approximations with Spherelets</title>
    <summary>  Data lying in a high dimensional ambient space are commonly thought to have a
much lower intrinsic dimension. In particular, the data may be concentrated
near a lower-dimensional subspace or manifold. There is an immense literature
focused on approximating the unknown subspace, and in exploiting such
approximations in clustering, data compression, and building of predictive
models. Most of the literature relies on approximating subspaces using a
locally linear, and potentially multiscale, dictionary. In this article, we
propose a simple and general alternative, which instead uses pieces of spheres,
or spherelets, to locally approximate the unknown subspace. Theory is developed
showing that spherelets can produce lower covering numbers and MSEs for many
manifolds. We develop spherical principal components analysis (SPCA). Results
relative to state-of-the-art competitors show gains in ability to accurately
approximate the subspace with fewer components. In addition, unlike most
competitors, our approach can be used for data denoising and can efficiently
embed new data without retraining. The methods are illustrated with standard
toy manifold learning examples, and applications to multiple real data sets.
</summary>
    <author>
      <name>Didong Li</name>
    </author>
    <author>
      <name>Minerva Mukhopadhyay</name>
    </author>
    <author>
      <name>David B. Dunson</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08263v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08263v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05428v4</id>
    <updated>2018-07-30T20:53:57Z</updated>
    <published>2018-03-13T21:14:46Z</published>
    <title>A Hierarchical Latent Vector Model for Learning Long-Term Structure in
  Music</title>
    <summary>  The Variational Autoencoder (VAE) has proven to be an effective model for
producing semantically meaningful latent representations for natural data.
However, it has thus far seen limited application to sequential data, and, as
we demonstrate, existing recurrent VAE models have difficulty modeling
sequences with long-term structure. To address this issue, we propose the use
of a hierarchical decoder, which first outputs embeddings for subsequences of
the input and then uses these embeddings to generate each subsequence
independently. This structure encourages the model to utilize its latent code,
thereby avoiding the "posterior collapse" problem which remains an issue for
recurrent VAEs. We apply this architecture to modeling sequences of musical
notes and find that it exhibits dramatically better sampling, interpolation,
and reconstruction performance than a "flat" baseline model. An implementation
of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.
</summary>
    <author>
      <name>Adam Roberts</name>
    </author>
    <author>
      <name>Jesse Engel</name>
    </author>
    <author>
      <name>Colin Raffel</name>
    </author>
    <author>
      <name>Curtis Hawthorne</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML Camera Ready Version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.05428v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05428v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08010v2</id>
    <updated>2018-07-30T19:48:40Z</updated>
    <published>2018-06-20T22:17:08Z</published>
    <title>Fairness Without Demographics in Repeated Loss Minimization</title>
    <summary>  Machine learning models (e.g., speech recognizers) are usually trained to
minimize average loss, which results in representation disparity---minority
groups (e.g., non-native speakers) contribute less to the training objective
and thus tend to suffer higher loss. Worse, as model accuracy affects user
retention, a minority group can shrink over time. In this paper, we first show
that the status quo of empirical risk minimization (ERM) amplifies
representation disparity over time, which can even make initially fair models
unfair. To mitigate this, we develop an approach based on distributionally
robust optimization (DRO), which minimizes the worst case risk over all
distributions close to the empirical distribution. We prove that this approach
controls the risk of the minority group at each time step, in the spirit of
Rawlsian distributive justice, while remaining oblivious to the identity of the
groups. We demonstrate that DRO prevents disparity amplification on examples
where ERM fails, and show improvements in minority group user satisfaction in a
real-world text autocomplete task.
</summary>
    <author>
      <name>Tatsunori B. Hashimoto</name>
    </author>
    <author>
      <name>Megha Srivastava</name>
    </author>
    <author>
      <name>Hongseok Namkoong</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version for ICML2018, corrects typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11545v1</id>
    <updated>2018-07-30T19:37:56Z</updated>
    <published>2018-07-30T19:37:56Z</published>
    <title>Call Detail Records Driven Anomaly Detection and Traffic Prediction in
  Mobile Cellular Networks</title>
    <summary>  Mobile networks possess information about the users as well as the network.
Such information is useful for making the network end-to-end visible and
intelligent. Big data analytics can efficiently analyze user and network
information, unearth meaningful insights with the help of machine learning
tools. Utilizing big data analytics and machine learning, this work contributes
in three ways. First, we utilize the call detail records (CDR) data to detect
anomalies in the network. For authentication and verification of anomalies, we
use k-means clustering, an unsupervised machine learning algorithm. Through
effective detection of anomalies, we can proceed to suitable design for
resource distribution as well as fault detection and avoidance. Second, we
prepare anomaly-free data by removing anomalous activities and train a neural
network model. By passing anomaly and anomaly-free data through this model, we
observe the effect of anomalous activities in training of the model and also
observe mean square error of anomaly and anomaly free data. Lastly, we use an
autoregressive integrated moving average (ARIMA) model to predict future
traffic for a user. Through simple visualization, we show that anomaly free
data better generalizes the learning models and performs better on prediction
task.
</summary>
    <author>
      <name>Kashif Sultan</name>
    </author>
    <author>
      <name>Hazrat Ali</name>
    </author>
    <author>
      <name>Zhongshan Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2018.2859756</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2018.2859756" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access Journal paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2018.2859756</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.11545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04153v7</id>
    <updated>2018-07-30T18:44:13Z</updated>
    <published>2018-01-12T12:49:32Z</published>
    <title>Bayesian Quadrature for Multiple Related Integrals</title>
    <summary>  Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then prove convergence rates for the method in the well-specified and
misspecified cases, and demonstrate its efficiency in the context of
multi-fidelity models for complex engineering systems and a problem of global
illumination in computer graphics.
</summary>
    <author>
      <name>Xiaoyue Xi</name>
    </author>
    <author>
      <name>François-Xavier Briol</name>
    </author>
    <author>
      <name>Mark Girolami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 35th International Conference on Machine Learning
  (ICML), PMLR 80:5369-5378, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04153v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04153v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03234v3</id>
    <updated>2018-07-30T18:29:03Z</updated>
    <published>2018-03-08T18:07:40Z</published>
    <title>Improving Optimization for Models With Continuous Symmetry Breaking</title>
    <summary>  Many loss functions in representation learning are invariant under a
continuous symmetry transformation. For example, the loss function of word
embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate
all word and context embedding vectors. We show that representation learning
models for time series possess an approximate continuous symmetry that leads to
slow convergence of gradient descent. We propose a new optimization algorithm
that speeds up convergence using ideas from gauge theory in physics. Our
algorithm leads to orders of magnitude faster convergence and to more
interpretable representations, as we show for dynamic extensions of matrix
factorization and word embedding models. We further present an example
application of our proposed algorithm that translates modern words into their
historic equivalents.
</summary>
    <author>
      <name>Robert Bamler</name>
    </author>
    <author>
      <name>Stephan Mandt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the proceedings of International Conference on Machine Learning
  (ICML 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03234v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03234v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03654v3</id>
    <updated>2018-07-30T18:02:11Z</updated>
    <published>2018-02-10T22:22:03Z</published>
    <title>Beyond the One Step Greedy Approach in Reinforcement Learning</title>
    <summary>  The famous Policy Iteration algorithm alternates between policy improvement
and policy evaluation. Implementations of this algorithm with several variants
of the latter evaluation stage, e.g, $n$-step and trace-based returns, have
been analyzed in previous works. However, the case of multiple-step lookahead
policy improvement, despite the recent increase in empirical evidence of its
strength, has to our knowledge not been carefully analyzed yet. In this work,
we introduce the first such analysis. Namely, we formulate variants of
multiple-step policy improvement, derive new algorithms using these definitions
and prove their convergence. Moreover, we show that recent prominent
Reinforcement Learning algorithms are, in fact, instances of our framework. We
thus shed light on their empirical success and give a recipe for deriving new
algorithms for future study.
</summary>
    <author>
      <name>Yonathan Efroni</name>
    </author>
    <author>
      <name>Gal Dalal</name>
    </author>
    <author>
      <name>Bruno Scherrer</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03654v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03654v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11459v1</id>
    <updated>2018-07-30T17:34:24Z</updated>
    <published>2018-07-30T17:34:24Z</published>
    <title>Improving Transferability of Deep Neural Networks</title>
    <summary>  Learning from small amounts of labeled data is a challenge in the area of
deep learning. This is currently addressed by Transfer Learning where one
learns the small data set as a transfer task from a larger source dataset.
Transfer Learning can deliver higher accuracy if the hyperparameters and source
dataset are chosen well. One of the important parameters is the learning rate
for the layers of the neural network. We show through experiments on the
ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range
of 127% can be obtained by proper choice of learning rates. We also show that
the images/label parameter for a dataset can potentially be used to determine
optimal learning rates for the layers to get the best overall accuracy. We
additionally validate this method on a sample of real-world image
classification tasks from a public visual recognition API.
</summary>
    <author>
      <name>Parijat Dube</name>
    </author>
    <author>
      <name>Bishwaranjan Bhattacharjee</name>
    </author>
    <author>
      <name>Elisabeth Petit-Bois</name>
    </author>
    <author>
      <name>Matthew Hill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures, 2 tables, Workshop on Domain Adaptation for
  Visual Understanding (Joint IJCAI/ECAI/AAMAS/ICML 2018 Workshop) Keywords:
  deep learning, transfer learning, finetuning, deep neural network,
  experimental</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11455v1</id>
    <updated>2018-07-30T17:23:50Z</updated>
    <published>2018-07-30T17:23:50Z</published>
    <title>Factor analysis of dynamic PET images: beyond Gaussian noise</title>
    <summary>  Factor analysis has proven to be a relevant tool for extracting tissue
time-activity curves (TACs) in dynamic PET images, since it allows for an
unsupervised analysis of the data. To provide reliable and interpretable
outputs, it requires to be conducted with respect to a suitable noise
statistics. However, the noise in reconstructed dynamic PET images is very
difficult to characterize, despite the Poissonian nature of the count-rates.
Rather than explicitly modeling the noise distribution, this work proposes to
study the relevance of several divergence measures to be used within a factor
analysis framework. To this end, the $\beta$-divergence, widely used in other
applicative domains, is considered to design the data-fitting term involved in
three different factor models. The performances of the resulting algorithms are
evaluated for different values of $\beta$, in a range covering Gaussian,
Poissonian and Gamma-distributed noises. The results obtained on two different
types of synthetic images and one real image show the interest of applying
non-standard values of $\beta$ to improve factor analysis.
</summary>
    <author>
      <name>Yanna Cruz Cavalcanti</name>
    </author>
    <author>
      <name>Thomas Oberlin</name>
    </author>
    <author>
      <name>Nicolas Dobigeon</name>
    </author>
    <author>
      <name>Simon Stute</name>
    </author>
    <author>
      <name>Maria-Joao Ribeiro</name>
    </author>
    <author>
      <name>Clovis Tauber</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07525v2</id>
    <updated>2018-07-30T16:43:23Z</updated>
    <published>2018-07-19T16:35:59Z</published>
    <title>Emulating malware authors for proactive protection using GANs over a
  distributed image visualization of dynamic file behavior</title>
    <summary>  Malware authors have always been at an advantage of being able to
adversarially test and augment their malicious code, before deploying the
payload, using anti-malware products at their disposal. The anti-malware
developers and threat experts, on the other hand, do not have such a privilege
of tuning anti-malware products against zero-day attacks pro-actively. This
allows the malware authors to being a step ahead of the anti-malware products,
fundamentally biasing the cat and mouse game played by the two parties. In this
paper, we propose a way that would enable machine learning based threat
prevention models to bridge that gap by being able to tune against a deep
generative adversarial network (GAN), which takes up the role of a malware
author and generates new types of malware. The GAN is trained over a reversible
distributed RGB image representation of known malware behaviors, encoding the
sequence of API call ngrams and the corresponding term frequencies. The
generated images represent synthetic malware that can be decoded back to the
underlying API call sequence information. The image representation is not only
demonstrated as a general technique of incorporating necessary priors for
exploiting convolutional neural network architectures for generative or
discriminative modeling, but also as a visualization method for easy manual
software or malware categorization, by having individual API ngram information
distributed across the image space. In addition, we also propose using
smart-definitions for detecting malwares based on perceptual hashing of these
images. Such hashes are potentially more effective than cryptographic hashes
that do not carry any meaningful similarity metric, and hence, do not
generalize well.
</summary>
    <author>
      <name>Vineeth S. Bhaskara</name>
    </author>
    <author>
      <name>Debanjan Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 12 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07525v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07525v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11429v1</id>
    <updated>2018-07-30T16:38:51Z</updated>
    <published>2018-07-30T16:38:51Z</published>
    <title>Kalman Filter-based Heuristic Ensemble: A New Perspective on Ensemble
  Classification Using Kalman Filters</title>
    <summary>  A classifier ensemble is a combination of multiple diverse classifier models
whose outputs are aggregated into a single prediction. Ensembles have been
repeatedly shown to perform better than single classifier models, therefore
ensembles has been always a subject of research. The objective of this paper is
to introduce a new perspective on ensemble classification by considering the
training of the ensemble as a state estimation problem. The state is estimated
using noisy measurements, and these measurements are then combined using a
Kalman filter, within which heuristics are used. An implementation of this
perspective, Kalman Filter based Heuristic Ensemble (KFHE), is also presented
in this paper. Experiments performed on several datasets, indicate the
effectiveness and the potential of KFHE when compared with boosting and
bagging. Moreover, KFHE was found to perform comparatively better than bagging
and boosting in the case of datasets with noisy class label assignments.
</summary>
    <author>
      <name>Arjun Pakrashi</name>
    </author>
    <author>
      <name>Brian Mac Namee</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11419v1</id>
    <updated>2018-07-30T16:13:57Z</updated>
    <published>2018-07-30T16:13:57Z</published>
    <title>High-dimensional estimation via sum-of-squares proofs</title>
    <summary>  Estimation is the computational task of recovering a hidden parameter $x$
associated with a distribution $D_x$, given a measurement $y$ sampled from the
distribution. High dimensional estimation problems arise naturally in
statistics, machine learning, and complexity theory.
  Many high dimensional estimation problems can be formulated as systems of
polynomial equations and inequalities, and thus give rise to natural
probability distributions over polynomial systems. Sum-of-squares proofs
provide a powerful framework to reason about polynomial systems, and further
there exist efficient algorithms to search for low-degree sum-of-squares
proofs.
  Understanding and characterizing the power of sum-of-squares proofs for
estimation problems has been a subject of intense study in recent years. On one
hand, there is a growing body of work utilizing sum-of-squares proofs for
recovering solutions to polynomial systems when the system is feasible. On the
other hand, a general technique referred to as pseudocalibration has been
developed towards showing lower bounds on the degree of sum-of-squares proofs.
Finally, the existence of sum-of-squares refutations of a polynomial system has
been shown to be intimately connected to the existence of spectral algorithms.
In this article we survey these developments.
</summary>
    <author>
      <name>Prasad Raghavendra</name>
    </author>
    <author>
      <name>Tselil Schramm</name>
    </author>
    <author>
      <name>David Steurer</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08317v2</id>
    <updated>2018-07-30T16:12:32Z</updated>
    <published>2018-06-21T16:53:02Z</published>
    <title>Fashion-Gen: The Generative Fashion Dataset and Challenge</title>
    <summary>  We introduce a new dataset of 293,008 high definition (1360 x 1360 pixels)
fashion images paired with item descriptions provided by professional stylists.
Each item is photographed from a variety of angles. We provide baseline results
on 1) high-resolution image generation, and 2) image generation conditioned on
the given text descriptions. We invite the community to improve upon these
baselines. In this paper, we also outline the details of a challenge that we
are launching based upon this dataset.
</summary>
    <author>
      <name>Negar Rostamzadeh</name>
    </author>
    <author>
      <name>Seyedarian Hosseini</name>
    </author>
    <author>
      <name>Thomas Boquet</name>
    </author>
    <author>
      <name>Wojciech Stokowiec</name>
    </author>
    <author>
      <name>Ying Zhang</name>
    </author>
    <author>
      <name>Christian Jauvin</name>
    </author>
    <author>
      <name>Chris Pal</name>
    </author>
    <link href="http://arxiv.org/abs/1806.08317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11408v1</id>
    <updated>2018-07-30T16:01:53Z</updated>
    <published>2018-07-30T16:01:53Z</published>
    <title>Local Linear Forests</title>
    <summary>  Random forests are a powerful method for non-parametric regression, but are
limited in their ability to fit smooth signals, and can show poor predictive
performance in the presence of strong, smooth effects. Taking the perspective
of random forests as an adaptive kernel method, we pair the forest kernel with
a local linear regression adjustment to better capture smoothness. The
resulting procedure, local linear forests, enables us to improve on asymptotic
rates of convergence for random forests with smooth signals, and provides
substantial gains in accuracy on both real and simulated data.
</summary>
    <author>
      <name>Rina Friedberg</name>
    </author>
    <author>
      <name>Julie Tibshirani</name>
    </author>
    <author>
      <name>Susan Athey</name>
    </author>
    <author>
      <name>Stefan Wager</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11407v1</id>
    <updated>2018-07-30T16:00:08Z</updated>
    <published>2018-07-30T16:00:08Z</published>
    <title>HybridNet: Classification and Reconstruction Cooperation for
  Semi-Supervised Learning</title>
    <summary>  In this paper, we introduce a new model for leveraging unlabeled data to
improve generalization performances of image classifiers: a two-branch
encoder-decoder architecture called HybridNet. The first branch receives
supervision signal and is dedicated to the extraction of invariant
class-related representations. The second branch is fully unsupervised and
dedicated to model information discarded by the first branch to reconstruct
input data. To further support the expected behavior of our model, we propose
an original training objective. It favors stability in the discriminative
branch and complementarity between the learned representations in the two
branches. HybridNet is able to outperform state-of-the-art results on CIFAR-10,
SVHN and STL-10 in various semi-supervised settings. In addition,
visualizations and ablation studies validate our contributions and the behavior
of the model on both CIFAR-10 and STL-10 datasets.
</summary>
    <author>
      <name>Thomas Robert</name>
    </author>
    <author>
      <name>Nicolas Thome</name>
    </author>
    <author>
      <name>Matthieu Cord</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10278v2</id>
    <updated>2018-07-30T15:46:11Z</updated>
    <published>2018-07-26T02:57:49Z</published>
    <title>Structured Point Cloud Data Analysis via Regularized Tensor Regression
  for Process Modeling and Optimization</title>
    <summary>  Advanced 3D metrology technologies such as Coordinate Measuring Machine (CMM)
and laser 3D scanners have facilitated the collection of massive point cloud
data, beneficial for process monitoring, control and optimization. However, due
to their high dimensionality and structure complexity, modeling and analysis of
point clouds are still a challenge. In this paper, we utilize multilinear
algebra techniques and propose a set of tensor regression approaches to model
the variational patterns of point clouds and to link them to process variables.
The performance of the proposed methods is evaluated through simulations and a
real case study of turning process optimization.
</summary>
    <author>
      <name>Hao Yan</name>
    </author>
    <author>
      <name>Kamran Paynabar</name>
    </author>
    <author>
      <name>Massimo Pacella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technometrics, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10278v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10278v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11398v1</id>
    <updated>2018-07-30T15:40:54Z</updated>
    <published>2018-07-30T15:40:54Z</published>
    <title>Preference-based Online Learning with Dueling Bandits: A Survey</title>
    <summary>  In machine learning, the notion of multi-armed bandits refers to a class of
online learning problems, in which an agent is supposed to simultaneously
explore and exploit a given set of choice alternatives in the course of a
sequential decision process. In the standard setting, the agent learns from
stochastic feedback in the form of real-valued rewards. In many applications,
however, numerical reward signals are not readily available -- instead, only
weaker information is provided, in particular relative preferences in the form
of qualitative comparisons between pairs of alternatives. This observation has
motivated the study of variants of the multi-armed bandit problem, in which
more general representations are used both for the type of feedback to learn
from and the target of prediction. The aim of this paper is to provide a survey
of the state of the art in this field, referred to as preference-based
multi-armed bandits or dueling bandits. To this end, we provide an overview of
problems that have been considered in the literature as well as methods for
tackling them. Our taxonomy is mainly based on the assumptions made by these
methods about the data-generating process and, related to this, the properties
of the preference-based feedback.
</summary>
    <author>
      <name>Robert Busa-Fekete</name>
    </author>
    <author>
      <name>Eyke Hüllermeier</name>
    </author>
    <author>
      <name>Adil El Mesaoudi-Paul</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06688v2</id>
    <updated>2018-07-30T14:57:02Z</updated>
    <published>2017-09-20T00:48:43Z</published>
    <title>Property Testing in High Dimensional Ising models</title>
    <summary>  This paper explores the information-theoretic limitations of graph property
testing in zero-field Ising models. Instead of learning the entire graph
structure, sometimes testing a basic graph property such as connectivity, cycle
presence or maximum clique size is a more relevant and attainable objective.
Since property testing is more fundamental than graph recovery, any necessary
conditions for property testing imply corresponding conditions for graph
recovery, while custom property tests can be statistically and/or
computationally more efficient than graph recovery based algorithms.
Understanding the statistical complexity of property testing requires the
distinction of ferromagnetic (i.e., positive interactions only) and general
Ising models. Using combinatorial constructs such as graph packing and strong
monotonicity, we characterize how target properties affect the corresponding
minimax upper and lower bounds within the realm of ferromagnets. On the other
hand, by studying the detection of an antiferromagnetic (i.e., negative
interactions only) Curie-Weiss model buried in Rademacher noise, we show that
property testing is strictly more challenging over general Ising models. In
terms of methodological development, we propose two types of correlation based
tests: computationally efficient screening for ferromagnets, and score type
tests for general models, including a fast cycle presence test. Our correlation
screening tests match the information-theoretic bounds for property testing in
ferromagnets.
</summary>
    <author>
      <name>Matey Neykov</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">72 pages, 10 figures; revised version</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06688v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06688v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11346v1</id>
    <updated>2018-07-30T13:45:16Z</updated>
    <published>2018-07-30T13:45:16Z</published>
    <title>Dropout-GAN: Learning from a Dynamic Ensemble of Discriminators</title>
    <summary>  We propose to incorporate adversarial dropout in generative multi-adversarial
networks, by omitting or dropping out, the feedback of each discriminator in
the framework with some probability at the end of each batch. Our approach
forces the single generator not to constrain its output to satisfy a single
discriminator, but, instead, to satisfy a dynamic ensemble of discriminators.
We show that this leads to a more generalized generator, promoting variety in
the generated samples and avoiding the common mode collapse problem commonly
experienced with generative adversarial networks (GANs). We further provide
evidence that the proposed framework, named Dropout-GAN, promotes sample
diversity both within and across epochs, eliminating mode collapse and
stabilizing training.
</summary>
    <author>
      <name>Gonçalo Mordido</name>
    </author>
    <author>
      <name>Haojin Yang</name>
    </author>
    <author>
      <name>Christoph Meinel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM KDD'18 Deep Learning Day</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11320v1</id>
    <updated>2018-07-30T12:44:26Z</updated>
    <published>2018-07-30T12:44:26Z</published>
    <title>Kernel Density Estimation-Based Markov Models with Hidden State</title>
    <summary>  We consider Markov models of stochastic processes where the next-step
conditional distribution is defined by a kernel density estimator (KDE),
similar to Markov forecast densities and certain time-series bootstrap schemes.
The KDE Markov models (KDE-MMs) we discuss are nonlinear, nonparametric, fully
probabilistic representations of stationary processes, based on techniques with
strong asymptotic consistency properties. The models generate new data by
concatenating points from the training data sequences in a context-sensitive
manner, together with some additive driving noise. We present novel EM-type
maximum-likelihood algorithms for data-driven bandwidth selection in KDE-MMs.
Additionally, we augment the KDE-MMs with a hidden state, yielding a new model
class, KDE-HMMs. The added state variable captures non-Markovian long memory
and signal structure (e.g., slow oscillations), complementing the short-range
dependences described by the Markov process. The resulting joint Markov and
hidden-Markov structure is appealing for modelling complex real-world processes
such as speech signals. We present guaranteed-ascent EM-update equations for
model parameters in the case of Gaussian kernels, as well as relaxed update
formulas that greatly accelerate training in practice. Experiments demonstrate
increased held-out set probability for KDE-HMMs on several challenging natural
and synthetic data series, compared to traditional techniques such as
autoregressive models, HMMs, and their combinations.
</summary>
    <author>
      <name>Gustav Eje Henter</name>
    </author>
    <author>
      <name>Arne Leijon</name>
    </author>
    <author>
      <name>W. Bastiaan Kleijn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M10, 62G07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00337v1</id>
    <updated>2018-07-30T11:17:43Z</updated>
    <published>2018-07-30T11:17:43Z</published>
    <title>The Importance of Context When Recommending TV Content: Dataset and
  Algorithms</title>
    <summary>  Home entertainment systems feature in a variety of usage scenarios with one
or more simultaneous users, for whom the complexity of choosing media to
consume has increased rapidly over the last decade. Users' decision processes
are complex and highly influenced by contextual settings, but data supporting
the development and evaluation of context-aware recommender systems are scarce.
In this paper we present a dataset of self-reported TV consumption enriched
with contextual information of viewing situations. We show how choice of genre
associates with, among others, the number of present users and users' attention
levels. Furthermore, we evaluate the performance of predicting chosen genres
given different configurations of contextual information, and compare the
results to contextless predictions. The results suggest that including
contextual features in the prediction cause notable improvements, and both
temporal and social context show significant contributions.
</summary>
    <author>
      <name>Miklas S. Kristoffersen</name>
    </author>
    <author>
      <name>Sven E. Shepstone</name>
    </author>
    <author>
      <name>Zheng-Hua Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04300v2</id>
    <updated>2018-07-30T08:41:52Z</updated>
    <published>2018-03-12T15:10:45Z</published>
    <title>Neural Conditional Gradients</title>
    <summary>  The move from hand-designed to learned optimizers in machine learning has
been quite successful for gradient-based and -free optimizers. When facing a
constrained problem, however, maintaining feasibility typically requires a
projection step, which might be computationally expensive and not
differentiable. We show how the design of projection-free convex optimization
algorithms can be cast as a learning problem based on Frank-Wolfe Networks:
recurrent networks implementing the Frank-Wolfe algorithm aka. conditional
gradients. This allows them to learn to exploit structure when, e.g.,
optimizing over rank-1 matrices. Our LSTM-learned optimizers outperform
hand-designed as well learned but unconstrained ones. We demonstrate this for
training support vector machines and softmax classifiers.
</summary>
    <author>
      <name>Patrick Schramowski</name>
    </author>
    <author>
      <name>Christian Bauckhage</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1610.05120 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04300v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04300v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11228v1</id>
    <updated>2018-07-30T08:39:47Z</updated>
    <published>2018-07-30T08:39:47Z</published>
    <title>Predicting Conversion of Mild Cognitive Impairments to Alzheimer's
  Disease and Exploring Impact of Neuroimaging</title>
    <summary>  Nowadays, a lot of scientific efforts are concentrated on the diagnosis of
Alzheimer's Disease (AD) applying deep learning methods to neuroimaging data.
Even for 2017, there were published more than a hundred papers dedicated to AD
diagnosis, whereas only a few works considered a problem of mild cognitive
impairments (MCI) conversion to the AD. However, the conversion prediction is
an important problem since approximately 15% of patients with MCI converges to
the AD every year. In the current work, we are focusing on the conversion
prediction using brain Magnetic Resonance Imaging and clinical data, such as
demographics, cognitive assessments, genetic, and biochemical markers. First of
all, we applied state-of-the-art deep learning algorithms on the neuroimaging
data and compared these results with two machine learning algorithms that we
fit using the clinical data. As a result, the models trained on the clinical
data outperform the deep learning algorithms applied to the MR images. To
explore the impact of neuroimaging further, we trained a deep feed-forward
embedding using similarity learning with Histogram loss on all available MRIs
and obtained 64-dimensional vector representation of neuroimaging data. The use
of learned representation from the deep embedding allowed to increase the
quality of prediction based on the neuroimaging. Finally, the current results
on this dataset show that the neuroimaging does affect conversion prediction,
however, cannot noticeably increase the quality of the prediction. The best
results of predicting MCI-to-AD conversion are provided by XGBoost algorithm
trained on the clinical and embedding data. The resulting accuracy is 0.76 +-
0.01 and the area under the ROC curve - 0.86 +- 0.01.
</summary>
    <author>
      <name>Yaroslav Shmulev</name>
    </author>
    <author>
      <name>Mikhail Belyaev</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09871v2</id>
    <updated>2018-07-30T08:04:17Z</updated>
    <published>2018-05-24T19:53:58Z</published>
    <title>Confidence region of singular vectors for high-dimensional and low-rank
  matrix regression</title>
    <summary>  Let ${\bf M}\in\mathbb{R}^{m_1\times m_2}$ be an unknown matrix with $r={\rm
rank}({\bf M})\ll \min(m_1,m_2)$ whose thin singular value decomposition is
denoted by ${\bf M}={\bf U}{\bf \Lambda}{\bf V}^{\top}$ where ${\bf
\Lambda}={\rm diag}(\lambda_1,\cdots,\lambda_r)$ contains its non-increasing
singular values. Low rank matrix regression refers to instances of estimating
${\bf M}$ from $n$ i.i.d. copies of random pair $\{({\bf X}, y)\}$ where ${\bf
X}\in\mathbb{R}^{m_1\times m_2}$ is a random measurement matrix and
$y\in\mathbb{R}$ is a noisy output satisfying $y={\rm tr}({\bf M}^{\top}{\bf
X})+\xi$ with $\xi$ being stochastic error independent of ${\bf X}$. The goal
of this paper is to construct efficient estimator (denoted by $\hat{\bf U}$ and
$\hat{\bf V}$) and confidence region of ${\bf U}$ and ${\bf V}$. In particular,
we characterize the distribution of $$ {\rm dist}^2\big[(\hat{\bf U},\hat{\bf
V}), ({\bf U},{\bf V})\big]=\|\hat{\bf U}\hat{\bf U}^{\top}-{\bf U}{\bf
U}^{\top}\|_{\rm F}^2+\|\hat{\bf V}\hat{\bf V}^{\top}-{\bf V}{\bf
V}^{\top}\|_{\rm F}^2. $$ We prove the asymptotic normality of properly
centered and normalized ${\rm dist}^2\big[(\hat{\bf U},\hat{\bf V}), ({\bf
U},{\bf V})\big]$ with data-dependent centering and normalization when
$r^{5/2}(m_1+m_2)^{3/2}=o(n/\log n)$, based on which confidence region of ${\bf
U}$ and ${\bf V}$ is constructed achieving any pre-determined confidence level
asymptotically.
</summary>
    <author>
      <name>Dong Xia</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09871v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09871v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11205v1</id>
    <updated>2018-07-30T07:40:44Z</updated>
    <published>2018-07-30T07:40:44Z</published>
    <title>Highly Scalable Deep Learning Training System with Mixed-Precision:
  Training ImageNet in Four Minutes</title>
    <summary>  Synchronized stochastic gradient descent (SGD) optimizers with data
parallelism are widely used in training large-scale deep neural networks.
Although using larger mini-batch sizes can improve the system scalability by
reducing the communication-to-computation ratio, it may hurt the generalization
ability of the models. To this end, we build a highly scalable deep learning
training system for dense GPU clusters with three main contributions: (1) We
propose a mixed-precision training method that significantly improves the
training throughput of a single GPU without losing accuracy. (2) We propose an
optimization approach for extremely large mini-batch size (up to 64k) that can
train CNN models on the ImageNet dataset without losing accuracy. (3) We
propose highly optimized all-reduce algorithms that achieve up to 3x and 11x
speedup on AlexNet and ResNet-50 respectively than NCCL-based training on a
cluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the
state-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes
and achieved 74.9\% top-1 test accuracy, and another KNL-based system with 2048
Intel KNLs spent 20 minutes and achieved 75.4\% accuracy. Our training system
can achieve 75.8\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40
GPUs. When training AlexNet with 95 epochs, our system can achieve 58.7\% top-1
test accuracy within 4 minutes, which also outperforms all other existing
systems.
</summary>
    <author>
      <name>Xianyan Jia</name>
    </author>
    <author>
      <name>Shutao Song</name>
    </author>
    <author>
      <name>Wei He</name>
    </author>
    <author>
      <name>Yangzihao Wang</name>
    </author>
    <author>
      <name>Haidong Rong</name>
    </author>
    <author>
      <name>Feihu Zhou</name>
    </author>
    <author>
      <name>Liqiang Xie</name>
    </author>
    <author>
      <name>Zhenyu Guo</name>
    </author>
    <author>
      <name>Yuanzhou Yang</name>
    </author>
    <author>
      <name>Liwei Yu</name>
    </author>
    <author>
      <name>Tiegang Chen</name>
    </author>
    <author>
      <name>Guangxiao Hu</name>
    </author>
    <author>
      <name>Shaohuai Shi</name>
    </author>
    <author>
      <name>Xiaowen Chu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1803.03383 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10623v2</id>
    <updated>2018-07-30T05:58:00Z</updated>
    <published>2018-07-26T17:55:35Z</published>
    <title>Learning low dimensional word based linear classifiers using Data Shared
  Adaptive Bootstrap Aggregated Lasso with application to IMDb data</title>
    <summary>  In this article we propose a new supervised ensemble learning method called
Data Shared Adaptive Bootstrap Aggregated (AdaBag) Lasso for capturing low
dimensional useful features for word based sentiment analysis and mining
problems. The literature on ensemble methods is very rich in both statistics
and machine learning. The algorithm is a substantial upgrade of the Data Shared
Lasso uplift algorithm. The most significant conceptual addition to the
existing literature lies in the final selection of bag of predictors through a
special bootstrap aggregation scheme. We apply the algorithm to one simulated
data and perform dimension reduction in grouped IMDb data (drama, comedy and
horror) to extract reduced set of word features for predicting sentiment
ratings of movie reviews demonstrating different aspects. We also compare the
performance of the present method with the classical Principal Components with
associated Linear Discrimination (PCA-LD) as baseline. There are few
limitations in the algorithm. Firstly, the algorithm workflow does not
incorporate online sequential data acquisition and it does not use sentence
based models which are common in ANN algorithms . Our results produce slightly
higher error rate compare to the reported state-of-the-art as a consequence.
</summary>
    <author>
      <name>Ashutosh K. Maurya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1204.1177 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10623v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10623v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11169v1</id>
    <updated>2018-07-30T04:34:59Z</updated>
    <published>2018-07-30T04:34:59Z</published>
    <title>Online Learning with an Almost Perfect Expert</title>
    <summary>  We study the online learning problem where a forecaster makes a sequence of
binary predictions using the advice of $n$ experts. Our main contribution is to
analyze the regime where the best expert makes at most $b$ mistakes and to show
that when $b = o(\log_4{n})$, the expected number of mistakes made by the
optimal forecaster is at most $\log_4{n} + o(\log_4{n})$. We also describe an
adversary strategy showing that this bound is tight.
</summary>
    <author>
      <name>Simina Brânzei</name>
    </author>
    <author>
      <name>Yuval Peres</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11167v1</id>
    <updated>2018-07-30T04:28:52Z</updated>
    <published>2018-07-30T04:28:52Z</published>
    <title>A Group-Theoretic Approach to Abstraction: Hierarchical, Interpretable,
  and Task-Free Clustering</title>
    <summary>  Abstraction plays a key role in concept learning and knowledge discovery.
While pervasive in both human and artificial intelligence, it remains
mysterious how concepts are abstracted in the first place. We study the nature
of abstraction through a group-theoretic approach, formalizing it as a
hierarchical, interpretable, and task-free clustering problem. This clustering
framework is data-free, feature-free, similarity-free, and globally
hierarchical---the four key features that distinguish it from common clustering
models. Beyond a theoretical foundation for abstraction, we also present a
top-down and a bottom-up approach to establish an algorithmic foundation for
practical abstraction-generating methods. Lastly, using both a theoretical
explanation and a real-world application, we show that the coupling of our
abstraction framework with statistics realizes Shannon's information lattice
and even further, brings learning into the picture. This gives a first step
towards a principled and cognitive way of automatic concept learning and
knowledge discovery.
</summary>
    <author>
      <name>Haizi Yu</name>
    </author>
    <author>
      <name>Igor Mineyev</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11143v1</id>
    <updated>2018-07-30T02:21:07Z</updated>
    <published>2018-07-30T02:21:07Z</published>
    <title>ARM: Augment-REINFORCE-Merge Gradient for Discrete Latent Variable
  Models</title>
    <summary>  To backpropagate the gradients through discrete stochastic layers, we encode
the true gradients into a multiplication between random noises and the
difference of the same function of two different sets of discrete latent
variables, which are correlated with these random noises. The expectations of
that multiplication over iterations are zeros combined with spikes from time to
time. To modulate the frequencies, amplitudes, and signs of the spikes to
capture the temporal evolution of the true gradients, we propose the
augment-REINFORCE-merge (ARM) estimator that combines data augmentation, the
score-function estimator, permutation of the indices of latent variables, and
variance reduction for Monte Carlo integration using common random numbers. The
ARM estimator provides low-variance and unbiased gradient estimates for the
parameters of discrete distributions, leading to state-of-the-art performance
in both auto-encoding variational Bayes and maximum likelihood inference, for
discrete latent variable models with one or multiple discrete stochastic
layers.
</summary>
    <author>
      <name>Mingzhang Yin</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01189v2</id>
    <updated>2018-07-30T01:55:15Z</updated>
    <published>2018-04-03T23:10:36Z</published>
    <title>Real-Time Prediction of the Duration of Distribution System Outages</title>
    <summary>  This paper addresses the problem of predicting duration of unplanned power
outages, using historical outage records to train a series of neural network
predictors. The initial duration prediction is made based on environmental
factors, and it is updated based on incoming field reports using natural
language processing to automatically analyze the text. Experiments using 15
years of outage records show good initial results and improved performance
leveraging text. Case studies show that the language processing identifies
phrases that point to outage causes and repair steps.
</summary>
    <author>
      <name>Aaron Jaech</name>
    </author>
    <author>
      <name>Baosen Zhang</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <author>
      <name>Daniel S. Kirschen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in IEEE Transactions on Power Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11121v1</id>
    <updated>2018-07-29T23:14:21Z</updated>
    <published>2018-07-29T23:14:21Z</published>
    <title>Neural Mesh: Introducing a Notion of Space and Conservation of Energy to
  Neural Networks</title>
    <summary>  Neural networks are based on a simplified model of the brain. In this
project, we wanted to relax the simplifying assumptions of a traditional neural
network by making a model that more closely emulates the low level interactions
of neurons. Like in an RNN, our model has a state that persists between time
steps, so that the energies of neurons persist. However, unlike an RNN, our
state consists of a 2 dimensional matrix, rather than a 1 dimensional vector,
thereby introducing a concept of distance to other neurons within the state. In
our model, neurons can only fire to adjacent neurons, as in the brain. Like in
the brain, we only allow neurons to fire in a time step if they contain enough
energy, or excitement. We also enforce a notion of conservation of energy, so
that a neuron cannot excite its neighbors more than the excitement it already
contained at that time step. Taken together, these two features allow signals
in the form of activations to flow around in our network over time, making our
neural mesh more closely model signals traveling through the brain the brain.
Although our main goal is to design an architecture to more closely emulate the
brain in the hope of having a correct internal representation of information by
the time we know how to properly train a general intelligence, we did benchmark
our neural mash on a specific task. We found that by increasing the runtime of
the mesh, we were able to increase its accuracy without increasing the number
of parameters.
</summary>
    <author>
      <name>Jacob Beck</name>
    </author>
    <author>
      <name>Zoe Papakipos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03625v2</id>
    <updated>2018-07-29T21:56:06Z</updated>
    <published>2017-09-11T23:43:30Z</published>
    <title>Budgeted Experiment Design for Causal Structure Learning</title>
    <summary>  We study the problem of causal structure learning when the experimenter is
limited to perform at most $k$ non-adaptive experiments of size $1$. We
formulate the problem of finding the best intervention target set as an
optimization problem, which aims to maximize the average number of edges whose
directions are resolved. We prove that the corresponding objective function is
submodular and a greedy algorithm suffices to achieve
$(1-\frac{1}{e})$-approximation of the optimal value. We further present an
accelerated variant of the greedy algorithm, which can lead to orders of
magnitude performance speedup. We validate our proposed approach on synthetic
and real graphs. The results show that compared to the purely observational
setting, our algorithm orients the majority of the edges through a considerably
small number of interventions.
</summary>
    <author>
      <name>AmirEmad Ghassami</name>
    </author>
    <author>
      <name>Saber Salehkaleybar</name>
    </author>
    <author>
      <name>Negar Kiyavash</name>
    </author>
    <author>
      <name>Elias Bareinboim</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">35th International Conference on Machine Learning (ICML), PMLR
  80:1719-1728, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.03625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04378v2</id>
    <updated>2018-07-29T21:49:01Z</updated>
    <published>2018-01-13T04:03:04Z</published>
    <title>Fairness in Supervised Learning: An Information Theoretic Approach</title>
    <summary>  Automated decision making systems are increasingly being used in real-world
applications. In these systems for the most part, the decision rules are
derived by minimizing the training error on the available historical data.
Therefore, if there is a bias related to a sensitive attribute such as gender,
race, religion, etc. in the data, say, due to cultural/historical
discriminatory practices against a certain demographic, the system could
continue discrimination in decisions by including the said bias in its decision
rule. We present an information theoretic framework for designing fair
predictors from data, which aim to prevent discrimination against a specified
sensitive attribute in a supervised learning setting. We use equalized odds as
the criterion for discrimination, which demands that the prediction should be
independent of the protected attribute conditioned on the actual label. To
ensure fairness and generalization simultaneously, we compress the data to an
auxiliary variable, which is used for the prediction task. This auxiliary
variable is chosen such that it is decontaminated from the discriminatory
attribute in the sense of equalized odds. The final predictor is obtained by
applying a Bayesian decision rule to the auxiliary variable.
</summary>
    <author>
      <name>AmirEmad Ghassami</name>
    </author>
    <author>
      <name>Sajad Khodadadian</name>
    </author>
    <author>
      <name>Negar Kiyavash</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04378v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04378v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02679v2</id>
    <updated>2018-07-29T19:20:19Z</updated>
    <published>2018-06-07T13:41:56Z</published>
    <title>Semi-Supervised Learning via Compact Latent Space Clustering</title>
    <summary>  We present a novel cost function for semi-supervised learning of neural
networks that encourages compact clustering of the latent space to facilitate
separation. The key idea is to dynamically create a graph over embeddings of
labeled and unlabeled samples of a training batch to capture underlying
structure in feature space, and use label propagation to estimate its high and
low density regions. We then devise a cost function based on Markov chains on
the graph that regularizes the latent space to form a single compact cluster
per class, while avoiding to disturb existing clusters during optimization. We
evaluate our approach on three benchmarks and compare to state-of-the art with
promising results. Our approach combines the benefits of graph-based
regularization with efficient, inductive inference, does not require
modifications to a network architecture, and can thus be easily applied to
existing networks to enable an effective use of unlabeled data.
</summary>
    <author>
      <name>Konstantinos Kamnitsas</name>
    </author>
    <author>
      <name>Daniel C. Castro</name>
    </author>
    <author>
      <name>Loic Le Folgoc</name>
    </author>
    <author>
      <name>Ian Walker</name>
    </author>
    <author>
      <name>Ryutaro Tanno</name>
    </author>
    <author>
      <name>Daniel Rueckert</name>
    </author>
    <author>
      <name>Ben Glocker</name>
    </author>
    <author>
      <name>Antonio Criminisi</name>
    </author>
    <author>
      <name>Aditya Nori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented as a long oral in ICML 2018. Post-conference camera ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02679v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02679v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05506v3</id>
    <updated>2018-07-29T19:02:30Z</updated>
    <published>2017-09-16T12:30:40Z</published>
    <title>A statistical interpretation of spectral embedding: the generalised
  random dot product graph</title>
    <summary>  A generalisation of a latent position network model known as the random dot
product graph model is considered. The resulting model may be of independent
interest because it has the unique property of representing a mixture of
connectivity behaviours as the corresponding convex combination in latent
space. We show that, whether the normalised Laplacian or adjacency matrix is
used, the vector representations of nodes obtained by spectral embedding
provide strongly consistent latent position estimates with asymptotically
Gaussian error. Direct methodological consequences follow from the observation
that the well-known mixed membership and standard stochastic block models are
special cases where the latent positions live respectively inside or on the
vertices of a simplex. Estimation via spectral embedding can therefore be
achieved by respectively estimating this simplicial support, or fitting a
Gaussian mixture model. In the latter case, the use of $K$-means, as has been
previously recommended, is suboptimal and for identifiability reasons unsound.
Empirical improvements in link prediction, as well as the potential to uncover
much richer latent structure (than available under the mixed membership or
standard stochastic block models) are demonstrated in a cyber-security example.
</summary>
    <author>
      <name>Patrick Rubin-Delanchy</name>
    </author>
    <author>
      <name>Carey E. Priebe</name>
    </author>
    <author>
      <name>Minh Tang</name>
    </author>
    <author>
      <name>Joshua Cape</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages; 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05506v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05506v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07230v2</id>
    <updated>2018-07-29T15:54:41Z</updated>
    <published>2017-11-20T09:52:25Z</published>
    <title>Regret Analysis for Adaptive Linear-Quadratic Policies</title>
    <summary>  In the classical problem of Linear-Quadratic (LQ) control, when the
parameters of the system's dynamics are unknown, an adaptive policy is needed
to learn those parameters and also plan a control action. The resulting
trade-off between accurate parameter estimation (exploration) and effective
control (exploitation) represents the main challenge in the area of adaptive
control. Asymptotic approaches have been extensively studied in the literature,
but there is a dearth of non-asymptotic results that in addition are rather
incomplete.
  This study establishes high probability regret bounds for the aforementioned
problem that are optimal up to logarithmic factors. The results on finite time
analysis of the regret are obtained under very mild assumptions, requiring: (i)
stabilizability of the system's dynamics, and (ii) limiting the degree of
heaviness of the noise distribution. To establish such bounds, certain novel
techniques are introduced to comprehensively address the probabilistic behavior
of dependent random matrices with heavy-tailed distributions.
</summary>
    <author>
      <name>Mohamad Kazem Shirani Faradonbeh</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <author>
      <name>George Michailidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.07230v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07230v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11074v1</id>
    <updated>2018-07-29T15:44:55Z</updated>
    <published>2018-07-29T15:44:55Z</published>
    <title>Visual Analogies between Atari Games for Studying Transfer Learning in
  RL</title>
    <summary>  In this work, we ask the following question: Can visual analogies, learned in
an unsupervised way, be used in order to transfer knowledge between pairs of
games and even play one game using an agent trained for another game? We
attempt to answer this research question by creating visual analogies between a
pair of games: a source game and a target game. For example, given a video
frame in the target game, we map it to an analogous state in the source game
and then attempt to play using a trained policy learned for the source game. We
demonstrate convincing visual mapping between four pairs of games (eight
mappings), which are used to evaluate three transfer learning approaches.
</summary>
    <author>
      <name>Doron Sobol</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.12338v2</id>
    <updated>2018-07-29T14:14:23Z</updated>
    <published>2018-05-31T06:38:07Z</published>
    <title>Hallucinating robots: Inferring Obstacle Distances from Partial Laser
  Measurements</title>
    <summary>  Many mobile robots rely on 2D laser scanners for localization, mapping, and
navigation. However, those sensors are unable to correctly provide distance to
obstacles such as glass panels and tables whose actual occupancy is invisible
at the height the sensor is measuring. In this work, instead of estimating the
distance to obstacles from richer sensor readings such as 3D lasers or RGBD
sensors, we present a method to estimate the distance directly from raw 2D
laser data. To learn a mapping from raw 2D laser distances to obstacle
distances we frame the problem as a learning task and train a neural network
formed as an autoencoder. A novel configuration of network hyperparameters is
proposed for the task at hand and is quantitatively validated on a test set.
Finally, we qualitatively demonstrate in real time on a Care-O-bot 4 that the
trained network can successfully infer obstacle distances from partial 2D laser
readings.
</summary>
    <author>
      <name>Jens Lundell</name>
    </author>
    <author>
      <name>Francesco Verdoja</name>
    </author>
    <author>
      <name>Ville Kyrki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In 2018 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.12338v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.12338v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05413v2</id>
    <updated>2018-07-29T12:27:31Z</updated>
    <published>2018-06-14T08:46:36Z</published>
    <title>Learning Dynamics of Linear Denoising Autoencoders</title>
    <summary>  Denoising autoencoders (DAEs) have proven useful for unsupervised
representation learning, but a thorough theoretical understanding is still
lacking of how the input noise influences learning. Here we develop theory for
how noise influences learning in DAEs. By focusing on linear DAEs, we are able
to derive analytic expressions that exactly describe their learning dynamics.
We verify our theoretical predictions with simulations as well as experiments
on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise
allows DAEs to ignore low variance directions in the inputs while learning to
reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs
to standard regularised autoencoders, we show that noise has a similar
regularisation effect to weight decay, but with faster training dynamics. We
also show that our theoretical predictions approximate learning dynamics on
real-world data and qualitatively match observed dynamics in nonlinear DAEs.
</summary>
    <author>
      <name>Arnu Pretorius</name>
    </author>
    <author>
      <name>Steve Kroon</name>
    </author>
    <author>
      <name>Herman Kamper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures, accepted at the 35th International Conference on
  Machine Learning (ICML) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08237v2</id>
    <updated>2018-07-29T12:07:43Z</updated>
    <published>2018-07-22T05:59:41Z</published>
    <title>Learning Deep Hidden Nonlinear Dynamics from Aggregate Data</title>
    <summary>  Learning nonlinear dynamics from diffusion data is a challenging problem
since the individuals observed may be different at different time points,
generally following an aggregate behaviour. Existing work cannot handle the
tasks well since they model such dynamics either directly on observations or
enforce the availability of complete longitudinal individual-level
trajectories. However, in most of the practical applications, these
requirements are unrealistic: the evolving dynamics may be too complex to be
modeled directly on observations, and individual-level trajectories may not be
available due to technical limitations, experimental costs and/or privacy
issues. To address these challenges, we formulate a model of diffusion dynamics
as the {\em hidden stochastic process} via the introduction of hidden variables
for flexibility, and learn the hidden dynamics directly on {\em aggregate
observations} without any requirement for individual-level trajectories. We
propose a dynamic generative model with Wasserstein distance for LEarninG dEep
hidden Nonlinear Dynamics (LEGEND) and prove its theoretical guarantees as
well. Experiments on a range of synthetic and real-world datasets illustrate
that LEGEND has very strong performance compared to state-of-the-art baselines.
</summary>
    <author>
      <name>Yisen Wang</name>
    </author>
    <author>
      <name>Bo Dai</name>
    </author>
    <author>
      <name>Lingkai Kong</name>
    </author>
    <author>
      <name>Sarah Monazam Erfani</name>
    </author>
    <author>
      <name>James Bailey</name>
    </author>
    <author>
      <name>Hongyuan Zha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Conference on Uncertainty in Artificial
  Intelligence (UAI), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08237v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08237v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08352v2</id>
    <updated>2018-07-29T12:02:52Z</updated>
    <published>2018-02-23T00:02:59Z</published>
    <title>Learning to Make Predictions on Graphs with Autoencoders</title>
    <summary>  We examine two fundamental tasks associated with graph representation
learning: link prediction and semi-supervised node classification. We present a
novel autoencoder architecture capable of learning a joint representation of
both local graph structure and available node features for the multi-task
learning of link prediction and node classification. Our autoencoder
architecture is efficiently trained end-to-end in a single learning stage to
simultaneously perform link prediction and node classification, whereas
previous related methods require multiple training steps that are difficult to
optimize. We provide a comprehensive empirical evaluation of our models on nine
benchmark graph-structured datasets and demonstrate significant improvement
over related methods for graph representation learning. Reference code and data
are available at https://github.com/vuptran/graph-representation-learning
</summary>
    <author>
      <name>Phi Vu Tran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at IEEE DSAA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08352v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08352v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11027v1</id>
    <updated>2018-07-29T09:25:37Z</updated>
    <published>2018-07-29T09:25:37Z</published>
    <title>Consistent polynomial-time unseeded graph matching for Lipschitz
  graphons</title>
    <summary>  We propose a consistent polynomial-time method for the unseeded node matching
problem for networks with smooth underlying structures. Despite widely
conjectured by the research community that the structured graph matching
problem to be significantly easier than its worst case counterpart, well-known
to be NP-hard, the statistical version of the problem has stood a challenge
that resisted any solution both provable and polynomial-time. The closest
existing work requires quasi-polynomial time. Our method is based on the latest
advances in graphon estimation techniques and analysis on the concentration of
empirical Wasserstein distances. Its core is a simple yet unconventional
sampling-and-matching scheme that reduces the problem from unseeded to seeded.
Our method allows flexible efficiencies, is convenient to analyze and
potentially can be extended to more general settings. Our work enables a rich
variety of subsequent estimations and inferences.
</summary>
    <author>
      <name>Yuan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
